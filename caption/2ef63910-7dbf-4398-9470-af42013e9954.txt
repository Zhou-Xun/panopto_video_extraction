1
00:00:00,180 --> 00:00:05,420
It's like I know when you're not coming back because I just started August.

2
00:00:05,670 --> 00:00:11,640
Well, that's a long time. Well, but this is really.

3
00:00:12,450 --> 00:00:17,500
It's like what it does something.

4
00:00:18,000 --> 00:00:22,730
Something like to come up with something negative.

5
00:00:25,470 --> 00:00:32,910
So I was able to start asking what I'm trying to do over the years.

6
00:00:33,370 --> 00:00:50,200
Not only that, but I know I have you question like yeah, yeah, you know, that's already on the side teachers for years.

7
00:00:50,970 --> 00:00:59,120
If you feel like you may need it, be on top of your belt at this part.

8
00:00:59,830 --> 00:01:13,360
More work to be involved in a little bit more now orders.

9
00:01:15,180 --> 00:01:23,670
And I remember saying that it was like no but actually we are we're just reporting but we are not nominated offering a hybrid option.

10
00:01:24,730 --> 00:01:31,440
I don't think we just say how, you know, I just it's not okay.

11
00:01:32,010 --> 00:01:36,719
Yeah, that's I think that would be helpful to me. Yeah.

12
00:01:36,720 --> 00:01:43,230
And I think we have the person in this role.

13
00:01:43,680 --> 00:01:50,010
You're welcome. You don't have to use against them all the time.

14
00:01:54,420 --> 00:01:58,450
Yeah, it is. I mean, some of the people out there now.

15
00:01:58,470 --> 00:02:08,340
So I guess my question is not so much of a crisis, but certainly, you know, you can build.

16
00:02:08,610 --> 00:02:11,790
I am here. There in Canada.

17
00:02:16,810 --> 00:02:20,850
Oh, yeah, that's good.

18
00:02:21,090 --> 00:03:04,030
Yeah, we have a lot of weekends like mine, except for the fact that I got to tell you about this great country right now with 0.3 million.

19
00:03:06,060 --> 00:03:16,270
I know. I don't have ever heard about it.

20
00:03:17,850 --> 00:03:29,850
So but this is I mean, I was going to do more public holidays.

21
00:03:30,270 --> 00:03:53,430
Yeah. It's wrong for us to have another spend my time enjoying your life and dealing with terrorism credit.

22
00:03:53,680 --> 00:04:06,510
Sorry, I could get you on the staff right now.

23
00:04:06,520 --> 00:04:12,180
It was in different countries, but my wife.

24
00:04:13,620 --> 00:04:24,560
Oh, so she's now. She's in the premier radiation oncology firm, the department's product and marketing and imaging.

25
00:04:24,660 --> 00:04:30,000
They actually have the other side of the topic with the of the seminar.

26
00:04:30,690 --> 00:04:34,590
They just hired her. Right. Okay, so you'd see more.

27
00:04:34,740 --> 00:04:43,830
But that would give you one, I guess, and make it a lot easier because this works out.

28
00:04:43,830 --> 00:04:58,820
A lot of things work in this power and it's never any other aspect that she does that is very, very difficult.

29
00:05:02,880 --> 00:05:24,060
I've been through a lot and I guess I could have done exactly I just the value in my life as you know this calculator you know it's a very you know,

30
00:05:28,290 --> 00:05:44,550
trying to get to the point where it wasn't there was no I'm not going to hurt you.

31
00:05:45,810 --> 00:05:51,370
Oh, oh, yes.

32
00:05:51,390 --> 00:06:01,890
You once were innocent. This is the transaction. No, no, it was before the senior official reported.

33
00:06:02,160 --> 00:06:08,340
So there is a part of the report within the context of the group that I was

34
00:06:08,340 --> 00:06:23,040
aware that we were working with was probably the one about the Cold War in.

35
00:06:26,550 --> 00:06:29,460
But, you know, I have I just you know,

36
00:06:29,910 --> 00:06:47,940
I think that there has to be a strong focus in the sense that it's not just the record of the total will be inside the Congress,

37
00:06:47,940 --> 00:06:56,159
but probably some talk about that.

38
00:06:56,160 --> 00:07:04,420
It's going to be one of the best players in the world.

39
00:07:07,870 --> 00:07:46,920
So you guys have been working on this issue of no are usually humorous when it comes

40
00:07:47,050 --> 00:08:07,710
to the outcome of what you do because we've got all the courts and are said to be.

41
00:08:16,720 --> 00:08:33,400
Just to be fair, I didn't know she could be encouraging her to go and get these crowds.

42
00:08:34,850 --> 00:08:58,130
I mean, you know, I don't know exactly what's going on in the show itself, but yeah, it's pretty cool.

43
00:08:59,510 --> 00:09:03,950
Yeah. You know, that's pretty.

44
00:09:05,030 --> 00:09:13,730
Say you and you find yourself.

45
00:09:19,850 --> 00:09:26,330
I mean, like I had some covered and I think those things.

46
00:09:27,710 --> 00:09:38,390
Yeah. Yeah. It's that free lunch where you get to the extent that you have a reason to, I guess, you know.

47
00:09:40,590 --> 00:09:55,160
All sorts of conflicts. I'm sorry, but I find that, you know, I mean, you know, I still think maybe someone is going to happen.

48
00:09:57,250 --> 00:10:00,320
Yeah, that's probably going to be possible.

49
00:10:00,710 --> 00:10:04,000
Yeah. I was looking at some of his papers.

50
00:10:05,150 --> 00:10:15,800
I feel like I don't look a lot like my colleagues.

51
00:10:16,250 --> 00:10:30,410
Yeah, we like to think that we can really do something like this.

52
00:10:30,590 --> 00:10:38,480
Basically know she's a major.

53
00:10:39,740 --> 00:10:49,129
No, that's fine. If I wasn't a biological, I was like, you want me to be a part of this music?

54
00:10:49,130 --> 00:11:01,350
Or you realize, Oh, I'm just trying to do a little research.

55
00:11:02,360 --> 00:11:09,200
Yeah, like, I don't know, like, I didn't have that problem. That's, you know, everyone is eager to get started here.

56
00:11:10,460 --> 00:11:14,390
So we're very excited today to have Dr. Weizmann presenting to us.

57
00:11:14,990 --> 00:11:24,200
Our son's a professor of physics and Fred Hodge and affiliated professor in the Department of Justice and the University of Washington and New Jersey,

58
00:11:24,770 --> 00:11:30,110
North Carolina, Chapel Hill here in the Department of Statistics at UCLA.

59
00:11:30,860 --> 00:11:37,280
You know that seven before joining professions you know getting in the faculty in the Department of biostatistics at Chapel Hill.

60
00:11:39,680 --> 00:11:43,310
And he's worked on many statistical methods to analyze different types of omics.

61
00:11:44,690 --> 00:11:50,040
And currently he's mainly working on sickle cell ailments and spatial transcriptomic data.

62
00:11:50,570 --> 00:11:53,770
So I think, you know, just figuring.

63
00:11:57,520 --> 00:12:01,010
All right. No one here. Me? Okay. All right.

64
00:12:01,970 --> 00:12:09,260
Thanks, Tom. Reuniting with Hasan. The nice thing to that suit guy to be able to come here, talk my research.

65
00:12:09,770 --> 00:12:20,200
Yeah. This is my first in-person seminar of the pandemic, so I got my first dose of vaccine in case we hadn't done it.

66
00:12:20,290 --> 00:12:26,780
I'm here so you can see some old friends and meet new friends and be able to share my research here.

67
00:12:29,220 --> 00:12:36,310
Hey, so the topic of Pogba is on to the physical measures, mostly within our basic data.

68
00:12:38,070 --> 00:12:42,510
So this is online the first time I saw and talk about this called ideas.

69
00:12:43,260 --> 00:12:45,180
So this measure is already published.

70
00:12:45,330 --> 00:12:55,709
If you are interested in finding more data in the paper, I have this refrain give you the overall concept and areas are now we call this punctuation.

71
00:12:55,710 --> 00:12:58,020
They were all single cell RNA seq data.

72
00:12:58,500 --> 00:13:05,819
Basically you got how many thousands of cell per individual and then you run the data at the same time to certain individuals.

73
00:13:05,820 --> 00:13:10,229
So you want to do the testing, comparing the gene expression across individuals,

74
00:13:10,230 --> 00:13:15,330
for example, differential expression between autism patients and the controls.

75
00:13:17,370 --> 00:13:22,050
And then the second, the method is on supervised deep learning with the general patient.

76
00:13:22,350 --> 00:13:29,760
And the idea is you want to compare the gene expression between two or cells,

77
00:13:30,120 --> 00:13:38,280
and then you want to borrow information from the genome notation and feel free to stop me any time during the talk.

78
00:13:38,520 --> 00:13:43,620
Ask questions. So somehow, you know, the single goes out.

79
00:13:43,660 --> 00:13:51,270
I think they do very well. But some of you may not get a very brief introduction to the figure on the on the last.

80
00:13:51,400 --> 00:13:55,889
It's what people usually see the single salaries equate to wage.

81
00:13:55,890 --> 00:13:59,890
Your lifestyle is like to have additional projects and properties in your mind.

82
00:14:00,220 --> 00:14:05,920
Each point needs a cell and a user is a car that based on black cross curves.

83
00:14:06,190 --> 00:14:09,230
So different color from different coffers.

84
00:14:10,080 --> 00:14:12,790
Now I look at the data is the table on the right.

85
00:14:13,180 --> 00:14:19,930
So you know how each row is a chain, each column is a cell, and then you can divide a cell based on, say,

86
00:14:20,320 --> 00:14:26,410
which individual like here individual y has three cells, individual to ourselves,

87
00:14:26,650 --> 00:14:31,180
and the number in God is actually lost the majority on zero like 90%.

88
00:14:31,480 --> 00:14:36,520
And then it's very sparse. And then you have that zero one, two, three.

89
00:14:37,690 --> 00:14:41,679
So there are much more about the single salary data.

90
00:14:41,680 --> 00:14:45,370
This is just a field that has been developing where we fast.

91
00:14:45,760 --> 00:14:50,170
For example, one thing we've been looking at here recently is the so called the plant mystique.

92
00:14:50,440 --> 00:14:59,860
So you can do gene editing, you can find know college in using this Nobel Prize and technical CRISPR-Cas9 are there other

93
00:14:59,860 --> 00:15:06,220
techniques and then once you do that you can also minority gene expression data from single cell.

94
00:15:06,430 --> 00:15:15,670
So we've got the same data matrix in the bottom you'll call this a y row is same for each cell, which gene you'll have knockout.

95
00:15:16,120 --> 00:15:23,109
So that's like you doing doing a corporation thousands or millions of cells and that's it's

96
00:15:23,110 --> 00:15:29,870
really opened the opportunity to do many things mean them how you can infer those you like

97
00:15:29,870 --> 00:15:36,170
play the graph of gene expression regulation is probably not important to the detail like

98
00:15:36,190 --> 00:15:40,690
that you can have their fundamental the here transcription by a collaborator Gene Franklin,

99
00:15:40,690 --> 00:15:43,809
and then you can optimize the gene expression all of the they will tell you

100
00:15:43,810 --> 00:15:47,470
that all kinds of they can genetic variation kind of language and expression.

101
00:15:47,980 --> 00:15:51,160
So these are not they we use those information trying to find the directions.

102
00:15:51,190 --> 00:15:56,110
Now there are interventions so you can gather like directly the graph and then you

103
00:15:56,110 --> 00:16:00,610
can ask and say if I now call this gene or the exam probably is going to have,

104
00:16:00,940 --> 00:16:06,520
you know, if you have a particular a target that you are interested in,

105
00:16:06,530 --> 00:16:13,089
like a related with disease that can give lots of information and then also spatial transcriptomics I know this

106
00:16:13,090 --> 00:16:20,020
John here is a expert in the in the area of his lab has many how you have publications and the field has also

107
00:16:20,020 --> 00:16:27,760
grow very fast and nowadays there is this new technique like there is a space also some colleagues the using

108
00:16:27,790 --> 00:16:34,420
sequencing data if you can match for each spot the way the 1 to 10 cells and now there is inside you imaging data.

109
00:16:35,110 --> 00:16:41,319
What I saw here is from the nano stream cosmic so they are just next door neighbor to Hodge.

110
00:16:41,320 --> 00:16:45,740
I got a chance to visit to their company. See their machine over here?

111
00:16:45,740 --> 00:16:51,990
Almost. You see the inside. That is quite exciting. And then here is this.

112
00:16:52,000 --> 00:16:55,840
You know, now the genomic data become literally imaging data.

113
00:16:56,320 --> 00:17:02,560
For those of you who work on imaging data staring into the single cell data may not be as hard as you expect.

114
00:17:02,860 --> 00:17:07,689
They are imaging and then they are the image that each pixel can give.

115
00:17:07,690 --> 00:17:13,480
You can say there are the cell and each cell is each pixel has a thousand different colors.

116
00:17:13,790 --> 00:17:17,049
You have RTP, you have three colors per pixel.

117
00:17:17,050 --> 00:17:20,800
Now your cell, you have a thousand colors, a thousand genes.

118
00:17:21,580 --> 00:17:25,950
And then this this is just basically the key, the image, your name to me.

119
00:17:26,250 --> 00:17:30,400
And then this is why you have this image in the cell phone to me.

120
00:17:30,410 --> 00:17:35,709
So lazzari state. And then coming back.

121
00:17:35,710 --> 00:17:39,920
I just hope that that you are all exciting for the single cell RC Band Aid how you win.

122
00:17:40,750 --> 00:17:46,690
So these are the main work on the Ionic field because it's very clear you also reach the aid higher and then it

123
00:17:46,690 --> 00:17:51,560
may require some statistical method of mulching the rest of the takes a measure to be applied in the staging.

124
00:17:52,030 --> 00:17:55,839
So the first topic I'm going to go to be in more detail is this individual I will

125
00:17:55,840 --> 00:18:02,020
differentially express an analysis of was our artistic data in the early days singles out

126
00:18:02,020 --> 00:18:06,129
the I'll be inside the there are many measure of the out of the default differentially

127
00:18:06,130 --> 00:18:11,860
expressed and testing to compare the gene expression between two groups of cells.

128
00:18:11,980 --> 00:18:20,770
So you value the data and then regardless the cancer patient in response to treatment who are responsive to the open tenor.

129
00:18:21,040 --> 00:18:24,940
And now any reason the study connected the single cell data for multiple reasons.

130
00:18:25,300 --> 00:18:32,000
Now you want to transmit across individuals. In the early days, you actually see they do the testing.

131
00:18:32,200 --> 00:18:38,260
They have three case full control. They pool all the cell phone cases, they pull all the cell phone controls.

132
00:18:38,500 --> 00:18:43,450
Now, your golf has all in cell phone cases, plenty of cells in the cell from control to acasti.

133
00:18:43,660 --> 00:18:46,899
You've got very small Q eight, but that's wrong. Statistical.

134
00:18:46,900 --> 00:18:52,480
You may well know that the sample size is three or worse as to the individual you have.

135
00:18:52,750 --> 00:18:58,810
And then if you look at the cell, they will knock a much larger sample size, but you cannot do all statistical inference.

136
00:19:00,490 --> 00:19:08,800
And then so alternative approach is to create those cell types by saving pseudo bulk RNA SEQ data.

137
00:19:09,130 --> 00:19:13,600
So think about the table. I so far for each individual have multiple cells.

138
00:19:13,840 --> 00:19:21,580
Now you just try to on that each cell type you just add up the comes across all the cells you've got one number.

139
00:19:21,940 --> 00:19:26,980
So now when I do the differential expression testing, I do that you might treat a one gene.

140
00:19:27,250 --> 00:19:29,680
I got 100,000 in this individual.

141
00:19:29,770 --> 00:19:36,760
So those 100 numbers, you start out looking at the 100 numbers, I take it there are some I got one number, so that's created a zero.

142
00:19:37,120 --> 00:19:41,559
Now the data is a nice matrix, always individual.

143
00:19:41,560 --> 00:19:45,030
I have one number and then you can do the testing.

144
00:19:45,560 --> 00:19:54,129
It is very five to my seven and also the information is lost the way you are taking the summation of five them all the gene expression different.

145
00:19:54,130 --> 00:20:02,200
So maybe in the areas rather than me, I can do this amazing truth I can do so our my feeling is actually quite simple.

146
00:20:02,200 --> 00:20:13,240
Conceptually, the implementation has three steps. So first, for each gene, we want to estimate the distribution of three expression, the radio.

147
00:20:13,540 --> 00:20:20,109
So basically for each gene and you look at why individually 100 cells, you also look at 100 number.

148
00:20:20,110 --> 00:20:26,140
If you ask me, the distribution could be a negative binomial zero infinity.

149
00:20:26,150 --> 00:20:30,670
The negative I now it's probably not necessary. Most of the days are single cell data.

150
00:20:31,060 --> 00:20:39,190
I can do this just as kernel based estimates. And now the second smaller the advantage of this voluntary delta approach.

151
00:20:39,550 --> 00:20:47,470
And then once you got the distribution, now the data is a little bit more clean for one gene, your got one distribution per individual.

152
00:20:47,650 --> 00:20:54,490
Now you want to compare gene expression across individuals. You are comparing distributions so we can calculate that distance.

153
00:20:54,490 --> 00:21:03,280
So those distributions between the only two individuals, for example, the transition and the emergence and I was just in distance.

154
00:21:03,610 --> 00:21:09,759
And then after that you just pass the weather of the distance across.

155
00:21:09,760 --> 00:21:15,520
The individual is associated with a variable interest basically was you got the distance of passing individuals.

156
00:21:15,520 --> 00:21:20,080
It's like you call the kernel matrix and then the posting reads kind of a kernel regression.

157
00:21:20,500 --> 00:21:25,840
So the concept is very simple and this is a schematic part to illustrate idea.

158
00:21:26,050 --> 00:21:30,310
So you start the way that this matrix are gene expression data.

159
00:21:30,490 --> 00:21:34,209
You should always assume each column is a cell and are here.

160
00:21:34,210 --> 00:21:36,550
I just group them based on say there are two cases,

161
00:21:36,550 --> 00:21:45,430
three control centers and following them all in each individual want to have a couple of cells and then you do your normalization transformation,

162
00:21:45,430 --> 00:21:54,819
the noise in your call, the new data matrix. And this is actually very crucial stuff to know how how the data processing has been done.

163
00:21:54,820 --> 00:22:00,850
I talk a little bit now the noisy later and now you look at the one gene across

164
00:22:00,850 --> 00:22:06,130
all the sound by individual you can got two distributions so here the fault

165
00:22:06,220 --> 00:22:14,320
for those individual others gene the right way you got one distribution the other individual there is a cell or cell they call another distribution.

166
00:22:14,560 --> 00:22:19,530
So I got this two distribution and I can calculate the first.

167
00:22:19,540 --> 00:22:23,670
And so those two distribution and I got the distance matrix, right?

168
00:22:23,800 --> 00:22:26,350
So this is the way if can't get the distance between.

169
00:22:27,640 --> 00:22:33,010
So here is like focusing on why don't you think about this, those are any two individuals and then I can come to that of.

170
00:22:33,580 --> 00:22:43,000
Paralyzed individuals like all the distance metrics and then combine those with additional calories.

171
00:22:43,210 --> 00:22:52,840
And then you've had we all hypothesis testing. The basic idea is you can estimate a pseudo life statistics like within wedding similarity

172
00:22:52,840 --> 00:22:58,070
divide in about 18 groups in America and then you can calculate humanity or by permutations.

173
00:22:58,090 --> 00:23:00,850
If you look at a regression, you can do this in parsing a few items.

174
00:23:01,060 --> 00:23:06,459
But the similar size that we call is usually smaller from a patient is more reliable and from a data is pretty fast.

175
00:23:06,460 --> 00:23:14,500
That because you already got the distance made it normally the distance matrix that is actually the computationally intensive part.

176
00:23:14,770 --> 00:23:21,790
Why does you call that those times matrix? You'll get this from the other rows and columns and calculates the statistic that's pretty fast.

177
00:23:23,680 --> 00:23:30,880
So and then so to even say that this idea was in some simulations.

178
00:23:31,540 --> 00:23:41,410
So there are two groups of as way we compare the first group are those like small grain and they are the mice or they designed the file.

179
00:23:43,180 --> 00:23:50,530
So no comparison. So here you go, simulation and we simulate 23 individuals.

180
00:23:50,530 --> 00:23:57,520
I think sodium ten is worse. It's ten controls because they just match up to the the real data we have.

181
00:23:57,550 --> 00:24:05,410
And then we also have simulate simulated panel that we have file, case file console, try to look at the result of in the sample that is smaller.

182
00:24:06,340 --> 00:24:13,960
And then on the the the, on the left, the side here is type of error.

183
00:24:14,770 --> 00:24:19,149
So, so the way we simulate every single of the 8000 genes and then we know 1000 genes

184
00:24:19,150 --> 00:24:24,340
is differential expression in six or meanwhile you and the other 1000 genes

185
00:24:24,340 --> 00:24:29,350
that they have differentially expressing per change are the awareness and the

186
00:24:29,350 --> 00:24:33,490
other 6000 that they are equivalently expressed between patient and control.

187
00:24:33,700 --> 00:24:38,590
So we basically are looking at among those 6000 and to estimate our error.

188
00:24:38,770 --> 00:24:43,809
And I think exactly that if you do a cell level testing, you know, you may have one case, one control.

189
00:24:43,810 --> 00:24:55,560
You got small particles because there are many cells and there's one nicer I want to measure is this mass to tie in the air?

190
00:24:55,570 --> 00:25:01,180
Mass itself is a it's a zero inflated the regression and the geology.

191
00:25:01,180 --> 00:25:04,569
R is a to put that in the army to see by model.

192
00:25:04,570 --> 00:25:08,559
So now I might be modeling those 1000 cells from y individual.

193
00:25:08,560 --> 00:25:17,050
So using the mix effect model as we can still look at the data across all of the cells and so they still have

194
00:25:17,080 --> 00:25:24,729
inflated the type of error is much lower inflated the type of error and then the well on the right side is the power.

195
00:25:24,730 --> 00:25:28,360
If you have a difference of expression, you mean awareness.

196
00:25:28,600 --> 00:25:33,179
And there was the power of different mice are. So.

197
00:25:33,180 --> 00:25:39,350
And then and then there is a case with a lot of a similar size of a slightly smaller number ourselves.

198
00:25:39,420 --> 00:25:46,590
Basically, the conclusion is similar to this you need to design your masato to do the testing on the individual and I world.

199
00:25:47,220 --> 00:25:50,790
I didn't. So. So. There is a few of my circuitry here.

200
00:25:50,790 --> 00:25:55,320
We compound for the individual and our testing these two is. So the bulk measure.

201
00:25:55,440 --> 00:26:02,550
So you just got a single log and then run the easy way next to binomial regression while borrowing information across genes.

202
00:26:03,030 --> 00:26:07,620
And then the idea is we have actually two words in there.

203
00:26:08,370 --> 00:26:17,459
One is that this is our this Tuesday and that's all by now the the iambic zero you play the negative binomial

204
00:26:17,460 --> 00:26:22,570
but when I implement the Y axis is a negative energy and the kitty and just colonel density estimation.

205
00:26:22,590 --> 00:26:25,889
So this is why you asked me that you should use our gene expression to use

206
00:26:25,890 --> 00:26:32,040
parametric down parametric measures and then their parameters are all similar.

207
00:26:34,680 --> 00:26:44,210
And then this is a in the simulation, the data is the example to so where if you look at the distribution, you actually see the difference.

208
00:26:44,220 --> 00:26:48,150
So the right and the blue are two groups case in control.

209
00:26:48,330 --> 00:26:52,260
If you look at the curve, one curve is y individual right.

210
00:26:52,260 --> 00:26:57,569
So here one car way in the radio because we asked me the gene expression you look at two groups are curves.

211
00:26:57,570 --> 00:27:00,900
They are clearly separated by why even do the pseudo bok.

212
00:27:01,410 --> 00:27:04,030
You will see that that they are now significantly different.

213
00:27:04,470 --> 00:27:17,620
So this expression the situation and and the comparing the basic tool and our method is clear when the gene expression difference is on where

214
00:27:17,620 --> 00:27:25,290
was the basic tool that still the bulk of that doesn't have power because when you take the mean you actually remove the where it's part.

215
00:27:27,320 --> 00:27:32,050
And then there's the application for autism study.

216
00:27:32,490 --> 00:27:44,299
The real data applications. So the data we look at is from the those on science paper with the 13 authors and

217
00:27:44,300 --> 00:27:50,000
patients and the ten controls and they only got 6000 cells from 17 cell types.

218
00:27:50,300 --> 00:27:55,390
So when we do this type of analysis, we are already up to the south by cell type separately.

219
00:27:57,050 --> 00:28:05,510
So once you start looking at real data and then you start learning new things from that, you know, things you can get from simulation.

220
00:28:06,290 --> 00:28:13,460
So the idea or idea, the idea, her idea is very simple, but in practice,

221
00:28:14,150 --> 00:28:19,220
the success really depends on how you can estimate that distribution because the data are sparse.

222
00:28:19,580 --> 00:28:29,420
And then there is also important to koalas who handle this is all right so now are we that they say this is a different cell.

223
00:28:29,420 --> 00:28:30,980
There are many types these are different.

224
00:28:31,310 --> 00:28:38,719
And then you think about this the easy part when you have one collaborate in this model and these are not distribution,

225
00:28:38,720 --> 00:28:46,940
but because the data is very sparse, you think about the data you've got these like 10% are one, 2% are to all the other zero.

226
00:28:47,060 --> 00:28:51,440
And now you want to do a regression. As far as sorry, that is actually pretty hard.

227
00:28:52,850 --> 00:28:56,719
And it's even harder if you I mean,

228
00:28:56,720 --> 00:29:03,390
it's it's not clear how to do that the exactly in the non parametric way properly

229
00:29:03,680 --> 00:29:08,450
and the faster enough what we're doing is kind of doing a linear approximation.

230
00:29:09,110 --> 00:29:18,950
So so here the this panel on the right that is basically the correlation are the two approach.

231
00:29:19,220 --> 00:29:26,720
If you are estimating the distribution using parametric measure or non parametric measure, how consists another result?

232
00:29:27,080 --> 00:29:36,860
And then we divide the the in full. There are groups and the way you exclude all the students is the expression is a zero you might 80% those cells.

233
00:29:37,130 --> 00:29:48,170
So that's a much harder case. And then on the remaining ones, if the zero proposal is like 70 to 80% and the correlation to measure can be much lower,

234
00:29:48,470 --> 00:29:55,470
and that beginning is just like Senate, it's hard for us to see how consistent the result and then find a third friends and then we

235
00:29:55,470 --> 00:30:02,360
would basically try to look kind examples and in total comparator is harder and and now into so

236
00:30:02,360 --> 00:30:08,340
the result here basically I would do this number as a way basically doing linear regression

237
00:30:08,370 --> 00:30:13,700
regress all the relapse account the residual to ask me the density become quite highly unstable.

238
00:30:14,060 --> 00:30:19,550
So we mainly focus on the parametric, the way to estimate the distribution.

239
00:30:20,180 --> 00:30:26,509
Another interesting lesson we learned here is the noisy in the single cell data.

240
00:30:26,510 --> 00:30:33,140
However, not so that the noise in my server you can just think about as a way to power information across genes,

241
00:30:33,180 --> 00:30:38,659
across cells and try to reduce the noise in the gene expression data.

242
00:30:38,660 --> 00:30:43,910
In the early days, if you are calling to imputation myself slightly fewer than zero and I think

243
00:30:43,910 --> 00:30:49,610
an eight year low appreciated the smaller the noisy rather than an location.

244
00:30:50,000 --> 00:30:53,720
But those are the some of the poorer distribution.

245
00:30:53,780 --> 00:30:58,170
They are both very nice few. I know this is not quite this is the time of humanities values solution.

246
00:30:58,190 --> 00:31:03,079
What we look at there is a peak hour on zero in the pipeline and so forth.

247
00:31:03,080 --> 00:31:08,660
But once this one on the last days we followed the noisy days after the noise.

248
00:31:08,900 --> 00:31:20,480
So uniting can increase the power a lot we try to the noisy using and I put up a measure to see what BCA is and they both gave consistent results.

249
00:31:21,590 --> 00:31:26,480
And then on the panel on the right, they just try to ask.

250
00:31:26,720 --> 00:31:30,049
We do differentially express interesting we find the genes are they are

251
00:31:30,050 --> 00:31:35,629
differentially expressed and there is there any evidence to confirm the findings.

252
00:31:35,630 --> 00:31:45,590
So for the autism study there is a good of curating the null so high risk which is basically for all those autism patients.

253
00:31:46,490 --> 00:31:49,940
They have this kind of rare somatic mutations.

254
00:31:50,450 --> 00:31:55,889
When they have this somatic mutation, they have a much higher risk of to have autism.

255
00:31:55,890 --> 00:32:04,550
Now, they are rare mutations, but they give kind of a partial list of autism related the genes.

256
00:32:05,120 --> 00:32:13,999
So so here is just comparing the gene levels that we find or say to those are high risk the genes so that they are actually different

257
00:32:14,000 --> 00:32:21,319
than in the sense like we are looking at differentially expression and now the high rates of genes defined by hyper mutation.

258
00:32:21,320 --> 00:32:28,900
There must be factors separate those two gene lots of our our mice out of the idea and

259
00:32:29,150 --> 00:32:34,190
is of the the standard idea of mass of using actual binomial to fit that distribution.

260
00:32:34,190 --> 00:32:34,960
So the idea is.

261
00:32:35,170 --> 00:32:46,479
Here is using our maser replies to the idea noisy so and then that easy to instill the bulk and then the exact same they just different cell types.

262
00:32:46,480 --> 00:32:55,130
So they are neurons. This idea is you have it there in your eyes and then goes out with three of our five six.

263
00:32:55,150 --> 00:33:00,910
They are excitatory neurons on different they are the body today inhibitory neuron

264
00:33:00,910 --> 00:33:06,100
will will inhibit things that you excited in your own will excite scenes and and so

265
00:33:06,230 --> 00:33:11,230
for different cell type of high stated that basically the genes and we find that in

266
00:33:11,230 --> 00:33:16,570
some cell types the hides are significant or life within the high risk of genes.

267
00:33:22,270 --> 00:33:25,900
And another thing we're also looking at is in the top of an hour.

268
00:33:26,680 --> 00:33:35,850
I don't know how much you know, that's another reason that this concern hovering on Twitter at anchor doing differential pricing,

269
00:33:35,860 --> 00:33:44,680
they find that a traditional measure to likely seek out high inflation or temporary certain situations and they say use this to raise some tax.

270
00:33:44,750 --> 00:33:50,500
That's the same. Good enough. But I mean, in practice, these are as good the way you do the analysis.

271
00:33:50,500 --> 00:33:59,079
You promote the case control labels to arrive again. Because when they see the the result of that after the noisy,

272
00:33:59,080 --> 00:34:06,489
I'll measure a much higher power with a big worry like whether this can reduce due to some type of error aberration.

273
00:34:06,490 --> 00:34:12,580
We don't we wasn't able to look at these in your region so this is a, you know, real data analysis.

274
00:34:13,450 --> 00:34:22,570
I mean, here we basically estimate have an error in the real data by applying different measures on commuting and case control labels.

275
00:34:23,260 --> 00:34:35,230
And then we count. And then basically for each type of error, say, HPI to account for on the panel 8.01 Panel B 8.05 HPI To cut off,

276
00:34:35,740 --> 00:34:39,130
we calculate the proposal to change the way the p value is smaller than the cutoff.

277
00:34:39,550 --> 00:34:45,160
And then and then for each milestone and we have several few items because we do have 17 cell types.

278
00:34:45,640 --> 00:34:49,300
So we look at the kind of of IRA across our types.

279
00:34:49,960 --> 00:34:56,830
So the the rap song must are the cell level testing.

280
00:34:56,830 --> 00:35:03,159
So we know they have an error. And then we also see in the real data in the mass that here is what I mentioned,

281
00:35:03,160 --> 00:35:10,059
this generalized linear mix effect model to try to account for that many cells from the same individual.

282
00:35:10,060 --> 00:35:17,770
So we still have made and have an error and our measure of how well kind of low so console.log is has an error.

283
00:35:17,770 --> 00:35:24,309
And the the noise in measure is somewhat that you have all conservative in this we

284
00:35:24,310 --> 00:35:30,250
are showing that our result is not biased by by any inflation or type of error.

285
00:35:30,700 --> 00:35:38,780
And of course this permutation is not ideal the way that if you actually have differential expression between case control and the formula,

286
00:35:38,790 --> 00:35:47,169
you actually. You know, I actually kind of look at the data where the one important the clarity,

287
00:35:47,170 --> 00:35:51,909
the missing from your them and then you'll get one wrong case control label.

288
00:35:51,910 --> 00:35:55,760
So the full case control label is the important, the clarity omission.

289
00:35:55,850 --> 00:36:02,110
So I'm always kind of me specify I may increase the prevalence estimates in your

290
00:36:02,110 --> 00:36:07,420
model may reduce the power but all we see that the temporal control is okay here.

291
00:36:10,750 --> 00:36:17,050
So there is a quick summary to the our major idea of what's in the bag.

292
00:36:17,770 --> 00:36:24,229
So the Volcker idea, to summarize these files on one January, individual power distribution and then the CO2 bottle,

293
00:36:24,230 --> 00:36:27,520
because summarize the surprising ones, only one individual by summation.

294
00:36:28,630 --> 00:36:32,080
You know, this isn't all like some of these cut for me in other areas.

295
00:36:32,120 --> 00:36:37,030
There is also a really subtle theme. This is think about where do the summation is.

296
00:36:37,630 --> 00:36:41,500
If you have 100 cells, one cell has a one. The other cell has one.

297
00:36:41,770 --> 00:36:45,250
Where do the summation? Actually, that one cell has much higher contribution.

298
00:36:45,700 --> 00:36:49,570
And then when we do the distribution, each cell has equal contribution.

299
00:36:49,600 --> 00:36:57,490
So that can also create a sum difference. And that's probably depend on the situation, which is that they make more sense.

300
00:36:58,780 --> 00:37:04,630
And then one thing is that we do not mean to say ideas for the replace syllable.

301
00:37:04,720 --> 00:37:14,590
I think the ball syllable is kind of robust and convenient to serve people so that until about an idea is like this or you can identify

302
00:37:14,590 --> 00:37:24,129
a model reference maybe is the difference of by variation other and using wise difference and the y idea is a can perform better.

303
00:37:24,130 --> 00:37:27,490
I think I already mentioned that that basically is firstly account for some non

304
00:37:27,790 --> 00:37:32,230
means fixed the signals and also that the noisy power can improve the power.

305
00:37:34,530 --> 00:37:39,510
So this is on your pause here, if I have any questions.

306
00:37:41,550 --> 00:37:54,830
And I'm curious about the part of the part of your your figures where you showed the variance in differential expression and in practice,

307
00:37:55,400 --> 00:38:00,709
kind of what you see with those genes that don't have a mean difference.

308
00:38:00,710 --> 00:38:08,600
Right, but might have a greater variability across cell types and how you got within a certain cell type and how you think about that.

309
00:38:09,410 --> 00:38:23,780
All right. So thanks. I think the question is really how how does the change where it's being captured and it means like a lot like a nosedive.

310
00:38:25,460 --> 00:38:26,880
So, I mean, this is a great question.

311
00:38:26,900 --> 00:38:33,440
Lensing is actually it's hard to see it because you've got this negative binomial me and whoever is stone incarnate.

312
00:38:33,440 --> 00:38:39,379
And so we need to try to look at that and we use the like in pseudo or this first in primary term.

313
00:38:39,380 --> 00:38:48,050
So use the negative binomial ratios are like where is the minus mule square minus meal divided amu square.

314
00:38:48,200 --> 00:38:52,940
So regardless because negative binomial with the variance is neoplasms quarantine the

315
00:38:52,990 --> 00:38:57,230
oldest time to actually account for the all of dispersing parameter in the real data.

316
00:38:57,470 --> 00:39:05,930
And then we ask, you know, all those genes we identify the pseudo bulk of cannot find and then you know if we

317
00:39:05,930 --> 00:39:11,670
have the pseudo if we have this over the squares and finally turns so it's okay,

318
00:39:11,690 --> 00:39:13,040
maybe I should rephrase them.

319
00:39:13,040 --> 00:39:20,810
So you got to take certain data for each gene in each individual can be the natural binomial and then you can estimate the or this person.

320
00:39:20,990 --> 00:39:27,320
And now if I got ten ks certain control, I can't just cancel with this person or this person.

321
00:39:27,710 --> 00:39:30,800
I can actually do the testing of some test.

322
00:39:31,160 --> 00:39:35,930
And then I find that the mice are not identified by our mice are by means of ideas,

323
00:39:36,170 --> 00:39:42,200
are more likely to have significance when you compare those over the source of planetary.

324
00:39:42,500 --> 00:39:45,560
Well, I actually try to compare the variance. It's not much difference.

325
00:39:45,800 --> 00:39:49,280
And now we realize the whereas is very strongly correlated with me,

326
00:39:49,340 --> 00:39:55,819
you only see the way you try to several separate after the mean fact is that I'm sorry.

327
00:39:55,820 --> 00:39:59,299
Your question. Yes, thank you. I have a simple question.

328
00:39:59,300 --> 00:40:01,070
When you analyze also data,

329
00:40:01,070 --> 00:40:10,960
do you look at a boys and girls and are there any differences in that you discoveries across different gender categories we do in Congress.

330
00:40:11,000 --> 00:40:14,270
Okay. Is it calories? But the sample size is pretty small.

331
00:40:14,600 --> 00:40:17,960
We wouldn't be able to see much big difference.

332
00:40:20,580 --> 00:40:25,290
So. So this is look like a well-executed, intuitive idea.

333
00:40:25,500 --> 00:40:28,770
So my question is about the noise art. Mm hmm.

334
00:40:29,680 --> 00:40:33,089
No, no. Part of you said this. The other people's work.

335
00:40:33,090 --> 00:40:40,530
I just curious, how do you tell those is why you say noise is probably something in the residual.

336
00:40:40,530 --> 00:40:48,500
So let's think about the regression. You know, those are not correlated with your your point of interest like case control early on.

337
00:40:49,290 --> 00:40:54,059
So how do you kill a noise? It's already a noise. Not funny. Yeah, yeah, it's a great question.

338
00:40:54,060 --> 00:41:02,840
And I think that it's actually kind of key to, you know, how you know, the way of the noise, even the removed the case can tell you or me I,

339
00:41:03,050 --> 00:41:14,580
I it's very the strong signal in the data cell type difference and then the those the noise in mice our database I can I try

340
00:41:14,580 --> 00:41:24,210
to tie to kind of doing damage reduction on the data try to look for the stronger signal which is cell type of different.

341
00:41:24,810 --> 00:41:32,620
So we. And then the remaining things are removed.

342
00:41:32,770 --> 00:41:41,140
So, I mean, I'm sorry, I cannot say like why that the noise in our ears help make it probably really a case by case.

343
00:41:41,470 --> 00:41:47,710
We found this study and they were also analyzed another way the data in both case we find that the noisy.

344
00:41:48,020 --> 00:41:53,850
Firstly you'll have to apply that to all cell types together. You cannot delete the cell type by cell type because when you do a cell type,

345
00:41:53,860 --> 00:42:01,180
I found that you don't have the strong across cell type SICA they'll help you to distinguish information in the noise,

346
00:42:01,450 --> 00:42:07,020
but how the case control information being coupled with all these noise that I'm actually not.

347
00:42:07,030 --> 00:42:13,900
I'm very sure this is a great question. But the thing in practice is really people who run the.

348
00:42:13,990 --> 00:42:20,160
You actually need to run both versions in check. Yes. All right.

349
00:42:21,780 --> 00:42:26,280
Okay, then I'm going to the next part on the stand.

350
00:42:26,310 --> 00:42:31,040
This is a still ongoing work. This is my problem.

351
00:42:31,050 --> 00:42:34,740
The student had finished most of the work. I just didn't put them into paper yet.

352
00:42:35,970 --> 00:42:41,880
The supervisor, they were learning with a general notation. So this is the master, in a way.

353
00:42:42,240 --> 00:42:45,690
Here we may consider a differential expression between two groups ourselves.

354
00:42:45,900 --> 00:42:49,890
So previously I was seeing differences of expression between two groups or individuals.

355
00:42:50,160 --> 00:42:53,730
And now in some situations you do want to compare two groups of cells.

356
00:42:54,150 --> 00:43:04,860
And then the challenge. And the motivation here is that when you compare to group cells, you often got lots of cells and then you got small pupils.

357
00:43:05,340 --> 00:43:12,569
And then, you know, in the previous study, a differential expression, which is one to find the enough genes.

358
00:43:12,570 --> 00:43:21,270
And so we can do like inside the arrangement and that is in and and and now because there are so many genes the few

359
00:43:21,270 --> 00:43:29,100
are two can be extremely small like ten to next page 110 to -100 to their ranking is also not that meaningful.

360
00:43:29,400 --> 00:43:37,760
And so here we try to think about maybe we can the do classification, accuracy in testing data and then many studies.

361
00:43:37,760 --> 00:43:40,079
And the final goal is not to identify the individual genes.

362
00:43:40,080 --> 00:43:46,110
When we do the differential expression testing, what we want to find is was the underlying biological person.

363
00:43:46,550 --> 00:43:55,400
So. So why now if you actually identify the biological process that can classify the cells and this is not a new idea,

364
00:43:55,410 --> 00:44:00,810
there are many efforts to combine gene notation and the differential expression testing.

365
00:44:01,050 --> 00:44:08,880
So what we do here is are we try to borrow this from neural network idea, totally model of the genome notation as a graph.

366
00:44:09,720 --> 00:44:13,350
So this is one graph protein. Protein interaction.

367
00:44:14,430 --> 00:44:17,160
It's. Most pretty,

368
00:44:17,170 --> 00:44:27,190
most evil people who are not familiar with I'm pretty clean or people who know about the reality is is actually much more messy than this one.

369
00:44:28,480 --> 00:44:36,450
So this the reason why is from the agree that it's a annotation on protein protein

370
00:44:36,450 --> 00:44:42,219
interactions and then so we want to use a graph to represent the the tuna annotation.

371
00:44:42,220 --> 00:44:47,520
Basically, our graph is no data chain and they are connected if they have, say,

372
00:44:47,530 --> 00:44:53,580
protein protein interactions and they want to model either by a graph neural network on graph convolutional networks.

373
00:44:54,250 --> 00:45:00,130
And for some of you know, if America's just try to introduce an idea.

374
00:45:00,700 --> 00:45:04,690
Convolutional neural network is very successful applying for the imaging.

375
00:45:04,870 --> 00:45:11,889
The convolution part of means you apply the same operation here like on each pixel.

376
00:45:11,890 --> 00:45:17,860
So this is I actually try to so imaging the one on the left image you can you can see the rise

377
00:45:17,860 --> 00:45:24,520
of graph is a graph of very regular data points like they have equal distance across each other,

378
00:45:24,790 --> 00:45:27,580
each pixel they have equal distance across their neighbors.

379
00:45:27,940 --> 00:45:33,730
And then you have well, basically the common user means you have a filter, an X three by three filter,

380
00:45:33,910 --> 00:45:38,860
and then you're try to scan the image and then basically the filter has the same parameter.

381
00:45:38,860 --> 00:45:43,600
Right? It looks like I want to have two and line. So that's the filter in this kind of image.

382
00:45:44,380 --> 00:45:49,360
And then so that's why the cameras and future those apply the same operation to all

383
00:45:49,360 --> 00:45:53,770
the positions and they all do a weighted summation of all pixels or know the region.

384
00:45:54,160 --> 00:45:58,059
And then this way it's a shared across region. So that's about the convolution.

385
00:45:58,060 --> 00:46:05,500
And that is actually one of the main reason the convolutional neural network was so successful because it's doesn't need that much parameters.

386
00:46:05,560 --> 00:46:12,160
It's the one parameter everywhere in the image find this and position independent the second one but they can graph is

387
00:46:12,160 --> 00:46:19,989
different think about the neighbor different from those two know right if I want to do the color of the graph one.

388
00:46:19,990 --> 00:46:24,520
No, they have three neighbors. The other knows how five neighbors and the distance across neighbors.

389
00:46:24,520 --> 00:46:27,790
I mean, they are they are not the same. One node is different from the other node.

390
00:46:28,060 --> 00:46:32,860
So how do I do this? And the neighbor do not match.

391
00:46:33,610 --> 00:46:43,149
So the the the graph and you're an algorithm is a big field and this is just one of the basic approach has to parse your aggregate

392
00:46:43,150 --> 00:46:50,590
of the neighborhood and then the neural network operates and learn the representation of each node based on its attributes.

393
00:46:51,010 --> 00:46:53,200
So if we think of all the gene expression data we have,

394
00:46:53,200 --> 00:47:00,400
a graph is nowadays a gene and then it's attributes is the expression you and say has all the cells.

395
00:47:01,090 --> 00:47:06,399
Okay, now my data is in the other way like one g and have the expression heads on themselves.

396
00:47:06,400 --> 00:47:12,460
That's a high dimensional scene when they want to find a group of genes, I want to reduce that.

397
00:47:12,880 --> 00:47:20,680
So the aggregation party's predetermined point, then we just take an average of this node attribute and all of these neighborhoods.

398
00:47:20,950 --> 00:47:22,719
So that average is fine.

399
00:47:22,720 --> 00:47:31,660
You can apply the same operation to all the nodes because they just take an average and then the the different parties learn the representation.

400
00:47:31,930 --> 00:47:40,120
So here the h t minus one is basically either T minus one star of your data at the very beginning.

401
00:47:40,120 --> 00:47:50,900
So it's just observable the data and then w it's a way the matrix it would be estimate hey and then you can think about the data is the,

402
00:47:51,720 --> 00:48:00,370
the, the, the common missionary or primate earth and then you go through the idealized nonlinear activation function.

403
00:48:00,370 --> 00:48:05,980
They account for this new output. So this is same operation applied to all the nodes.

404
00:48:06,040 --> 00:48:08,980
So that's kind of like convolutional operation.

405
00:48:09,970 --> 00:48:18,790
And so, so in the, you know, the parallel here to quantify the change in network and then we use protein protein action from our great database.

406
00:48:19,060 --> 00:48:24,070
And this location is a uniform database far from accurate because you know,

407
00:48:24,070 --> 00:48:30,100
this is maybe the two protein interacting neural cells and non who have muscle cells.

408
00:48:30,280 --> 00:48:38,830
It's not relevant but they are important to oh and then, you know, there's a gene that actually people don't know the disease present across cells.

409
00:48:39,040 --> 00:48:44,500
And the goal here is more like a just classroom, which is I want to find the group of our genes.

410
00:48:44,800 --> 00:48:48,129
And so that in that unit we're doing a cluster affair or something similar to your

411
00:48:48,130 --> 00:48:53,470
expression to have more connection within the graph and then they input the data.

412
00:48:54,270 --> 00:49:01,960
Well, to this goes for each gene, we, we try to use that all using the observer, the gene expression across all the cells.

413
00:49:01,970 --> 00:49:06,100
We try to gather low dimensional representation by the ground for neural network.

414
00:49:06,790 --> 00:49:13,330
And so there may be a question to ask and like, why do you want to at a low dimensional representation?

415
00:49:13,390 --> 00:49:23,010
I know you just directly use our data to do that. So one thing is the thing with our data is is small noisy is most fires.

416
00:49:23,250 --> 00:49:31,560
And so this is like that. The noisy part, if you recall the data from say 10,000 cells, you can try to reduce the dimension.

417
00:49:31,650 --> 00:49:37,650
It's almost like the noisy process. And then if you think about the cell from like 20 cell types,

418
00:49:37,660 --> 00:49:44,310
you're kind of capturing the cell type information and the same dimension reduction is applied to all the genes.

419
00:49:44,580 --> 00:49:53,190
So, so this is like, you know, you try to because while watching the neural network, you kind of borrow information across genes.

420
00:49:53,400 --> 00:50:01,620
So this is like I try to reduce the dimensions on clustering cells so you can reduce the noise in the cells.

421
00:50:02,160 --> 00:50:13,020
And then for many people who look to the neural network, there's a good question to ask is why do you use this thing like this is a black box?

422
00:50:13,020 --> 00:50:22,559
I don't like it when I use a simple laser. So you my understanding here is this situation is a good set of neural network.

423
00:50:22,560 --> 00:50:27,030
And so we have got a number of cells, we have a large sample size and there are also strong signals.

424
00:50:27,030 --> 00:50:32,669
The gene expression can be very different or from cells, and that's where the neural network can help.

425
00:50:32,670 --> 00:50:34,080
We will have high dimensional data.

426
00:50:34,110 --> 00:50:44,220
You want to go learn that low dimensional representation and the sigmoid sort and and this is a protein data of one when

427
00:50:44,250 --> 00:50:51,540
I try to learn the neural network on those are the imaging data and then the parts of our highways like why they work,

428
00:50:51,540 --> 00:50:55,860
like you have a neural network with the targeting parameters, it's with our feet.

429
00:50:56,730 --> 00:51:03,240
But I rarely do see anything. So the even is so strong. If you want to classify a car, the dog will see it.

430
00:51:03,240 --> 00:51:07,530
You'll see. You'll know that. I mean, the signal is strong means you know their difference.

431
00:51:08,010 --> 00:51:12,540
But the difference is not clearly to tell. You don't know exactly the feature.

432
00:51:12,720 --> 00:51:16,310
So what you need to learn is the feature and the crime.

433
00:51:16,320 --> 00:51:21,450
Because in the neural network other ways, it's more like loading on the PTA,

434
00:51:21,660 --> 00:51:26,639
on something that you don't need a silver wise function, you don't need you.

435
00:51:26,640 --> 00:51:33,640
In the case control label, you can turn, learn off. So so you can you'll have a neural network who is very well over randomize.

436
00:51:33,670 --> 00:51:39,780
They can still generalize pretty well because because you you do rely on the strong signal.

437
00:51:39,780 --> 00:51:44,489
You can just do unsupervised learning and then get lost all the ways already.

438
00:51:44,490 --> 00:51:47,840
Ask me if you get. Okay.

439
00:51:47,840 --> 00:51:56,290
So again, this is a schematic a problem though our method so start away way that the the

440
00:51:56,300 --> 00:52:03,740
database is the expression of change here actually is like on the south side

441
00:52:03,740 --> 00:52:07,390
of change and you start out using all genes and we use like a more labor of

442
00:52:07,760 --> 00:52:11,900
color of a polymorphisms is like top 1000 genes that you guys really express.

443
00:52:12,320 --> 00:52:19,820
And then we call this data and then we do this estimation a graph neural network.

444
00:52:19,860 --> 00:52:27,740
You kind of gave you this length in the component. So it's called graph pulling its operation in the graph neural network you can put them.

445
00:52:28,070 --> 00:52:33,830
And then when you do the graph putting you actually wow the new no here actually

446
00:52:33,830 --> 00:52:40,760
take a improve the from all the no the in the right and then so so here is that

447
00:52:40,760 --> 00:52:44,930
they put this latent component that you can consider like one column the orange

448
00:52:44,990 --> 00:52:50,569
here is like the weights are all the edges and then we say the our exactly.

449
00:52:50,570 --> 00:52:53,990
The function title may make the weight of the assignment about the script.

450
00:52:54,290 --> 00:52:59,059
So one j is always one based on the component. And then once you go this method,

451
00:52:59,060 --> 00:53:04,520
the component that this is basically tell you how the group is into clusters and

452
00:53:04,520 --> 00:53:10,069
then you can transform your original data matrix to cells worth is Nathan the

453
00:53:10,070 --> 00:53:17,750
components so you actually think about the from from here to here what we have is the

454
00:53:17,750 --> 00:53:26,600
expression data in 1000 genes forces the size of data in say 28 in the component.

455
00:53:26,600 --> 00:53:35,270
So we kind of reduce the dimensional gene. And then you can put this data to another neural network to do classification.

456
00:53:35,600 --> 00:53:41,420
So the the beauty of this deep learning framework is either as those operations is differentiable,

457
00:53:41,630 --> 00:53:45,390
again, you just do a matrix, multiplication is differentiable.

458
00:53:45,410 --> 00:53:50,240
So you can push back by propagate everything to estimate.

459
00:53:50,480 --> 00:53:54,410
So here we actually try to estimate the two neural network.

460
00:53:54,650 --> 00:53:59,270
Why is the graph neural network? Why does supervised learning network?

461
00:54:01,410 --> 00:54:06,090
And then the moat just give you a little bit more detail on the operation.

462
00:54:06,090 --> 00:54:13,260
So for example, with how I saw in June, the 5000 South for the first time was not a massive reduction by dropping your network.

463
00:54:13,590 --> 00:54:22,350
So I was always using with 1972. And then what we actually used is a little bit more complicated that there is also escape connection.

464
00:54:22,680 --> 00:54:33,030
Basically this means that any copies of an activity matrix, it's a of a normalized the connectivity matrix.

465
00:54:33,270 --> 00:54:40,880
And then eight times actually is like graphs most the data and then multiply W is doing the dimension reduction and the W scale

466
00:54:40,890 --> 00:54:46,650
just means I don't want to just take an average with my neighbors because you are learning a represent representation and

467
00:54:46,650 --> 00:54:55,950
there's no so there's no the ten highlights for weight and so actually all is the output all the hours either there is

468
00:54:55,960 --> 00:55:05,370
normalize a connection matrix and w the double escape are two way the matrix and then the next stop is doing the graph pulling.

469
00:55:05,370 --> 00:55:12,770
So that's like the first that is. Reduce dimension on cells and the second the slightly reduced dimension on tubes.

470
00:55:13,160 --> 00:55:19,610
So you can say from 30 to 4, it's a two to the original 220 a component.

471
00:55:21,260 --> 00:55:26,780
And then the last answer and follow the supervisor training. Why is the classification apathetic?

472
00:55:26,780 --> 00:55:32,870
Which is how to improve ourselves? And the other is try to use the graph.

473
00:55:32,960 --> 00:55:38,360
It's a pilot panic. How does my gene component are distributed on the graph?

474
00:55:38,360 --> 00:55:43,280
White are the genes on one component. They tend to have multiple connections in the crop.

475
00:55:43,550 --> 00:55:56,950
So we use this pilot. Hey, from, from this notation here is basically where similar to spike from clustering pilot and using hypoxia to imagine is.

476
00:55:59,350 --> 00:56:05,930
So we apply this to Lukather the more they all died from COVID 19.

477
00:56:08,020 --> 00:56:14,100
So. We hope we can put recover that behind us.

478
00:56:15,870 --> 00:56:24,059
Formosa us. I think he's really worried about the disease of life of some high risk individual like particularly immunocompromised

479
00:56:24,060 --> 00:56:31,260
the person and then like other people there are still risk comes with the amount of more severe disease.

480
00:56:31,650 --> 00:56:37,020
And so while we try to learn things like what's the difference between mouth and the severe disease?

481
00:56:37,380 --> 00:56:45,990
So I think now there's enough of us that the, you know, the consensus is really that it's the T-cell is the important.

482
00:56:46,350 --> 00:56:53,070
I think the antibody is from B cells is help us prevent the from infection.

483
00:56:53,460 --> 00:56:57,570
But T-cell is really the one to prevent the from severe disease.

484
00:56:57,690 --> 00:57:03,839
So that's why we got the vaccine and then with all the immune memory and the next time we see the where we see the virus,

485
00:57:03,840 --> 00:57:09,390
then the T-cell will start immediately to get rid of the virus.

486
00:57:09,420 --> 00:57:14,040
So it only has the chance to become more severe and healthier.

487
00:57:14,040 --> 00:57:20,910
We want to look at the cd8+ t cells while the T cells between now and the severe COVID 19 patients.

488
00:57:21,930 --> 00:57:28,709
We spread it out with half of the sample for training and the other half of testing and then among the treating

489
00:57:28,710 --> 00:57:37,680
samples so 10% as by the days and then snack by some of the plastic 1000 genes as they improve with our the ionizer.

490
00:57:37,700 --> 00:57:47,820
I think a few things I want to point out words are first studies are the those is the classroom, those are the cells and then this phase of the label.

491
00:57:48,000 --> 00:57:55,640
So our label itself is phase from severe amount of patients. So the of B cells are the severe amount is not well separated.

492
00:57:55,650 --> 00:58:02,900
If we were to supervise the analysis, they don't belong to different clusters and then we run our problem neural network actually.

493
00:58:02,910 --> 00:58:10,830
And finally finally is the way the things we call basically the way we assign the genes to late in the component.

494
00:58:11,130 --> 00:58:15,960
So once you are nice we find that here is. So each row is a chin.

495
00:58:15,970 --> 00:58:19,140
Each column is a late and the component. We gave 40 components.

496
00:58:19,380 --> 00:58:23,080
But the neural network automatically is like the novel component we have.

497
00:58:23,350 --> 00:58:29,810
How do you look at the some columns and they don't have wings and then they also assign genes at all.

498
00:58:29,820 --> 00:58:35,880
Usually one chain is assigned to one component. So it's very clear from structure and they're like.

499
00:58:37,170 --> 00:58:41,670
So 100 units. They don't really clear the signs when they leave in the component.

500
00:58:41,970 --> 00:58:45,030
So that's just learned through the supervisor task.

501
00:58:45,270 --> 00:58:46,820
If you want to do Crosby Hazen,

502
00:58:47,100 --> 00:58:53,820
you're going to need to use that 300 units and then put them into those components and maybe direct 20 some components.

503
00:58:54,240 --> 00:59:01,180
And then once you got that, they would classify the cells and this is the second that these videos will be out there.

504
00:59:01,200 --> 00:59:11,160
But basically this first of all, by means of the number of cells with a scar on either one of 02.1, the next one is point one, 2.2.

505
00:59:11,760 --> 00:59:20,220
So basically, if you have the cd8+ t cell from a patient with a severe cold and then you go out and higher a skull more likely to be severe,

506
00:59:20,400 --> 00:59:24,430
and then if they smile, they would go for the severe patients.

507
00:59:24,430 --> 00:59:29,159
So some of the T cells will have a lower scar. I think this is indicating some biology.

508
00:59:29,160 --> 00:59:33,420
Not all the t's out there, as has the issue and normally a subset of them.

509
00:59:33,780 --> 00:59:40,079
And then once you call the cell line, will predicts those are all person B and C and B was in both the cell.

510
00:59:40,080 --> 00:59:45,360
And I will prediction you'll just take an average of the patients and they will plow the scar.

511
00:59:45,360 --> 00:59:51,419
You can find that you can actually separate the the the patients now they the severe rare.

512
00:59:51,420 --> 00:59:55,020
Well, okay. And then this is the summary.

513
00:59:56,010 --> 01:00:04,649
So the first match of ideas is the individual line or differential expression and then it's the forecast is doing the

514
01:00:04,650 --> 01:00:09,860
testing across individuals and they can also be applied in the whole training space in between to improve ourselves.

515
01:00:10,140 --> 01:00:14,400
What you want to compare is the distribution rather than their movie set.

516
01:00:14,730 --> 01:00:17,940
And then the stem is supervised deep learning that you have a patient.

517
01:00:17,940 --> 01:00:22,979
So a comparative expression between to prove ourselves and hope to identify a the sites

518
01:00:22,980 --> 01:00:27,930
that collectively classify the cells and then we incorporate the translation as a graph.

519
01:00:29,190 --> 01:00:30,950
And this is the first part.

520
01:00:30,970 --> 01:00:39,640
The idea is mainly about will post up once here the slide the stands by operating still that are from your W and our stop here.

521
01:00:39,690 --> 01:00:49,300
Thank you for your attention. In many seems.

522
01:00:51,860 --> 01:00:55,360
So very nice work as I have a question of that book.

523
01:00:55,380 --> 01:01:01,280
So it's very nice that you have a very sparse representation of energy and set to the affected audience.

524
01:01:01,580 --> 01:01:06,140
Is this a feature of this in your own network, or did you put any specific assumptions on that?

525
01:01:06,440 --> 01:01:11,650
Oh, it's just automatically from this. Yeah, great question.

526
01:01:11,660 --> 01:01:20,450
I shouldn't take the credit. There is actually from that paper citation, the spectrum pilot, I think is the spark from pilot here.

527
01:01:20,450 --> 01:01:25,879
You know, this one to the ground for pilot to try to encourage more connection for the change within cost.

528
01:01:25,880 --> 01:01:33,920
Or there is also another panel to kind of try to make the way the more discrete like kind of entropy type of pilot.

529
01:01:34,340 --> 01:01:38,480
So you want to make one thing to one component only property of many components.

530
01:01:38,690 --> 01:01:39,919
Oh, I see. That's nice.

531
01:01:39,920 --> 01:01:48,350
And is that the inference tables that if you run with a different set of weights, that what do you identify similar in factor relationships?

532
01:01:49,220 --> 01:02:00,709
I think so. But since you pervasive Paris attack numbers on individual I was pretty stable but yeah I mean the

533
01:02:00,710 --> 01:02:06,680
the cell my well that was we run a few times and I haven't really timed exactly the change tool.

534
01:02:12,530 --> 01:02:22,290
So the so the the, you know, the the suppression of that the case I'm talking about the recording 19 example is it's quite striking.

535
01:02:22,310 --> 01:02:31,280
So the so the question that you only pick this up with the neural network does imply or these are nonlinear additive effect,

536
01:02:31,280 --> 01:02:36,819
or they're you have to look into the interactions and then get these type of these factors.

537
01:02:36,820 --> 01:02:39,830
So let's just want to get your take on this.

538
01:02:40,240 --> 01:02:44,200
Yeah, great question. So for this neural network,

539
01:02:44,200 --> 01:02:51,679
I with one one feature like we were thinking is how to make the result as easy to

540
01:02:51,680 --> 01:02:57,680
interpret as possible for the we call these the insights so they are they are very

541
01:02:57,680 --> 01:03:02,329
easy to explain and then so we can tell your network wise across from your network

542
01:03:02,330 --> 01:03:06,230
but the other is just the speed of our neural network to do the classification.

543
01:03:06,500 --> 01:03:13,250
I think one thing we should try is actually replace the site in the neural network by things obviously revising or something.

544
01:03:13,580 --> 01:03:22,610
And in this case, because they're all together like 100 samples, we have 52 new 50 testing and then we have like 20 component.

545
01:03:22,760 --> 01:03:28,069
So it's partly the standard process request and partly still hard,

546
01:03:28,070 --> 01:03:38,809
but my fear is that a component is not I important like was more important than the nonlinear RJ part on the neural

547
01:03:38,810 --> 01:03:46,129
network is really on the graph neural network where we reduce the dimensional to 10,000 cells to a low dimension.

548
01:03:46,130 --> 01:03:50,870
So it kind of denies the single cell data and how few classroom to choose.

549
01:03:52,010 --> 01:03:55,770
Yeah, I, I think if there is and in your article is,

550
01:03:55,850 --> 01:04:03,290
oh they are rather in the later step and then it's, it's, you know, if you replace the silver lines,

551
01:04:03,290 --> 01:04:06,140
the learning neural network with the something else,

552
01:04:06,470 --> 01:04:15,590
otherwise the you can still back propagate all the gradient to then you probably can adjust learning based on this simple simple amount of their.

553
01:04:19,940 --> 01:04:26,679
You know, American. So for the first part, the differential expression analysis is in downpour.

554
01:04:26,680 --> 01:04:29,860
It's ten is south five separately. Yeah.

555
01:04:29,980 --> 01:04:35,050
So there are a way that they can borrow information across different cell types are different genes.

556
01:04:35,770 --> 01:04:39,360
Um. No, sir.

557
01:04:39,870 --> 01:04:44,060
Borrowing information across our times. I mean, it's a very practical question.

558
01:04:44,520 --> 01:04:49,650
Some cell types may be relatively small, and then it's both her.

559
01:04:49,890 --> 01:04:52,770
If I can look and see more of a cell type to borrow information,

560
01:04:53,160 --> 01:05:02,900
but I'm I'm just not sure exactly how to quantify how much to borrow information in terms of borrowing information across chains.

561
01:05:02,910 --> 01:05:10,959
I think that that's more feasible. We haven't tried that, but we kind of already borrow information on how students buy during the day.

562
01:05:10,960 --> 01:05:17,160
And I think that's one way, our way, our information with our saying like parking information.

563
01:05:17,460 --> 01:05:27,840
But yeah, but that's in the testing part, particularly for the part we need to adjust for that.

564
01:05:27,840 --> 01:05:36,479
If I go like retailers, like, you know, the success is actually along those of those seems a minor part like have you had to

565
01:05:36,480 --> 01:05:40,860
ask for readouts is something that is in question but that's a very contingent result.

566
01:05:40,860 --> 01:05:46,160
Well, I think that part we can definitely borrow information across chains to make it more robust

567
01:05:46,260 --> 01:05:51,330
to to adjust for readouts like if you just the way that I read actually it doesn't work.

568
01:05:53,330 --> 01:05:58,340
If you do the requirement in Beijing, it may or the way to live.

569
01:05:58,790 --> 01:06:02,290
So maybe the good point is somewhere in between. Yeah.

570
01:06:04,460 --> 01:06:10,010
If you go back to the slide where you show the the COVID 19, the two groups, the severe.

571
01:06:11,120 --> 01:06:19,070
So when when I look at that graph right by disease state it looks like there's a huge difference.

572
01:06:20,390 --> 01:06:28,360
In that in that graph in the blue rights right type of south in the middle kind of the streak at the top of it.

573
01:06:28,370 --> 01:06:31,880
So in graph B. Oh yeah. Right, right there.

574
01:06:32,060 --> 01:06:39,250
It looks like there's a whole cell type missing in the mild compared to the year.

575
01:06:39,620 --> 01:06:44,060
Right. So is what you're picking up just that missing cell type?

576
01:06:44,930 --> 01:06:48,670
Oh, good question. Um, yeah.

577
01:06:48,680 --> 01:06:56,360
I mean, this is the kind of more likely that although I do not know, I mean, how many blue are hidden behind the orange.

578
01:06:56,390 --> 01:07:02,210
But I guess, I mean, your point is that there is a definite racial part of the blue are all right.

579
01:07:02,510 --> 01:07:06,260
And if you look across from these like this cluster.

580
01:07:07,130 --> 01:07:18,980
Yeah I mean that's definitely likely some of those severe are well mixed the way the mild for some severe a stand out.

581
01:07:19,840 --> 01:07:25,209
Yeah yeah. Thank you for your time.

582
01:07:25,210 --> 01:07:32,520
This I think it's quite likely. So I have another question.

583
01:07:32,550 --> 01:07:35,670
I go first to some kind of follow Laura's earlier question, though.

584
01:07:36,120 --> 01:07:41,870
Your model is very nice. You can use test distribution's difference between engines versus controls.

585
01:07:42,360 --> 01:07:47,400
So you can capture is a mean difference over dispersion difference or anything else.

586
01:07:47,840 --> 01:07:51,930
And so this reminds me of some work done by Steven's group.

587
01:07:51,970 --> 01:07:56,430
They tried to detect risks associated with that dispersion differences in single cell studies,

588
01:07:56,760 --> 01:08:01,810
and which they found is very hard to detect over dispersion differences compared with the main differences.

589
01:08:01,840 --> 01:08:04,530
So I was wondering, you know, more than what you did have those genes,

590
01:08:04,530 --> 01:08:09,510
how many are due to them being how many are due to dispersion and how many are potentially due to others?

591
01:08:09,540 --> 01:08:15,380
Yeah, we have a supplementary table and in the paper the difference is actually not that much.

592
01:08:15,390 --> 01:08:24,580
And figure exactly. Number is like. I don't know, like, if our fault falls to the wall, because they may find that out.

593
01:08:24,860 --> 01:08:34,239
You know, I wouldn't presume to do it for with this person. And then I would measure is like 16% that who are with this person and and then it's also

594
01:08:34,240 --> 01:08:40,630
harder to tell what exactly is going on with this person the way we ask me to assume.

595
01:08:41,010 --> 01:08:46,120
So they were like, oh, is this person private or do our exam pass thing to prove that you might have come up with this and that.

596
01:08:46,540 --> 01:08:49,689
So I agree. I think it's it's actually hard.

597
01:08:49,690 --> 01:08:59,499
I mean, what we see the business is not dramatic. Like we do see the trend like our visitor can identify other friends, deal with this person by 16%.

598
01:08:59,500 --> 01:09:08,160
That's not something big. Yeah, I think another difference is what I mentioned there is like I think that is a good thing to actually do is

599
01:09:08,170 --> 01:09:15,340
sell equal weight and that can be different than from taking the size of their with a moderate expression,

600
01:09:15,340 --> 01:09:18,700
do not forget. So that can be another reason.

601
01:09:18,700 --> 01:09:23,139
Cause the difference is I have a question.

602
01:09:23,140 --> 01:09:27,220
So what is pushing is that there's no difference in the distribution of work.

603
01:09:27,490 --> 01:09:32,500
And now that like I can give you what I want is when I'm the technician.

604
01:09:32,920 --> 01:09:42,280
Oh it's the individual I what kind of you you're, you're kind of assuming assumed are some people in terms of the distribution.

605
01:09:44,680 --> 01:09:48,419
Yeah. Yeah.

606
01:09:48,420 --> 01:09:52,380
That's right. That's right. That's. We don't.

607
01:09:54,600 --> 01:10:00,410
I mean, we don't have. Sell out distribution and sell.

608
01:10:00,440 --> 01:10:04,879
We want to have one number because here we are as a country.

609
01:10:04,880 --> 01:10:09,140
Imagine separately for each gene, we only for one say we only have one number.

610
01:10:09,530 --> 01:10:13,190
So we only got the distribution of one individual.

611
01:10:14,120 --> 01:10:17,570
So. Yeah, and you can.

612
01:10:22,330 --> 01:10:27,190
Right. So so yeah, it's automatic. You assume in the South where they follow the same description.

613
01:10:27,490 --> 01:10:30,700
Yeah, yeah, yeah, yeah. So yeah.

614
01:10:31,800 --> 01:10:35,080
And that, that may or may not be the right assumption.

615
01:10:35,090 --> 01:10:43,540
Yeah. So is there any batch, in fact that you are ready to do any preprocessing kind of write out actually before you apply any estimation.

616
01:10:43,810 --> 01:10:48,370
Yeah. For those who are on, we do not. This is a relatively small study,

617
01:10:48,370 --> 01:10:58,749
but I know several and I think some sort of yeah there was a another CO and another

618
01:10:58,750 --> 01:11:04,300
point the data for the on the stand by another out of that idea or some basic in fact

619
01:11:04,690 --> 01:11:15,620
but yeah it could be then you have to try to incorporate that in in because that batch

620
01:11:15,640 --> 01:11:22,520
is usually on the individual that is rarely sequenced by individual itself anyway.

621
01:11:22,540 --> 01:11:25,860
Individual user ID from one batch. Right.

622
01:11:25,990 --> 01:11:30,969
And then if you have quite a few fact across things that can be put into to collaborate.

623
01:11:30,970 --> 01:11:35,200
So I would do the current immigration. Yeah. Thank you.

624
01:11:40,860 --> 01:11:56,240
Looks like I figure again. I learned a great.

625
01:12:02,940 --> 01:12:23,390
Even with all the way to the end of It's Last You.

626
01:12:26,650 --> 01:12:31,890
I found the first episode in the coming year.

627
01:12:35,050 --> 01:12:51,720
I know that. I know it's going to be kind of like.

