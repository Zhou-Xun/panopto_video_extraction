1
00:00:01,450 --> 00:00:08,139
It should be up and running now. Okay.

2
00:00:08,140 --> 00:00:11,680
So here's just a little logistics.

3
00:00:12,220 --> 00:00:17,020
I think things are settling down in terms of who's going to be in the class. So we're just going to kind of wait through today.

4
00:00:18,250 --> 00:00:24,430
And then it looks like we're going to have, you know, somewhere in the forties for student totals.

5
00:00:24,970 --> 00:00:29,410
So that's going to be too big for my plan for our final projects individually.

6
00:00:30,280 --> 00:00:33,460
So my current plan is to put people together in triples.

7
00:00:35,260 --> 00:00:40,540
So that would be about probably about 15, 16 or 17 groups, depending on how things shake out,

8
00:00:42,130 --> 00:00:50,890
which I think would be enough to both magically deal with for me, and also maybe more importantly for you guys to be able to present.

9
00:00:52,120 --> 00:01:01,510
So I'm going to go ahead and sign those. So I'm going to start next people's backgrounds and and, you know, submit level on the program and so forth.

10
00:01:03,370 --> 00:01:07,150
So you can look forward to getting that by, I guess, by the student.

11
00:01:08,230 --> 00:01:17,590
And so that allow you, I think time to get working should have well over a month, I think, going to get your one page proposal together as a group.

12
00:01:19,300 --> 00:01:25,030
So we'll see how that goes. I mean, I don't know, for some reason that's not working, then we'll probably just default to a final exam.

13
00:01:25,780 --> 00:01:29,350
But I'd prefer to have a project for this type of class.

14
00:01:31,810 --> 00:01:36,040
Okay. Quick questions about that. I guess I just dropped it on you, so.

15
00:01:39,450 --> 00:01:43,040
All right. So wonderful. Okay.

16
00:01:43,080 --> 00:01:51,659
So when I pick up and kind of ramp up our discussion of randomized trials and

17
00:01:51,660 --> 00:01:54,270
then we're going to go look into this concept of propensity scores today.

18
00:01:54,900 --> 00:02:05,640
So just to remind everybody, we were looking at a kind of a toy version of observations from this and it's program.

19
00:02:07,530 --> 00:02:14,490
So basically looking at the effect of job training in a randomized trial,

20
00:02:15,570 --> 00:02:20,460
we've just six individuals randomized to either three to treatment or three to control.

21
00:02:21,540 --> 00:02:25,530
So we're going to do a little bit of modeling. We're going to do this approach of trying to basically do imputation.

22
00:02:27,270 --> 00:02:38,710
So we work through an example and without covariates, right?

23
00:02:38,730 --> 00:02:42,300
We basically just have our Ys here.

24
00:02:45,400 --> 00:02:52,750
And we set our because we're imputing we have to say something about this association within the individual.

25
00:02:53,410 --> 00:03:01,600
So we're going to say that an associated for fun you could we write this and plug it a different value for a row and see how things change.

26
00:03:02,850 --> 00:03:04,030
You can do that here right now.

27
00:03:05,050 --> 00:03:17,920
So we worked out from our basic Bayesian results that the posterior distribution of the means for treating the variance estimates

28
00:03:17,920 --> 00:03:30,210
is fixed and known is going to be normally distributed with means given by the sample means and under treatment and control.

29
00:03:30,550 --> 00:03:34,960
We're imputing here. So we we're filling in this this table.

30
00:03:36,670 --> 00:03:38,050
So we're able to compute these.

31
00:03:39,700 --> 00:03:52,150
And then with variance is given by these estimates of the treatment variance and the control variance divided by sample space.

32
00:03:53,350 --> 00:03:57,460
So we kind of circle back and forth that and we got these draws.

33
00:03:58,400 --> 00:04:05,049
We pointed out also that super population inference was a was a little wider than the population

34
00:04:05,050 --> 00:04:16,120
inference in terms of the posterior credible intervals because of the fact that you're in the former,

35
00:04:16,120 --> 00:04:22,689
you're actually trying to impute out to the whole super population of essentially an infinite number of individuals in the

36
00:04:22,690 --> 00:04:31,780
latter years trying to make it if it's about the six where the only uncertainty is in the unobserved potential outcome.

37
00:04:33,880 --> 00:04:37,900
So just one or two. So anyway, so any quick questions about that?

38
00:04:38,320 --> 00:04:45,780
I get people's heads back. Don't be afraid to speak up.

39
00:04:45,810 --> 00:04:49,170
I know it's a big class, but I'm happy to take the time.

40
00:04:51,450 --> 00:05:01,980
Okay. So what about if we have available covariance x so we can use regression incorporated information about the associations with these covariance.

41
00:05:03,180 --> 00:05:11,980
So this is essentially trying to use essentially try to reduce variance, right?

42
00:05:12,360 --> 00:05:22,919
Essentially trying to sort of take out whatever information we might be able to get about the covariate

43
00:05:22,920 --> 00:05:32,370
effects and really just focus down on this residual error and and account for that in our inference.

44
00:05:33,660 --> 00:05:42,310
So I'm going to two and again, I'm not trying to turn this completely into a base class, but I did put out a pretty extensive review here.

45
00:05:43,440 --> 00:05:47,520
And just to remind people, some of you may not have seen this,

46
00:05:48,330 --> 00:05:57,570
and I'm going to go through all the details of that, but I am going to jump in about linear regression.

47
00:05:58,170 --> 00:06:02,159
So we have a linear regression model, right?

48
00:06:02,160 --> 00:06:09,629
So we have a mean for why it's a linear combination of covariance and the known parameters

49
00:06:09,630 --> 00:06:15,510
beta and then a residual commonly distributed error term variance sigma squared.

50
00:06:16,860 --> 00:06:19,379
So if we want to go ahead,

51
00:06:19,380 --> 00:06:31,350
if we treat this sigma squared as known and then are only very our only parameter in this a known parameter, this model is beta.

52
00:06:32,160 --> 00:06:35,250
So now we want to get posterior inference about beta.

53
00:06:35,460 --> 00:06:36,540
So how can we do that?

54
00:06:37,650 --> 00:06:47,520
Well, we'd start by fitting the posterior distribution, which is always going to be proportional to the likelihood times the prior.

55
00:06:49,110 --> 00:06:52,290
So here we're going to assume essentially a flat prior.

56
00:06:52,290 --> 00:07:01,500
No, no more weight on any one value of beta than the other works in this case because there's a lot of information about beta in the data.

57
00:07:02,790 --> 00:07:09,540
And so it's just for the likelihood part, right, which is just going to be one over.

58
00:07:11,040 --> 00:07:16,560
It's going to be the essentially the the true residuals squared, some of those.

59
00:07:16,680 --> 00:07:24,390
Right. One minus six beta transpose six times one over two sigma squared.

60
00:07:26,310 --> 00:07:36,790
So. To try and get this into a known form.

61
00:07:38,560 --> 00:07:41,890
We're going to play a little trick here of.

62
00:07:45,600 --> 00:07:51,410
But in beta hat's right, our usual estimate, our maximum likelihood estimate of beta happen.

63
00:07:53,310 --> 00:08:08,790
And now I've written out all the details, but you can kind of see here, if we add if we take Y minus x, beta transpose times, Y minus X be the hat.

64
00:08:10,230 --> 00:08:15,300
So this is the observed residuals on the squares.

65
00:08:16,050 --> 00:08:25,830
And add that on to this piece here that we essentially get back to this part here.

66
00:08:26,730 --> 00:08:34,710
And what happens here is essentially this interior piece of x beta hat transpose times, x beta hat.

67
00:08:37,520 --> 00:08:43,850
Ends up. Canceling with this.

68
00:08:45,410 --> 00:08:47,180
This times, this part here.

69
00:08:47,810 --> 00:08:59,299
So if you work out all of the algebra so you have a y transpose y minus two x being ahead x transpose y from this times this I'm sorry,

70
00:08:59,300 --> 00:09:13,010
y z that times that and then the algebra you write it transpose of x to hat is also better hat transpose,

71
00:09:13,010 --> 00:09:19,700
x transpose and x transpose beta and then you work out the same part here.

72
00:09:20,810 --> 00:09:30,860
So replacing beta hat with everything in terms of Y and X gives us this piece here.

73
00:09:32,140 --> 00:09:34,070
You know, there's a bunch of cancelations.

74
00:09:35,270 --> 00:09:49,900
So essentially this you're able to use this x transpose x x transpose x inverse cancels and then I'm left with a minus 2yxx transpose x intersection.

75
00:09:49,940 --> 00:09:56,960
So as y plus two y transpose x x x inverse x equals y.

76
00:09:57,620 --> 00:10:01,070
So these guys cancel so.

77
00:10:12,860 --> 00:10:16,600
Right. So I'm just going to be left with this part here.

78
00:10:18,580 --> 00:10:26,740
You're going to get some cancelation there. I'm just going to the Spartans over here to of course, this is just the expansion of this part here.

79
00:10:27,760 --> 00:10:32,080
So bottom line is, I can rewrite this into this form.

80
00:10:37,810 --> 00:10:50,310
Which. Mean, sorry to here.

81
00:10:53,350 --> 00:10:57,070
Right. So this doesn't involve any unknown parameters.

82
00:10:57,280 --> 00:11:01,820
Right. This is these are all whys and exits, which are data, right.

83
00:11:02,320 --> 00:11:07,930
Which I'm conditioning on. So the only part that involves an unknown parameter here is this piece.

84
00:11:10,840 --> 00:11:22,180
Right. And so this is the essentially the kernel of multivariate normal distribution to have with variance given by this support in the middle here.

85
00:11:22,270 --> 00:11:31,479
Right. So right by general multivariate normal distribution to you the minus one half you transpose

86
00:11:31,480 --> 00:11:43,720
the you where Q is the is going to be sorry say X minus you know the mean variance per here.

87
00:11:45,340 --> 00:11:52,060
So. Right. Digging out your old one and have thought about it a little bit.

88
00:11:53,020 --> 00:12:01,210
But the game, ultimate game here is to get this into a form that we can use to estimate from observed data.

89
00:12:03,080 --> 00:12:08,700
Right. So everything here is in terms of observed data. All right.

90
00:12:08,700 --> 00:12:11,939
This is Peter. You know, freedom.

91
00:12:11,940 --> 00:12:28,470
I sit in squares now and. So if I don't have sigma squared known, it's the next little piece.

92
00:12:31,630 --> 00:12:40,860
So here I am going to consume essentially the same plat player for beta reflect planar sigma squared as is possible.

93
00:12:40,870 --> 00:12:52,179
We have enough data. It's a little bit better form in terms of the four imputed sampling properties.

94
00:12:52,180 --> 00:12:58,840
The second spirit is to put a sort of flat prayer and log of sigma squared,

95
00:12:59,050 --> 00:13:04,930
which is also on the original scale is basically the reciprocal of seven squared.

96
00:13:06,670 --> 00:13:11,920
So so now we have the same thing right there.

97
00:13:12,100 --> 00:13:16,630
This joint posterior just dissipated sigma squared likelihood times this prior.

98
00:13:18,160 --> 00:13:26,500
So now my likelihood because now I have to worry about my sigma squared piece.

99
00:13:26,500 --> 00:13:36,459
Right? That's now a uncertainty. So I have the kernel of the multivariate normal and dropping the two pi part because that's constant.

100
00:13:36,460 --> 00:13:42,760
I don't need to worry about it and I have my prior here is not just one so I didn't

101
00:13:43,090 --> 00:13:48,130
factor that in and I know from my previous work I can rewrite this into this form here.

102
00:13:50,500 --> 00:13:59,079
So, so if we consider this factored posterior right so essentially we're going with game we're going to

103
00:13:59,080 --> 00:14:07,480
play here is we want to get ultimately a draw from this marginal distribution of sigma squared,

104
00:14:08,290 --> 00:14:13,960
plug that in here and get a draw of beta. Right. We already know how to draw beta and sigma squared to nil.

105
00:14:15,400 --> 00:14:21,310
So we just need this marginal part.

106
00:14:22,870 --> 00:14:27,400
So to do that, we can work that out from the joint distribution integrating out over beta.

107
00:14:29,230 --> 00:14:36,340
Right? I've already written this part here into this form, so we're just going to pull that down here.

108
00:14:37,570 --> 00:14:42,440
So we have this right. We have a constant right.

109
00:14:42,550 --> 00:14:47,140
So this part here is a constant. So I'm going to pull that out.

110
00:14:47,380 --> 00:14:51,970
So then we pretend to deal with the integration on the respective betas, this piece here.

111
00:14:53,740 --> 00:15:00,960
So that's that part there. So I'm just going to rewrite this as RSS residuals somewhat squares.

112
00:15:02,170 --> 00:15:04,810
So bringing that down there, that down there.

113
00:15:05,650 --> 00:15:13,719
And then to do this inner go, I'm going to this is a game that's often played when you're trying to get in political solutions in Bayesian situations.

114
00:15:13,720 --> 00:15:18,600
You may have this this looks like a multivariate normal, but only up to a constant.

115
00:15:18,640 --> 00:15:24,940
So you've got to multiply by that constant in the divide, by that constant on the outside, right?

116
00:15:24,940 --> 00:15:33,729
So everything stays balanced. And then I can integrate this thing because it's just a known distributes a known distribution.

117
00:15:33,730 --> 00:15:37,660
So it integrates to what? What?

118
00:15:38,050 --> 00:15:46,960
Yes. Perfect. So I'm missing this sigma squared exponentials x, the negative one, the terms of that two one half.

119
00:15:47,890 --> 00:15:53,170
So negative one half. So I'm going to plug that in and so multiply by the positive one half.

120
00:15:53,170 --> 00:16:03,850
So these things are it's well, all right, so we know this is one because this is a multivariate normal senator beta hat with this variance here.

121
00:16:05,290 --> 00:16:10,440
So it just leaves me with this and right.

122
00:16:10,460 --> 00:16:17,860
Because I'm not going to rewrite the determinant here and write a constant is basically this is going to take a constant outside the determinant.

123
00:16:18,760 --> 00:16:24,880
Then it's basically going to be the rank this is Q by Q Q So you get covariance,

124
00:16:25,570 --> 00:16:33,910
so that becomes sigma squared raised to power a few times one half with of do I can put all that together like that.

125
00:16:35,110 --> 00:16:38,680
So I've just rewritten the residual some of the squares out here in the fourth form.

126
00:16:39,550 --> 00:16:51,850
Okay. So there's everything in terms of data and sigma squared and it turns out that is if you look up your tables of known

127
00:16:51,850 --> 00:16:59,650
distributions in inverse chi square with n minus two degrees of freedom and this little scaling constant x squared.

128
00:17:00,820 --> 00:17:05,050
So that's this part here. So.

129
00:17:09,090 --> 00:17:12,930
So I'm going to divide that back by in Q minus one.

130
00:17:13,650 --> 00:17:24,660
So if you look up the this kind of cancels because this gets multiplied by this in the kernel of the exponential function.

131
00:17:24,990 --> 00:17:29,940
And so so this this is this comes out with this.

132
00:17:31,140 --> 00:17:37,260
So where you can just provide it as the residuals in the squares.

133
00:17:39,570 --> 00:17:45,810
So now we know how to draw sigma squared from this inverse chi square function for distribution.

134
00:17:46,590 --> 00:17:52,470
Once we draw sigma squared, we can draw data from this multivariate normal distribution.

135
00:17:52,770 --> 00:17:57,830
And then we have a drive by identical squared. So it took 682.

136
00:17:57,840 --> 00:17:59,490
You remember all this? You haven't.

137
00:18:00,630 --> 00:18:10,080
You probably see at least parts of this in your sort of introduction is the little base stuff we touch on sort of crumbling inference forces.

138
00:18:10,210 --> 00:18:14,520
So, okay. So that's basically just what I said there.

139
00:18:18,080 --> 00:18:27,260
So. So when we have a simulation, then the marginal distribution of beta can then be determined just by inspection from the drawings.

140
00:18:28,520 --> 00:18:34,280
A little more calculus can actually show that you can do this from this multivariate t distribution so you can avoid doing the draw.

141
00:18:34,280 --> 00:18:39,499
C2 squared, I guess nowadays is probably is sort of the default.

142
00:18:39,500 --> 00:18:42,500
I think the car has this somewhere.

143
00:18:42,500 --> 00:18:43,340
It might be in a package,

144
00:18:43,700 --> 00:18:52,850
but but if you want to get if you want to get the sigma squared aspects themselves and obviously you can forget about that the do the joint cool.

145
00:18:53,900 --> 00:18:56,389
So okay,

146
00:18:56,390 --> 00:19:10,970
so that was a little warp speed but very fast review hopefully for most of you or an introduction to Bayesian inference with linear regression.

147
00:19:14,510 --> 00:19:18,590
Pause for a second. I see a lot of heads nodding, so it's good, I think.

148
00:19:21,880 --> 00:19:29,370
All right. Okay. So that definition in the beginning.

149
00:19:29,590 --> 00:19:41,770
Okay. So so I have these covariates, so I can just do the same the same game that I did before in terms of imputations.

150
00:19:43,600 --> 00:19:48,220
But now I'm going to do it conditional on these axes. So.

151
00:19:49,660 --> 00:19:58,360
Right. So instead of doing a draw just from the overall mean of Y, I'm going to do a draw of beta hat, right?

152
00:19:59,170 --> 00:20:05,790
So that's my beta hat to I'm trying to derive of, of,

153
00:20:05,900 --> 00:20:16,360
of beta one of the regression parameters that govern the association between correlates X and the potential outcome under treatment.

154
00:20:17,360 --> 00:20:21,340
Right, which is multivariate normal centered at beta hat.

155
00:20:22,330 --> 00:20:36,820
Right. If regress my Y one's on x with this again 20 signal one squared here is known with this residual covariance matrix.

156
00:20:37,930 --> 00:20:43,870
And then once I have a draw a beta, I can do a drive sigma squared as inverse.

157
00:20:45,280 --> 00:20:48,580
And this.

158
00:20:51,770 --> 00:20:57,499
Right. So before I showed I didn't use it the proper distribution.

159
00:20:57,500 --> 00:21:00,649
But here I'm actually using this inverse gamma proper distribution.

160
00:21:00,650 --> 00:21:03,830
So small values of A and B are typically used here.

161
00:21:04,520 --> 00:21:08,179
So perhaps somewhat old fashioned.

162
00:21:08,180 --> 00:21:14,270
There are some arguments for moving toward things like have coaches and so forth.

163
00:21:14,270 --> 00:21:19,669
But the nice thing about this is it gives the a so called conjugate result, right?

164
00:21:19,670 --> 00:21:24,410
So the posterior distribution is in the same family as the priors.

165
00:21:24,950 --> 00:21:32,150
You just update the parameter values and in particular you now have an plus eight with two degrees of freedom.

166
00:21:33,020 --> 00:21:40,860
And then this. The residual sum of squares for here.

167
00:21:42,470 --> 00:21:47,720
Plus B so they can play the same game for the control outcome.

168
00:21:49,130 --> 00:21:56,240
And then you can impute, right, the missing values under control.

169
00:21:56,990 --> 00:22:03,200
Right. You're assigned a treatment from their means and parents.

170
00:22:04,250 --> 00:22:11,090
And similarly for the missing values under treatment if you're assigned to control from their predicted values.

171
00:22:11,540 --> 00:22:17,210
Yes. Sorry, this might already cover this, but we are really trying to find.

172
00:22:21,160 --> 00:22:31,330
Right? Yes. Is this the right time to do so?

173
00:22:34,270 --> 00:22:40,180
Right. Excuse me. So they should have a Q in their.

174
00:22:50,690 --> 00:22:58,120
Right before. Yeah.

175
00:22:58,120 --> 00:23:12,230
I think I miss you, too. I should. I should be subtracting off the. Yeah.

176
00:23:12,380 --> 00:23:16,750
I think I might have the wrong values there. Oh.

177
00:23:30,540 --> 00:23:34,050
Yeah, I think. I'm sorry. I think there's a problem. 235.

178
00:23:47,550 --> 00:23:50,910
It should be like n minus Q and then.

179
00:23:54,830 --> 00:23:58,910
This residual sum, of course, bought for the draft suit and squared.

180
00:24:04,490 --> 00:24:07,940
All right. But conceptual, you get the idea. So my question.

181
00:24:07,940 --> 00:24:13,400
Yeah. Is it the same, Andy, that we put on Sigma one squared as a consequence zero squared?

182
00:24:14,990 --> 00:24:20,629
Well, you could put different ones on, but usually using just some fairly small arbitrary value.

183
00:24:20,630 --> 00:24:40,010
So typically the same. Okay.

184
00:24:41,540 --> 00:24:45,979
So you one else has been.

185
00:24:45,980 --> 00:24:55,520
I'm going to move on to. A third set of class students.

186
00:24:59,950 --> 00:25:05,560
To introduce this concept of propensity scores. When I get in this stuff, it's a little.

187
00:25:08,160 --> 00:25:11,560
The broader. Okay. So.

188
00:25:13,470 --> 00:25:21,240
So a lot of the kind of game here that we're going to be we're going to be playing as we move forward,

189
00:25:21,870 --> 00:25:26,579
is have you sort of set this foundation with randomized trials is basically how we can get

190
00:25:26,580 --> 00:25:31,440
something that's not a randomized trial to behave like a randomized trial in terms of influence.

191
00:25:32,610 --> 00:25:37,470
So one of the most fundamental, fundamental and easiest ways to do this is through this concept of fancy scores.

192
00:25:39,930 --> 00:25:45,630
So we're going to go ahead and define it along with this concept of a balancing score.

193
00:25:46,290 --> 00:25:53,580
There's an amazingly simple proof. The propensity score is a balancing score, which basically allows us as long as the underlying assumptions are met.

194
00:25:53,980 --> 00:26:07,830
This is often a big deal. But is is if those assumptions are met to basically obtain average causal effect estimates using propensity score.

195
00:26:09,360 --> 00:26:11,220
And we're to talk about a couple of approaches here.

196
00:26:13,350 --> 00:26:17,820
The next set of notes, which we haven't seen loaded up yet because I haven't finished writing them,

197
00:26:18,330 --> 00:26:22,590
is basically going to be an extension to this idea of matching using propensity scores.

198
00:26:23,100 --> 00:26:27,540
But we'll and we'll get into that a little bit.

199
00:26:28,050 --> 00:26:33,930
And I realize I didn't quite clear this out in order to save a bit of time, I think I've sort of cut that part out.

200
00:26:34,620 --> 00:26:40,410
So we're going to focus on the steady propensity scores in situations of binary

201
00:26:40,620 --> 00:26:49,380
action or the most multi sort of multivariate but not continuous treatments.

202
00:26:54,070 --> 00:26:58,930
Right. So. Okay.

203
00:27:01,150 --> 00:27:07,180
So yeah, as I've going through the class this year, I'm trying to sort of trim things down so we can kind of get a little further into

204
00:27:07,180 --> 00:27:14,410
some stuff that I didn't have time to get to the first time I taught this class. So there may be an occasional you might see some of the seams slip.

205
00:27:16,210 --> 00:27:19,870
So. Okay. Observational studies.

206
00:27:20,360 --> 00:27:29,560
Right. So this idea of randomized designs either in assignment clinical trials or in sampling for probability surveys was really a big leap,

207
00:27:29,680 --> 00:27:35,650
I think in scientific just in changing the science in general, particularly in medicine and public health.

208
00:27:36,940 --> 00:27:44,740
So as we've seen, randomization allows causal inference to be made sense of potential confounders or independent of treatment assignment.

209
00:27:45,010 --> 00:27:50,020
Right. And that's just the nature of the design.

210
00:27:50,080 --> 00:27:53,200
So you kind of bake that in to the way you're collecting the data.

211
00:27:54,550 --> 00:28:00,930
But, you know, there are limits to what we can accomplish in randomized trials. So there are some ethical limitations.

212
00:28:00,940 --> 00:28:07,780
And particularly if you have a team that you think is harmful, then, you know, like smoking being a classic,

213
00:28:07,780 --> 00:28:16,180
one's essentially impossible to run randomized trials, smoking in human beings because it's believed quite correctly to be detrimental.

214
00:28:17,500 --> 00:28:24,370
But, you know, there were some controversy about the but but in the end, there was just it was considered unethical to do it.

215
00:28:25,240 --> 00:28:30,310
There can be issues of compliance. So let's take a look at that a little bit coming down the road.

216
00:28:31,510 --> 00:28:36,540
So side effects or just the difficulty taking the treatment might mean that, well,

217
00:28:36,550 --> 00:28:42,910
you start out with random randomized in terms of assignment that the actual

218
00:28:42,910 --> 00:28:47,440
people taking the treatment may no longer be randomized because who takes that?

219
00:28:47,440 --> 00:28:59,049
Who's able to take the treatment may not be completely random with respect to the whole population, conceivably in reverse.

220
00:28:59,050 --> 00:29:03,680
I mean, there are situations where people can maybe access the treatment, even with the right control and so forth.

221
00:29:03,680 --> 00:29:08,559
So okay. So and then finally, just practical limitations.

222
00:29:08,560 --> 00:29:14,620
It's awfully expensive to do this, you know, recruiting people when you're going to tell them, well, you might get the treatment or you might not.

223
00:29:15,760 --> 00:29:26,950
That isn't always a big recruitment, positive recruitment material. So that's those are all things that don't sort of stop you sort randomized trials.

224
00:29:26,950 --> 00:29:33,280
So we're going to use this idea propensity score to move toward causal inference using data collected under non-randomized designs.

225
00:29:34,990 --> 00:29:38,200
Okay. So what is going to be score?

226
00:29:39,340 --> 00:29:48,999
Well, basically, it's a way of thinking about the assignment in the most the broadest possible case here, like how we assign a treatment.

227
00:29:49,000 --> 00:29:56,080
A could depend on how we assign treatments to other people or sample the population,

228
00:29:56,830 --> 00:30:00,700
could depend on covariance, could depend on our potential outcomes.

229
00:30:01,810 --> 00:30:10,840
So the pre treatment coverage so so now so there are several assumptions we need to make here to make progress.

230
00:30:11,350 --> 00:30:15,400
But I want to use as a side for mechanism to make causal inference.

231
00:30:16,030 --> 00:30:23,290
Some are pretty weak and others are stronger. They may be thought as approximations to improve the party's inference rather than perfect it.

232
00:30:24,820 --> 00:30:31,420
So we kind of go through those and discuss them collectively.

233
00:30:31,420 --> 00:30:36,100
These are sort of the ideas of a regular assignment mechanism which allows this construction of propensity scores.

234
00:30:38,140 --> 00:30:45,250
Okay. So the first one we've we've already seen this is the stable unit treatment value idea.

235
00:30:47,230 --> 00:30:57,040
So it basically means however you assign to other people, it doesn't matter how I'm going to sign you.

236
00:30:57,970 --> 00:31:10,030
So basically which basically drops off that first part there and then an extension of suit for which I think given to Rubin

237
00:31:10,030 --> 00:31:17,050
referred to as individualistic assumption basically means the assignment is independent of the values for the other individuals.

238
00:31:17,980 --> 00:31:20,970
So these two together kind of blend together for.

239
00:31:21,070 --> 00:31:30,850
So basically what everyone is saying, you can only depend on data relevant to you or all dependent pieces.

240
00:31:32,020 --> 00:31:35,469
So this may not be always reasonable assumptions,

241
00:31:35,470 --> 00:31:45,880
but at least are ones that are probably easily testable and no knowable because of the way assignment positivity

242
00:31:47,050 --> 00:31:53,940
basically means all units have a non-zero chance of being assigned to all possible treatments or not.

243
00:31:53,980 --> 00:31:59,080
So simple case of a treatment control. Everybody has some chance of being under control.

244
00:31:59,310 --> 00:32:04,140
But he has some chance of being on the treatment again given to Rubin's calls, individualistic.

245
00:32:07,770 --> 00:32:10,220
And so this gets to be a little hairier, right?

246
00:32:10,230 --> 00:32:18,780
I mean, so it could be that that some individuals really can't access the treatment or can't access the control based on the nature of of,

247
00:32:19,170 --> 00:32:24,360
you know, things that are, you know, things that are sort of specific to them.

248
00:32:26,940 --> 00:32:34,770
But is is is ultimately going to be typically testable because these are things that we can see,

249
00:32:35,880 --> 00:32:41,700
at least with respect to ex, maybe not so much with respect to both of these,

250
00:32:42,360 --> 00:32:49,509
but we can start to to move in that direction, particularly with one confounder in this, that we can certainly test positivity and confounder.

251
00:32:49,510 --> 00:32:55,710
This means that the assignment is independent potential outcomes conditional on X.

252
00:32:56,580 --> 00:33:01,770
So this is a really big one. It's also one that's really hard to test impossible settings.

253
00:33:02,730 --> 00:33:04,710
But but it is critical. Right.

254
00:33:04,720 --> 00:33:15,000
So basically says everything about the assignments based on on covariates and has nothing to do with the actual content.

255
00:33:16,170 --> 00:33:20,040
Right. So it doesn't mean it's necessarily randomized 5050.

256
00:33:20,230 --> 00:33:28,049
Right. Could be that, you know, given covariates, it it it could be more even given coverage,

257
00:33:28,050 --> 00:33:36,090
maybe more people in a certain type of treatment or certain type of control. But that it doesn't the point doesn't depend on the potential outcomes.

258
00:33:36,960 --> 00:33:45,180
So a little example of this, right? So it might be if you're in a setting, particularly where, of course, we just went through COVID.

259
00:33:45,180 --> 00:33:49,950
So while people were running trials there also a lot of places, especially early on,

260
00:33:49,950 --> 00:33:52,380
that were just simply trying to throw whatever they could at somebody.

261
00:33:53,220 --> 00:33:59,340
So it might be that sicker patients may be more likely to have gotten a certain type of treatment.

262
00:33:59,610 --> 00:34:08,040
Right. So if you just sort of looked blindly like who got, you know, that the outcome of the treatment and the outcome under control,

263
00:34:08,670 --> 00:34:11,370
it might look like, oh, this treatment didn't work very well,

264
00:34:12,480 --> 00:34:21,660
but if you account so if X is some sort of measure of sort of baseline illness at the time that the treatment was given,

265
00:34:22,410 --> 00:34:31,800
then you might think that sort of within sort of bands of ill degrees of illness that whether you got treat or not, might be essentially random.

266
00:34:32,850 --> 00:34:40,350
So based on factors that are sort of outside of the individual would have nothing to do with their potential outcome.

267
00:34:41,280 --> 00:34:49,980
So if you just we'll see in a second. Bill Prince, of course, based on that baseline sickness, you would then be able to move forward.

268
00:34:50,310 --> 00:34:55,560
You're going to get better results. Yes. Yeah. So basically, I would call you where we're beginning off.

269
00:34:56,840 --> 00:35:00,840
We talk about the definition of randomized, right? So basically.

270
00:35:00,860 --> 00:35:04,440
Q The definition of randomization already in place.

271
00:35:05,850 --> 00:35:13,739
Exactly. Very good. So it's a matter of fact, because this is independent of why index typically.

272
00:35:13,740 --> 00:35:16,950
And then then it's going to be on compounded.

273
00:35:16,950 --> 00:35:22,980
This is a little weaker assumption. So this is conditional in ex conditional on baseline things I've observed.

274
00:35:24,480 --> 00:35:29,559
It's it's randomized it. Yes.

275
00:35:29,560 --> 00:35:55,570
Good point. So under these four assumptions, we essentially are distribution of this treatment assignment.

276
00:35:56,110 --> 00:36:07,530
It's just going to be a Bernoulli variable with probability of treatment assignment, additional XY given by this key of Excite.

277
00:36:07,900 --> 00:36:15,940
Right. So this is just the really distribution, you know, y28 times one minus I'm sorry,

278
00:36:18,280 --> 00:36:22,240
theta day to one minus data to one minus eight for theta here is this probability of

279
00:36:22,470 --> 00:36:27,390
to one or the quality of x side because that's just a sort of standard notation,

280
00:36:27,410 --> 00:36:33,790
this idea of the propensity score. So, right,

281
00:36:33,790 --> 00:36:43,659
so this positivity unconfirmed also implied that this propensity score has to be greater than zero and less than one within that everybody

282
00:36:43,660 --> 00:36:57,970
in the population and that treatment is independent right to the number of Y here given side independent of potential outcomes given.

283
00:37:03,910 --> 00:37:07,130
Okay. So that's the definition.

284
00:37:09,690 --> 00:37:18,459
At this point. Okay.

285
00:37:18,460 --> 00:37:23,080
So a little bit about I sort of hinted at this before about testing positive union contracts.

286
00:37:24,100 --> 00:37:28,120
So so positivity,

287
00:37:29,500 --> 00:37:36,520
implicitly assuming competitiveness can be tested within the available data by modeling the probability selection as a function of covariance.

288
00:37:37,780 --> 00:37:49,170
So if we have substantial the numbers of subjects where their probability of being assigned to treatment is essentially zero in this,

289
00:37:49,180 --> 00:37:53,170
or probability of assigning controls, essentially one will probably be assigned to treatment.

290
00:37:53,210 --> 00:38:04,120
Essentially one or those are probably the same controls zero. That suggests problems of positivity rate in a particular right if we sort of look over.

291
00:38:04,870 --> 00:38:09,000
So this this notation is a little nasty, but conceptually the idea is pretty straightforward.

292
00:38:09,010 --> 00:38:13,690
So we can compute our probabilities being assigned to treatment.

293
00:38:14,800 --> 00:38:17,800
Then we look among all the individuals that were actually assigned to treatment.

294
00:38:18,490 --> 00:38:22,990
We find that smallest value. That's what this this represents.

295
00:38:23,830 --> 00:38:39,970
And if that value is still larger than some of the values for being on treatment for some of the control individuals, then that suggests a problem.

296
00:38:40,750 --> 00:38:49,210
And conversely, if you sort of look at the maximum probability to being assigned to treatment among those who have control,

297
00:38:49,840 --> 00:38:56,320
and there are people in a treatment that have treatment assignment probabilities that are higher than that, that suggest potential issues.

298
00:38:57,070 --> 00:39:00,580
So, I mean, that kind of makes sense, right?

299
00:39:00,580 --> 00:39:03,370
You've got presumably the people that are actually assigned to treatment are

300
00:39:03,370 --> 00:39:06,610
tend to going to have fairly high probabilities of being assigned to treatment.

301
00:39:07,570 --> 00:39:13,000
So if we look at among those under control, we might see some people that have very low probabilities.

302
00:39:13,960 --> 00:39:20,770
And conversely, if we look at the people being assigned to the treatment that are under control,

303
00:39:20,770 --> 00:39:23,980
we'd assume that they would have very lower probabilities on average.

304
00:39:24,460 --> 00:39:31,240
And if there are some individuals in treatment have high probabilities, then this is sometimes referred to lack of overlap.

305
00:39:32,470 --> 00:39:38,680
You basically don't have elements in the data on the opposite treatment arm have similar probability assignment.

306
00:39:39,340 --> 00:39:46,630
So to look at that simple example, maybe for looking at, you know, sort of baseline sickness as being treatment assignment,

307
00:39:47,290 --> 00:39:56,050
it might be that patients who were at a certain level of illness, how we're measuring that, none of them got all of them got treatment.

308
00:39:56,110 --> 00:39:58,810
None of them were or were on the control group.

309
00:39:59,530 --> 00:40:05,200
And conversely, there may be some patients that were at the sort of least sick and couldn't get treatment at all.

310
00:40:05,440 --> 00:40:06,490
They were all controls.

311
00:40:07,260 --> 00:40:16,720
So so that's problematic because it means the sort of space that we can make inference in with respect to the coverage is more limited.

312
00:40:18,910 --> 00:40:21,969
We're going to dig into this a little bit further down the road.

313
00:40:21,970 --> 00:40:27,100
But but in terms of dealing with it, you can sort of ignore it if it's small.

314
00:40:28,360 --> 00:40:33,580
Otherwise, you can drop cases and restrict inference to a segment of the population that meets the positivity assumption.

315
00:40:34,360 --> 00:40:39,340
Or I didn't write it on the thing here. Or you can live with it even if it's large.

316
00:40:39,910 --> 00:40:43,210
And essentially what you're doing is making extrapolations.

317
00:40:43,780 --> 00:40:48,280
No, no data behind it. Out to these these parts.

318
00:40:48,730 --> 00:40:52,120
You can think of this acts as maybe some sort of high dimensional space.

319
00:40:52,570 --> 00:40:59,350
And there are sort of pieces of this where people don't get dropped in to both treatment and control.

320
00:40:59,350 --> 00:41:05,889
And so it's hard to say what's going to happen under those circumstances without essentially looking at maybe closer,

321
00:41:05,890 --> 00:41:11,560
you know, the closer to values of X and essentially moving back out into your model.

322
00:41:13,270 --> 00:41:20,650
So importantly, it's it's not testable.

323
00:41:24,220 --> 00:41:27,129
So it implies.

324
00:41:27,130 --> 00:41:38,350
Right, that basically the distribution of the control cases, given X II is the same, whether you were assigned to treatment or control and vice versa.

325
00:41:39,430 --> 00:41:46,840
Right. The distribution of your treated cases, whether you were assigned to treatment or control given X is the same.

326
00:41:48,130 --> 00:41:55,690
So basically it means implied behind that is that you can essentially impute, if you will.

327
00:41:55,750 --> 00:42:08,770
Right. You can look at the individuals with a given X and fill in their control values or fill in their treatment values.

328
00:42:09,400 --> 00:42:13,810
Now, that's impossible to test as this fundamental problem causal inference you.

329
00:42:15,430 --> 00:42:20,080
So. So you didn't have to assume it or possibly created by design.

330
00:42:21,040 --> 00:42:27,219
So it might be that within, you know, we have some sort of situation where maybe you just want to do a simple randomization,

331
00:42:27,220 --> 00:42:31,630
but maybe you can get away with randomizing within strata of X.

332
00:42:32,290 --> 00:42:36,670
Right. So you're if you're a researcher, you're the physician at the hospital.

333
00:42:37,570 --> 00:42:43,570
So it might be that, again, ethical issues could be could be a point here, but, you know,

334
00:42:43,570 --> 00:42:50,559
you could could try to sort of of over underweight assignment, especially if you're really uncertain about whether treatment's going to be helpful.

335
00:42:50,560 --> 00:43:00,160
Harmful, right. There's a good place for that. And then two of the things are kind of related to sort of natural experiment.

336
00:43:01,900 --> 00:43:07,480
And it there's really this concept of an instrumental variable which will then kind of get more detail down the road.

337
00:43:07,990 --> 00:43:11,800
So this is basically a variable that's related to assignment but independent of the outcome.

338
00:43:12,850 --> 00:43:24,969
So a classic example of this might be well, the classic example this dates way back now, but to when I was a kid, there was a war in this country,

339
00:43:24,970 --> 00:43:33,670
the Vietnam War, and there was a draft going on, but they didn't draft everybody their own then and then in those days.

340
00:43:34,360 --> 00:43:41,860
And what they did do was they ordered your birth date between one and 365 based on the day of the year.

341
00:43:42,760 --> 00:43:49,240
And then they randomly ordered those numbers and they would select down.

342
00:43:50,320 --> 00:43:57,670
So if July 17th was whatever that is, when 200 or something was the very first day,

343
00:43:58,210 --> 00:44:02,800
then you would actually think certain to be drafted and they sort of work their way on down to

344
00:44:02,800 --> 00:44:07,540
they had enough men drafted that that was what they needed for their for their for the military.

345
00:44:08,320 --> 00:44:13,389
So that that became a kind of a classic instrumental variable.

346
00:44:13,390 --> 00:44:20,620
Right. So presumably there was very little about your birthday that was going to be associated with your outcome of being exposed to going to the war.

347
00:44:21,310 --> 00:44:23,050
But it had a lot to do with it.

348
00:44:23,080 --> 00:44:31,570
You actually went so these these kinds of that's a and then also to be viewed as a sort of natural unnatural experiment in this case.

349
00:44:32,200 --> 00:44:38,259
But but these are all in that's a that's a that's conceptually the idea here.

350
00:44:38,260 --> 00:44:42,790
And you can see there's there's a there's a big gain, especially in economics,

351
00:44:42,790 --> 00:44:49,420
but I think increasingly in sort of bio sciences, his idea, the instrumental variables.

352
00:44:51,700 --> 00:44:59,140
Otherwise if you've just got some observed data, then making this assumption,

353
00:44:59,350 --> 00:45:02,380
I mean, the underlying science behind things will presumably play a role.

354
00:45:03,190 --> 00:45:15,340
And I guess just to go right back to this point here, maybe you're not going to get perfect, but you're going to do better than if you didn't.

355
00:45:19,250 --> 00:45:20,530
And in particular, my condition.

356
00:45:20,720 --> 00:45:26,390
You're going to do better than if you just completely ignore it and just treat the assignment as completely minimized.

357
00:45:27,900 --> 00:45:31,830
Okay. So. All right.

358
00:45:33,290 --> 00:45:38,270
So we haven't quite put all the pieces together here yet. So look for the next few slides to do that.

359
00:45:38,750 --> 00:45:47,380
So how do we use this thing to estimate causal effects? So well, we could just use x ray.

360
00:45:48,730 --> 00:45:59,709
I mean, what's the point of the propensity score? And indeed, if X is really small dimensional, that might be a perfectly reasonable way to go.

361
00:45:59,710 --> 00:46:05,710
Right? So you basically look at the estimates of the causal effects within each level of X,

362
00:46:06,790 --> 00:46:16,450
and then you average those effects with respect to the distribution of X to get an unbiased estimate for the average causal effects.

363
00:46:19,090 --> 00:46:25,750
So that's impractical many settings because of the absence of randomization, we kind of want to use a large set of covariates.

364
00:46:26,320 --> 00:46:32,620
So hopefully we have something more than 100 number two gender and age, a couple of age categories or something.

365
00:46:33,400 --> 00:46:40,900
So to, to, to use as covariates and so propensity scores here, basically, that's where they start to play the role.

366
00:46:44,450 --> 00:46:50,630
So so this background of this, we want to have this idea of a balancing score,

367
00:46:51,680 --> 00:47:02,450
and that's basically any scale or function of covariates so that A.I. is going to be independent of X side given this balancing score, right?

368
00:47:02,450 --> 00:47:10,340
This function that takes this high dimensional X, primes it into a little machine and you come back with holes, it'll be an X scalar value.

369
00:47:11,570 --> 00:47:17,750
And it basically summarizes all the information about the association between the treatment and the baseline covariates.

370
00:47:18,950 --> 00:47:22,520
So it might seem that it's impossible to come up with something like that,

371
00:47:23,450 --> 00:47:28,280
but it turns out the propensity score is a balancing score and it meets regular assignment mechanisms.

372
00:47:29,630 --> 00:47:43,910
Right? So essentially that is independent of X given E of x where the probability of I given x in the of x is just the probability of a equal,

373
00:47:44,240 --> 00:47:51,670
the probability of a i given years. This is amazingly a two line proof.

374
00:47:55,470 --> 00:48:06,810
So the first part simply points out that if you condition on both x and he is x because he of x is a function

375
00:48:06,810 --> 00:48:18,600
of x sign that the probability of equal to one is just the same thing as you get conditional and x,

376
00:48:18,600 --> 00:48:22,559
which we have from our definition of that. In the second part.

377
00:48:22,560 --> 00:48:26,730
And here's a there's a typo, a corrected R on the slides.

378
00:48:26,730 --> 00:48:33,320
But on your version, unless you've downloaded like 15 minutes before class, you won't see a corrected.

379
00:48:33,330 --> 00:48:37,710
So this your version probably says a an x given he should say exit naked.

380
00:48:39,450 --> 00:48:47,010
So. So.

381
00:48:51,680 --> 00:49:01,570
Basically. So this probability of I given of X or the expected value of eight I right from the definition of the binary variable.

382
00:49:02,530 --> 00:49:07,300
So now I'm going to write this out as the sort of smoothing function here of

383
00:49:08,290 --> 00:49:12,840
conditional expectation of any given acts and then take the expectation of X,

384
00:49:12,850 --> 00:49:19,570
right. So I already worked out write this piece in one.

385
00:49:22,930 --> 00:49:30,310
So that's just the events. But of course, the expectation of a value conditional on itself is just the value itself.

386
00:49:31,330 --> 00:49:37,740
So. So I've equated these two equations here.

387
00:49:38,640 --> 00:49:45,230
And so I essentially have proof theory of X as a balancing equation about your score against.

388
00:49:46,480 --> 00:49:58,830
She did get extra conditions in the upper double feature of being an extra parentheses extra condition.

389
00:49:59,680 --> 00:50:04,480
Oh. No, I don't think so.

390
00:50:05,410 --> 00:50:09,280
Right here. This is the marginalizing over.

391
00:50:09,730 --> 00:50:19,410
I think I'm missing the parentheses, though. Right. So I don't want to hear on conditioning on X.

392
00:50:20,460 --> 00:50:24,780
I don't want a condition on X at this peace or expectation for us.

393
00:50:28,390 --> 00:50:31,990
Right. So we have. Why is.

394
00:50:33,250 --> 00:50:37,959
He of x. He of y.

395
00:50:37,960 --> 00:50:41,690
Given x. Yes.

396
00:50:42,000 --> 00:50:52,070
Right. So I guess my question is, what way is a culture conditioned on to have faith and in the definition of faith?

397
00:50:56,510 --> 00:51:04,120
Oh. Because I'm just fixing this quantity.

398
00:51:04,120 --> 00:51:10,570
This, this, this. This is just a fixed quantity I condition on the whole way through.

399
00:51:11,560 --> 00:51:17,440
So I'm treating X and I've excise separate things here. So you can think of this as.

400
00:51:19,750 --> 00:51:26,620
I think the is a Z and I'm conditioning on an X, right.

401
00:51:26,640 --> 00:51:35,310
The goal here is to show that once I conditioned on this piece here, I don't have any extra information in the exercise.

402
00:51:36,900 --> 00:51:41,820
So even though I know the X is there, it's kind of it's kind of been just treated as a.

403
00:51:45,350 --> 00:51:53,060
It has its own quantity. Mutation.

404
00:52:11,920 --> 00:52:16,299
Would you be able to ride out the actual expansion from the step on the right to the next one?

405
00:52:16,300 --> 00:52:22,540
Because I still find it a little iffy. Or I can ask.

406
00:52:22,600 --> 00:52:26,170
Yeah, that's fine. I'm sorry. Let me just try to make a note of my. Here, here.

407
00:52:31,710 --> 00:52:35,120
Okay. So I'm sorry. So you're fine.

408
00:52:35,130 --> 00:52:38,580
So which part? It's confusing. From the one to the most.

409
00:52:38,580 --> 00:52:42,240
To the right. This step to the next one. The step to this.

410
00:52:42,270 --> 00:52:47,640
Yeah. Okay. So I said you get experiences here, but essentially this part,

411
00:52:49,020 --> 00:52:58,390
we've already computed that up here because the expected value of A's, the probability area was one given it's right.

412
00:52:59,570 --> 00:53:10,050
Oh. So this entire thing, it's just. Yeah. Experience, but it's also conditioned on X and not you can't just substitute ye of excite in it because.

413
00:53:11,820 --> 00:53:19,500
Like I thought the idea of a given x like the entire thing within the outer parentheses is going to be your backside.

414
00:53:20,910 --> 00:53:25,230
So I was a little confused about why there's this kind of condition where this part comes in.

415
00:53:25,420 --> 00:53:38,040
Yeah, right. So. Well, it's sort of where that that's actually where that stuff is happening.

416
00:53:38,760 --> 00:53:46,200
I might maybe in retrospect. All right. This is something else, because we're just sort of conditioned on its value.

417
00:53:46,200 --> 00:53:52,300
We're not thinking about it as a function. We're not thinking about taking expectations back to the exercise in it.

418
00:53:52,320 --> 00:54:00,990
It's just something we conditioned on, because the goal here is to show in the end that when you conditioned on this piece that this.

419
00:54:01,050 --> 00:54:06,720
This is irrelevant. This is irrelevant. Yeah. So keeping that that conditional part in there.

420
00:54:09,720 --> 00:54:14,950
So I think maybe the problem actually, I think the parentheses go here. I think maybe that's the issue.

421
00:54:14,970 --> 00:54:18,330
I think that would be okay. Yeah. Yeah. Everything's confused.

422
00:54:18,330 --> 00:54:27,060
Why? You get rid of it? Yeah. Yeah. So I think the extra parentheses I went to go to.

423
00:54:31,350 --> 00:54:41,190
Right, because I'm condition at the holy fruit. But now I want to rewrite eBay, give Exide light condition.

424
00:54:42,060 --> 00:54:51,750
So I'm expanding this out here because there's no air conditioning on.

425
00:54:55,700 --> 00:55:00,670
There's a few marginal exploitation of the IE over overheads.

426
00:55:01,130 --> 00:55:09,020
Now I want a conditional annex of this part here. Yes, that comma should be a parentheses, Mr. Parentheses.

427
00:55:12,570 --> 00:55:18,200
So then the expectation is that you do that and of course conditions with.

428
00:55:19,740 --> 00:55:39,290
So. So for Yellowstone, a little further reconditioning your backside.

429
00:55:39,330 --> 00:55:41,780
So we're just going to treat that as a fixed quantity.

430
00:55:42,470 --> 00:55:50,420
Or we can still think about the general distribution of EXI and how things vary with respect to that, keeping that thing fixed.

431
00:55:51,170 --> 00:55:56,300
So it turns out that an expectation, right?

432
00:55:59,510 --> 00:56:05,180
Once we condition is side, then that's all we need, right.

433
00:56:05,510 --> 00:56:08,870
Is expected value. They are getting excited. He is excited. So.

434
00:56:15,060 --> 00:56:19,110
Once I have that, and I already know that.

435
00:56:21,690 --> 00:56:28,930
Conditional on both exciting and exciting. That this is equal to you get.

436
00:56:30,080 --> 00:56:34,059
Right. So here here a conditional next slide here.

437
00:56:34,060 --> 00:56:38,500
I didn't and I got to the same point. Either way, you know what you're saying.

438
00:56:40,180 --> 00:56:49,360
So whether I conditional next or not is probably like equal to one is always propensity score.

439
00:56:50,710 --> 00:56:55,510
And so conditional next side gives me nothing if I know what you've x.

440
00:56:55,750 --> 00:57:01,500
So that makes the definition of a relative score. Okay.

441
00:57:01,890 --> 00:57:06,420
Sorry. So there's a typo that's confusing and shouldn't be.

442
00:57:08,150 --> 00:57:15,280
Should be apprentices. So.

443
00:57:26,880 --> 00:57:30,450
So I have uncomfortableness rights of my distribution to the I.

444
00:57:34,730 --> 00:57:38,070
Given access doesn't depend on my initial outcomes.

445
00:57:39,080 --> 00:57:46,040
Then if I condition on this balancing score, by definition it depends on the balancing score.

446
00:57:46,050 --> 00:57:51,900
And since this is a balancing score, then under unpreparedness,

447
00:57:53,100 --> 00:57:59,669
this distribution of ages just depends on this idea that say this is super critical because this is a scalar, right?

448
00:57:59,670 --> 00:58:09,180
I've gone from maybe a thousand dimensional X down to a single one dimensional scalar, so that allows me to get a lot done.

449
00:58:10,260 --> 00:58:13,770
It's also the coarsest balancing score. So it's,

450
00:58:13,770 --> 00:58:19,140
it's sort of the you there's there's no there's no point in getting something that's

451
00:58:21,660 --> 00:58:29,400
has more support points than your backside to because it doesn't give you anything.

452
00:58:29,520 --> 00:58:35,160
So the sort of proof on that, it's also not too tricky to go into it here.

453
00:58:42,770 --> 00:58:45,770
So why do we care about.

454
00:58:51,760 --> 00:58:53,710
My propensity scores. All right.

455
00:58:54,460 --> 00:59:01,600
If we haven't confirmed this, it would seem that we could just revive the invitation approach, this sort of randomized experiment thing we did before.

456
00:59:02,980 --> 00:59:06,130
So we could just go through. Right.

457
00:59:07,390 --> 00:59:10,540
Or target for population.

458
00:59:11,780 --> 00:59:16,599
Effect. Right. It's just going to be. Or estimated means under treatment.

459
00:59:16,600 --> 00:59:19,659
Minus or estimated means or control. Right.

460
00:59:19,660 --> 00:59:25,870
So it has to be a means for treatment. Add up all the means and treatment and then there are predicted values under control.

461
00:59:26,680 --> 00:59:32,890
And subtract those from the observed values and or control and the predicted values under control from those in treatment.

462
00:59:33,930 --> 00:59:38,410
So all right. So I kind of bring this together a little bit here.

463
00:59:42,250 --> 00:59:56,050
So the parts involve this difference between the observed values and the predicted values under control treated in this reverse group here.

464
00:59:58,780 --> 01:00:09,580
So. Basically this becomes the difference between the treated and predicted control values,

465
01:00:10,360 --> 01:00:22,090
the the mean and the predicted mean control values, and then the predicted mean treatment values minus the observed control values.

466
01:00:23,620 --> 01:00:27,660
So so that all makes sense conceptually, right?

467
01:00:27,670 --> 01:00:34,149
This is kind of an estimate of the treatment effect supposed to get treated and the treatment effect

468
01:00:34,150 --> 01:00:42,190
once the folks who didn't get those up and then wait them by the fraction of treatment control.

469
01:00:46,040 --> 01:00:51,800
So I guess just to show to get from this line down to this line,

470
01:00:54,140 --> 01:01:05,030
we have the fact that the mean values under the treatment correspond to their predicted values in the control.

471
01:01:09,300 --> 01:01:12,840
Right. From the fact that the residuals have to do, some of them have to be zero.

472
01:01:13,980 --> 01:01:18,900
Right. And so, again. Right, the B of the residuals is zero.

473
01:01:19,590 --> 01:01:27,659
And similarly, the mean the residuals for the prediction for the treatment groups are between

474
01:01:27,660 --> 01:01:31,260
the predicted values of your treatment and the actual ads and treatments. Also zero.

475
01:01:32,070 --> 01:01:36,690
So. So basically we can rewrite.

476
01:01:39,440 --> 01:01:45,549
The. This part here.

477
01:01:45,550 --> 01:01:48,970
This means that the white ones transpose.

478
01:01:52,020 --> 01:02:02,100
Better known here as the observed differences in control, minus this adjustment here for the exes.

479
01:02:03,810 --> 01:02:10,080
And then similarly for this part here is the observed differences.

480
01:02:11,860 --> 01:02:16,000
Under with an adjustment for the axes.

481
01:02:17,980 --> 01:02:36,700
And so if we plug this in your previous result, you basically get two and one over in this part here and not over in this part here.

482
01:02:36,760 --> 01:02:40,870
I'm replacing this with this.

483
01:02:42,860 --> 01:02:47,380
And this part. This.

484
01:02:50,120 --> 01:02:53,570
So. And in fact, you're not my.

485
01:02:57,320 --> 01:03:07,630
So. So I'm going to factor out my y one bar minus y zero bar.

486
01:03:08,590 --> 01:03:13,390
I get in one plus not around that, which is just an array of one.

487
01:03:15,520 --> 01:03:19,900
And this is essentially a a constant.

488
01:03:20,980 --> 01:03:31,059
But I can factor out and multiply it more around who's better half not and then not has been have one so right so this

489
01:03:31,060 --> 01:03:41,170
is basically the derivation here to try and show that under this sort of model based approach and this is what you get.

490
01:03:41,170 --> 01:03:50,680
But I'm not going to spend a ton of time on this because that's what it relies heavily on having the correct regression model for beta.

491
01:03:52,900 --> 01:04:00,320
Right. So if I don't have this beta right, this adjustment for the imbalance in the covariance is going to be wrong.

492
01:04:01,610 --> 01:04:05,600
And the randomized trial setting women aren't so worried about that.

493
01:04:21,330 --> 01:04:36,220
I've already sort of been hinting at that answer by talking about imbalance here. So if I take the expectation of this quantity, right.

494
01:04:36,300 --> 01:04:43,470
So I'll get the expected value of one bar when I bar and then the expected value of this part here.

495
01:04:43,650 --> 01:04:48,300
Right. So what's that expectation to be in randomized trials?

496
01:04:52,600 --> 01:04:55,900
So these are the X's assigned to treatment and these are the experts assigned to control.

497
01:05:00,970 --> 01:05:05,720
Five. Large samples.

498
01:05:06,680 --> 01:05:17,740
This is do tend toward what? Yes.

499
01:05:18,310 --> 01:05:29,440
It's a zero. Right. So essentially the distribution of these Xs should be this identical would be identical in two samples.

500
01:05:29,440 --> 01:05:34,000
But but in the expectation, these two things are going to be equal to each other.

501
01:05:35,500 --> 01:05:40,120
Right. So because whether I'm assigned a treatment assigned to control completely random.

502
01:05:41,230 --> 01:05:46,690
Right. So the distribution of my needs and my access to treatment on these maps is under control.

503
01:05:46,750 --> 01:05:51,790
I'm going to get going to be small and I'm going to get smaller laws you might see up against on average.

504
01:05:52,870 --> 01:05:57,520
So this whole thing goes away. I don't care then about how well I model beta.

505
01:05:59,140 --> 01:06:04,150
Right. Because I am assured with with randomization that I'll be okay.

506
01:06:04,330 --> 01:06:12,960
Now I sort of care because obviously in practical settings some people in smaller samples are really lousy adjustment here.

507
01:06:12,970 --> 01:06:17,530
Could, could. If I have a really bad model it could even into worse off than when I started.

508
01:06:17,890 --> 01:06:25,150
It certainly may not be getting a great deal of correction, but but if I'm in a non-randomized setting,

509
01:06:26,590 --> 01:06:33,670
particularly where there's a big difference there between these Xs, that the modeling of this becomes really critical.

510
01:06:33,940 --> 01:06:42,350
Is that right? So if I have if this is even off a modest amount, then it could blow up.

511
01:06:42,370 --> 01:06:46,200
If these differences are really big. And then I have.

512
01:06:48,500 --> 01:06:56,830
So yeah, so that's what the propensity score is going to allow us to get a bit around.

513
01:06:56,860 --> 01:07:04,459
And so we're going to move away from imputation, although hopefully we have time before it into the course.

514
01:07:04,460 --> 01:07:14,780
We're going to come back to it because imputing based on the propensity score is a different matter that that would have allowed you to get

515
01:07:14,780 --> 01:07:25,730
away from these biases in particular is if you use both the propensity score and the model for Y given X companies idea of double robustness.

516
01:07:26,570 --> 01:07:29,960
So essentially one corrects the other.

517
01:07:29,990 --> 01:07:37,250
If one of them is right, you're always going to be okay. So but that's quite a bit further down the road.

518
01:07:38,610 --> 01:07:46,939
All right. So just try and finish up for today here. We're going to we're not going to finish this whole section today,

519
01:07:46,940 --> 01:07:56,750
but we'll just get through the a little bit of the mechanics and theory behind the idea of using weights with propensity scores.

520
01:07:57,740 --> 01:08:01,730
So there's a couple of approaches.

521
01:08:02,420 --> 01:08:05,120
One is to use the propensity score is directly to create weights.

522
01:08:06,290 --> 01:08:12,620
So you can think of these weights as basically multiplying your observed values by the reciprocal

523
01:08:13,010 --> 01:08:20,840
of the propensity for the treatment arm there in taking the sum of those combining by hand.

524
01:08:22,250 --> 01:08:30,230
And then similarly on the control side divided by the propensity be a control.

525
01:08:32,210 --> 01:08:41,360
And so that's so that's the mechanics of doing that.

526
01:08:41,360 --> 01:08:51,709
No no why does it work well under on confounder this right if we think about her expected value of Y

527
01:08:51,710 --> 01:09:00,020
given X given treatment assignment so we can replace that observed value with this potential outcome.

528
01:09:05,730 --> 01:09:14,960
So. We kind of go back to the same game that we saw in the randomized trial setting.

529
01:09:15,980 --> 01:09:19,790
Now, if you think about the expectation of.

530
01:09:21,410 --> 01:09:30,380
The sum of our observed values in the treatment arm divided by their propensity to be in treatment.

531
01:09:31,130 --> 01:09:43,760
We can rewrite that as the sum of the entire sample, but now multiplying by 88 times they will know why.

532
01:09:43,760 --> 01:09:51,560
One odds. So now remember, we're conditioning on accidentally of x the whole way through here.

533
01:09:52,490 --> 01:10:02,330
So if we think about the expected value in respect to a, this quantity probably should say conditional in X.

534
01:10:19,320 --> 01:10:25,110
So the expected value, they, of course, is just going to be a propensity score.

535
01:10:25,710 --> 01:10:30,280
So if I divide it by the propensity score, these things cancel, right?

536
01:10:30,360 --> 01:10:39,970
And I'm just left with this. But because of this, my observed data corresponds to my potential outcome.

537
01:10:42,030 --> 01:10:46,349
So. So that gives me.

538
01:10:46,350 --> 01:10:51,080
Exactly. The meaning of life.

539
01:10:52,070 --> 01:11:01,550
One bar in the population more. If we assume a simple random sample, then this whole thing is going to converge to be one.

540
01:11:02,390 --> 01:11:09,080
Now there's a little bit of extra P0 of conditioned on X and this is unconditional in x.

541
01:11:09,710 --> 01:11:14,320
And what's happening here is I'm averaging over the empirical distribution of the axis, right?

542
01:11:14,570 --> 01:11:25,970
So if my distribution X, my sample makes my distribution of X in the population, if I look at these conditional expectations of Y one given x,

543
01:11:27,380 --> 01:11:36,620
then just the sample mean of those population universe will correspond to the super population quantity.

544
01:11:37,520 --> 01:11:40,429
And I could play the same game, right, because I get the same cancelation.

545
01:11:40,430 --> 01:11:48,950
But now I'm canceling the expected value of a I in the control groups, which is one area of X.

546
01:11:48,950 --> 01:11:51,980
Right. So that converges to the United.

547
01:11:58,020 --> 01:12:10,180
Lots of questions about her. So you can think of this as being like and when we did the randomized trial setting, we had the fact that Evi,

548
01:12:10,210 --> 01:12:17,560
he of XY is just going to be the fraction of subjects assigned to treatment and

549
01:12:17,560 --> 01:12:21,160
one minus IV exercise going to be the fractions of subjects assigned to control.

550
01:12:22,090 --> 01:12:25,810
So we took the expectation of that, of course, that cancel.

551
01:12:27,070 --> 01:12:30,970
So here it's a little fancier because they're not constant anymore.

552
01:12:31,180 --> 01:12:39,459
Right. They could vary by XY, but still patients at linear function, it all work itself out.

553
01:12:39,460 --> 01:12:45,630
And again. Okay.

554
01:12:45,630 --> 01:12:50,660
So I think the last little bit here, this idea of stabilized estimates.

555
01:12:51,690 --> 01:12:55,800
So, you know, I was going through and I was coming by or dividing by in Norway through here.

556
01:12:55,810 --> 01:13:02,850
Right. So if these quantities are close to zero,

557
01:13:03,720 --> 01:13:14,280
it turns out that replacing that end with with the weighted sums of these propensity scores actually is more helpful.

558
01:13:15,850 --> 01:13:22,350
So, so essentially.

559
01:13:23,460 --> 01:13:34,710
Right. So an expectation. Right. We would assume that if we sum up over these scores, they're going to equal the sample size, the population size.

560
01:13:35,340 --> 01:13:42,360
Similarly, on the on the control side, I think that should be equal to zero.

561
01:13:53,840 --> 01:14:08,510
So essentially this is just the reciprocal, the propensity score times one and the reciprocal of propensity score on the control side, times one.

562
01:14:09,650 --> 01:14:18,140
So if I go through and plug those in, my end cancels my in here.

563
01:14:23,360 --> 01:14:29,990
And I'm left with this some on the bottom, the same thing on the control side.

564
01:14:30,890 --> 01:14:35,810
So essentially then I'm looking at this idea of the weighted mean.

565
01:14:37,160 --> 01:14:45,500
So I take my treatment individuals and I multiply them by the reciprocal of you would be

566
01:14:45,500 --> 01:14:53,210
treated and divide that by the sum of those reciprocal of those entities or weights.

567
01:14:54,020 --> 01:15:03,280
The same thing on the control side and I take the difference in that and that gives me my estimated of the super population to.

568
01:15:07,040 --> 01:15:13,280
Okay. So. All right, I'll stop here.

569
01:15:19,100 --> 01:15:25,310
So, you know, negative. This is going to go to far too many questions at this point.

570
01:15:26,450 --> 01:15:34,760
Yes. So how does this actually help with the issue of is the way to get your stories right?

571
01:15:35,000 --> 01:15:39,560
So I'll start off by illustrating the super simple example.

572
01:15:39,730 --> 01:15:48,770
Suppose I just had one observation in treatment and through sort of the reasons I know that, you know, example,

573
01:15:48,770 --> 01:15:54,230
you only had one person assigned treatment, the very low probability of getting assigned treatment but they got it.

574
01:15:55,190 --> 01:15:59,090
It's a P of exciting is really small so.

575
01:16:01,550 --> 01:16:18,240
This. Essentially if I take this and then divide by in, I could end up with a really gigantic value with a lot of instability in that.

576
01:16:19,050 --> 01:16:25,680
Whereas if I just divide by itself, I'm actually just going to come back to this observed value so it'll be much more stable.

577
01:16:26,430 --> 01:16:28,800
Essentially, you can sort of play with this in practice.

578
01:16:29,280 --> 01:16:37,320
If you look at very if you look at a distribution of the X, which is sort of has values close to zero,

579
01:16:38,100 --> 01:16:42,140
let's say just for the treatment side, of course, of the sum,

580
01:16:42,160 --> 01:16:49,200
there are close to one in the control side for this part and divide look at this total and divide

581
01:16:49,200 --> 01:16:55,800
it by a constant in you will find that's far more unstable than dividing by these quantities here.

582
01:16:57,870 --> 01:17:09,060
That might be a good home of problem so so that's the in again you're getting a little like

583
01:17:10,110 --> 01:17:13,650
side trip into in the survey sampling but there's something called the high I guess Demeter,

584
01:17:14,310 --> 01:17:23,790
which is this idea of sort of estimating your population total. There's a really silly example that this so-called Harvest Thompson estimate,

585
01:17:23,790 --> 01:17:31,229
or if you put it to two really extreme settings you get where you basically take the sum of the outcomes,

586
01:17:31,230 --> 01:17:35,760
times the probability of selection and divide that by the population size.

587
01:17:36,030 --> 01:17:40,350
Very similar idea here would be of exercise of probabilities of selection with propensity score.

588
01:17:41,250 --> 01:17:44,770
So you can get extremely crazy results though.

589
01:17:44,880 --> 01:17:51,510
So if there's a famous example called Basu's Elephant, you can look at that in the web where there's a circuit statistician involved.

590
01:17:51,510 --> 01:17:55,580
So it's fired sheep.

591
01:17:56,940 --> 01:18:02,280
So that's because they're wedded to this or it's Thompson estimate or it's unbiased.

592
01:18:02,940 --> 01:18:09,269
So it turns out there's actually a slight bias to this because in small samples,

593
01:18:09,270 --> 01:18:15,450
this isn't a perfect estimate or there's actually a solid biased estimate of of an event.

594
01:18:16,350 --> 01:18:19,740
But it's much more stable, it's much more sensible to do so.

595
01:18:20,490 --> 01:18:25,690
Typically, when you have weighted means that you're trying to estimate you,

596
01:18:25,920 --> 01:18:32,550
you use the weights to some of the weights that are nominated or even if you have to know the population total,

597
01:18:32,940 --> 01:18:38,910
it tends to be a lower variance distributor with sort of a bit of a bias variance tradeoff.

598
01:18:40,240 --> 01:18:44,650
So. And this is exactly the same game here just with grocery stores.

599
01:18:51,660 --> 01:18:58,680
All right. Anything else? You mean it over?

600
01:18:59,850 --> 01:19:07,860
Look, I will have office hours tomorrow, and I will see you all.

601
01:19:08,280 --> 01:19:09,270
See you then. I'll see you Monday.

