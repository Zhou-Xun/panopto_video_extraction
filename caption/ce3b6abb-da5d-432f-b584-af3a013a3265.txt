1
00:00:01,940 --> 00:00:09,769
So welcome to the mid-term review. So first thing first, we need to do some announcements about logistics.

2
00:00:09,770 --> 00:00:15,710
So we will not have the regular lecture next Monday. So you can use a time towards midterm, if you will.

3
00:00:18,450 --> 00:00:38,670
So here's what I'm going to do. October 31st, 20.

4
00:00:56,590 --> 00:01:01,010
All right. So.

5
00:01:01,190 --> 00:01:04,630
So that's the first thing. The second thing is about the homework.

6
00:01:04,640 --> 00:01:09,050
So we have already posted the homework to solutions.

7
00:01:09,530 --> 00:01:15,649
So please feel free to check your answers against the solutions and hopefully that

8
00:01:15,650 --> 00:01:19,310
can give you some direct feedback regarding where you can do some corrections.

9
00:01:20,420 --> 00:01:29,360
Today we are going to do the mid-term review, so I'm returning to the syllabus which is organizing this table.

10
00:01:29,360 --> 00:01:34,490
So we are here. I would do an interview, so I would post the notes today.

11
00:01:35,150 --> 00:01:36,840
It is going to be a handwritten note today.

12
00:01:36,860 --> 00:01:43,850
So just to give you a conceptual review of what you've learned so far and the instruction about midterm, it doesn't hurt to repeat that again.

13
00:01:44,510 --> 00:01:48,590
So the exam will be open at 5 p.m. October 26, 2022.

14
00:01:48,620 --> 00:01:54,110
Today the exam will be closed at 11:59 p.m. October 31st.

15
00:01:55,100 --> 00:02:02,510
So for whatever reason, there is a issue with a you know, you started like 50 minutes prior to deadline.

16
00:02:02,810 --> 00:02:06,250
There is no way I can give you another extension. Okay, so you have to start early.

17
00:02:06,260 --> 00:02:09,680
You have to guarantee that you have the required time to finish.

18
00:02:09,680 --> 00:02:12,730
The work on the exam will be 90 minutes.

19
00:02:12,740 --> 00:02:19,850
For those of you who request accommodation, I have granted you additional time according to your submitted document.

20
00:02:20,780 --> 00:02:25,310
You can do this wherever you want with internet access and the exam will be given via the campus.

21
00:02:25,460 --> 00:02:30,870
Okay, so you can do this via the quiz. It will have 50 multiple choices.

22
00:02:31,230 --> 00:02:37,590
Only one question, one answer for question. So sometimes I'll be asking, what's the incorrect answer?

23
00:02:38,160 --> 00:02:40,320
You choose the one that's incorrect. Okay.

24
00:02:42,870 --> 00:02:49,349
And primarily it is emphasizing on conceptual understanding of the techniques and also the terminologies that we have

25
00:02:49,350 --> 00:02:57,060
been practicing implement more and other techniques and you will be tested on whether you can interpret model results.

26
00:02:58,170 --> 00:03:04,230
You can do some pen paper calculation if you want, but that does not is not part of the evaluation of the exam.

27
00:03:05,310 --> 00:03:13,890
So, Rose, first there are a few rules, any deviation from the rules with resulting zeros for midterm score for all the parties involved.

28
00:03:15,360 --> 00:03:17,640
The answer to the question should represent homework only.

29
00:03:18,240 --> 00:03:25,710
So if you refer to any materials outside of this class Internet, it will be treated as if you have violated rules.

30
00:03:26,700 --> 00:03:33,000
You can use the open book. You can use a textbook. You can use notes you have taken in class, printed, or electronic version.

31
00:03:34,800 --> 00:03:38,160
You do not. You are not allowed to do internet search.

32
00:03:38,370 --> 00:03:43,980
So what is Remo and Google? And that is not allowed and no group work is allowed.

33
00:03:45,210 --> 00:03:50,310
So how can we tell? Canvas Quest has the capability of monitoring all the activities.

34
00:03:51,280 --> 00:03:56,769
So what are the materials covered? Handouts. 10107 Homework.

35
00:03:56,770 --> 00:04:07,840
One and two. So one is graded. Two is due was due yesterday and you have the solutions textbook chapter 184 free to use them anyway so.

36
00:04:09,340 --> 00:04:23,240
Any questions before we proceed to do the review? All right.

37
00:04:23,250 --> 00:04:26,550
I don't see any questions right now. But please feel free to ask questions if you have any.

38
00:04:28,320 --> 00:04:31,370
Okay, so now let me. Do the story.

39
00:04:31,370 --> 00:04:47,970
You here? So in this class so far we have been focusing on one theme singularly,

40
00:04:48,000 --> 00:04:55,140
which is trying to extend the techniques you've learned in society to deal with our observations that are dependent.

41
00:04:55,470 --> 00:04:59,520
So we were dealing with linear models.

42
00:05:01,050 --> 00:05:06,120
Okay. And in 650, we have been doing independent observations.

43
00:05:13,750 --> 00:05:17,190
And in general, those are recorded by ABC.

44
00:05:17,560 --> 00:05:25,900
And why they're right. Because for one person I you may have a vector of information that's relevant to be used as a predictor.

45
00:05:26,110 --> 00:05:38,259
Why is a single is a scalar outcome which is often continuous and we have or you have learned maximum likelihood and also other test techniques,

46
00:05:38,260 --> 00:05:42,850
right? Like a racial test wall test and score test.

47
00:05:42,850 --> 00:05:51,370
Right. So you have learned all these variability, all these varieties, and you have learned about, I believe, interactions.

48
00:05:53,300 --> 00:05:56,900
Model with the instructions you have. Learn the technique to ordinary squares.

49
00:05:57,230 --> 00:06:00,500
You have learned the terminology is a residual sum of squares.

50
00:06:00,950 --> 00:06:09,090
So I am not reviewing 650 for you. But just to list out in general, what are the terms or techniques you've learned in 653?

51
00:06:09,110 --> 00:06:14,930
Clearly, our goal is to generalize to dependent observations.

52
00:06:37,670 --> 00:06:48,110
Okay. So there we have to introduce a second index, which is index multiple occasions for the observation of response for one subject.

53
00:06:48,440 --> 00:06:56,459
So that's where often we have why I as a vector outcomes, which is comprised of Y, one or two, y,

54
00:06:56,460 --> 00:07:08,450
I and I and we have a design matrix back side and that is basically and I buy

55
00:07:08,450 --> 00:07:13,220
P where each row correspond to the occasion is column correspond to cover.

56
00:07:13,400 --> 00:07:22,250
For example, we can do xij1 up to exi j you know, p right.

57
00:07:22,280 --> 00:07:29,210
So you may have other rows as well. So this is representing you have pieces of information to be used as predictor.

58
00:07:30,920 --> 00:07:35,990
So usually we record data in this way, linear model again.

59
00:07:36,140 --> 00:07:41,630
So this warning here often means that we are saying the mean.

60
00:07:42,730 --> 00:07:50,210
Meanwhile, J. Equals a function of beta and thus f is.

61
00:07:51,670 --> 00:07:57,460
Anemia function. In Beta.

62
00:07:59,640 --> 00:08:00,390
For example.

63
00:08:02,830 --> 00:08:17,680
My J equals beta zero plus beta one times h i j so the meaning of the eighth person's j occasion can be explained by the age of the measurement.

64
00:08:18,170 --> 00:08:22,900
Right? So this is what we call unit models. And these are all for.

65
00:08:24,090 --> 00:08:34,660
Continuous outcomes. So for the midterm, we're only going to focus on continuous outcomes.

66
00:08:36,490 --> 00:08:40,960
Now, when we are doing the extension, how do we specify the model?

67
00:08:41,170 --> 00:08:44,500
So there are two components. One is the mean model.

68
00:08:48,670 --> 00:08:51,790
The other is the variance covariance model.

69
00:08:59,380 --> 00:09:02,680
Variance covariance model.

70
00:09:03,620 --> 00:09:06,530
Right. So in terms of notation, how do we represent that?

71
00:09:07,640 --> 00:09:17,540
The first is often the conditional expectation of Y given the design matrix equals exi transpose beta.

72
00:09:17,910 --> 00:09:23,300
Right. The second piece is basically the variance of why I give an x.

73
00:09:24,230 --> 00:09:29,330
This is off to represent my Sigma II. So this is what we call the total.

74
00:09:30,410 --> 00:09:34,100
Total variance covariance matrix, right?

75
00:09:39,010 --> 00:09:45,340
So the reason why we use total is to prepare for the introduction of like between subject variability.

76
00:09:50,600 --> 00:09:53,810
It's too hot. Let me. So.

77
00:10:07,180 --> 00:10:11,870
So let's talk about how to specify the ME model. So it is this part.

78
00:10:12,310 --> 00:10:16,180
Okay. We have two sets of techniques.

79
00:10:16,540 --> 00:10:25,180
The first one is called analysis of. Analysis of response profiles.

80
00:10:31,790 --> 00:10:36,530
The other set is what is parametric curves for the ME model?

81
00:10:48,500 --> 00:10:51,800
Okay. So what are the differences?

82
00:10:52,580 --> 00:10:56,090
First, it is most flexible.

83
00:11:00,080 --> 00:11:07,910
Second it only works for. Only works will balance the data.

84
00:11:13,920 --> 00:11:19,900
Okay. Which often means that the number of occasion across people is the same.

85
00:11:20,320 --> 00:11:26,220
And also we have a common. Set of occasions.

86
00:11:33,920 --> 00:11:38,970
All right. Now, in contrast, we have.

87
00:11:42,300 --> 00:11:45,320
Parametric curves, which often is more parsimonious.

88
00:11:45,330 --> 00:11:58,510
For example, we can have linear trend. Which means that if we specify music as a function of, say, time.

89
00:12:01,580 --> 00:12:12,489
Then this F is nine year. So this warning here basically is to say the relationship between the mean and the time are,

90
00:12:12,490 --> 00:12:18,010
Nina, it's not the nine year as in the Nina model, which was Nina in terms of data.

91
00:12:18,640 --> 00:12:26,940
And clearly, there are other extensions like quadratic. Or other polynomials.

92
00:12:35,370 --> 00:12:39,390
Okay. And also, we have learned something that we called splints.

93
00:12:40,920 --> 00:12:44,400
Right. Most prominently, we have learned Nina Splice.

94
00:12:52,440 --> 00:12:58,139
Some people call this piecewise. I need your models.

95
00:12:58,140 --> 00:13:02,360
Right. Which is most straightforward.

96
00:13:02,480 --> 00:13:06,200
It's trying to concatenate multiple segments of straight lines.

97
00:13:09,400 --> 00:13:15,020
So that's pretty much the specification part. By the way, if you have any questions, please feel free to stop me.

98
00:13:15,740 --> 00:13:22,550
Because, you know, I'm just going to proceed by talking about things that I think most relevant to midterm.

99
00:13:23,450 --> 00:13:26,560
So how about the various governance model part?

100
00:13:26,840 --> 00:13:36,130
We can do them here. I want to ask you, what are the two possible options to do this modeling?

101
00:13:50,300 --> 00:14:01,870
So the first one is covariance pattern model that. So this is to model the total variance covariance.

102
00:14:07,200 --> 00:14:10,200
The second part, what is second option?

103
00:14:12,960 --> 00:14:20,900
The one you just turned in. Nina makes models.

104
00:14:24,890 --> 00:14:41,030
Mix models. But what's the difference relative to cover inside a model?

105
00:14:44,580 --> 00:14:51,540
The most interesting feature is that it decomposes the total variance Coburn's into the between subject.

106
00:14:55,430 --> 00:15:01,850
And within subject. Of arrogance.

107
00:15:02,060 --> 00:15:12,650
So this is the defining feature of this. Well, for the one on the left, you have no way of modeling the two sources of variance separately.

108
00:15:13,820 --> 00:15:20,660
Now let's consider what are they best for? So the governance coherence model is kind of best for.

109
00:15:22,130 --> 00:15:32,410
Balanced data. Even better for.

110
00:15:35,570 --> 00:15:45,420
Four evenly spaced. Time means.

111
00:15:47,780 --> 00:15:51,590
So it is to say that balance data does not mean that the timings are evenly spaced.

112
00:15:55,250 --> 00:15:59,360
What do I mean by that? Well, let's look at three people.

113
00:16:01,520 --> 00:16:14,380
We have these occasions and measurements. So for these three people, you know, they have different numbers of measurements.

114
00:16:14,390 --> 00:16:19,830
Right? And we sort of claim that, hey, the convergence pattern model works best for balanced data,

115
00:16:19,860 --> 00:16:25,429
works best for evenly spaced timings, but they are also applicable to this situation.

116
00:16:25,430 --> 00:16:25,610
Right.

117
00:16:25,640 --> 00:16:33,950
For example, if you're going to impose complete symmetry, which is to say that regardless of which pair choose, the correlation will be the same.

118
00:16:34,850 --> 00:16:57,400
So actually it works for this kind of situation to. Okay.

119
00:16:58,180 --> 00:17:04,400
And, um. You have other extensions to.

120
00:17:10,240 --> 00:17:15,760
To have a say to not force identical arrows for any pairs of measurements.

121
00:17:15,760 --> 00:17:20,950
Right. What are they? They are exponential.

122
00:17:23,640 --> 00:17:27,900
They are. They are one. They are banded.

123
00:17:31,140 --> 00:17:36,810
Toeplitz. And there is a hybrid.

124
00:17:39,360 --> 00:17:42,810
Say one plus some compound symmetry, i.

125
00:17:43,880 --> 00:17:54,520
Clearly you also have the unstructured. So you have all kinds of extensions and you know.

126
00:17:55,580 --> 00:18:00,440
A good goal to review these is to make sure that you understand what these patterns are.

127
00:18:00,950 --> 00:18:09,689
So what does exponential mean? So the correlation between any J and pair of measurements.

128
00:18:09,690 --> 00:18:12,720
Right. That correlation is going to be exponential. What?

129
00:18:14,170 --> 00:18:20,390
Minus alpha times. The difference between the timing of. What is alarming.

130
00:18:24,240 --> 00:18:33,420
You know I means you, Jake. Roger K equals the, uh, ro j minus K.

131
00:18:33,570 --> 00:18:40,480
All right. But J does not. So what does bended mean?

132
00:18:42,330 --> 00:18:48,720
It basically means that, you know, Rogic is going to be zero.

133
00:18:48,750 --> 00:18:52,860
If J minus K is greater than, say, I don't know, two.

134
00:18:53,610 --> 00:18:56,819
If they're two, more than two time means two occasions apart.

135
00:18:56,820 --> 00:19:08,670
They're not going to be correlated. I will not repeat other ones, but I think that you can find them in the handout.

136
00:19:09,270 --> 00:19:16,830
The point here is that we started by imposing certain patents upon these total various currencies,

137
00:19:17,250 --> 00:19:24,240
and they have a lot of our lot of our relation with a study of time series exponential error one bend.

138
00:19:24,300 --> 00:19:33,800
These are tools from time series. So returning to the second approach, which is an economics model.

139
00:19:34,650 --> 00:19:39,570
These are genuine non time series people's invention.

140
00:19:39,990 --> 00:19:47,310
So they are trying to petition the modeling to two parts and you may be wondering why do I group them into the technique?

141
00:19:47,880 --> 00:19:55,190
One of the techniques to model variance coherence that is because we introduce the between subject variation.

142
00:19:55,200 --> 00:20:00,079
So let's review the. Formulation.

143
00:20:00,080 --> 00:20:06,380
So for Nina mix model, you basically have a better outcome, explained them in three parts.

144
00:20:12,390 --> 00:20:16,560
For this part, we call the fixed effects. For this part, we call this random effects.

145
00:20:16,890 --> 00:20:20,340
For this part, we call them within subject errors.

146
00:20:27,840 --> 00:20:31,230
Often they can also be interpreted as measurement errors.

147
00:20:37,510 --> 00:20:45,360
Right. So you have a data point I observed in terms of why you think there are three sources of information contributing to it.

148
00:20:47,210 --> 00:20:55,680
What are the general assumptions? We do have a few.

149
00:20:55,770 --> 00:21:04,110
First we have IPS and Y equals, if you like, one, two, optional I and I.

150
00:21:05,010 --> 00:21:12,690
Okay. And we often assume if i j follows a Gaussian assumption would mean zero variance.

151
00:21:12,690 --> 00:21:17,570
Sigma Square. And we also assume that the errors.

152
00:21:19,800 --> 00:21:24,880
Our Independent if. We set?

153
00:21:26,640 --> 00:21:30,400
Sorry if we. Focus on.

154
00:21:49,500 --> 00:21:54,090
So this is one special case and more more generally.

155
00:21:54,090 --> 00:22:03,860
Right. People can have the variance of if shall I equals r and in the above.

156
00:22:07,340 --> 00:22:13,140
We have. Set our equals sigma squared times in a nine.

157
00:22:14,360 --> 00:22:18,530
So this is the assumption about the errors.

158
00:22:19,580 --> 00:22:23,720
Number two, it is about the random effects.

159
00:22:29,330 --> 00:22:34,830
So for one subject, you may have queue running if that's if you have random intercept and queue.

160
00:22:34,850 --> 00:22:38,570
Here is one. If you have random intercept at random slopes query close to.

161
00:22:38,840 --> 00:22:42,410
But more generically, you may have more than two random effects.

162
00:22:43,970 --> 00:22:47,360
And we also make some assumptions about its distribution.

163
00:22:49,250 --> 00:22:59,270
It hasn't been zero and various covariance matrix g and g is often Q by Q and you can represent them in the matrix.

164
00:22:59,840 --> 00:23:09,440
You can call them G one, one, g, two, two up to G and I and I and you have all the after again, elements which I will not write.

165
00:23:14,640 --> 00:23:18,510
And we often assume that by is independent, by prime.

166
00:23:19,320 --> 00:23:25,530
Which means that if you randomly draw one person's random effects, that will not be informative of another person's random effect.

167
00:23:34,810 --> 00:23:39,190
I do want to add in the above that if you have the errors from two people.

168
00:23:41,910 --> 00:23:51,130
I prime the air is also being dependent. Okay.

169
00:23:51,140 --> 00:23:56,140
Now we also need to talk about the stochastic dependance between the BI and I.

170
00:23:56,260 --> 00:24:00,610
So we often assume these are independent,

171
00:24:01,060 --> 00:24:05,560
which means that the measurement error is going to be independent of whether this person

172
00:24:05,560 --> 00:24:08,980
say is high responder or low responder if you are dealing with a random intercept.

173
00:24:12,360 --> 00:24:16,440
So this sets of assumptions.

174
00:24:17,010 --> 00:24:25,500
The set of assumptions specifies a nina mixed model and returning to the main point that this is a modeling of total variance we need to derive.

175
00:24:26,600 --> 00:24:34,100
The variance covariance matrix but average over the bits.

176
00:24:34,100 --> 00:24:37,280
Right. So here we do not have by conditioned upon.

177
00:24:38,440 --> 00:24:43,410
So we have. Denoted this thing by Sigma I.

178
00:24:45,760 --> 00:24:49,000
I want to make a note that this is not conditional.

179
00:24:50,670 --> 00:24:55,530
Not conditional on by. Okay.

180
00:25:04,640 --> 00:25:14,860
And we have worked on this annotation, which is sorry, this is a formula which equals Z Times, G and Z transpose plus Sigma Square.

181
00:25:15,140 --> 00:25:20,420
And I here, here I am using the simplifying assumption like this.

182
00:25:20,420 --> 00:25:23,660
But clearly if you want to play with more general ah, you can do that.

183
00:25:28,890 --> 00:25:32,580
So. This speaks to the main point, right?

184
00:25:32,590 --> 00:25:37,810
If we have a mixed model, the total advanced governance model is now very structured.

185
00:25:38,050 --> 00:25:42,040
It is characterized by whatever it is in G, whatever that is in Z.

186
00:25:42,160 --> 00:25:49,990
Right. So if we set Z I to B, say one one, 1t1 tie to tie three.

187
00:25:50,800 --> 00:25:55,270
So for this person, I have three occasions just to illustrate.

188
00:25:55,300 --> 00:26:03,370
Okay. So if you plug this whole thing in, you can see that this variance covariance matrix Sigma II is going to be dependent upon.

189
00:26:06,200 --> 00:26:14,620
Depends on t right. So we have created a situation where more generally.

190
00:26:18,400 --> 00:26:22,570
Sigma. I can be time independent, so time dependent.

191
00:26:23,840 --> 00:26:24,980
All time. Very well.

192
00:26:27,630 --> 00:26:36,450
If you contrast this work, contrast this with the covariance pattern model here, you don't see often the variance covariance changing with ti.

193
00:26:37,140 --> 00:26:41,640
Chloe may argue or whatever you do heterogeneous variance, core independent model.

194
00:26:41,670 --> 00:26:49,620
Yeah, that's true. Then you will have time points, specific variances that you really did not make the correlation to be changed over time.

195
00:26:50,160 --> 00:26:56,850
So here we have achieved that by specifying a particular set of Z, for example,

196
00:26:57,120 --> 00:27:03,660
the intercept and the time you have derived a total variance covariance that's dependent on time.

197
00:27:10,370 --> 00:27:15,830
Any questions so far regarding the how to put things into buckets about things you've learned?

198
00:27:24,540 --> 00:27:41,520
Okay. So next part is about inference, right? So what does the word inference mean?

199
00:27:42,090 --> 00:27:48,760
You got to estimate. Say estimate. Beta Sigma.

200
00:27:49,290 --> 00:27:56,090
Right. And then quantify. Quantify the uncertainty.

201
00:28:00,530 --> 00:28:08,720
Of the estimates. Usually in terms of the standard error or a confidence interval.

202
00:28:23,780 --> 00:28:28,920
Okay. So what's the primary tool we use to conduct inference?

203
00:28:29,820 --> 00:28:37,440
Well, for estimation, we have learned that maximum likelihood or some extensions may work.

204
00:28:37,620 --> 00:28:42,090
So let's talk about those. We use likelihood function.

205
00:28:50,240 --> 00:28:58,350
So this often would require a Gaussian assumption. Otherwise you cannot write down a real life function.

206
00:29:01,990 --> 00:29:12,309
So we basically assume that why this VEP responses from subject AI is going to follow a mode of very normal distribution with any dimensions.

207
00:29:12,310 --> 00:29:17,610
The mean is going to be. Exhibitor.

208
00:29:19,820 --> 00:29:24,370
All right. The variance is going to be sigma I. Okay.

209
00:29:24,820 --> 00:29:30,190
So this is assumption. Now you can write down the, um, multiplier Gaussian likelihood.

210
00:29:35,750 --> 00:29:40,250
Basically the likelihood of Theta Sigma.

211
00:29:43,730 --> 00:29:46,730
Can I save some just space by just removing the eye here?

212
00:29:47,610 --> 00:29:52,460
So I assume the same. So I'm just going to write one single sigma here and all the data.

213
00:29:54,280 --> 00:30:03,010
So this likelihood will be well over two pi times the determinant of sigma and uh.

214
00:30:03,850 --> 00:30:07,970
And over two times exponential. Uh.

215
00:30:09,250 --> 00:30:17,420
Y minus x. Beta sigma inverse transpose y minus x.

216
00:30:17,420 --> 00:30:21,220
Y. Beta. Okay.

217
00:30:21,250 --> 00:30:26,410
So this is the. Likelihood if you take log.

218
00:30:34,180 --> 00:30:37,989
If you take a log, then you have whatever you will derive.

219
00:30:37,990 --> 00:30:46,420
I will not write that down. Um. So do you have to remember the multiplayer Gaussian laggard?

220
00:30:46,450 --> 00:30:51,090
You don't have to. At least when I was your age, I can write down that pretty easily.

221
00:30:51,100 --> 00:31:00,010
So try to at least know the structures. But when you have to console the slides, you've got to understand what are the unknowns in there?

222
00:31:00,550 --> 00:31:06,700
So these are the unknowns, right? These things appear here.

223
00:31:07,240 --> 00:31:10,390
So the data appear here under the.

224
00:31:11,920 --> 00:31:16,460
Emphasize about triangles, right? So you have something that's a mix of data and unknown.

225
00:31:17,740 --> 00:31:24,820
How do we what do we mean by doing estimation? For example, if you are going to parameter sigma by compound symmetry.

226
00:31:30,280 --> 00:31:37,990
Sigma is parameters by two parameters, the variance for each of the time points and the correlation.

227
00:31:38,380 --> 00:31:49,270
So this is homogeneous compound symmetry. Then basically your likelihood will be just Beta and Sigma Square Rho and data.

228
00:31:50,060 --> 00:31:56,740
All right. So essentially your goal is trying to find the best beta and Sigma Square and Rho.

229
00:32:04,290 --> 00:32:08,940
So in general, how do you do that? Let's consider maximum likelihood.

230
00:32:11,380 --> 00:32:15,650
We have a. Wonderful interactive process.

231
00:32:25,210 --> 00:32:31,640
Which is to say that we. We estimate the variance covariance.

232
00:32:33,210 --> 00:32:39,870
A time point to plus one given. The current estimate of reality.

233
00:32:42,770 --> 00:32:48,530
Are you flexibility you solve for the sigma and then you solve for the next one?

234
00:32:51,130 --> 00:32:56,650
Given the most recent, most recently updated variance covariance matrix.

235
00:33:01,080 --> 00:33:07,260
Okay. Clearly you need some initialization and you can use ordinary squared initialize if you will.

236
00:33:08,730 --> 00:33:15,270
You can enter many times and they will converge often for the step number one.

237
00:33:15,420 --> 00:33:21,150
It is internally very hard because it is very hard to obtain close form solutions.

238
00:33:22,350 --> 00:33:28,870
However, for two, we have a very good formula which is what we call the g less.

239
00:33:29,550 --> 00:33:34,240
What a g. What the g lets me. The generalized squares.

240
00:33:34,240 --> 00:33:39,250
Right? Generalized. Least squares.

241
00:33:42,640 --> 00:33:50,230
This estimate says that for whatever variance covariance estimate you plug in.

242
00:33:50,230 --> 00:34:06,420
So I'm using S as a placeholder. Whatever you plug in, the best data you can do, you can get essentially is this one.

243
00:34:14,530 --> 00:34:20,360
This is the best you can do. Okay.

244
00:34:20,930 --> 00:34:27,790
And later on when we replace S by the REMO estimate, this whole beta will just be called Remo.

245
00:34:27,800 --> 00:34:33,020
But for this particular iterative process, we were focusing on the maximum likelihood.

246
00:34:35,740 --> 00:34:41,080
So there is often the question about. Testing.

247
00:34:41,380 --> 00:34:46,100
Right. Do we use email or not? So.

248
00:34:47,430 --> 00:34:52,530
And L. Can be used.

249
00:34:55,200 --> 00:35:02,410
Four point estimate. Point estimation.

250
00:35:05,080 --> 00:35:10,580
And while test. You can use that.

251
00:35:11,450 --> 00:35:15,420
But do I prefer that? I don't prefer that.

252
00:35:15,540 --> 00:35:26,740
So I don't recommend this. What's the reason?

253
00:35:27,520 --> 00:35:32,620
It is often because the sigma had eml is often.

254
00:35:33,810 --> 00:35:38,530
More biased. Then.

255
00:35:39,780 --> 00:35:47,470
A better solution. A better one, which we call Remo, and we will talk about that pretty soon.

256
00:35:51,960 --> 00:35:59,670
So this brings us to talking about an alternate method for estimating the beta and sigma.

257
00:36:01,240 --> 00:36:08,920
But why do we need to bother ourselves with, you know, a new technique when everything else seems to be working fine?

258
00:36:17,300 --> 00:36:23,460
So what's the motivation? To put it in terms you understand, why bother?

259
00:36:28,620 --> 00:36:37,650
Again, it is primarily because of the finite sample. Bias of.

260
00:36:40,060 --> 00:36:43,630
Rimmel is going to be smaller than that of EML.

261
00:36:46,380 --> 00:36:51,980
But the bias of what the this one. The variance covariance matrix.

262
00:36:55,750 --> 00:37:05,380
Sorry. This is. How did people discover this?

263
00:37:05,560 --> 00:37:10,959
Well, it is just you know, there are simple examples like what you have worked on homework one.

264
00:37:10,960 --> 00:37:18,310
Number one, you've found that email gives you a denominator of one of N, while the Remo gives you denominator one over minus one.

265
00:37:18,550 --> 00:37:24,760
And you know pretty well that denominator of one event will give you a biased Sigma Square estimation.

266
00:37:25,120 --> 00:37:31,660
So from that, you know, there needs to be some alternative techniques that can give you less biased estimate,

267
00:37:32,050 --> 00:37:39,520
and that was generalize to much broader sets of the various governments estimation.

268
00:37:39,790 --> 00:37:49,770
And those are unified under the theory of remote. So can people recall what's the reason why we call it the Remo?

269
00:37:50,010 --> 00:37:54,830
What does it represent? What?

270
00:37:56,270 --> 00:37:59,390
Restricted. Yeah. So why do we call it? Why do we want to restrict?

271
00:38:28,440 --> 00:38:32,760
So one problem we've found is that in during maximum local estimation,

272
00:38:33,930 --> 00:38:40,980
we treated beta as if it's known because we jointly maximize both abate and sigma.

273
00:38:41,820 --> 00:38:47,520
So later on, people found this contributed to the bias of the maximum lager estimation for Sigma.

274
00:38:51,270 --> 00:38:57,570
Okay. So what they do is that they want to get rid of beta during estimation of sigma.

275
00:38:57,580 --> 00:39:34,590
So there are two ways to do it. So the first approach is to do transformation.

276
00:39:34,600 --> 00:39:42,940
So we transform. Transform Y into a y so that.

277
00:39:45,420 --> 00:39:49,460
It's distribution. There's not.

278
00:39:51,070 --> 00:39:54,290
Depend on. Peter.

279
00:39:55,480 --> 00:40:01,520
And then we can write down the El Star for the transformed data.

280
00:40:01,540 --> 00:40:04,660
If I represent that using data star and.

281
00:40:06,300 --> 00:40:15,650
Sigma. So. When we have a distribution that does not depend on beta, you can write down the transform data distribution just in terms of sigma.

282
00:40:16,250 --> 00:40:23,300
And now, of course, you got to make it clear that you are talking about the transfer, the distribution channels, one data.

283
00:40:23,870 --> 00:40:27,080
So this is one way of dealing with how to get rid of beta.

284
00:40:28,540 --> 00:40:34,780
And I have to assure you, that said, this is something you have done pretty frequently in 650.

285
00:40:38,730 --> 00:40:43,940
Okay. So when, when you were doing like Y minus x I.

286
00:40:44,910 --> 00:40:52,850
Beta in 650. Does this distribution depend on data?

287
00:40:58,540 --> 00:41:04,060
It doesn't. So the residual does not have a the distribution residual does not depend on the data.

288
00:41:06,240 --> 00:41:10,170
So it is not surprising that you can do this in longitudinal case as well.

289
00:41:10,200 --> 00:41:14,430
It is just that you have to make you have to deal with the disease.

290
00:41:16,640 --> 00:41:20,780
Number two is a brute force approach, which is to do the integrated.

291
00:41:22,930 --> 00:41:38,090
Likelihood. Well, we have l beta sigma theta.

292
00:41:40,400 --> 00:41:45,920
And what we do is just we integrate our data. Then we have something that does not depend on.

293
00:41:50,290 --> 00:41:52,750
Does not depend on Peter. Okay. I should not put it out there.

294
00:42:00,360 --> 00:42:05,489
Because here I am not transforming data, so I'm not going to emphasize the transform data.

295
00:42:05,490 --> 00:42:12,890
But indeed the two things will be identical. It is a good thing, right?

296
00:42:12,930 --> 00:42:17,010
Because you want to have, you know, two rounds towards the same thing.

297
00:42:20,390 --> 00:42:23,990
Now. What do you do to conduct remo estimation?

298
00:42:24,590 --> 00:42:29,180
What you do is simply to find the sigma heads, which we call Remo.

299
00:42:30,070 --> 00:42:35,810
That is the argument. That maximizes the restricted likelihood.

300
00:42:36,620 --> 00:42:43,920
The argument that maximizes the restricted likelihood among all possible sigma.

301
00:42:43,940 --> 00:42:52,430
So you can take. So this is called Remo for the Sigma, the various components estimation.

302
00:42:52,730 --> 00:42:56,420
Now how do we obtain the Remo for the beta? It is simple.

303
00:42:56,930 --> 00:43:05,700
We just do the glass. And then there you have it.

304
00:43:06,060 --> 00:43:11,820
We have conducted the point estimation for beta. We have conducted point estimation for sigma.

305
00:43:13,960 --> 00:43:19,300
Once again, can you use remote to conduct testing for the main structure?

306
00:43:22,420 --> 00:43:34,379
If you're using all test. Yes, you can. Both Rama or Emil?

307
00:43:34,380 --> 00:43:39,390
Och. Because it just involves some estimates divided by the stent error.

308
00:43:39,390 --> 00:43:43,530
Right. I'm just going to make it simple. Who cares about how you get better?

309
00:43:45,240 --> 00:43:51,990
But there is one thing that I prefer, which is I want a less biased estimation of the data and less biased estimation of this dinner.

310
00:43:52,680 --> 00:43:55,710
So again, both okay, but I prefer this one.

311
00:43:56,190 --> 00:44:00,720
If you only have to choose one, right? Answer, choose Remo. Okay.

312
00:44:00,810 --> 00:44:06,200
That's what I'm going to say. So similarities.

313
00:44:10,120 --> 00:44:16,770
So both. Sigma eml and Sigma had Remo.

314
00:44:17,740 --> 00:44:21,910
Although they have finite sample differences, when you increase the number of people,

315
00:44:22,240 --> 00:44:27,190
this will converge in probability to the truth with the number of people.

316
00:44:28,910 --> 00:44:34,320
Goes to infinity. But in finite samples.

317
00:44:38,670 --> 00:44:42,530
We prefer. Remo.

318
00:44:44,270 --> 00:44:52,230
Because it's a less pious. By that I mean the REMO estimate for beta and Remo estimate for.

319
00:44:53,140 --> 00:44:56,490
For the variants coherence. So another terminology.

320
00:44:56,500 --> 00:44:58,149
So when you whenever you talk about Rambo,

321
00:44:58,150 --> 00:45:06,250
you have to ask yourself the rama of what I can tell about Rambo four beta sigma or the Rambo objective that has been optimized.

322
00:45:06,500 --> 00:45:14,770
Right. So you just got to be clear about what is the quantity you're talking about.

323
00:45:17,330 --> 00:45:21,260
Let's take 5 minutes, bro. Let's come back at 354.

324
00:46:15,530 --> 00:46:22,440
Quick question. Just confused with the schedule because I have yet to meet.

325
00:46:24,400 --> 00:46:27,900
It's not my place. There's no place for this.

326
00:46:29,660 --> 00:46:39,840
So it was a constructive response. I haven't been to number 11, but I've been insisting that this is not my personal consultancy.

327
00:46:41,440 --> 00:46:45,350
This section. Material is. So then.

328
00:46:47,640 --> 00:46:52,030
And the big question. They were on different servers.

329
00:46:53,870 --> 00:46:58,250
Yeah. Okay. Have you been taking their yet? Yes, I've been doing whatever it is that whatever, like.

330
00:46:58,700 --> 00:47:02,300
So I'm just kind of like. Okay.

331
00:47:03,170 --> 00:47:07,700
So I've been following the. I've been following this kind of.

332
00:47:09,470 --> 00:47:15,840
And judging from the slides and making quite. But this is not the thing.

333
00:47:16,880 --> 00:47:21,470
We're similar, but they're not exactly the same, said the. Okay.

334
00:47:22,450 --> 00:47:25,710
Have you taken the midterm? Yes. Well, it will be so useful.

335
00:47:25,720 --> 00:47:31,580
But, you know, it's just not required any of the 27. I mean, I've been attending this one, but I think that.

336
00:47:31,630 --> 00:47:35,430
Oh, really? Oh, you cannot attend this one. How can you attend this?

337
00:47:36,730 --> 00:47:40,320
I've been following this. So when the association his named.

338
00:47:42,310 --> 00:47:46,510
Thomas Broom. Yeah, I'm not surprised. But, yeah.

339
00:47:46,900 --> 00:47:51,340
I mean, because I've been, I've been following more along with the cannabis based on my health and.

340
00:47:52,350 --> 00:47:55,440
Right. But I thought I'd give you thought in my class.

341
00:47:55,450 --> 00:47:59,160
How can you access to the sections videos you don't have?

342
00:47:59,620 --> 00:48:02,620
Yeah, I've been watching those other videos. Okay. Yeah.

343
00:48:03,580 --> 00:48:09,040
Putting on this number. Thank you. Sorry. At the time.

344
00:48:59,610 --> 00:49:06,350
Yeah. So.

345
00:49:13,030 --> 00:50:13,500
Just. Yeah, there's.

346
00:50:19,960 --> 00:50:23,130
Oh, excuse me. We have to be crystal clear.

347
00:50:24,440 --> 00:50:28,390
So it's good. Yeah.

348
00:50:29,680 --> 00:50:32,740
You actually care about it? It's the last question.

349
00:50:34,180 --> 00:50:42,040
So is it okay to increase their ownership of those books?

350
00:50:43,740 --> 00:50:48,170
Oh, yeah, I would recommend, but.

351
00:50:51,240 --> 00:50:54,850
Yeah. Yeah. Mean me almost.

352
00:50:55,270 --> 00:50:58,430
Yeah, I just.

353
00:51:02,600 --> 00:51:13,860
Thank you. All right.

354
00:51:15,630 --> 00:51:21,390
Let's get back to work. So we have, well, pretty much some more materials to cover.

355
00:51:22,350 --> 00:51:29,160
Um. So we have the next set of topics, which is on.

356
00:51:30,720 --> 00:51:38,010
Model selections. Or rather, if you want to call comparisons, that's okay to.

357
00:51:41,860 --> 00:51:44,950
So we have to be clear that there are two sets of our models.

358
00:51:45,610 --> 00:52:01,420
So to set two components in the model we have expected value of y i given xy equals exi beta and we have the variance convergence of y i given xy.

359
00:52:01,780 --> 00:52:09,630
Right. So this is the sigma here. So we need to choose both models.

360
00:52:10,560 --> 00:52:19,170
But usually there is a recommended authority to do so. That reason is pretty nuanced and I'm going to see that.

361
00:52:19,620 --> 00:52:26,700
Describe that. So as you can see that when you are partitioning models into its data and errors,

362
00:52:27,510 --> 00:52:31,920
if you have a lousy model, whatever that's not model, it goes into the error part, right?

363
00:52:32,790 --> 00:52:37,380
And vice versa. So the two parts are going to be interdependent.

364
00:52:38,250 --> 00:52:41,340
So what we recommend is the following recipe.

365
00:52:41,520 --> 00:52:47,220
I don't like to do cookbook statistics, but this is a good practice.

366
00:52:47,580 --> 00:52:50,610
So you generally do use a maximal.

367
00:52:53,670 --> 00:52:57,730
Maximum model. All right.

368
00:52:57,760 --> 00:53:02,600
That's made me model. Which means that be as saturated as you can.

369
00:53:12,820 --> 00:53:19,620
Right. So anybody has anybody has problem understanding what saturated.

370
00:53:28,980 --> 00:53:43,140
So let's look at a balance data. So imbalanced data setting the saturated model or the maximum warming model is basically analyzes.

371
00:53:44,400 --> 00:53:52,290
Of. Response profiles. Okay.

372
00:53:53,100 --> 00:54:00,270
So if you are looking at two groups, one sorry for occasions you will have.

373
00:54:01,300 --> 00:54:06,160
These kind of tables where group one of two time point one, two, three,

374
00:54:06,160 --> 00:54:15,010
four and you can see there are eight cells each characterizing the mean response at one time point in a column in one group.

375
00:54:15,280 --> 00:54:21,070
Right. So the saturation means that you let the eight parameters to vary freely.

376
00:54:21,100 --> 00:54:29,290
They do not constrain each other. So this is this means we have one, two, three, four, five, six, seven, eight, eight parameters for the mean.

377
00:54:29,770 --> 00:54:33,460
And this is exactly what analysis of response profile means.

378
00:54:34,420 --> 00:54:38,110
It does not care whether one observation came before the other one.

379
00:54:38,230 --> 00:54:41,890
It just says, Hey, they're different and we're going to give them different parameters.

380
00:54:43,720 --> 00:54:52,780
Number two. Because analysis of results profile often is only applicable to balance data context.

381
00:54:52,840 --> 00:54:59,770
What if you have data that are super unbalanced? Then you have to redefine the maximum model.

382
00:55:00,430 --> 00:55:03,670
So in that case, we generally go one step.

383
00:55:06,080 --> 00:55:11,290
Beyond. The. Memorial.

384
00:55:14,430 --> 00:55:22,480
Your scientific team. Would you be willing?

385
00:55:27,340 --> 00:55:33,530
To entertain. Or investigate.

386
00:55:37,920 --> 00:55:43,499
So essentially this is context dependent, right? You can talk with your collaborator and say, hey, you know, I have this model.

387
00:55:43,500 --> 00:55:50,310
Do you think this is too restrictive? If that person says yes. And then you ask in which way you add, you add another covariate, another interaction.

388
00:55:50,730 --> 00:55:54,550
You ask him or her, Hey, is this good enough? Is it roughly good enough?

389
00:55:54,570 --> 00:56:04,710
Then you go one level beyond that, and you can also ask their opinion about what is a reasonable how to say generalization of that model.

390
00:56:07,180 --> 00:56:12,490
So now after you have fixed the Maxwell model.

391
00:56:14,460 --> 00:56:18,990
You will be able to do model comparison but only for the variance covariance part.

392
00:56:18,990 --> 00:56:23,860
Right. So you have the me model. Okay.

393
00:56:23,970 --> 00:56:32,690
Let me just make this clearer. So you have model one.

394
00:56:34,150 --> 00:56:46,040
Versus Model two, right? Clearly you have the MI model one and the variance covariance of one, the ME model to the variance coherence model two.

395
00:56:46,040 --> 00:56:51,110
Right. So in this situation you want to make them the same, but they are not the same.

396
00:56:51,120 --> 00:56:54,290
Right. So that you are choosing between the two variants, current models.

397
00:56:57,600 --> 00:57:02,480
And you want to fix them to be the maximal. Model.

398
00:57:05,040 --> 00:57:08,609
For example, analysis of response profiles.

399
00:57:08,610 --> 00:57:16,530
If you have balanced data now for the variance covariance components, you basically say are the two various current models are different.

400
00:57:16,530 --> 00:57:19,740
For example, one is compound symmetry, the other is unstructured.

401
00:57:21,980 --> 00:57:26,170
Hopefully now you're familiar with that, given that. You have done homework to.

402
00:57:32,190 --> 00:57:36,180
Right now. The question is, how do you make the choice or to make the choice?

403
00:57:37,250 --> 00:57:43,910
So there are two sets of possibilities, right? What if they are nested?

404
00:57:44,570 --> 00:57:53,570
What if the nested in those cases? I'm going to draw this kind of figure where the model one is nested, the model two.

405
00:57:54,080 --> 00:57:59,600
And there you can use like a racial test. Which is true.

406
00:58:00,950 --> 00:58:08,320
Which is great because it provides a probability characterization of the test statistic.

407
00:58:18,140 --> 00:58:24,160
What should we call sampling distribution? Of the test statistic.

408
00:58:29,580 --> 00:58:43,240
Under the No. So, uh, just to check, do you guys know the meaning of the sampling distribution?

409
00:58:56,200 --> 00:59:02,590
Well, often it just means a distribution of a statistic, but.

410
00:59:11,200 --> 00:59:23,530
Of a statistic. Okay. So, for example, if you have x one, two X and each follows a girls in distribution with mean zero and they're in sigma squared.

411
00:59:24,280 --> 00:59:27,700
So people do not call this sampling distribution.

412
00:59:27,700 --> 00:59:31,000
People just call this the distribution of the X, right?

413
00:59:31,300 --> 00:59:34,750
But you can calculate the sample mean what's its distribution?

414
00:59:36,370 --> 00:59:40,810
Gaussian with zero and variance covariance. So variance sigma squared over N right.

415
00:59:42,040 --> 00:59:47,800
And you can see that it's going to have a distribution that's a much more concentrated near zero, but variance.

416
00:59:49,800 --> 00:59:55,020
Of Sigma scared of either end. So of the one that's more concentrated, we call that sampling distribution.

417
00:59:55,890 --> 01:00:03,440
We primarily. Is using that term to distinguish from the original distribution for a single brand verbal.

418
01:00:04,200 --> 01:00:10,290
So that's why I say that sampling distribution is often the distribution of a statistic in this case,

419
01:00:10,290 --> 01:00:15,120
because in LRT what you do is just you do minus two.

420
01:00:15,720 --> 01:00:20,100
L has the smaller model minus L had the bigger model.

421
01:00:21,830 --> 01:00:25,100
Right. So this is a this is a quantity that depends on data.

422
01:00:26,360 --> 01:00:31,270
And data itself has distribution. So this is a statistic that has its own distribution.

423
01:00:31,280 --> 01:00:36,020
We call that sampling distribution. And we know that often.

424
01:00:38,260 --> 01:00:47,320
We will run into two situations where. And one is not on the boundary.

425
01:00:52,700 --> 01:01:01,460
Of too. Then, you know, the null distribution is that of Chi Square distribution with the difference in the degree of freedom.

426
01:01:02,090 --> 01:01:07,670
Right. What if and one is.

427
01:01:08,830 --> 01:01:15,940
On the boundary. Of and to.

428
01:01:16,660 --> 01:01:24,310
Then this will have a weird distribution and this distribution is only going to be considering the special case.

429
01:01:26,420 --> 01:01:29,600
That and one has. Q Random effects.

430
01:01:31,380 --> 01:01:36,960
And to has q plus one random effects then now distribution will be a mixture.

431
01:01:40,430 --> 01:01:45,340
Of these two things. So it's sort of a weird.

432
01:01:47,520 --> 01:01:50,760
So what are the examples of M1 on the boundary of them too?

433
01:01:51,030 --> 01:02:00,020
So for example. I think I can talk about this later, but they are primarily.

434
01:02:01,540 --> 01:02:07,330
They are easily distinguished when you are testing for the necessity of one additional random effects.

435
01:02:10,710 --> 01:02:27,590
Okay. Whatever.

436
01:02:27,590 --> 01:02:30,590
You do not use it. If you use what statistic?

437
01:02:42,130 --> 01:02:49,780
You basically do something like a beta, have a transpose sigma inverse, something like this,

438
01:02:49,780 --> 01:02:55,330
and this is going to follow a chi square distribution of the degree of freedom.

439
01:02:56,960 --> 01:03:00,770
Well, actually, this is sorry, I should not do this because I'm testing for the various currents.

440
01:03:00,770 --> 01:03:03,920
So let's leave this to later. Apologies.

441
01:03:11,930 --> 01:03:23,570
Okay. Let's return to this note. What if they're not nested? We have some criteria.

442
01:03:23,900 --> 01:03:30,860
I see OPEC. The smaller, the better, the larger, the worse.

443
01:03:34,770 --> 01:03:38,160
So these are called scoring of scoring comparisons, right?

444
01:03:41,260 --> 01:03:43,780
The difference between this and the like.

445
01:03:43,780 --> 01:03:52,390
Original test based approach is that unlike LRT where we have a normal distribution, these is differences does not have a distribution.

446
01:03:53,080 --> 01:04:00,740
So you cannot provide a. Distribution and characterization of how significant the difference is.

447
01:04:02,720 --> 01:04:08,930
So yeah, I see. It is defined by two terms l hart plus two C.

448
01:04:11,160 --> 01:04:19,110
You may be you may be asking. Okay. Let's stay ahead here, because we compare the Covance, I would say, using ramp of.

449
01:04:21,150 --> 01:04:28,160
And see here is a number of parameters. In the various Covance model.

450
01:04:43,520 --> 01:04:53,860
Actually, plus the main model, right. But because in this comparison we are fixing the maximum model.

451
01:04:54,100 --> 01:04:59,500
So it doesn't really matter whether you count the number of parameters in the model or not.

452
01:05:00,280 --> 01:05:06,400
So if you only report the number of parameters, you mean in the current model, that's good enough.

453
01:05:07,610 --> 01:05:10,819
So this is see and we choose. We fit different models.

454
01:05:10,820 --> 01:05:16,760
We choose a smaller, smaller model. Sorry, choose a model with a smaller AC.

455
01:05:49,140 --> 01:05:54,820
Okay. So.

456
01:05:56,360 --> 01:06:01,220
At high level, we have two approaches to deal with the various governance selection.

457
01:06:02,030 --> 01:06:06,770
Now, let's dove a little bit more on the on this part.

458
01:06:07,970 --> 01:06:13,430
Which is a new thing that you learned probably in the past two weeks.

459
01:06:13,940 --> 01:06:20,060
So one example is that if the M1 is why J e calls Excite.

460
01:06:21,270 --> 01:06:35,340
Peter Transpose plus the random intercept plus error and two is y j because excite j transpose beta plus VII zero plus something.

461
01:06:37,520 --> 01:06:44,750
Uh. Yeah. Let's do this one. Plus Egyptian is right.

462
01:06:45,350 --> 01:06:50,470
So the question is whether. The small model is good enough.

463
01:06:52,870 --> 01:06:59,650
However, to specify that all you basically need to specify that the variance of being one is zero, right?

464
01:07:00,310 --> 01:07:06,490
The alternative is that the variance of by one is greater than zero.

465
01:07:07,520 --> 01:07:17,210
Right. So in this case, you can see the no fly zone, the entire on the boundary of the entire parameter space.

466
01:07:17,780 --> 01:07:22,100
Y Well, uh, the variance of by y is greater than or equal to zero.

467
01:07:22,460 --> 01:07:25,490
The null is asking whether it is zero.

468
01:07:25,520 --> 01:07:34,900
So it is on a boundary. And this isn't all.

469
01:07:35,330 --> 01:07:43,479
And this is the boundary situation. And we have talking about the direction of conservative this.

470
01:07:43,480 --> 01:07:46,990
Right. So you have two distributions.

471
01:07:51,400 --> 01:07:55,570
So I'm going to first ask, what's the what's the naive?

472
01:07:56,290 --> 01:08:00,369
What's a naive knowledge distribution?

473
01:08:00,370 --> 01:08:04,090
If you have not learned that there is a possibility that knowledge distribution may be a mixture?

474
01:08:09,560 --> 01:08:14,900
Basically. Well, it will assume that it's a Chi Square distribution, but with how many degrees of freedom?

475
01:08:24,960 --> 01:08:37,740
Well if you consider the bigger model, right. What's the variance of bi0 by one it is what is g11g22g12 under another which elements are zero?

476
01:08:44,550 --> 01:08:48,460
Is this zero? Not necessarily.

477
01:08:49,870 --> 01:08:55,510
This is zero. This is what we said. If you have a variance of one one.

478
01:08:57,690 --> 01:09:02,400
One verbal zero. How about correlation? The Coalition is also zero.

479
01:09:04,660 --> 01:09:14,600
So you have two things. That's zero. So you have a chi square. Distribution of two degrees of freedom if you are naive about this procedure.

480
01:09:15,170 --> 01:09:20,140
However, as we know, the true north will be a chi square will be a mixture of what?

481
01:09:20,150 --> 01:09:25,600
Mixture of. I'm going to draw this half chi square.

482
01:09:25,600 --> 01:09:29,860
One and a half. Chi Square two.

483
01:09:29,890 --> 01:09:33,440
How do I know? It's always on the left. Well,

484
01:09:34,160 --> 01:09:37,580
it is taking half of Chi Square too and change that to chi square one and chi

485
01:09:37,580 --> 01:09:44,450
square one has a is stochastic a smaller because you has a mean of one so.

486
01:09:47,230 --> 01:09:51,600
If you're going to apply for it, you're going to do the rejection procedure, right?

487
01:09:52,120 --> 01:10:01,690
What you would naively do is to get the .05 under the naive null distribution, which is incorrect, and say, hey,

488
01:10:02,020 --> 01:10:11,080
this is a cutoff that we have to have the test statistic go beyond and then we can reject the null, right?

489
01:10:12,450 --> 01:10:19,020
But under the true enol, that area is much smaller than .05.

490
01:10:20,590 --> 01:10:22,750
Which means that if you are going to.

491
01:10:25,100 --> 01:10:32,570
If you're going to repeatedly calculate this like racial test statistic, you'll have very small chance of making the cut.

492
01:10:33,080 --> 01:10:44,790
So. It is harder to reject than all the consequences that you will be stuck with a model say that is random in set only.

493
01:10:45,070 --> 01:10:52,110
Although although in truth you probably would want to use a model with random slope.

494
01:10:52,350 --> 01:11:02,310
Right. So this is what we call. This is why we want to pay attention to the different distribution.

495
01:11:02,520 --> 01:11:05,940
When you are testing for the additional random effect.

496
01:11:07,520 --> 01:11:13,780
So what's the remedy for this? Well, if you still insist on using the.

497
01:11:15,250 --> 01:11:18,690
Naive. Blue Nile distribution.

498
01:11:18,840 --> 01:11:24,210
You'd better want to be a bit more lenient, right, in terms of the cut off.

499
01:11:24,780 --> 01:11:29,760
So the hope is that this area will be roughly 8.5.

500
01:11:29,940 --> 01:11:37,380
Right. Roughly. So sometimes people just try to use the 8.1 as the ad hoc cutoff.

501
01:11:38,760 --> 01:11:47,149
This technique is not needed when you have this situation where the no district, no model has Q random effects and the bigger model has Q plus one.

502
01:11:47,150 --> 01:11:51,540
And if we know exactly what's in our distribution, so you do not need this approximation,

503
01:11:51,990 --> 01:11:57,090
but as in your homework there is a question was asking whether to run of effects can be

504
01:11:57,090 --> 01:12:03,030
simultaneous removed in those cases we do not have a mathematical result for the null distribution.

505
01:12:03,390 --> 01:12:06,370
So the solution there is we can see in the solution in the post,

506
01:12:06,400 --> 01:12:17,730
the solution is to do the naive Chi Square distribution with the non regular number of degrees of freedom and then make make a cutoff to be lower.

507
01:12:18,030 --> 01:12:27,120
Right. Using the significance level point one. So that's the that's the new thing in this class.

508
01:12:30,750 --> 01:12:34,320
I have 4 minutes. I guess I can finish them. I don't think I have a lot of materials.

509
01:12:35,340 --> 01:12:40,860
The final piece essentially is the recipe for memorial selection.

510
01:12:48,740 --> 01:12:56,000
As I promised, this is in the midterm exam. So we often assume that we have the same.

511
01:13:00,410 --> 01:13:06,050
Variance covariance model. How could we have done that?

512
01:13:06,080 --> 01:13:11,780
Well, in the previous step, we have decided to we have fixed them all at the maximum model,

513
01:13:11,780 --> 01:13:15,230
and we have chosen the various governance model, and that's the one we're happy with.

514
01:13:15,260 --> 01:13:23,270
Okay. And then we fixed the various cameras model at that, selected various governance model and then entertained with different models.

515
01:13:37,490 --> 01:13:41,030
Same thing. We have nested models.

516
01:13:41,450 --> 01:13:45,820
We have non nested. MI models.

517
01:13:50,200 --> 01:14:02,500
So what does the model mean? Well, you may have for example, you may have expected value of Y, given ASI equals, say, two models,

518
01:14:02,860 --> 01:14:11,079
beta zero plus beta one times CIJ, the other one is beta zero plus beta two and the CIJ plus beta three times 80 squared.

519
01:14:11,080 --> 01:14:18,760
Right. So these two model nested because you can set beta 3 to 0 and achieve a retrieve the smaller model.

520
01:14:21,350 --> 01:14:26,110
Oh. Why do I do that? Yeah. Beta two zero.

521
01:14:26,680 --> 01:14:30,190
And the alternative is beta two does not equal zero.

522
01:14:36,600 --> 01:14:49,090
Now if you are going to do LRT. You have two things, which is the app optimized log likelihood.

523
01:14:49,810 --> 01:14:53,380
The other one is optimized, Remo. Which one should you use to make the comparison?

524
01:15:00,350 --> 01:15:08,070
A or B. Well.

525
01:15:09,310 --> 01:15:21,190
If you use world test. Do use beta hat transpose remo eml sigma hat in verse eml.

526
01:15:22,740 --> 01:15:27,390
Beta email or do use beta had to read more.

527
01:15:38,240 --> 01:15:41,890
C or D. We've got to make this work, okay?

528
01:15:42,430 --> 01:15:44,740
If you don't know the answer, you will lose two points at least.

529
01:15:48,180 --> 01:15:55,590
So for the if you're going to use LRT, we definitely want to do something like to L had.

530
01:15:56,950 --> 01:16:01,390
For the small one, right. Minus the owl heads for the larger one.

531
01:16:02,200 --> 01:16:05,829
And my question is for the question mark part.

532
01:16:05,830 --> 01:16:10,070
Do you want to fill email, Remo? Come on.

533
01:16:10,180 --> 01:16:13,420
If you know the answer, just say it. We can save, like, 30 seconds, I guess.

534
01:16:18,490 --> 01:16:22,990
I have a haircut at 430, so I do have to go after this. Come on, guys.

535
01:16:25,570 --> 01:16:31,780
ML oc ml. If you have question yourself, it is because Remo has removed beta.

536
01:16:32,780 --> 01:16:35,900
Okay. So it cannot be used to compare something about better.

537
01:16:42,420 --> 01:16:48,840
Second question, if using world test statistic, are you going to use email or email?

538
01:16:50,010 --> 01:16:53,040
You may say both. Okay, but which one is preferred?

539
01:16:56,180 --> 01:16:59,420
What is that? Remo. Okay.

540
01:16:59,440 --> 01:17:08,820
Yeah. Remo is preferred. Anyway, for the sake you know, if you have to choose one, choose Remo.

541
01:17:09,750 --> 01:17:14,910
Both are okay because here you have the world test statistic taking that form.

542
01:17:15,180 --> 01:17:19,980
So all you need to do is just plug in a beat estimate. But once you have her estimate, who cares about how you get it right?

543
01:17:20,280 --> 01:17:25,500
But clearly, it matters whether you got a higher quality beta or a higher quality sigma.

544
01:17:25,680 --> 01:17:32,010
And according to our discussion, the quality of sigma and better estimate are higher when you use Remo.

545
01:17:34,040 --> 01:17:37,790
So when you have non-listed models, you can use AIC or.

546
01:17:38,390 --> 01:17:43,330
Right. Okay.

547
01:17:43,340 --> 01:17:52,260
So for the. Yeah, I see here. So here the C is basically the number of parameters.

548
01:17:55,330 --> 01:18:02,090
In the memorial, right. In the memo.

549
01:18:02,540 --> 01:18:05,840
So if we're here. Which one do you use? Remote or email?

550
01:18:17,710 --> 01:18:23,230
Which one to choose. Well, again, it's about the mean.

551
01:18:23,320 --> 01:18:26,580
So you've got to have something that has something to do with beta.

552
01:18:26,590 --> 01:18:38,460
So you use enough. Okay.

553
01:18:39,900 --> 01:18:43,800
I promise. This is the final piece. I do want to be responsible for whatever I'm going to cover.

554
01:18:44,130 --> 01:18:47,130
So love is our final piece.

555
01:18:47,490 --> 01:18:52,420
It is the best nine year. I'm biased.

556
01:18:53,880 --> 01:18:58,090
Prediction. For what?

557
01:18:58,120 --> 01:19:03,020
For a. For Bae. And of course, you can do this for.

558
01:19:06,790 --> 01:19:16,380
Do this for a while as well, which will be a trajectory. Right. It is first introduced in the Nina mix effects model.

559
01:19:16,870 --> 01:19:21,630
Second, it can be used to predict. Individual trajectory.

560
01:19:27,140 --> 01:19:30,080
So this is what we call individual trajectory, right?

561
01:19:30,440 --> 01:19:36,380
Because if you plug in the beta hat, if you plug it by hand, you will have a vector of y hat there.

562
01:19:36,410 --> 01:19:42,890
Right now, the question is, can you predict at the time point when this person was not measured at all?

563
01:19:44,090 --> 01:19:54,380
Yes, you can, because you can plug in an additional row in X, Y or Z NZEI to represent that time.

564
01:19:54,860 --> 01:19:59,480
That's outside of the sets of observations for this person.

565
01:20:00,680 --> 01:20:07,440
And three, we have talking about the shrinkage. Shrinkage interpretation.

566
01:20:10,300 --> 01:20:14,620
Because we just talk about that yesterday. So I am not going to say that again.

567
01:20:29,610 --> 01:20:40,400
And the final. Parting question. Can be two hats.

568
01:20:40,640 --> 01:20:44,540
Remo. Be equal to beta ml.

569
01:20:46,540 --> 01:20:52,690
Sometimes. So the answer is yes.

570
01:20:53,700 --> 01:20:58,410
Or no? So I'm going to leave you this as a question.

571
01:21:01,020 --> 01:21:04,320
Is that painful? That. Should I just tell you the answer?

572
01:21:07,210 --> 01:21:10,880
Yes. Yes. So that's how it is. So this is.

573
01:21:10,930 --> 01:21:14,020
Yes. Okay. So.

574
01:21:15,660 --> 01:21:19,360
So I'm going to say, what's the example? So why I will be.

575
01:21:19,860 --> 01:21:29,190
So this is a model you're going to sit and we assume the errors follow normal distribution with being zero and Sigma Square and I.

576
01:21:30,980 --> 01:21:41,150
So you will be able to see that Peter has goals for any placeholder of the various coins estimate.

577
01:21:41,360 --> 01:21:48,210
It will be. So this formula you're familiar with.

578
01:21:49,590 --> 01:21:55,830
The point is that regardless of whatever Sigma hat you plug in here.

579
01:21:59,220 --> 01:22:04,050
Beards and nail. Or Sigma hat Remo.

580
01:22:05,780 --> 01:22:10,970
They will be canceled because you have this term with the inverse and you have this term.

581
01:22:11,210 --> 01:22:20,210
So in this very special case, where regardless of what Sigma had number you plug in, be it from a remote procedure or email procedure.

582
01:22:20,480 --> 01:22:24,580
The beta will be exactly the same. You ask, what's that data?

583
01:22:25,210 --> 01:22:31,460
Well, if you look at this equation, what does it mean? It just says that across all the occasions the me is the same.

584
01:22:32,380 --> 01:22:41,430
If I have five numbers, how do you ask me to? The average that you care about how variable it is, the best thing is average.

585
01:22:41,790 --> 01:23:04,330
So it is not surprising that beta had a just the average. So this is one example where they can be exactly the same.

586
01:23:04,660 --> 01:23:08,530
Okay, so I will stop here. That's all I have prepared.

587
01:23:09,070 --> 01:23:19,180
If you have any additional questions. You can ask me, but I will not try to bias you towards answering the midterm questions.

588
01:23:19,570 --> 01:23:23,780
But I probably forgot the question, so maybe it's okay.

589
01:23:25,690 --> 01:23:40,800
All right. Good luck with the good luck with the midterm. You have a really quick super late for that.

590
01:23:40,810 --> 01:23:41,470
So everything.

