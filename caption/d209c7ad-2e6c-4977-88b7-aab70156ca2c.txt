1
00:00:00,530 --> 00:00:03,330
Hello. Welcome to this lecture

2
00:00:03,330 --> 00:00:05,835
on the Measures of
Association and Regression.

3
00:00:05,835 --> 00:00:07,950
In this lecture, we'll
discuss how we use

4
00:00:07,950 --> 00:00:10,275
inference with our
linear regression model,

5
00:00:10,275 --> 00:00:11,730
and the concept of

6
00:00:11,730 --> 00:00:14,610
goodness-of-fit of a
linear regression model.

7
00:00:14,610 --> 00:00:17,250
Now, at this point, our
statistical toolbox is

8
00:00:17,250 --> 00:00:19,995
almost fully stocked
with everything we need.

9
00:00:19,995 --> 00:00:23,100
Nonetheless, although we
might now understand how

10
00:00:23,100 --> 00:00:26,115
we fit a line through data
we see in a scatter-plot,

11
00:00:26,115 --> 00:00:28,410
we do need to go
one step further.

12
00:00:28,410 --> 00:00:32,010
We have yet to understand if
the line we have created is

13
00:00:32,010 --> 00:00:33,900
a good representation of what

14
00:00:33,900 --> 00:00:37,135
the line in the entire
population might be.

15
00:00:37,135 --> 00:00:39,980
So our learning objectives
are first to understand

16
00:00:39,980 --> 00:00:43,010
the specific mathematical
relationship between

17
00:00:43,010 --> 00:00:44,660
our slope coefficient b

18
00:00:44,660 --> 00:00:46,819
and Pearson's
correlation coefficient,

19
00:00:46,819 --> 00:00:49,160
r. To use the results of

20
00:00:49,160 --> 00:00:50,510
our fitted regression
line to make

21
00:00:50,510 --> 00:00:52,915
inference back to the population,

22
00:00:52,915 --> 00:00:54,500
to then correctly interpret

23
00:00:54,500 --> 00:00:56,780
the coefficient of
determination and

24
00:00:56,780 --> 00:00:58,790
understand its
mathematical relationship

25
00:00:58,790 --> 00:01:01,240
with Pearson's
correlation coefficient,

26
00:01:01,240 --> 00:01:03,140
and then finally,
to be able to use

27
00:01:03,140 --> 00:01:05,150
a table of computer
output to make

28
00:01:05,150 --> 00:01:07,160
inference from the
fitted regression line

29
00:01:07,160 --> 00:01:09,365
back to the population.

30
00:01:09,365 --> 00:01:11,840
At this point now, we
have two measures of

31
00:01:11,840 --> 00:01:15,395
association between two
continuous measures.

32
00:01:15,395 --> 00:01:17,130
We talked about r,

33
00:01:17,130 --> 00:01:19,440
which is Pearson's
Correlation Coefficient,

34
00:01:19,440 --> 00:01:21,105
and we talked about b,

35
00:01:21,105 --> 00:01:22,370
which is the slope of

36
00:01:22,370 --> 00:01:23,840
the least-squares regression line

37
00:01:23,840 --> 00:01:26,005
fitting through the data.

38
00:01:26,005 --> 00:01:29,270
Now, I've showed you
the equations that we

39
00:01:29,270 --> 00:01:32,015
use to compute both r and b.

40
00:01:32,015 --> 00:01:34,130
Looking at these two equations,

41
00:01:34,130 --> 00:01:35,990
we can begin to examine how

42
00:01:35,990 --> 00:01:39,050
these two quantities are
connected to each other.

43
00:01:39,050 --> 00:01:42,110
So from the two
equations, for r and b,

44
00:01:42,110 --> 00:01:44,330
I first move the denominators in

45
00:01:44,330 --> 00:01:46,760
the fractions over to
the left-hand side.

46
00:01:46,760 --> 00:01:49,670
So that I have the sample
covariance of x and

47
00:01:49,670 --> 00:01:53,225
y expressed two different ways.

48
00:01:53,225 --> 00:01:57,440
If I then set those two
expressions equal to each other,

49
00:01:57,440 --> 00:01:59,900
and then I get b by itself,

50
00:01:59,900 --> 00:02:02,120
I have the relationship between

51
00:02:02,120 --> 00:02:05,600
the slope coefficient and
Pearson's correlation.

52
00:02:05,600 --> 00:02:08,960
The slope coefficient is
Pearson's correlation times

53
00:02:08,960 --> 00:02:10,880
the ratio of the
standard deviation

54
00:02:10,880 --> 00:02:13,430
of y and the standard
deviation of x.

55
00:02:13,430 --> 00:02:17,960
Now, this equation is
rarely used in practice,

56
00:02:17,960 --> 00:02:19,340
but its interpretation is

57
00:02:19,340 --> 00:02:22,760
important because it allows
us to see first that

58
00:02:22,760 --> 00:02:25,025
both the slope coefficient and

59
00:02:25,025 --> 00:02:27,200
Pearson's correlation
coefficient must

60
00:02:27,200 --> 00:02:28,565
have the same sign.

61
00:02:28,565 --> 00:02:30,320
The standard deviation of y and

62
00:02:30,320 --> 00:02:33,185
the standard deviation of x
are both positive quantities,

63
00:02:33,185 --> 00:02:34,925
so their ratio is positive.

64
00:02:34,925 --> 00:02:36,680
So if r is negative,

65
00:02:36,680 --> 00:02:38,540
that means that b
must be negative,

66
00:02:38,540 --> 00:02:41,150
and if r is positive,
b must be positive.

67
00:02:41,150 --> 00:02:44,330
Furthermore, if r shows us to

68
00:02:44,330 --> 00:02:47,660
have statistical significance
in a hypothesis test,

69
00:02:47,660 --> 00:02:49,820
then the slope
coefficient must also

70
00:02:49,820 --> 00:02:52,565
lead us to the exact
same conclusion.

71
00:02:52,565 --> 00:02:54,770
If there's positive
association and it's

72
00:02:54,770 --> 00:02:57,680
significant through Pearson's
correlation coefficient,

73
00:02:57,680 --> 00:03:00,980
then we must find that we
have positive association and

74
00:03:00,980 --> 00:03:02,750
statistical significance if we

75
00:03:02,750 --> 00:03:04,765
think about a slope coefficient.

76
00:03:04,765 --> 00:03:06,800
Furthermore, we can see that if

77
00:03:06,800 --> 00:03:08,870
Pearson's correlation is zero,

78
00:03:08,870 --> 00:03:12,350
then the slope coefficient
must also be zero.

79
00:03:12,350 --> 00:03:16,760
However, all other values
of Pearson's correlation

80
00:03:16,760 --> 00:03:21,350
do not have a single corresponding
value for the slope,

81
00:03:21,350 --> 00:03:24,290
which is represented in
the following figure.

82
00:03:24,290 --> 00:03:26,615
In both of these scatter plots,

83
00:03:26,615 --> 00:03:28,160
the yellow line represents

84
00:03:28,160 --> 00:03:30,650
the best fitting line
through both sets of data.

85
00:03:30,650 --> 00:03:35,890
Both of these lines have the
same slope equal to 0.49.

86
00:03:35,890 --> 00:03:37,520
However, we can see that

87
00:03:37,520 --> 00:03:39,920
the scatter plot on the
left its points are

88
00:03:39,920 --> 00:03:41,570
much more tightly
scattered around

89
00:03:41,570 --> 00:03:43,925
the line than the
plot on the right.

90
00:03:43,925 --> 00:03:47,855
Pearson's correlation for
the left plot is 0.77

91
00:03:47,855 --> 00:03:52,810
and Pearson's correlation
on the right is only 0.20.

92
00:03:52,810 --> 00:03:55,670
Again, Pearson's
correlation coefficient

93
00:03:55,670 --> 00:03:58,010
is related to the
slope coefficient,

94
00:03:58,010 --> 00:04:01,565
but it is also a function of
the variability in the data.

95
00:04:01,565 --> 00:04:03,745
The more spread out the data are,

96
00:04:03,745 --> 00:04:06,750
Pearson's correlation
begins to decrease.

97
00:04:06,750 --> 00:04:08,240
Now, just as we can use

98
00:04:08,240 --> 00:04:10,445
Pearson's correlation
coefficient to make

99
00:04:10,445 --> 00:04:12,890
inference about the
population parameter Rho,

100
00:04:12,890 --> 00:04:14,690
the correlation in
the population,

101
00:04:14,690 --> 00:04:16,595
we can also make inference

102
00:04:16,595 --> 00:04:18,650
regarding the slope
coefficient Beta

103
00:04:18,650 --> 00:04:22,565
in the population through our
estimate b from our data.

104
00:04:22,565 --> 00:04:25,130
Again, the null hypothesis
says that x and

105
00:04:25,130 --> 00:04:27,665
y have no association
with each other.

106
00:04:27,665 --> 00:04:28,940
If that's true, then

107
00:04:28,940 --> 00:04:31,730
the slope coefficient in the
population must be zero,

108
00:04:31,730 --> 00:04:33,410
and therefore, the
alternative says that

109
00:04:33,410 --> 00:04:35,705
the slope is something
other than zero.

110
00:04:35,705 --> 00:04:37,730
Again, we need a
test statistic to

111
00:04:37,730 --> 00:04:39,335
compare these two hypotheses,

112
00:04:39,335 --> 00:04:42,550
the test statistic is a
ratio of signal to noise,

113
00:04:42,550 --> 00:04:45,350
the signal is my best
estimate of the slope,

114
00:04:45,350 --> 00:04:49,190
which is the magnitude of my
least-squares line slope,

115
00:04:49,190 --> 00:04:51,440
and there is the
representation for

116
00:04:51,440 --> 00:04:52,730
the standard error or

117
00:04:52,730 --> 00:04:55,190
the denominator that would
go in my test statistic.

118
00:04:55,190 --> 00:04:56,240
Again, there is

119
00:04:56,240 --> 00:04:59,605
no intuitive interpretation
for this value.

120
00:04:59,605 --> 00:05:03,080
However, it then plays a
role in my test statistic,

121
00:05:03,080 --> 00:05:05,030
because I take the ratio of

122
00:05:05,030 --> 00:05:07,070
the slope coefficient
to its standard error

123
00:05:07,070 --> 00:05:10,850
then is applied to the data
from Siegel and Rothman,

124
00:05:10,850 --> 00:05:15,140
the standard error of the
slope coefficient is 0.03,

125
00:05:15,140 --> 00:05:17,660
and therefore, my test statistic

126
00:05:17,660 --> 00:05:19,010
for testing whether
or not there is

127
00:05:19,010 --> 00:05:21,200
an association between x and y is

128
00:05:21,200 --> 00:05:23,710
the ratio of the slope
to its standard error,

129
00:05:23,710 --> 00:05:28,085
and for these data that
statistic is 10.82.

130
00:05:28,085 --> 00:05:30,775
Now, the statistics
should look familiar.

131
00:05:30,775 --> 00:05:33,395
It is the same test
statistic we got

132
00:05:33,395 --> 00:05:36,490
when we used Pearson's
correlation coefficient.

133
00:05:36,490 --> 00:05:39,385
Again, the p-value
is very, very small,

134
00:05:39,385 --> 00:05:43,159
because this test statistic
is very far from two,

135
00:05:43,159 --> 00:05:45,200
which is the typical
critical value for

136
00:05:45,200 --> 00:05:48,130
a p-value less than 0.05.

137
00:05:48,130 --> 00:05:51,290
Again, whether we used Pearson's
correlation coefficient

138
00:05:51,290 --> 00:05:52,970
or the slope coefficient,

139
00:05:52,970 --> 00:05:55,610
both of them have the same
test statistic and therefore,

140
00:05:55,610 --> 00:05:57,800
the same p-value, and both would

141
00:05:57,800 --> 00:06:00,275
tell us to reject
the null hypothesis.

142
00:06:00,275 --> 00:06:03,590
We conclude that the
firearm ownership rate and

143
00:06:03,590 --> 00:06:05,420
the male firearm
suicide rate have

144
00:06:05,420 --> 00:06:08,000
some association
in the population.

145
00:06:08,000 --> 00:06:10,129
Now, as with any statistic,

146
00:06:10,129 --> 00:06:13,100
we can convert it to a 95
percent confidence interval

147
00:06:13,100 --> 00:06:14,510
for the population slope,

148
00:06:14,510 --> 00:06:15,710
and in this case, I take

149
00:06:15,710 --> 00:06:18,230
the best fitting
line from my data,

150
00:06:18,230 --> 00:06:21,770
the slope of that plus or
minus two standard errors,

151
00:06:21,770 --> 00:06:24,040
and apply to the data
from Siegel and Rothman,

152
00:06:24,040 --> 00:06:26,390
we get a confidence
interval for the slope in

153
00:06:26,390 --> 00:06:31,300
the population from 0.26 to 0.38.

154
00:06:31,300 --> 00:06:32,900
Again, the interpretation of

155
00:06:32,900 --> 00:06:36,230
this confidence interval
states that in the population

156
00:06:36,230 --> 00:06:38,960
the slope coefficient
could be as low as

157
00:06:38,960 --> 00:06:43,450
0.26 or 0.38
according to my data.

158
00:06:43,450 --> 00:06:46,110
Zero is not in this
confidence interval,

159
00:06:46,110 --> 00:06:48,950
which is the null hypothesis
value for the slope.

160
00:06:48,950 --> 00:06:51,635
Because zero is not in
this confidence interval,

161
00:06:51,635 --> 00:06:53,975
I would reject that
null hypothesis.

162
00:06:53,975 --> 00:06:56,090
Therefore, the
confidence interval and

163
00:06:56,090 --> 00:06:59,690
my t-statistic both gave
me the same conclusion.

164
00:06:59,690 --> 00:07:01,850
Now, usually, the fit of

165
00:07:01,850 --> 00:07:03,410
the regression
model is summarized

166
00:07:03,410 --> 00:07:04,700
by a computer in

167
00:07:04,700 --> 00:07:06,935
a table that looks like
the one in front of you.

168
00:07:06,935 --> 00:07:08,410
There are two rows,

169
00:07:08,410 --> 00:07:09,500
the first row represents

170
00:07:09,500 --> 00:07:11,720
information on the
intercept for the model,

171
00:07:11,720 --> 00:07:13,190
and the second row represents

172
00:07:13,190 --> 00:07:15,620
information for the
slope in the model.

173
00:07:15,620 --> 00:07:17,345
The first column gives you

174
00:07:17,345 --> 00:07:19,775
the values for the
intercept and the slope.

175
00:07:19,775 --> 00:07:23,420
So again, the value of
one is what I had for

176
00:07:23,420 --> 00:07:27,500
a and 0.32 is the
value I had for b.

177
00:07:27,500 --> 00:07:29,060
The second column gives you

178
00:07:29,060 --> 00:07:31,609
the standard error for
each of these estimates,

179
00:07:31,609 --> 00:07:32,840
the third column gives you

180
00:07:32,840 --> 00:07:35,270
the ratio of the estimate
to the standard error,

181
00:07:35,270 --> 00:07:38,235
which becomes a test
statistic where a t-value,

182
00:07:38,235 --> 00:07:40,470
and then those t-values
are converted to

183
00:07:40,470 --> 00:07:43,835
a p-value from a standard
normal 0-1 distribution.

184
00:07:43,835 --> 00:07:47,750
Now again, inference regarding
the association of x and

185
00:07:47,750 --> 00:07:53,950
y is completely contained in
the second row of the table.

186
00:07:53,950 --> 00:07:57,470
There's a test
statistic of 10.82 and

187
00:07:57,470 --> 00:08:00,680
a corresponding p-value
that's less than 0.001.

188
00:08:00,680 --> 00:08:03,710
This p-value tells me to
reject the null hypothesis,

189
00:08:03,710 --> 00:08:05,990
and I find that there is
an association between

190
00:08:05,990 --> 00:08:09,730
firearm ownership and the
male firearm suicide rate.

191
00:08:09,730 --> 00:08:12,530
The information for
the intercept is not

192
00:08:12,530 --> 00:08:16,565
used to make inference about
x and y, and so therefore,

193
00:08:16,565 --> 00:08:18,170
these three values play

194
00:08:18,170 --> 00:08:21,365
no role in this course

195
00:08:21,365 --> 00:08:24,700
regarding assessing the
association of x and y.

196
00:08:24,700 --> 00:08:27,920
Now, the table also has
two values below it.

197
00:08:27,920 --> 00:08:30,110
The first on the left is
the mean squared error.

198
00:08:30,110 --> 00:08:32,330
This is the number that
I tried to minimize

199
00:08:32,330 --> 00:08:35,360
when I was fitting my
line through the data.

200
00:08:35,360 --> 00:08:38,525
The more important value
is the value on the right.

201
00:08:38,525 --> 00:08:41,345
This is a quantity
known as R-squared,

202
00:08:41,345 --> 00:08:44,710
and in this case
our value is 0.71.

203
00:08:44,710 --> 00:08:47,435
Now, technically
R-squared is called

204
00:08:47,435 --> 00:08:49,760
the coefficient of determination,

205
00:08:49,760 --> 00:08:52,400
and that tells us the
proportion of variation in

206
00:08:52,400 --> 00:08:54,140
our y variables that

207
00:08:54,140 --> 00:08:56,360
is explained by our
regression model.

208
00:08:56,360 --> 00:08:57,980
Again, the data were collected by

209
00:08:57,980 --> 00:08:59,390
Siegel and Rothman because

210
00:08:59,390 --> 00:09:02,434
the states all had
varying rates of suicide,

211
00:09:02,434 --> 00:09:04,580
and we were trying
to figure out why.

212
00:09:04,580 --> 00:09:08,090
We wanted to know if
the gun ownership had

213
00:09:08,090 --> 00:09:10,475
some ability to explain why

214
00:09:10,475 --> 00:09:13,130
the suicide rates varied
among the states,

215
00:09:13,130 --> 00:09:17,570
and R-squared tells us the
strength of that association.

216
00:09:17,570 --> 00:09:19,785
R-squared goes from 0-1,

217
00:09:19,785 --> 00:09:22,120
and the closer
R-squared is to one,

218
00:09:22,120 --> 00:09:26,600
the better my x variable
explains the variability in y,

219
00:09:26,600 --> 00:09:29,765
and the more the R-squared
value is to zero,

220
00:09:29,765 --> 00:09:32,840
that indicates less
explanatory power of

221
00:09:32,840 --> 00:09:36,310
that x variable for the
corresponding y variable.

222
00:09:36,310 --> 00:09:37,970
Now, the coefficient of

223
00:09:37,970 --> 00:09:40,384
determination is
called R-squared,

224
00:09:40,384 --> 00:09:41,930
because we know that for

225
00:09:41,930 --> 00:09:45,410
a given coefficient of
determination that value is

226
00:09:45,410 --> 00:09:47,390
exactly equal to the square of

227
00:09:47,390 --> 00:09:51,470
Pearson's correlation
coefficient, or r-squared.

228
00:09:51,470 --> 00:09:54,875
Now, if we start with a
coefficient of determination,

229
00:09:54,875 --> 00:09:56,645
we can take the square root,

230
00:09:56,645 --> 00:10:00,080
and that gives me the magnitude
of Pearson's correlation.

231
00:10:00,080 --> 00:10:01,970
However, I don't
know if I should put

232
00:10:01,970 --> 00:10:04,195
a positive or a
negative sign in front.

233
00:10:04,195 --> 00:10:06,440
But the way I do that is
to look at the slope,

234
00:10:06,440 --> 00:10:07,880
because the slope and

235
00:10:07,880 --> 00:10:11,045
Pearson's correlation
must have the same sign.

236
00:10:11,045 --> 00:10:13,970
Now, if Pearson's
correlation is zero,

237
00:10:13,970 --> 00:10:16,325
I've told you that the
slope must be zero,

238
00:10:16,325 --> 00:10:18,080
and we also know now that

239
00:10:18,080 --> 00:10:20,890
the coefficient of determination
must also be zero.

240
00:10:20,890 --> 00:10:24,635
X and y have absolutely no
association with each other.

241
00:10:24,635 --> 00:10:27,530
If that's true, then
any given value of

242
00:10:27,530 --> 00:10:31,490
x does not explain what
the value of y should be.

243
00:10:31,490 --> 00:10:35,090
X explains none of
the variability in y.

244
00:10:35,090 --> 00:10:36,950
There must be another x variable

245
00:10:36,950 --> 00:10:38,720
that I need to find in order to

246
00:10:38,720 --> 00:10:43,285
explain why the y variables
differ among the states.

247
00:10:43,285 --> 00:10:47,390
Now, R-squared is often
cited as the measure of how

248
00:10:47,390 --> 00:10:48,950
well a model fits

249
00:10:48,950 --> 00:10:51,725
the data or quantifies
the goodness-of-fit.

250
00:10:51,725 --> 00:10:55,315
As I said, as R-squared
goes from 0-1,

251
00:10:55,315 --> 00:10:57,470
we say that my x
variable provides

252
00:10:57,470 --> 00:11:01,190
more and more
information about y.

253
00:11:01,190 --> 00:11:03,860
So congratulations.
At this point,

254
00:11:03,860 --> 00:11:05,480
you have successfully reached

255
00:11:05,480 --> 00:11:08,090
the end of the material
in this course.

256
00:11:08,090 --> 00:11:11,120
But the regression
concepts that we've

257
00:11:11,120 --> 00:11:14,000
covered in this course
laid the foundation

258
00:11:14,000 --> 00:11:17,390
for many more advanced
regression methods that you

259
00:11:17,390 --> 00:11:19,235
might encounter in public health

260
00:11:19,235 --> 00:11:21,440
including multiple
linear regression,

261
00:11:21,440 --> 00:11:24,920
generalized linear models,
like logistic regression,

262
00:11:24,920 --> 00:11:26,585
and linear mixed models,

263
00:11:26,585 --> 00:11:30,456
which are used to analyze
correlated continuous data.

