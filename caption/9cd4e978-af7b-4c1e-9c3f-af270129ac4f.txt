1
00:00:00,420 --> 00:00:03,420
But as you say, what is it like? What can we make it into?

2
00:00:04,170 --> 00:00:09,380
We just know. Oh, okay.

3
00:00:09,390 --> 00:00:12,660
We can go with that for weeks. All right.

4
00:00:14,520 --> 00:00:24,810
Good afternoon, everybody. We have an economic cost and everybody thinks that after you hired everybody.

5
00:00:25,680 --> 00:00:29,610
All right. Let's go over a little bit of the exam one.

6
00:00:31,500 --> 00:00:36,480
I want to do it for three B, which is a prediction of random effects, which we called UPS.

7
00:00:37,770 --> 00:00:44,370
And then I'll go over the code that I have been developing for homework time for three that

8
00:00:44,370 --> 00:00:49,890
I'm hoping that gives you some idea of what you can do with random effects models in art.

9
00:00:52,350 --> 00:01:03,150
Canvas. In canvas. In the miscellaneous documents, there is a manuscript on how to use the package in our for mixed effects models.

10
00:01:03,750 --> 00:01:09,180
Let me ask you anyway.

11
00:01:09,210 --> 00:01:15,780
There's a document in there if you want to see more about what's available in that package and how you do things beyond what I show you in class,

12
00:01:16,530 --> 00:01:24,150
that's the best source of information. Again, in this class, we don't talk about derivation of these random effect variances.

13
00:01:24,480 --> 00:01:28,590
That document has some wonderful theory and development.

14
00:01:28,590 --> 00:01:33,300
If you want to see how people have kind of to estimate things they said, it's not trivial.

15
00:01:33,630 --> 00:01:39,180
And it took years and years and years of programing and development to make that happen for what we do today.

16
00:01:39,330 --> 00:01:46,440
So that's in there as well and would be covered probably in the course if these sorts of methods were not here.

17
00:01:48,060 --> 00:01:54,780
All right. When I told folks to say, what would you like me to go over on the exam,

18
00:01:56,610 --> 00:02:02,220
which would be the frontrunners were number four or number seven and number nine.

19
00:02:03,630 --> 00:02:11,370
I expected a couple of those. So I want to go through those three and go over maybe a couple others that people listed.

20
00:02:12,510 --> 00:02:16,320
If you have questions, your best source of information is office hours.

21
00:02:17,790 --> 00:02:21,210
If you can't come to sirs, I'm happy to meet other times.

22
00:02:21,270 --> 00:02:27,330
If you want to understand, if you want to discuss why you think your answers are right, I'm happy to hear that as well.

23
00:02:28,020 --> 00:02:35,610
If your answer is my answer is right because I want two points. That's not a very good argument, but I am very open.

24
00:02:36,090 --> 00:02:41,430
I mean, I've looked at a couple of these problems that people said, I want to go over. I can see where the questions lie, perhaps.

25
00:02:42,420 --> 00:02:48,820
So if you're puzzled and you think your answer is right, I'm happy to discuss it.

26
00:02:49,620 --> 00:02:57,419
So keep that in mind. Question number four. All right.

27
00:02:57,420 --> 00:03:10,710
Question four. I asked folks which of the following is true for restricted maximum age range?

28
00:03:10,740 --> 00:03:14,969
And I said that of the four responses, three of them are right answers.

29
00:03:14,970 --> 00:03:18,490
So the wrong one was that Remo should never be used in small samples of data.

30
00:03:19,740 --> 00:03:26,490
I don't believe anybody selected that that I can recall when I was going through the exams, so I don't think the issue lies there.

31
00:03:26,910 --> 00:03:30,690
Rebel is used to produce unbiased estimates of variance parameters.

32
00:03:31,950 --> 00:03:39,089
So was wrong one. Yeah.

33
00:03:39,090 --> 00:03:47,670
There you go. Sorry. Lecture two E. Uh, it's the best way to do this.

34
00:03:48,830 --> 00:03:55,710
So I just. He.

35
00:04:05,130 --> 00:04:15,430
So I sent a letter to a page ten to the letter, page nine, page ten link.

36
00:04:16,130 --> 00:04:25,510
Right. There is the likelihood we use for determining estimates for the variance and correlation parameters.

37
00:04:25,510 --> 00:04:31,840
Omega in everything that we did was conditional upon knowing what the value of beta is.

38
00:04:31,850 --> 00:04:36,670
So it's all conditional on beta at the fixed, what I call fixed effects parameters.

39
00:04:38,890 --> 00:04:43,000
Somebody came to me and after the exam, before I graded it, said,

40
00:04:43,000 --> 00:04:46,690
I just want to make sure that you by fixed effects, you didn't mean that it was fixed.

41
00:04:46,960 --> 00:04:50,980
Because I told you that removal is an iterative process, right?

42
00:04:51,430 --> 00:04:56,290
You get Omega, you get better, you go back to get Omega, head back and forth, back and forth.

43
00:04:56,590 --> 00:05:02,350
I didn't mean that beta head was fixed. I meant to use the terminology that we use in regression,

44
00:05:02,350 --> 00:05:08,140
that the mean parameters are called fixed defects and all of the other effects are called random effects.

45
00:05:08,560 --> 00:05:15,460
So I don't know if that's where the confusion lay for some people, but I did not intend for that to be the case.

46
00:05:16,450 --> 00:05:25,600
So everything we do in Remo for estimating the variance parameters and the correlation parameter is conditional upon what the fixed effects were.

47
00:05:26,050 --> 00:05:30,400
That's the whole point. We come up with a different likelihood that was penalized like the.

48
00:05:38,070 --> 00:05:42,750
In large samples, Rummel estimates. In a maximum likelihood, estimates are nearly equal.

49
00:05:46,740 --> 00:05:48,810
So I thought I said that in the lectures.

50
00:05:49,320 --> 00:05:59,970
And as I went through my lecture notes, what I said on page 13 was that beta had for maximum likelihood and beta from rebel for consistent unbiased.

51
00:06:01,140 --> 00:06:06,660
So that means that they're going to be nearly equal in large samples. I didn't say anything about the Omega.

52
00:06:06,690 --> 00:06:18,629
That's. Therefore the various covariance parameters for maximum likelihood and removal they ve nearly even large samples and they are again.

53
00:06:18,630 --> 00:06:23,790
The important thing to remember is I didn't go through this in great detail, but if you go back to ordinarily squares,

54
00:06:25,170 --> 00:06:30,600
we've taught you that to estimate the residual variance, you divide by n minus p, not n.

55
00:06:31,530 --> 00:06:34,790
That is a remo concept, right?

56
00:06:35,160 --> 00:06:41,700
So whether you use an in the denominator or n minus p is irrelevant in large samples because the denominator,

57
00:06:41,700 --> 00:06:50,760
for all intents and purposes, is the same number. So that is the the important concept there is that Remo is an issue in small samples.

58
00:06:54,310 --> 00:06:57,640
That are conditional upon the estimates. Right.

59
00:06:58,270 --> 00:07:03,010
And we're trying to get rid of bias. We're trying to get rid of bias when we do estimation.

60
00:07:03,040 --> 00:07:07,240
So that was what my train of thought was on those four, four points there.

61
00:07:08,650 --> 00:07:18,129
That helps. Whoever wanted to talk about this or further discussion is always produced.

62
00:07:18,130 --> 00:07:23,170
Unbiased estimates of variance parameter rumble produces an unbiased estimate.

63
00:07:23,440 --> 00:07:31,930
Does it always happen or. It's it's used to produce a less a less biased estimate of the variance.

64
00:07:32,880 --> 00:07:37,890
I believe it always produces an unbiased estimate, not less biased.

65
00:07:38,190 --> 00:07:43,300
Now, remember, as I keep saying in my class, that's really great.

66
00:07:44,370 --> 00:07:50,790
Across many, many samples of data, across many, many samples of data, we get unbiased results.

67
00:07:51,420 --> 00:07:54,660
That doesn't mean your result is unbiased or is right.

68
00:07:54,990 --> 00:07:59,100
There's a distribution. All of our estimates have an average.

69
00:07:59,100 --> 00:08:03,180
That is the right answer. Some of us are too high. Some of us are too low.

70
00:08:03,540 --> 00:08:07,860
Right. So it doesn't mean you're always going to get a better answer.

71
00:08:08,580 --> 00:08:15,420
It just means we have a better process. Yes, I have a similar questions.

72
00:08:16,050 --> 00:08:24,570
So I have referred to the textbook called the longitudinal analysis, which is the textbook for another section.

73
00:08:24,880 --> 00:08:31,200
Yup. And in this book, this the ramble is not always advised for various parameters.

74
00:08:31,380 --> 00:08:37,590
It is can it can produce last virus parameter demand in math.

75
00:08:37,770 --> 00:08:45,230
Okay. So I think the idea of. So it's it's so it's context dependent.

76
00:08:46,980 --> 00:08:50,540
Sometimes I'm biased. Sometimes it's less biased. Yes.

77
00:08:50,540 --> 00:09:02,360
So when I do this, I'm not sure when I do this question, but after a third exam, I go to this textbook and I think it's always unbiased.

78
00:09:02,850 --> 00:09:06,990
Okay. Oh, so there's something I have forgotten, but.

79
00:09:07,500 --> 00:09:11,340
Yeah, I'm not sure. Is it the textbook that I gave you a copy of?

80
00:09:12,210 --> 00:09:18,840
Or is it a different one? The really series. Did you find it from canvas or from somewhere else?

81
00:09:20,730 --> 00:09:25,780
Do you remember, Jim, and is part of the analysis?

82
00:09:25,840 --> 00:09:32,970
I who are the authors? But Maurice Fitzmaurice location where?

83
00:09:32,980 --> 00:09:36,250
It's okay. It's me. Okay.

84
00:09:37,480 --> 00:09:42,400
So it's not always unbiased. That you're right.

85
00:09:42,410 --> 00:09:45,700
It's less biased. Not necessarily biased.

86
00:09:46,330 --> 00:09:54,880
It's been a long time since I studied Remo. Okay. I'm not sure whether this sentence makes it always produce unbiased or not.

87
00:09:55,120 --> 00:10:02,950
Well, there is. What is my sentence mean? I'm sure we use it to produce unbiased estimates.

88
00:10:03,700 --> 00:10:07,970
Yeah. Does it produce? Always produce them? When I see this, I.

89
00:10:09,250 --> 00:10:13,750
I think it means it always produce unbiased, which is around for me.

90
00:10:13,750 --> 00:10:19,240
So I don't think. Okay, so I buy that argument so I can go.

91
00:10:19,330 --> 00:10:22,930
This is the beauty of a programed exam tube. I don't have you guys.

92
00:10:22,930 --> 00:10:26,380
I'll turn your exams back on so I'll fix that one.

93
00:10:26,470 --> 00:10:29,620
I thought it was always I'm biased, but it's to reduce bias.

94
00:10:30,880 --> 00:10:41,200
Cool. Okay, so if you picked that first one, if you didn't pick the first one, although they were promoted for each person, what did I say there?

95
00:10:41,260 --> 00:10:44,469
Well, that'll affect your grades somehow. Or this is one of those.

96
00:10:44,470 --> 00:10:48,670
That is three. Oh, I did manually do that.

97
00:10:48,730 --> 00:11:01,690
Okay. Number four. There we go. One of the first things, which is to.

98
00:11:04,490 --> 00:11:07,850
You owe so much. This is how much I study, Remo.

99
00:11:07,880 --> 00:11:11,000
[INAUDIBLE] notice I put very levels of San Remo in our analyzes.

100
00:11:12,980 --> 00:11:19,280
And again, remember that everything we do with our the default, I believe, is rumble goals.

101
00:11:19,970 --> 00:11:31,400
Alamy the default is rebel. It's not that because in large samples we're basically doing the same thing, in my opinion.

102
00:11:31,530 --> 00:11:35,400
So but I do want to give credit where credit is due.

103
00:11:35,420 --> 00:11:40,010
So I will fix that. Thank you. When?

104
00:11:40,970 --> 00:11:48,630
What I say seven. He was good to me.

105
00:11:49,830 --> 00:11:54,570
So I thought I wrote the answer clearly for number seven and the answer key,

106
00:11:54,900 --> 00:12:01,950
although I had the wrong my wrong answer in that box, but the right answer is up in the point totals.

107
00:12:03,180 --> 00:12:09,669
Again, I don't know if someone wants to discuss this further. This was my very first set of lecture notes.

108
00:12:09,670 --> 00:12:16,740
Lecture one and believe what I wrote. Lecture one and page eight and page 12.

109
00:12:17,610 --> 00:12:25,710
I talked about the fact that when we have Perry in a two sample group comparison, the variance is always reduced by one minus row.

110
00:12:27,510 --> 00:12:32,340
So when I had the standard error of 1.5, I had to reduce it by the square root of one minus row.

111
00:12:34,300 --> 00:12:37,690
And that produces 1.031.06. Excuse me.

112
00:12:38,940 --> 00:12:42,960
I was flattered that two individuals on the exam got the wrong answer, that I had an answer.

113
00:12:44,220 --> 00:12:48,510
So I'm not the only one who is not quite remembering where the square root.

114
00:12:48,610 --> 00:12:51,810
But anyway, so that was the point of that problem.

115
00:12:52,110 --> 00:12:56,439
So the really important concept. To know about longitudinal data.

116
00:12:56,440 --> 00:13:02,099
That's the whole reason we measure people over time. The correlation does give us some benefit.

117
00:13:02,100 --> 00:13:09,120
It gives us some efficiency. So standard errors get reduced in the setting.

118
00:13:10,500 --> 00:13:15,030
And then question number nine. This was my wild card question.

119
00:13:16,150 --> 00:13:21,270
It was I went back and forth several times on whether I should ask this or not.

120
00:13:22,950 --> 00:13:28,530
It was never explicitly in my lectures, but it was there implicitly.

121
00:13:32,400 --> 00:13:37,290
Let's see if I can say this verbally. I have some notes. Plot B is independence.

122
00:13:38,640 --> 00:13:44,910
So remember what correlation is. Correlation is a ratio of covariance to variance.

123
00:13:47,070 --> 00:13:51,750
How do observations vary with each other in a person relative to how noisy are they?

124
00:13:52,560 --> 00:13:58,950
And we're getting into this deeper now with random effects. I hope you're seeing how random effects connect everything we're doing.

125
00:14:00,470 --> 00:14:04,940
But Plan B, again, everybody's data is on top of everybody else.

126
00:14:05,690 --> 00:14:08,690
Correlation. We should see some sort of clustering by person.

127
00:14:08,690 --> 00:14:14,180
We should see distinct colors clustering together or distinct dots of the same color clustering together.

128
00:14:14,840 --> 00:14:23,000
So plan B is definitely independence because there we just don't see any relationship at all within a person's observations.

129
00:14:23,930 --> 00:14:26,330
And I think the majority of folks got that.

130
00:14:26,840 --> 00:14:32,420
And then I figured plot and plot C would be the two challenges because we hadn't really talked about this, but.

131
00:14:35,190 --> 00:14:38,250
It's again, a ratio of covariance to variance.

132
00:14:40,650 --> 00:14:47,410
We see less covariance in plots. See? As time goes on.

133
00:14:47,860 --> 00:14:51,130
My observations aren't as similar as they used to be.

134
00:14:52,270 --> 00:14:56,140
The other issue that's stronger here is that variance is growing over time here.

135
00:14:56,710 --> 00:15:01,360
The denominator of correlation is getting small is getting bigger and bigger and bigger.

136
00:15:03,320 --> 00:15:08,600
And so the correlation is just necessarily going to get smaller as we move along the X axis.

137
00:15:09,660 --> 00:15:16,559
And so plan C is auto regressive. And that leads to A to B compound symmetric.

138
00:15:16,560 --> 00:15:22,530
Again here, the variance is pretty constant over time. And there's a lot of covariance.

139
00:15:22,910 --> 00:15:28,190
My observations go with each other covering quite a bit and so do the other ones.

140
00:15:28,830 --> 00:15:32,630
So what a is exemplary of compound symmetric.

141
00:15:36,790 --> 00:15:42,520
This also relates to what we're doing now. Plot A if we want to start doing random effects.

142
00:15:43,030 --> 00:15:50,200
Plot I would fit a random intercept model. Everybody appears to have a different intercept, but similar slopes over time.

143
00:15:50,710 --> 00:15:57,490
And remember I told you that compound symmetry in a random random intercept model are the same thing.

144
00:15:58,650 --> 00:16:08,400
So that's why those two go together. Likewise, in Plot C, I might fit a random intercept random slope model because people have them model.

145
00:16:08,400 --> 00:16:11,430
They have the same intercept here. So maybe just a random slope model.

146
00:16:12,930 --> 00:16:19,080
A random slope model has variance that increases over time and it mimics an air one structure.

147
00:16:19,560 --> 00:16:24,270
So random slope models are related. So auto regressive type correlations.

148
00:16:24,750 --> 00:16:30,810
So, so those are the kind of plots we would expect Inglis data that we would fit as well.

149
00:16:31,500 --> 00:16:37,710
So but it all has to do with if we truly understand how correlation is a ratio of covariance and variance.

150
00:16:39,420 --> 00:16:45,990
And enough people got this right that I didn't feel like I was being a big jerk by putting it in the exam.

151
00:16:47,010 --> 00:16:52,530
I hope that wasn't by guessing. I imagine maybe some people got a plan B and then punted on the other two.

152
00:16:53,160 --> 00:16:56,190
But there were enough people that I didn't think it was random.

153
00:16:57,420 --> 00:17:02,610
So again, if most people had gotten this wrong, I would have done something about it.

154
00:17:02,730 --> 00:17:08,070
But I felt like something was going on that was was positive with that problem solver.

155
00:17:08,150 --> 00:17:12,420
So I left it as it was, at least gave it four points for getting one of them.

156
00:17:13,200 --> 00:17:17,040
And then, of course, you either got the other two or you didn't.

157
00:17:19,090 --> 00:17:22,560
I mean, just one other.

158
00:17:24,240 --> 00:17:29,040
I was looking at number 11. So so at least one person I looked at the polls at number 11.

159
00:17:31,140 --> 00:17:34,140
Again, this was we're looking at some data here.

160
00:17:34,140 --> 00:17:38,400
White blood cells, white blood cell counts, positively correlated.

161
00:17:39,240 --> 00:17:42,480
We've got two groups of individuals. We know there's positive correlation.

162
00:17:42,810 --> 00:17:48,510
Which of these designs would give us the most statistical power, the most efficiency for comparing these two groups?

163
00:17:50,100 --> 00:17:58,950
And as I looked at this, I wondered if some of you were thinking of my simulations and I was thinking about ignoring the correlation or not,

164
00:17:58,950 --> 00:18:05,130
and whether it was between or within person effect. And maybe that's where things went astray because that doesn't apply here at all.

165
00:18:08,550 --> 00:18:16,740
The whole issue here is, again, the correlation. This one minus row concept of correlation is a good thing when it's positive.

166
00:18:18,840 --> 00:18:24,180
And so I tried to order these break. Obviously, you want as much correlation as possible.

167
00:18:24,960 --> 00:18:29,310
So that's why it's that last choice, at least in the order I have here.

168
00:18:29,960 --> 00:18:37,080
Right. So the correlation coefficient a point for you, you benefit a little bit by having additional time points.

169
00:18:38,820 --> 00:18:42,899
But it didn't have a point for eight with three longitudinal. I didn't want to get into that issue.

170
00:18:42,900 --> 00:18:49,110
But so far, in a way, the important thing here is that you've got a correlation coefficient that's bigger than all the rest.

171
00:18:52,400 --> 00:18:55,970
If you're taking clinical trials, I think Kelly will cover this if she hasn't already.

172
00:18:56,870 --> 00:19:01,070
The second option, there are 70. So both all four options have 100 people.

173
00:19:02,710 --> 00:19:10,270
Again, one of the most important things you need to have in your brain when you work with other individuals is that when you are comparing two groups,

174
00:19:11,260 --> 00:19:16,540
the best study design in terms of efficiency is equal in both groups 5050.

175
00:19:18,460 --> 00:19:22,150
As soon as you start to imbalance the size of the two groups, you lose power.

176
00:19:22,160 --> 00:19:28,979
You lose efficiency. And again, for someone who struggles with that concept with you, I just say,

177
00:19:28,980 --> 00:19:36,000
think of a study which I have 2000 people in my group and two in the other group, and I figure out if anything's going on between those two groups.

178
00:19:36,010 --> 00:19:38,720
No. Right. That's an extreme example.

179
00:19:38,730 --> 00:19:43,770
But the best way to figure out if two groups are different is they have the same amount of information about groups.

180
00:19:44,340 --> 00:19:48,840
So but again, that had little correlation. So I wasn't getting into that concept as much.

181
00:19:49,680 --> 00:19:54,750
So again, it just has to do with correlations, a good thing, and the more of it that's positive.

182
00:19:55,470 --> 00:20:01,420
Again, as humans, we expect correlation to be positive. Yeah.

183
00:20:01,770 --> 00:20:10,110
So for correlation, what it's just like if it's more greater, it's better because like this, all my opinions,

184
00:20:10,350 --> 00:20:18,120
it's like if you have more longitudinal outcomes, if the correlation is really big, then you can predict one.

185
00:20:18,120 --> 00:20:21,210
I'll count based on another outcome, like kind of.

186
00:20:21,870 --> 00:20:24,899
So the use for like the use for information.

187
00:20:24,900 --> 00:20:30,420
Information for each outcome is small if the correlation is big, if the correlation is big.

188
00:20:30,450 --> 00:20:33,750
Each additional observation produces less information. Yeah.

189
00:20:33,850 --> 00:20:40,170
Yeah. And then that's produce less information than if you compare two groups.

190
00:20:40,590 --> 00:20:47,850
And so then the for a longitudinal outcomes was 0.48.

191
00:20:47,940 --> 00:20:54,880
Yep. Correlation will have less information compared to the one with 0.23 correlation of extreme

192
00:20:54,890 --> 00:21:01,230
prevention because that is the was in terrorism correlation is almost twice than the previous one.

193
00:21:01,230 --> 00:21:07,890
Correct. But what is the variance of your observations relative if you have a correlation of way to three?

194
00:21:08,580 --> 00:21:15,059
Right, the variance of your average value is going to be bigger than when the correlation is played.

195
00:21:15,060 --> 00:21:23,219
Four eight. There's that one minus row benefit. Yeah, that's like the currency of the prediction.

196
00:21:23,220 --> 00:21:27,450
I mean, it's not like the the smaller the p value is, the better.

197
00:21:27,990 --> 00:21:34,770
I think it's the more information you get from the data is better will that will produce a smaller p value.

198
00:21:36,720 --> 00:21:41,700
More information should leave you with more efficiency. Therefore the smaller p value.

199
00:21:43,880 --> 00:21:47,630
So we can discuss this more. If you again, I am always happy to be convinced that I'm wrong.

200
00:21:49,560 --> 00:21:52,190
Goodbye. Just related shootings.

201
00:21:52,220 --> 00:21:58,750
I was also thinking along this line that you had perfect correlation and there's almost like one off in person, right?

202
00:21:59,210 --> 00:22:06,320
But yeah, I agree that there's more noise if you have lower correlation than the average, the average, the very average.

203
00:22:06,620 --> 00:22:10,639
So your degrees of freedom is reduced, right? Perfect correlation.

204
00:22:10,640 --> 00:22:15,110
You don't have three times and you have an.

205
00:22:15,590 --> 00:22:23,600
Yeah, but if the correlation is almost zero, then you have for a longitudinal outcome for each person and you have 50%,

206
00:22:23,600 --> 00:22:31,850
then you will like have 50 times for like 200% because the correlation is almost zero.

207
00:22:32,150 --> 00:22:38,180
You will have four times the degrees of freedom, but you will have less efficiency.

208
00:22:39,900 --> 00:22:45,960
So let me work on that some more. They don't want to spend half an hour debating with you guys that saying that it's not worthwhile.

209
00:22:46,710 --> 00:22:50,440
But I hear your thoughts.

210
00:22:50,460 --> 00:22:54,380
And what I'm struggling with is the degrees of freedom. Again.

211
00:22:54,380 --> 00:23:01,150
What what you were saying and what I'm saying are the same thing. So there's two things going on.

212
00:23:02,590 --> 00:23:11,790
As I said earlier, in class, when you start getting correlated data, you increase the total number of observations which you don't add,

213
00:23:11,860 --> 00:23:20,250
which is the degrees of freedom, but you don't increase the total number of independent observations every time because of the correlation.

214
00:23:20,260 --> 00:23:27,010
You get less information with each additional observation. I still think it should be more efficient.

215
00:23:31,560 --> 00:23:38,100
I'm going to do some simulations. I mean, at some point, if you had like a thousand out of them.

216
00:23:38,220 --> 00:23:42,900
I agree. I mean, the thing was the the variance of the sample mean a sigma squared over n.

217
00:23:43,110 --> 00:23:47,000
So at some point the variance does. I agree with you, right?

218
00:23:47,010 --> 00:23:52,240
If I just wanted to study weight, I would study me a thousand times, right?

219
00:23:52,660 --> 00:23:57,990
Yes. But that's not a sample size of of a 10,000. That's a sample size of one, right.

220
00:23:58,260 --> 00:24:03,990
Yeah. So. You're looking at me like I'm totally nuts.

221
00:24:06,030 --> 00:24:09,280
So I want to talk.

222
00:24:09,290 --> 00:24:17,390
I want to I want to think about this some more. I thought it was a really cool question, but now you're making me in my own my own feeling.

223
00:24:18,840 --> 00:24:29,340
All right. Again, if you have thoughts like that, I am more than open to them if you want to write them down, if I'd rather talk with you in person.

224
00:24:30,210 --> 00:24:33,840
But I'm always open for discussion on this stuff.

225
00:24:38,390 --> 00:24:43,020
I'm going to use some terms today in my R code that relates to lecture two.

226
00:24:44,490 --> 00:24:45,780
Lecture three, be.

227
00:24:48,180 --> 00:24:57,450
So I would like to cover this today and I'm going off my plan of the semester, but I do want to get these concepts out in front of you in class.

228
00:24:59,500 --> 00:25:02,710
Before you start doing homework. Number three, write.

229
00:25:06,690 --> 00:25:09,719
So I'm going to show you the linear mixed model.

230
00:25:09,720 --> 00:25:16,770
And again, in terms of matrix notation, if you don't like matrix algebra again,

231
00:25:16,860 --> 00:25:20,640
you know, try to get yourself more comfortable with it, at least for another semester.

232
00:25:21,990 --> 00:25:29,790
We have a vector of observations. Why for a person I they are a mixture of fixed effects.

233
00:25:29,790 --> 00:25:35,489
So covariates x times beta a some random things, right?

234
00:25:35,490 --> 00:25:39,930
So again, z is usually an intercept. Maybe it's time, maybe it's something else.

235
00:25:40,530 --> 00:25:47,310
Times This VII is not a parameter, it has variance and then error terms.

236
00:25:48,600 --> 00:25:54,150
So because of all the assumptions we make, the random effects are independent of the errors.

237
00:25:55,450 --> 00:25:59,050
We know that the following to explain everything has been zero, right?

238
00:25:59,500 --> 00:26:07,329
That the mean of the wages is just the fixed effects excited data and the variance is a partitioning.

239
00:26:07,330 --> 00:26:11,320
That's a sum of two components it ZC transpose.

240
00:26:12,400 --> 00:26:18,010
That is the effect of the random intercept, random slope, other random parameter, other random effects.

241
00:26:18,010 --> 00:26:24,510
Excuse me. So again, that's between individuals and then there is an array.

242
00:26:24,520 --> 00:26:33,870
Again, usually our eye is an identity matrix times some variance parameter, but the sum of those two matrices give me the overall variance of the Y.

243
00:26:33,880 --> 00:26:38,950
I say so our eye is the variance of the residuals.

244
00:26:40,150 --> 00:26:47,080
That is no longer the variance of y is because we have another partitioning of both the random effects and the overall variance of sigma.

245
00:26:47,080 --> 00:26:51,840
I was. I fixed that.

246
00:26:51,850 --> 00:26:55,270
I know I fix that. Oh, no, answer the sunglass. This is a review.

247
00:26:55,270 --> 00:27:01,570
Okay. And GLS, right. The variance of the residuals is sigma I because there are no random effects.

248
00:27:01,990 --> 00:27:07,270
But once we get into an alarm again, we're partitioning the overall noise in the data into two pieces.

249
00:27:07,540 --> 00:27:12,370
How do we vary between each other and how each person's observations vary within a person?

250
00:27:15,590 --> 00:27:18,980
You can have random effects and correlated errors.

251
00:27:19,730 --> 00:27:27,000
You could have both. I have yet to see anybody in a general setting to do both.

252
00:27:27,480 --> 00:27:35,340
Either you do a random effects model with independent ears, or you do a glass bridge and you don't care about random things.

253
00:27:36,420 --> 00:27:43,770
But you could do both. You could find some very interesting setting where perhaps you want to auto regressive errors and also have random effects.

254
00:27:45,030 --> 00:27:50,730
And I would guarantee you, outside of longitudinal data that probably is used somewhere, but not in longitudinal data.

255
00:27:54,260 --> 00:27:55,940
Why do we use linear mix models?

256
00:27:56,360 --> 00:28:05,150
Because we might be interested in estimating this between subject variability separate from other within subject variability.

257
00:28:05,780 --> 00:28:08,810
So we may want to figure out what is my conditional mean.

258
00:28:09,470 --> 00:28:19,220
So the population mean is Zubaydah, but my own mean trajectory is that that mean plus some deviation above or below.

259
00:28:20,540 --> 00:28:27,019
And once I condition on my random effects, the only variance between everybody is the right matrix,

260
00:28:27,020 --> 00:28:30,130
which is usually just sigma squared times an identity.

261
00:28:30,980 --> 00:28:39,140
So this Z eight Times VII quantifies the difference between the mean of an individual of random effects by and the mean of the entire population.

262
00:28:39,170 --> 00:28:46,309
So now the way average is said. But we're never going to get those things because we don't know what fire is.

263
00:28:46,310 --> 00:28:50,090
It's random, it is unobserved, we don't have a number for it.

264
00:28:50,900 --> 00:28:58,910
So we have to come up with something. We have to estimate it or predict it, as we say, using the data we have at hand.

265
00:29:00,890 --> 00:29:02,630
And again, the rand effects are useful.

266
00:29:02,840 --> 00:29:09,680
They quantify the unknown characteristics of each individual when we fit a random index model, maybe a random intercept model.

267
00:29:10,280 --> 00:29:14,509
And there's this group of ten people who have really large intercepts relative to everybody else.

268
00:29:14,510 --> 00:29:19,400
That's interesting. Why are those ten people so different in their intercepts from everybody else?

269
00:29:19,790 --> 00:29:24,680
It's probably due to some latent factors. Can we then go back and figure out what those latent factors might be?

270
00:29:25,100 --> 00:29:28,940
Maybe we didn't measure some characteristic. Well.

271
00:29:29,880 --> 00:29:33,630
So basically, what's in my brain and what's on my slider, the same thing. All right.

272
00:29:34,470 --> 00:29:37,860
But the bizarre to get in are not parameters there.

273
00:29:37,860 --> 00:29:41,550
Variance is a parameter. Is a. Is it is it a fixed value?

274
00:29:42,240 --> 00:29:49,080
But these guys are random, so we can't estimate them. But we're going to give a prediction as to what we think they could be based upon the data.

275
00:29:50,370 --> 00:29:54,220
So if you have something, you don't know what it is. What do we do?

276
00:29:54,250 --> 00:29:59,080
Usually we say, well, let's plug in the average. It doesn't really work here.

277
00:29:59,440 --> 00:30:04,570
The average zero, we plug it in zero everywhere. Well, we're back to the population being that for everybody.

278
00:30:05,530 --> 00:30:10,719
But one possible solution is to predict these random effects using the data we have,

279
00:30:10,720 --> 00:30:17,500
which are the outcomes different people with different outcomes should maybe have different slopes or different intercepts.

280
00:30:18,490 --> 00:30:25,270
And so we have we call it prediction. So instead of a hat, we have a tilde in much sooner or later we talk about tilde by.

281
00:30:26,170 --> 00:30:32,440
And so that is on average what is what should my random effect be on average, given the data that I have?

282
00:30:33,230 --> 00:30:37,240
Okay. So how do we figure out a conditional mean?

283
00:30:38,380 --> 00:30:41,500
Well, we saw that. And I don't think you even need six of the two.

284
00:30:41,500 --> 00:30:46,690
I think you need six. So one, let's get rid of all of the covariates.

285
00:30:47,050 --> 00:30:53,530
Let's just we have an overall population mean so there's no exit beta everybody has the same mean doesn't vary by anything.

286
00:30:54,740 --> 00:30:56,360
And we have a random intercept.

287
00:30:56,870 --> 00:31:03,230
So there's a population average and then everybody's got an average above or below that, depending on their random effects, and then there's error.

288
00:31:03,890 --> 00:31:08,840
So we assume that this random intercept essentially has mean zero.

289
00:31:09,320 --> 00:31:13,160
Again, there's a matrix, but in this case, we only have one random affected sun.

290
00:31:13,160 --> 00:31:19,490
It's a scalar, it's a number. And the errors, again, there's a vector of errors.

291
00:31:20,830 --> 00:31:25,990
And they have a mean zero, Victor. And again, we're going to assume they're independent within a person.

292
00:31:27,130 --> 00:31:32,920
So to figure out what by tilde is the expected value of a random effect given the outcomes,

293
00:31:32,920 --> 00:31:37,540
why, we need to know the conditional distribution of these of these things.

294
00:31:38,140 --> 00:31:42,129
And so how do we find the conditional distribution? What do we have?

295
00:31:42,130 --> 00:31:49,600
We have the marginal distribution of b i. We have the conditional distribution of YRI given by.

296
00:31:51,630 --> 00:32:00,510
So we can go back to 601 really? 602 because we know that to reverse the conditioning,

297
00:32:01,620 --> 00:32:12,090
the distribution of the air can on Y is simply the product of Y given by times the distribution API divided by this normalizing constant,

298
00:32:12,540 --> 00:32:15,929
the distribution of Y. And so again,

299
00:32:15,930 --> 00:32:20,759
this conditional distribution is proportional to the two things we have to have

300
00:32:20,760 --> 00:32:26,550
y i given V that's normal and we have the distribution of B and that's normal.

301
00:32:28,700 --> 00:32:32,030
So what is the conditional distribution of VII given? Why at.

302
00:32:35,640 --> 00:32:40,680
You'll be taking 682 right now. You got a normal times.

303
00:32:40,680 --> 00:32:43,530
A normal? What's the other distribution when it's out?

304
00:32:43,560 --> 00:32:52,830
Dr. Johnson So again, patients would view this as a typical Bayesian equation where this is the posterior.

305
00:32:55,350 --> 00:32:58,890
This is the likelihood. And that is the prayer.

306
00:33:01,410 --> 00:33:08,550
And if we have a normal likelihood and we have a normal prayer, that's the posterior.

307
00:33:09,360 --> 00:33:11,400
It's normal, right?

308
00:33:11,610 --> 00:33:18,570
Conjugate the whole idea of controversy, which even if you're not seeing 62, I believe you're taught in 602 that just about what I meant.

309
00:33:19,710 --> 00:33:19,950
Right.

310
00:33:19,950 --> 00:33:29,879
So using Bayesian concepts because again, because why I give them by as normal and by as normal then by is conjugate at the distribution of base,

311
00:33:29,880 --> 00:33:37,650
conjugate to the conditional distribution of life. And therefore, we know that this thing must be normal.

312
00:33:39,100 --> 00:33:43,690
But that doesn't do me any good yet. I need the men of that distribution.

313
00:33:44,140 --> 00:33:49,390
So sappy I give away is a normal distribution. In order to predict to be, I have to figure out what the mean.

314
00:33:49,390 --> 00:33:54,850
As does Dr. Johnson make you do the security of this horrible algebra?

315
00:33:55,630 --> 00:33:59,500
Yes. That's like the classics. You know, he does such great.

316
00:34:00,070 --> 00:34:07,420
If I have a normal times, a normal and the next one is normal, it's ugly but so worthwhile.

317
00:34:07,510 --> 00:34:13,899
So here we go. So we have a kernel of something that looks like a density.

318
00:34:13,900 --> 00:34:21,580
Remember, we took away the normalizing constant. So we're going to see if something looks like a normal kernel and then we can figure

319
00:34:21,580 --> 00:34:25,600
out what the normalizing constant is because it's defined by what the kernel is.

320
00:34:26,230 --> 00:34:30,459
So this distribution would be a given, which again is the product of two normals.

321
00:34:30,460 --> 00:34:35,800
I've written them on the second line there, so I should say it's proportional right to those two, the product of those two.

322
00:34:36,400 --> 00:34:43,900
So again, f of why I given by is all of this stuff right here.

323
00:34:45,190 --> 00:34:54,070
This is just a normal multivariate, normal distribution of a bunch of independent observations and there is the normal kernel for b i.

324
00:34:56,630 --> 00:35:03,890
So then I'm going to put everything into the exponential. I can get rid of this and I can get rid of this because I'm back to proportionality.

325
00:35:03,890 --> 00:35:08,180
I don't care about all these other things that are floating around. So I end up with the exponential.

326
00:35:08,660 --> 00:35:17,630
And I has completed the square here because again, I'm looking for something that looks like a distribution for VI conditional on the wires.

327
00:35:18,320 --> 00:35:25,639
And so I've put all this in here. So does this look like a normal distribution to me with a specific mean and variance?

328
00:35:25,640 --> 00:35:26,120
Not yet.

329
00:35:26,960 --> 00:35:34,430
But again, the way I approach this, I know that in general, if I have a random variable that's normally distributed with mean M and variance v.

330
00:35:35,610 --> 00:35:39,090
Then the density of X as a kernel. Right.

331
00:35:39,390 --> 00:35:44,580
One thing I'm interested in is this stuff in the exponential. And I can write that as a quadratic equation.

332
00:35:44,700 --> 00:35:48,330
It's the exponential of eight times X squared plus B, times x plus C.

333
00:35:49,720 --> 00:35:54,430
And a is a function of the variance. B is the ratio of the mean in the variants.

334
00:35:55,090 --> 00:36:01,780
And from those I know that the mean if I have if I just give you I say this is the

335
00:36:01,780 --> 00:36:05,980
kernel of a normal distribution and maybe it's three x squared minus two X plus one.

336
00:36:07,040 --> 00:36:11,390
If I give you that, you can immediately tell me what the mean is of that distribution.

337
00:36:12,080 --> 00:36:19,400
It's negative B over to A. So there's a negative B, there's a B and A to A and here to be in an A.

338
00:36:19,880 --> 00:36:27,800
So I have to figure out what those two things are. And so in the linear mix model, we have this kernel here again and I start to combine.

339
00:36:28,340 --> 00:36:31,610
Right. There's a squared term of VII. That's what I'm interested in.

340
00:36:32,630 --> 00:36:36,890
There's a B and then there's everything else that doesn't involve BI.

341
00:36:36,890 --> 00:36:44,180
That's your capital C. And so with some algebra, I see that capital takes that form, capital B takes that form.

342
00:36:45,770 --> 00:36:53,719
And so then if I take negative B over to A and do a little algebra, there's the mean of this normal distribution and I could get the variance.

343
00:36:53,720 --> 00:36:56,930
I don't care about, I don't care about everything about the distribution.

344
00:36:57,260 --> 00:37:01,250
I just cared about the normality because it allowed me to get to the mean.

345
00:37:02,400 --> 00:37:13,370
There it is. So it's some positive fraction times the difference between my average and the population average.

346
00:37:18,590 --> 00:37:22,430
And so therefore there are flip our base elders.

347
00:37:23,270 --> 00:37:26,750
So the expected value of someone's random asset, given their data,

348
00:37:28,070 --> 00:37:35,060
is this weighted average of the mean of the observations less whatever the population mean is across all people.

349
00:37:36,410 --> 00:37:42,469
But again, the weight is less than one and it's a function of the between subject variance.

350
00:37:42,470 --> 00:37:45,890
DE Which is how many observations there are.

351
00:37:47,480 --> 00:37:51,710
For a person and the between the within subject variability sigma e squared.

352
00:37:54,010 --> 00:38:00,760
So we'll interpret this a little bit more. So we call this in the world of estimation of shrinkage estimate.

353
00:38:01,920 --> 00:38:06,700
And there's a lot of work that the shrinkage estimate in estimate is existing in a lot of fields.

354
00:38:07,330 --> 00:38:11,110
But what are we doing? We're taking the deviation of an individual's mean.

355
00:38:12,250 --> 00:38:19,060
Their sample mean from the population mean. We don't just use the difference between my mean and the average of everybody.

356
00:38:19,900 --> 00:38:22,930
We don't just take that number. We pull it towards zero.

357
00:38:23,140 --> 00:38:27,490
We shrink it towards zero right by the shrinkage factor.

358
00:38:29,620 --> 00:38:32,439
So if I know the population mean which of course I don't,

359
00:38:32,440 --> 00:38:39,100
because I'm going to have to estimate it from the data if I know the variance of the random effects and I know the the variance of the errors,

360
00:38:39,850 --> 00:38:44,050
then we can do a little algebra here and find that in fact the expected value.

361
00:38:47,060 --> 00:38:53,400
And in that condition, why there should be a conditioning of why either the thing is zero.

362
00:38:54,580 --> 00:38:57,030
Right. It's unbiased.

363
00:38:59,510 --> 00:39:06,770
And again, I'm not going to go through the algebra, but this has a property that it has the smallest variance among all linear functions of the Y.

364
00:39:08,600 --> 00:39:17,130
And so this thing has come to be called the blowup, a best linear, unbiased predictor of B, right?

365
00:39:17,480 --> 00:39:20,840
So if you hear it again, you'll hear me say it like, here are the blowups.

366
00:39:21,740 --> 00:39:27,770
Those are just the predictions of the random effects. In this case, there would be other random intercepts because those ones.

367
00:39:34,010 --> 00:39:41,360
Again. If none of those things are known and they never are, then there's no guarantee that these things are best.

368
00:39:42,730 --> 00:39:47,230
Theoretically when things are known, they're best, right? But then we always plug in estimates and we screw things up.

369
00:39:49,950 --> 00:39:58,020
So if we take estimates of the population mean and the variance of the random intercepts and the noise, then we have this thing here.

370
00:39:59,290 --> 00:40:07,090
Right. So we fit the model. We get our estimates of everything and then we use that to then predict to the random effects.

371
00:40:08,320 --> 00:40:13,420
So then people might refer to that as an Ebola with E for empirical.

372
00:40:13,840 --> 00:40:19,570
So we've taken a theoretical construct which is the blob, and we've plugged in empirical estimates of everything we need.

373
00:40:20,260 --> 00:40:25,530
So. And you might hear someone say Ebola. I think that's necessary, but.

374
00:40:26,960 --> 00:40:35,840
If those three things are known again, if those are known, then these random effect predictions are posterior means from a Bayesian perspective.

375
00:40:37,650 --> 00:40:41,580
So again this is something we're Bayesian stance around and say, yep,

376
00:40:41,580 --> 00:40:52,350
see you're using our ideas when you're frequentist and they're not strictly Bayesian because we take a Bayesian idea and then we plug in.

377
00:40:53,550 --> 00:40:57,570
Maximum likelihood or restricted maximum like that, estimates of the other things.

378
00:40:57,570 --> 00:41:02,070
That's a frequentist approach. So we don't get Bayesian estimates of everything.

379
00:41:02,940 --> 00:41:07,350
We get a Bayesian estimate of one thing and everything else is via frequentist methods.

380
00:41:07,710 --> 00:41:18,330
So as we talk about Empirical Bayes, so Empirical Bayes is a method in which we kind of merge the two ideas.

381
00:41:19,140 --> 00:41:24,180
We use a Bayesian idea and then plug in empirical estimates to get something useful, right?

382
00:41:24,870 --> 00:41:30,840
So again, if you ever hear of Empirical Bayes estimation, this is what people are talking about in longitudinal data.

383
00:41:33,140 --> 00:41:39,320
So again, these are deviations. Remember, these random intercepts are deviations from from the population.

384
00:41:39,860 --> 00:41:42,770
So now that we have these these as these predictions,

385
00:41:43,850 --> 00:41:50,300
I can figure out what someone's conditional mean is what is my trajectory given my random effect?

386
00:41:51,260 --> 00:41:57,140
And I call that mutant silver. And that is simply the population mean plus my deviation.

387
00:41:58,460 --> 00:42:01,310
And again, just to look at the intuition behind this,

388
00:42:01,310 --> 00:42:10,040
some algebra reveals that my mean my predicted mean is the sum of the population mean and my random intercept prediction.

389
00:42:10,820 --> 00:42:18,590
And I just plug in what the vital those are. And then I put the stuff with you together and the stuff with y bar together.

390
00:42:19,610 --> 00:42:26,780
And I see that my what you tell me my predicted mean is, is simply a weighted average of the mean of everybody.

391
00:42:27,710 --> 00:42:35,840
And what my data are. So this is the beauty of using everybody's data simultaneously.

392
00:42:36,920 --> 00:42:40,910
My predicted mien is not just my observed mean.

393
00:42:41,990 --> 00:42:49,610
It's a proportion of my observed mean and a proportion of everybody else's means for me in every priority population.

394
00:42:50,270 --> 00:42:55,530
So again, I don't just use my data to think what I would be because I remember this.

395
00:42:56,840 --> 00:43:01,640
It doesn't always work. So I take a weighted average of my data and all of the data.

396
00:43:02,600 --> 00:43:05,060
The idea of shrinkage and combining data.

397
00:43:05,720 --> 00:43:14,120
So the weights are a function of the variability between people and the variability within people and DeHart quantifies between.

398
00:43:14,420 --> 00:43:19,370
And Sigma Epsilon squared for the space within. So let's think about this.

399
00:43:20,770 --> 00:43:29,480
When people are very different from each other. When the between subject variance is much, much bigger than the noise within a person.

400
00:43:30,740 --> 00:43:34,250
The outcomes tend to cluster together within an individual right.

401
00:43:34,370 --> 00:43:38,090
My observations don't vary a lot, but they're very different from this person.

402
00:43:38,570 --> 00:43:44,120
There's clustering. There's very distinct clustering going on. My observations look very different from somebody else.

403
00:43:45,860 --> 00:43:54,720
So if that's true, then I should use my data to get my predictive me because I'm so distinct from everybody else.

404
00:43:54,740 --> 00:43:59,480
Why would I use those data when they have really nothing similar to mine?

405
00:44:00,560 --> 00:44:03,710
So the predictive mean is going to be very, very close to my data.

406
00:44:03,720 --> 00:44:05,690
I'm going to ignore everybody else, essentially.

407
00:44:06,530 --> 00:44:16,310
But when we have extremely noisy data, when the variability within when my data are bouncing around enough that they overlap with other people's data,

408
00:44:18,740 --> 00:44:26,870
then we see people starting to look like each other. And I don't really trust an individual's data to predict what the true values are.

409
00:44:27,350 --> 00:44:34,040
And so then I'll just say everybody gets the population mean. For lack of a better estimate, because there's just so much overlap.

410
00:44:34,370 --> 00:44:36,710
So again, the reality of some combination of those two,

411
00:44:37,520 --> 00:44:43,940
probably you understand that essentially when one of them goes to zero and not we get these two extremes,

412
00:44:44,330 --> 00:44:49,900
but that's usually a weighted average of the two. For a more general linear mix model.

413
00:44:49,920 --> 00:44:56,260
So that was the very simple as simplest model because then we can look at the algebra scores.

414
00:44:56,340 --> 00:45:02,130
If they had no covariates and I only had a random intercept.

415
00:45:03,540 --> 00:45:08,160
Usually there's going to be an exit beta instead of new. And there might be more than one random effect.

416
00:45:09,090 --> 00:45:13,950
And again, I don't want to go through all of the algebraic derivations, but the intuition should be the same.

417
00:45:14,580 --> 00:45:22,350
So for a more general linear, mixed model in which I have covariates, excited times, beta, and perhaps more random effects, that was the model there.

418
00:45:22,890 --> 00:45:27,300
What are the the blocks? The predicted random effects?

419
00:45:27,750 --> 00:45:29,969
If I had a random intercept and a random slope model,

420
00:45:29,970 --> 00:45:35,879
I'm not predicting two values for everybody the random intercept deviation and the random slope deviation.

421
00:45:35,880 --> 00:45:42,240
And they might have a covariance going the with them. There's the formula right there in front of you.

422
00:45:42,270 --> 00:45:47,670
That's been derived again, a bunch of matrix algebra, but it's the same sort of concept.

423
00:45:48,890 --> 00:45:53,540
The wait again. There's a deviation from my data in the population.

424
00:45:53,660 --> 00:45:57,640
Y. I minus x i. Beta. And there's a weight out in front of it.

425
00:45:57,670 --> 00:46:00,520
That's a function of the between and within subject variability.

426
00:46:01,850 --> 00:46:09,740
And the individual's predicted values are again equal to estimate ahead plus z times vital the.

427
00:46:11,030 --> 00:46:13,879
And if I plug in everything and I combine like terms,

428
00:46:13,880 --> 00:46:23,360
I get this right here that everybody's predicted values or wait a combination of their actual outcomes and

429
00:46:23,510 --> 00:46:29,750
the population mean I beta and the two weights are a function of the between and within subject variability.

430
00:46:30,320 --> 00:46:36,770
The whole point is, as we shrink an individual's predicted values from their observed values toward the population mean.

431
00:46:39,390 --> 00:46:46,530
We use the collective data and we pull people who are very far from the population and we pull them in a little bit.

432
00:46:47,280 --> 00:46:52,110
And people who are very below the population, we pull them in a little bit toward the population.

433
00:46:56,940 --> 00:47:02,310
So again, within individual variation is quantified by an R0 Sigma inverse.

434
00:47:02,400 --> 00:47:09,240
So when that's large in a matrix sense, then an individual's predicted outcomes are closer to the population mean.

435
00:47:09,510 --> 00:47:14,430
And when that thing is small, then the predictive values get closer to what was observed.

436
00:47:15,580 --> 00:47:19,810
And again, the weights are a function of am I, am I, is the number of observations per person.

437
00:47:21,280 --> 00:47:31,630
So shrinkage increases as MRI decreases. If I don't have a lot of data on, somebody's going to pull them toward the population more.

438
00:47:32,110 --> 00:47:38,410
If I have a lot of observations on somebody, I'm going to be willing to maybe pull away from the population a little bit more.

439
00:47:39,970 --> 00:47:43,660
So a lot of function of information. All right.

440
00:47:46,030 --> 00:47:49,270
So we're going to do all this in R by 330.

441
00:47:50,260 --> 00:47:56,260
All right. So they've talked about estimation, estimating the fixed effects better,

442
00:47:56,260 --> 00:48:06,940
hence estimating the variances of the random effects and then using those to get to these predicted values for the random effects.

443
00:48:08,420 --> 00:48:14,200
And we do all this in order to figure out again, is there a group effect in your data?

444
00:48:14,830 --> 00:48:16,150
Is there a time effect?

445
00:48:16,210 --> 00:48:23,260
And is there some sort of difference of the time effect across the groups or among the reason we're going to do that with a linear, mixed model?

446
00:48:24,800 --> 00:48:28,330
And so let's go back to my data sets.

447
00:48:29,450 --> 00:48:33,980
What did I just know here? This resume.

448
00:48:37,730 --> 00:48:42,470
So there are some interesting things that I want to talk about in my data that wasn't intentional.

449
00:48:43,940 --> 00:48:48,200
I think that are important in general for you as you're writing your reports.

450
00:48:50,540 --> 00:48:57,050
I forgot to tell you all somehow and always gives me wonderful feedback on your reports in a big,

451
00:48:57,080 --> 00:49:04,370
you know, not individually, but hey, this is happening whenever you report your results.

452
00:49:05,330 --> 00:49:08,920
Don't say the interaction was insignificant.

453
00:49:08,930 --> 00:49:13,440
P Greater than .05. We are statisticians.

454
00:49:13,440 --> 00:49:21,330
We don't do that. That's what collaborators do. So a better approach is to say the interaction term was.

455
00:49:23,390 --> 00:49:30,080
However, it wasn't significantly different from zero P-value equal to tell me what the p value was.

456
00:49:30,290 --> 00:49:33,410
None of this bigger than .05 crap. Right?

457
00:49:33,530 --> 00:49:37,850
That just isn't useful for people. That's what happens in medical journals.

458
00:49:38,300 --> 00:49:44,780
So you give details. Remember that P values are a function of sample sizes.

459
00:49:46,160 --> 00:49:52,870
When you get a p value of 0.06 from ten people. That's amazing, right?

460
00:49:52,890 --> 00:49:57,120
I always tell collaborators, you know, all the time, we can't publish this.

461
00:49:57,550 --> 00:50:01,900
What were you thinking? We had ten people who were going to publish.

462
00:50:02,650 --> 00:50:06,220
But it's pretty darn close. There's probably something going on.

463
00:50:06,880 --> 00:50:09,040
You just didn't get enough data, right?

464
00:50:09,430 --> 00:50:17,560
So don't always just jump on the P-value in deciding whether or not something is interesting or not or keep remembering what the sample size is.

465
00:50:18,850 --> 00:50:23,020
That's why you should report effect estimates always not just the p values.

466
00:50:23,350 --> 00:50:26,650
And that's going to happen here in these data. I didn't plan that.

467
00:50:27,370 --> 00:50:37,000
Again, these are data looking at pipe measurements, and I'm going to create the data from the original data set.

468
00:50:47,130 --> 00:50:52,410
You know what we now know. All right. So we have the idea of each individual.

469
00:50:52,410 --> 00:51:01,500
There are three groups in this dataset. There is a control, a hyper internally MC and a non hyper insulin mixed group of people.

470
00:51:02,820 --> 00:51:11,650
These data are balanced. So there are fixed number of time points and everybody has all the observations and all those time points.

471
00:51:12,700 --> 00:51:22,120
So again, remember that in GLS, when we fit exchangeable correlation with a balanced set of data,

472
00:51:23,170 --> 00:51:27,490
you get the same point estimate you did from independence. Standard errors are different.

473
00:51:29,580 --> 00:51:32,730
So it's going to carryover with a random intercept model.

474
00:51:33,900 --> 00:51:39,900
Random intercept is compound symmetry with normal data. So again, when you don't have balance data, again, all of that.

475
00:51:40,230 --> 00:51:46,380
So again, if you're analyzing data and you're seeing, you know, coefficient estimates of look the same, that's because perhaps they are.

476
00:51:50,850 --> 00:51:52,770
You're going to use the library LME for.

477
00:51:52,770 --> 00:51:58,379
So I asked you to get all these libraries at the beginning of the semester, but we have not used this one yet, so let me.

478
00:51:58,380 --> 00:52:01,410
Four will fit the linear mixed effects models with normal data.

479
00:52:02,280 --> 00:52:05,280
And again, I've asked you to look at sandwich standard errors.

480
00:52:07,000 --> 00:52:14,780
Through the library. So let me again show you a spaghetti clutch again.

481
00:52:15,120 --> 00:52:18,570
You guys have made me rethink the utility of spaghetti plots.

482
00:52:20,190 --> 00:52:24,930
I don't I don't like them very much, but they're useful for random effects models.

483
00:52:25,590 --> 00:52:30,630
They helped me see if the intercepts vary a lot. They help me see a slope are all over the place.

484
00:52:30,930 --> 00:52:39,810
Again, in smaller datasets, I try to put some color in here for the controls, the non hyperinsulinemia, the hyper insulin that groups.

485
00:52:48,240 --> 00:52:54,610
I considered a shrink this. It's hard to see here, but I don't like fitting a line.

486
00:52:54,610 --> 00:53:01,960
I don't like treating time as linear in these data. There is a general job for everybody and then a general increase in the data.

487
00:53:02,770 --> 00:53:06,669
Last time I analyzed these data, I said a piecewise linear model was a change.

488
00:53:06,670 --> 00:53:10,000
Find it to or not at two. So one line and one line.

489
00:53:10,390 --> 00:53:17,070
But for today I'm going to decide instead to fit a quadratic term, which is true because it's fun to do a different model.

490
00:53:17,080 --> 00:53:25,240
So I created another variable called TM two for the curvature.

491
00:53:25,780 --> 00:53:29,050
And again, I ask you all to start with the model of independence.

492
00:53:29,620 --> 00:53:35,469
So ignoring there's no random effects typical of, well, let's get a sandwich.

493
00:53:35,470 --> 00:53:46,240
Standard error for that is to get the AC in the BBC, the R-squared, which is really simple with with or less because it's the default in R.

494
00:53:53,930 --> 00:54:03,719
Right. So here's what I got. And so, again, looking at things, looking at the standard errors, the sandwich senators,

495
00:54:03,720 --> 00:54:07,480
again, remember, the standard errors here didn't account for repeated measures.

496
00:54:07,950 --> 00:54:17,250
So they're probably not very reliable. If I look at the sandwich, senators, if I decide to go with this, I see that maybe there's a time effect.

497
00:54:17,430 --> 00:54:25,950
Those curvature interactions at the bottom appear to be just marginally significant, not overwhelmingly great, but.

498
00:54:27,390 --> 00:54:33,960
So right now, I would say that it looks like there's a slight difference in the time effects and it's in this curvature.

499
00:54:34,740 --> 00:54:40,080
And I discussed this with a couple of folks in my office. There is this is really hard to explain in application.

500
00:54:40,590 --> 00:54:43,800
You can't just say time has this coefficient.

501
00:54:45,170 --> 00:54:51,680
Because there's a linear and a curvature overall, and then there's a deviation for the other two groups from the control group.

502
00:54:52,970 --> 00:54:57,050
I find pictures at this point are more useful than a table of coefficient estimates.

503
00:54:59,020 --> 00:55:05,530
Again when you get into 699, don't always decide that you're going to have a table of coefficient estimates.

504
00:55:06,130 --> 00:55:10,330
So one thing I've learned as a teacher over 20 years, everybody loves to have a table of coefficients.

505
00:55:11,200 --> 00:55:13,300
Don't give me those if I don't know what to do with them.

506
00:55:15,350 --> 00:55:22,460
You've all taken 651 and we're going to talk about this and gee, never give me a table of coefficients from a logistic regression.

507
00:55:23,480 --> 00:55:34,700
Give me odds ratios. Hey, don't give anybody next semester a table of coefficient estimates from logistic regression exponentially exponential them.

508
00:55:35,540 --> 00:55:41,450
We're trying to interpret things anyways so it looks like perhaps, you know,

509
00:55:41,540 --> 00:55:44,899
there's a time it's really hard to decide, you know, the group difference.

510
00:55:44,900 --> 00:55:50,930
I can't just look at those group that's factor group two and three because group is in the interactions.

511
00:55:51,440 --> 00:55:55,370
So sometimes you want to take the interactions out to try and tease out what's going on.

512
00:55:55,370 --> 00:56:02,550
But anyway, no random effects. Probably not going to be the final model in this class because we're fighting random effects.

513
00:56:03,060 --> 00:56:11,510
So now I'm going to sit around and intercept. The syntax is fairly understandable at this point.

514
00:56:11,510 --> 00:56:19,130
It took many years for them to make this useful and the syntax looks very much like the LM function or the gloss function.

515
00:56:19,460 --> 00:56:28,310
You express random effects by saying what those covariates are with a line and then telling me where the clustering lies.

516
00:56:28,310 --> 00:56:31,910
Where is the correlation? And this is within ID, within person.

517
00:56:31,940 --> 00:56:36,650
That's the variable that tells the computer where things are the same, the same person.

518
00:56:37,460 --> 00:56:41,270
So one with a line ID means just a random intercept.

519
00:56:43,220 --> 00:56:52,020
So that model will then be fit. And again, if you really want to, again, you can get sandwich standard errors.

520
00:56:54,410 --> 00:56:57,979
And we'll do this in a second. Combine those. Get the vaccine.

521
00:56:57,980 --> 00:57:02,060
The vaccine. Again, there is not a strict R squared.

522
00:57:02,180 --> 00:57:08,200
That is output. But I've given you a formula and I give you code that we use with GLS.

523
00:57:08,210 --> 00:57:14,090
It's the same code here. I want to compare the log likelihood of my model with some null model.

524
00:57:14,780 --> 00:57:22,969
The null model is just an intercept with no random aspects can just really matter what your known model is,

525
00:57:22,970 --> 00:57:26,900
as long as you use that consistently across all the models for fitting as a comparator.

526
00:57:27,720 --> 00:57:33,920
Right. So there we go, close to three.

527
00:57:34,700 --> 00:57:38,210
So here is a the results of the random intercept model.

528
00:57:39,110 --> 00:57:42,800
And again, you're going to get some estimates, you're going to get some standard errors.

529
00:57:43,190 --> 00:57:45,350
Now, these models are model base standard errors.

530
00:57:45,650 --> 00:57:53,809
I have a model in which I have a random intercept tells me basically the same story that I got from the sandwich estimates with

531
00:57:53,810 --> 00:58:03,050
independents that maybe there's just a very slight difference in the curvatures between groups two and three with the placebo group.

532
00:58:04,430 --> 00:58:13,310
What's the point of the sandwich errors? Nothing more other than for me to gauge if I think my model has gotten all of the correlation in the errors.

533
00:58:16,380 --> 00:58:18,900
For the most part, I am not too concerned here.

534
00:58:19,190 --> 00:58:25,920
I mean, the numbers maybe are different a little bit, but overall, it looks like perhaps random intercepts to an okay here.

535
00:58:29,310 --> 00:58:33,240
So the R-squared from the mouse model with no random effects.

536
00:58:34,020 --> 00:58:38,170
Was about .33. This model has an R squared.

537
00:58:38,580 --> 00:58:46,430
Much, much bigger. This is enough for me to certainly say that this model is better in terms of fitting everything.

538
00:58:46,980 --> 00:58:50,160
So it looks like perhaps we have a decent model here with a random intercept.

539
00:58:52,690 --> 00:59:01,510
I have this line here called Brand F two and the function in the library is var core of any model that you fit with random effects.

540
00:59:08,650 --> 00:59:16,720
And that's what you see there. So it gives you two variance components, but gives you the standard deviation, the square roots.

541
00:59:17,320 --> 00:59:22,600
So again, ID intercept. That's the between subject to standard error.

542
00:59:23,290 --> 00:59:26,980
If you squared that, you would get the between subject variability.

543
00:59:27,700 --> 00:59:32,019
That's the capital D and that's the residual is the leftover variance.

544
00:59:32,020 --> 00:59:35,440
The sigma squared and the square root of that is point four or five.

545
00:59:38,300 --> 00:59:44,600
So what is the correlation of observations from the same subjects?

546
00:59:46,010 --> 00:59:49,190
Larger than or less than point. No, we can't do that.

547
00:59:50,150 --> 00:59:53,660
What is the correlation of two observations from the same person?

548
00:59:53,960 --> 01:00:05,300
How would we get to that? Of course, you've all memorized that formula that I gave you last week, the other day.

549
01:00:05,810 --> 01:00:13,840
Right. Just the it is.

550
01:00:46,000 --> 01:00:55,030
So was a random. Seriously.

551
01:00:58,010 --> 01:01:04,459
Oh, wow. And now I have graph paper with a random intercept.

552
01:01:04,460 --> 01:01:07,730
The correlation of any two observation isn't a function of Jane Kay,

553
01:01:08,900 --> 01:01:14,150
it's simply a function of sigma squared B and the total variance, which is the sum of the two variance components.

554
01:01:15,350 --> 01:01:20,720
And so that is to them, right?

555
01:01:21,140 --> 01:01:34,610
We got 2.52 squared. Plus .45 squared.

556
01:01:41,720 --> 01:01:44,870
And to really make this represents about 25.

557
01:01:46,230 --> 01:01:51,500
Two five plus. I don't mean to.

558
01:01:56,200 --> 01:02:05,859
So yep, that's a correlation coefficient. If you went back to your data and you went back to GLS and fit a random [INAUDIBLE],

559
01:02:05,860 --> 01:02:12,550
no nothing random account on symmetric model, you would get a correlation coefficient output from that function.

560
01:02:13,240 --> 01:02:17,080
It should be the same value that this model gets. They are identical.

561
01:02:17,170 --> 01:02:20,740
It's not that they're similar approaches, random intercepts.

562
01:02:21,840 --> 01:02:28,800
With independent errors in compound symmetry in glass are identical with normal data.

563
01:02:30,500 --> 01:02:37,730
Only normal data. So when we get to other point, and I put this by my mouth, there's my covered right there.

564
01:02:38,120 --> 01:02:44,440
All right. Right.

565
01:02:44,980 --> 01:02:53,370
So just keep that in mind. Of course, last time I didn't have this curvature model, I had a random into to a piecewise linear.

566
01:02:53,370 --> 01:02:59,220
So I can't compare my owner from last time. And now I'm going to hit a random intercept, intercept and slope model.

567
01:02:59,730 --> 01:03:01,740
And remember, now that I have two random effects,

568
01:03:02,010 --> 01:03:06,720
they have marginal variances and they can also have a covariance, they have a by very normal distribution.

569
01:03:07,680 --> 01:03:12,840
So I'm going to assume right now that they're correlated. And so the way I do that is similar syntax.

570
01:03:14,010 --> 01:03:20,130
Now I say that within subject there is an intercept column of ones and then time.

571
01:03:22,690 --> 01:03:25,900
Now you might say, well, what in their tells the computer it's correlated.

572
01:03:26,510 --> 01:03:29,950
Well, I'll show you in a second how to tell the computer that they should be uncorrelated.

573
01:03:30,730 --> 01:03:32,830
Right. And so this model runs.

574
01:03:38,100 --> 01:03:45,630
And I'm going to save everything that I see in the BBC, get an R-squared value and let's look at the random effects of which.

575
01:03:48,830 --> 01:03:53,090
So there's what we get. So again, the bottom line is always the residual variance.

576
01:03:53,540 --> 01:03:56,750
That's .42. We then have a standard.

577
01:03:56,870 --> 01:04:00,559
That's the deviation. Deviation. I don't know why ours programs are deviations.

578
01:04:00,560 --> 01:04:08,129
Maybe because the numbers are so small sometimes. The with in-person again ID means within IDs.

579
01:04:08,130 --> 01:04:17,760
So as in-person I have a random intercept standard deviation, a random slope standard deviation, and there's a correlation between those two.

580
01:04:19,050 --> 01:04:25,530
So you can figure out what the DX matrix is. Again, we usually have intercepts and slopes that are negatively correlated.

581
01:04:25,530 --> 01:04:30,380
And so that last time we believe people who start out really high just tend to pull towards the mean.

582
01:04:30,590 --> 01:04:35,330
People who start low tend to falter at the meet. So there they are.

583
01:04:35,450 --> 01:04:39,200
There are the the variances and the correlation of the random effects.

584
01:04:39,200 --> 01:04:41,030
There's the variance of the error term.

585
01:04:43,580 --> 01:04:52,879
The output is really nice getting these if you ever in your life have to get these and put them somewhere saved off in some Stratosphere Boy,

586
01:04:52,880 --> 01:04:57,350
that's a hard channel. You have to use attributes and things that just aren't nice.

587
01:04:57,860 --> 01:05:06,800
Anyway, our squared from my random intercept model was .535854.

588
01:05:07,910 --> 01:05:13,040
Our square is now .550 yes.

589
01:05:13,040 --> 01:05:15,559
Hand. Go ahead. Yeah. I just had a question about something you said earlier.

590
01:05:15,560 --> 01:05:21,230
You said that if you do a random intercept, it's identically equal to compound symmetry.

591
01:05:21,230 --> 01:05:24,830
If you have normal data so you get them and they're unequal,

592
01:05:24,830 --> 01:05:29,030
would that be an indicator that you should be using something other than a normal model for your data?

593
01:05:29,960 --> 01:05:32,990
If they don't match each other, then that's a problem with the computer.

594
01:05:33,980 --> 01:05:41,900
It's not your data. If you take the same data and you use these two algorithms, yeah, they should give you exactly the same answer.

595
01:05:42,350 --> 01:05:49,940
So it isn't a question of which one fits better. They just they may both be crummy models, but they're going to give the same crummy results.

596
01:05:50,570 --> 01:05:54,620
One will never be better than the other. They're just identical in every aspect.

597
01:05:54,620 --> 01:06:03,500
Standard errors. Correlation. Yes. I also have a question about like because you mentioned that the sandwich of sorry the

598
01:06:03,860 --> 01:06:09,740
outcome of symmetry is identical to the oh I meant was to intercept random variable.

599
01:06:10,100 --> 01:06:22,040
So I'm wondering does does that mean does sandwich a variance for both of the models are the same should be put up for the same residuals.

600
01:06:22,220 --> 01:06:27,560
So both models should produce the same residuals from which the sandwich estimate is derived.

601
01:06:27,800 --> 01:06:30,050
Well, the residuals shouldn't be the same, right?

602
01:06:30,320 --> 01:06:40,400
Because for AM just like a random turn on the pie and the B I could understand it incorrectly with a pie is totally random based on each draw.

603
01:06:41,360 --> 01:06:50,930
So the epsilon well but remember epsilon is y minus x beta hat it is not y minus x beta hat minus z ibai.

604
01:06:52,100 --> 01:06:56,629
The residuals. Subtract off the population mean. Oh.

605
01:06:56,630 --> 01:06:59,660
Okay. You don't subtract off the random effects. I see.

606
01:06:59,840 --> 01:07:08,300
So they should produce the same result. We could get subjects specific residuals, but that's not what we use when we talk about residuals.

607
01:07:09,350 --> 01:07:12,770
Residuals are y minus y, the population mean?

608
01:07:13,440 --> 01:07:22,069
That's right. So these two models, I don't think the Earth's words are really worth saying.

609
01:07:22,070 --> 01:07:26,990
They're different from each other, probably. Mean, maybe you might say that's worth it.

610
01:07:26,990 --> 01:07:35,420
But I'm not saying that the random slope model is really any better than the random intercept model in terms of our square and C two.

611
01:07:38,120 --> 01:07:45,140
Oh, I see. Two. So there's see two from the R from the random intercepts model.

612
01:07:48,410 --> 01:07:54,960
Yeah. Nothing to really get too excited about here. In fact, I guess they're like, Well, it's not interesting.

613
01:07:57,290 --> 01:08:01,550
So as he went up, they both went up. Okay? No. Yes, he went down.

614
01:08:03,380 --> 01:08:07,280
He went up a little bit. Which tells you that computer doesn't know what to do.

615
01:08:07,580 --> 01:08:16,670
Right. They're about the same. If you really want to fit uncorrelated random intercepts and slopes, here is the syntax.

616
01:08:17,210 --> 01:08:21,050
Is there an explanation for why he is? He obviously went upstairs.

617
01:08:21,320 --> 01:08:28,550
Because they penalized differently when penalizes by the sample size, whereas the other one doesn't.

618
01:08:29,180 --> 01:08:38,420
So if you look at the two formulas, there's a again, if they're going in opposite directions, it usually means everything's close to each other.

619
01:08:38,450 --> 01:08:42,980
You shouldn't see, you know, as you go down by 100. Viki Go up by 100%.

620
01:08:43,850 --> 01:08:48,380
By a little bit. It's possible because they penalize for different things.

621
01:08:49,860 --> 01:09:01,710
Here is how you take correlation out of the random effects. You have to specify the intercept within it, plus the slope, minus the intercept within.

622
01:09:03,000 --> 01:09:06,990
And so you're fitting those two things. Again, don't typically do this.

623
01:09:07,470 --> 01:09:13,590
But if you're ever wondering, that's how you separate random effects from being correlated in the fitting process.

624
01:09:13,860 --> 01:09:20,550
Again, as one less parameter, there's one less parameter and mod four. There's no covariance term for the two random effects that has to be estimated.

625
01:09:24,340 --> 01:09:28,419
So just in fitting this image over and over, I saved the results.

626
01:09:28,420 --> 01:09:32,860
They're going to measure squared. Let's look at the random effects.

627
01:09:36,700 --> 01:09:45,610
Again, it is the random intercept. And then it that one just means the second thing nested within it, which was the slope that tells you it's time.

628
01:09:46,300 --> 01:09:50,620
But there's no covariance. There's no correlation now between the random intercept, the random slope.

629
01:09:51,070 --> 01:09:54,120
But it really doesn't have any effect on anything.

630
01:09:58,760 --> 01:10:04,840
R squared is essentially what it was for the other two approaches. So again, I'm thinking maybe random intercepts.

631
01:10:11,660 --> 01:10:18,830
Let's talk about forever. There was the question about should everything in the fixed effects be in the random effects?

632
01:10:19,790 --> 01:10:25,960
And the answer is theoretically no. They could be completely different sets of covariance.

633
01:10:26,740 --> 01:10:29,770
Often we make what's in the fixed effects.

634
01:10:30,100 --> 01:10:33,730
Possibly a random effect. Especially time. A fixed effect for time.

635
01:10:33,970 --> 01:10:39,160
We have an effect. Random if we want a random effect. So what I did was took the model I said before, but now you'll see.

636
01:10:39,160 --> 01:10:47,870
I have three random effects. And saying that there's a random intercept, there's a random linear component, and the curvature less time.

637
01:10:47,870 --> 01:10:50,720
I didn't have a curvature, random effect. I just had the linear path.

638
01:10:51,960 --> 01:10:59,030
So now I'm saying there's also this random curvature that varies from person to person and then go ahead and try to fit that.

639
01:10:59,180 --> 01:11:07,480
And again, this is what small data sets. I get something like this and you'll get these kind of messages all the time.

640
01:11:09,370 --> 01:11:13,240
And this computer is basically telling you you don't have enough data to do all of this.

641
01:11:13,930 --> 01:11:22,780
Right. I just can't I can't take the number of people in this dataset and fit population means and figure out how everybody differs by the intercept,

642
01:11:22,780 --> 01:11:26,050
their curvature and their linear component. They're just too much to estimate there.

643
01:11:26,710 --> 01:11:30,070
So whether you want to fit that model or not, it's irrelevant because it's not going to work.

644
01:11:33,520 --> 01:11:36,940
We did all this already. I compare to Seatbacks.

645
01:11:38,110 --> 01:11:44,110
Again, I'm and I'm looking at this one right here as being good enough.

646
01:11:44,470 --> 01:11:49,750
Again, if you want to argue that maybe you should have a random slope, that is fine.

647
01:11:50,470 --> 01:11:54,879
No issues there. This is what's interesting.

648
01:11:54,880 --> 01:12:01,900
And then I think we're getting toward the end here on a Friday, again, the intercept, the there's four interaction terms.

649
01:12:02,020 --> 01:12:05,830
I hate interactions. I would much rather not have to explain interactions.

650
01:12:07,180 --> 01:12:14,620
So let's see if I can take them out. Let's see if, in fact, all three groups ten tend to be similar with each other terms.

651
01:12:15,190 --> 01:12:18,310
There's just one overall curvature for all three groups. Same trend.

652
01:12:19,030 --> 01:12:23,170
So I'm going to set a model that doesn't have the interaction. Now, I just have a plus here.

653
01:12:23,320 --> 01:12:31,000
There's no times. So when I set that model, I pull out the likelihoods, slug likelihoods for the two models,

654
01:12:31,000 --> 01:12:35,110
and then I did the negative two log likelihood type test for degrees of freedom,

655
01:12:35,440 --> 01:12:38,830
because the two models differ by four parameters for interaction terms.

656
01:12:39,940 --> 01:12:43,330
And here's what I guess. This is what I want to talk about.

657
01:12:43,990 --> 01:12:49,460
It's been one eight. But if you remember.

658
01:12:50,740 --> 01:12:56,860
When I looked at the fit of the model of UPS three.

659
01:12:57,980 --> 01:13:03,000
Uh, you know, I was talking about these two p values drops.

660
01:13:03,010 --> 01:13:07,640
What's that like a sandwich? Maybe I should have it, and maybe I should have the curvature.

661
01:13:09,470 --> 01:13:13,850
So this is where you have to use your brain rather than just numbers from a computer.

662
01:13:15,080 --> 01:13:16,729
You could argue and say, you know what?

663
01:13:16,730 --> 01:13:21,770
I think maybe accounting for everything else, there's a bit of curvature that differs between the three groups.

664
01:13:21,980 --> 01:13:29,299
I'm going to keep it in. If you look at this p value here for all four terms being kept in the model, again,

665
01:13:29,300 --> 01:13:34,820
I don't think I want a model in which the curvature interaction stays in, but the linear interaction comes out.

666
01:13:35,480 --> 01:13:43,070
Then my head really starts to hurt. Right. But I might use this p value and say, you know, it's bigger than Plano five.

667
01:13:43,520 --> 01:13:47,030
Maybe the interactions are important, but remember.

668
01:13:52,120 --> 01:13:56,860
At 33 people have 33 people in this dataset among three groups.

669
01:13:57,730 --> 01:14:02,080
So that .18. Not bad. It was only 33 people.

670
01:14:03,160 --> 01:14:05,990
So again, p values. Use them wisely.

671
01:14:06,010 --> 01:14:11,200
Don't always just say it's going to be less than Plano five because with these data that you're not going to find much.

672
01:14:12,130 --> 01:14:15,370
So as I wrote here, what to do with the interaction? Should I keep them or remove them?

673
01:14:16,750 --> 01:14:27,250
I would go either way. Um, and then I have the set of the final model if you want to know what the.

674
01:14:29,320 --> 01:14:40,600
Subject specific coefficients are if you say coefficient of a model dollar sign I.D. So can I pick the random intercept model?

675
01:14:41,050 --> 01:14:44,110
So you'll see. What happened is everybody has a different intercept.

676
01:14:44,230 --> 01:14:48,160
It took the population intercept and added on the random component for you.

677
01:14:49,670 --> 01:14:54,010
And then you'll see that all the other things are the same because we didn't have random events for any of those.

678
01:14:54,020 --> 01:15:00,200
There's just a population. So you can get those things if you want.

679
01:15:01,070 --> 01:15:05,330
Someone's subject specific prediction, including the random effects.

680
01:15:05,330 --> 01:15:08,530
And I call them gloves here. Then you say predict.

681
01:15:08,540 --> 01:15:14,480
So you could use predict with alarm in 650 or you could use fitted set it or predict.

682
01:15:15,630 --> 01:15:19,100
It gets a little fuzzy here, what's fitted and what's predicted.

683
01:15:19,100 --> 01:15:22,190
And hence, go back to your air, your residuals. What are the residuals?

684
01:15:22,190 --> 01:15:24,110
Do I subtract off the random effects or not?

685
01:15:25,460 --> 01:15:33,860
So predict of a model gives you the predicted values will be including the random effects you could say already form

686
01:15:33,860 --> 01:15:39,170
equals and a and that will take the random assessment that just gives you the population averages for everybody.

687
01:15:42,580 --> 01:15:49,120
And just to finish up here way, let's get those.

688
01:15:50,190 --> 01:15:53,180
It's for those who want to share what man of the night.

689
01:15:53,240 --> 01:16:00,360
Again, this to me is more useful than a table of coefficients and necessarily like all the subject specific lines.

690
01:16:00,810 --> 01:16:04,350
But I could show the investigator, here's the results of my final model.

691
01:16:04,920 --> 01:16:08,100
There's a again, the thicker colored lines are the three groups.

692
01:16:08,670 --> 01:16:14,579
So we see that these three groups start out differently. They tend to have this similar bend over time.

693
01:16:14,580 --> 01:16:18,840
And then the curvature at the end changes with the three groups that they end up fairly close at the end.

694
01:16:20,200 --> 01:16:24,910
I wouldn't know how to say that from a table of coefficient estimates. Right. That's just worthless.

695
01:16:25,540 --> 01:16:28,540
And then you can see the subjects with the variability with the groups.

696
01:16:28,540 --> 01:16:31,690
There's there's the blue line. There is each person.

697
01:16:31,840 --> 01:16:35,380
They have the same curve. All the blue people have the same curve.

698
01:16:35,620 --> 01:16:44,200
It's just they moved up or down because of the random intercept. We'll start there and I'm going over and it's Friday.

699
01:16:46,210 --> 01:16:49,510
All right. So go forth with that information.

700
01:16:50,020 --> 01:17:00,550
I'm going to think about raising a question for. See you later or.

