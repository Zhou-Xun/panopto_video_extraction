1
00:00:15,680 --> 00:00:21,590
Okay. So I'd like to start off today.

2
00:00:28,700 --> 00:00:34,620
With the folks could make some name plates, tags and take a call.

3
00:00:34,670 --> 00:00:41,390
I really appreciate it. When you really start learning the things, you have some little markers.

4
00:00:47,210 --> 00:00:51,320
You have a good friend who is big enough that I can see.

5
00:00:55,130 --> 00:01:50,870
Maybe just hold on to these. It just looks right.

6
00:01:54,380 --> 00:01:59,620
It's just. It's so short. It's such a big piece, you know?

7
00:02:01,030 --> 00:02:22,490
You know, it seems to be your they come in here through the 12 letters and test you.

8
00:02:24,650 --> 00:02:28,280
Okay. So thanks for doing that. It's helped me, too.

9
00:02:28,910 --> 00:02:34,970
I know a few of you. Many of you. I don't see this well, so I call you out by name and get to know you a little bit.

10
00:02:35,150 --> 00:02:38,750
So I appreciate it. Okay.

11
00:02:38,990 --> 00:02:42,340
So second issue is the final project.

12
00:02:42,380 --> 00:02:45,950
So the vote was heavily in favor of having it still be groups.

13
00:02:46,880 --> 00:02:51,380
So we will do that. Right now I'm thinking about seven groups of three.

14
00:02:52,850 --> 00:02:59,450
I think I'll try to keep it in groups of three. That seems like maybe a nice balance between sort of giving people an opportunity to

15
00:02:59,450 --> 00:03:06,770
kind of spread things around a little bit and work each other and not but not be,

16
00:03:06,890 --> 00:03:13,250
you know, sort of overworked, make it too too complicated in terms of dynamics.

17
00:03:14,180 --> 00:03:20,060
So that's the current plan. I will try to get those groups out today.

18
00:03:24,970 --> 00:03:26,470
If I would just randomly assign them.

19
00:03:27,340 --> 00:03:35,889
I was trying to do a bit of balancing before because we had a more heterogeneous class, but now it's pretty heavily biased at investors.

20
00:03:35,890 --> 00:03:47,530
So. Okay, so then I'm going to sort of kick things off today by reviewing the first homework.

21
00:03:47,950 --> 00:03:52,430
So but first, is everybody ready?

22
00:03:52,450 --> 00:04:04,270
Scott, the American. Great, thank you so much to everybody here.

23
00:04:07,440 --> 00:04:10,650
You have to eat. Sorry. There you go.

24
00:04:13,740 --> 00:04:26,700
So I guess any. Well, first of all, any any questions about could people access their homework?

25
00:04:28,170 --> 00:04:33,719
Okay. So it was I think it was I think it was set up. I think the default was different on this one than it was two years ago.

26
00:04:33,720 --> 00:04:35,640
So you're all set to read. So it's good.

27
00:04:37,620 --> 00:04:52,170
And in terms of the final project, get I will get that out there, I guess, you know, feel free to hit me with questions about that.

28
00:04:52,380 --> 00:04:53,520
I mean, we do have a description,

29
00:04:53,730 --> 00:05:00,900
but one thing I will do is I will put up some examples of really good projects in the last class so you have something to work with.

30
00:05:00,990 --> 00:05:05,340
I think this is a target, particularly for those of you who may not have done something like this before.

31
00:05:05,540 --> 00:05:14,460
So so I do think it's a little bit question for a master's class if I wanted to do this, but I think it's a really good chance.

32
00:05:15,150 --> 00:05:22,320
It's a really good way to learn this. I mean, kind of doing Holbrook's in this stuff is a little I think a little go well.

33
00:05:22,940 --> 00:05:30,030
So so I think sort of getting you to sort of grapple with some like real world examples would be will be good.

34
00:05:30,540 --> 00:05:35,429
And I think with enough with just seven groups, I think hopefully people will come up with stuff they want to work with.

35
00:05:35,430 --> 00:05:41,820
If you're really stuck, let me know and I'll try to, you know, sort of put you in some directions of some ideas.

36
00:05:42,830 --> 00:05:51,620
So. Okay, uh, so in terms of the first homework, um, I just wanted to kind of quickly go over this.

37
00:05:51,620 --> 00:05:57,920
Most people that did quite well, I won't spend a ton of time on this, but I wanted to, to make sure that, you know,

38
00:05:58,080 --> 00:06:03,139
if things were confusing in the solutions that people had a chance to ask, you know,

39
00:06:03,140 --> 00:06:07,550
obviously you can continue if you have questions to do with office hours or send emails.

40
00:06:08,530 --> 00:06:18,769
Um, okay. So the first, first question was we have this little very tiny population where we have everything in masks so we could see some control.

41
00:06:18,770 --> 00:06:23,150
Maybe this is like some change in pain score or something over time.

42
00:06:25,310 --> 00:06:32,780
So under treatment or or or control or maybe this is like a side effect measure or something.

43
00:06:33,890 --> 00:06:39,650
So, okay, so we want to compute the causal effect in this population.

44
00:06:39,800 --> 00:06:41,720
So everybody got this right. You're right.

45
00:06:41,720 --> 00:06:48,770
You just look at the differences and take the mean of the differences, which gives you a measure of three and four.

46
00:06:48,770 --> 00:06:55,730
Part B again, most people got this, got this fine. But basically, I just sort of wanted people to consider the past,

47
00:06:55,790 --> 00:07:00,170
all the possible assignments of treatment control, where you put two people in treatment and two people in control.

48
00:07:02,210 --> 00:07:10,760
So and to show that if you take the average of that, so if you were to sort of do all the possible assignments and repeat them,

49
00:07:11,240 --> 00:07:16,010
then you would tend to your your estimate would converge to the true value.

50
00:07:16,520 --> 00:07:24,590
So you can see the six possible outcomes under these assignments and take the average and then you get back to three.

51
00:07:26,650 --> 00:07:31,600
So okay. Now for part two,

52
00:07:33,190 --> 00:07:40,450
we asked people to consider this sort of weird assignment mechanism back on to and where basically

53
00:07:43,300 --> 00:07:49,570
where everybody gets assigned to the treatment if their outcome in treatment is equal to one.

54
00:07:50,380 --> 00:07:55,540
And otherwise they get assigned to control if the outcome of the treatment is equal to zero.

55
00:07:56,500 --> 00:08:00,760
So where your options are is a dichotomous outcome.

56
00:08:00,760 --> 00:08:16,210
So it's one or zero of the outcome. So we showed in class that if we take the mean of of the possible outcomes, we're looking at the,

57
00:08:16,840 --> 00:08:24,340
at the number of individuals that were assigned to treatment get a value of one and assigned to treatment at a value of zero.

58
00:08:26,590 --> 00:08:30,640
And similarly, when assigned a control, get a value of one versus a value of zero.

59
00:08:31,360 --> 00:08:38,319
We can see that the overall treatment effect is basically the subjects that kind of go in the

60
00:08:38,320 --> 00:08:46,480
direction of treatment and and outcome means being well when there's a when there's an effect,

61
00:08:46,840 --> 00:08:50,980
sort of a positive effect of treatment. So they were able to one to treatment, zero and to control.

62
00:08:51,460 --> 00:08:57,480
And then you subtract off the proportion of individuals for whom there are zero under treatment or under control.

63
00:08:57,490 --> 00:09:03,250
So that gives you the overall treatment effect, which doesn't make sense, intuitively efficient to the algebra.

64
00:09:05,050 --> 00:09:15,730
And then we showed that Chow had P basically, if we look at the, at the mean of individuals assigned to treatment versus the mean assigned to control,

65
00:09:16,540 --> 00:09:20,979
that basically the mean assigned to treatment is always going to be one, right?

66
00:09:20,980 --> 00:09:24,940
Because if you recall, when you get that and then on the control side,

67
00:09:25,720 --> 00:09:30,190
it's this fraction of individuals that kind of flip among all the individuals

68
00:09:30,190 --> 00:09:34,959
that would be assigned to control because there's zero in your treatment. So that gives you this quantity here.

69
00:09:34,960 --> 00:09:41,230
Basically the subjects for whom there's no impact and they stay at zero divided by the subjects that are zero under treatment.

70
00:09:42,760 --> 00:09:49,570
So since there's only one way to assign this the expected value of this, how happy is is going to equal to how happy.

71
00:09:50,290 --> 00:09:55,210
And so if they're unbiased, then the two have to equal each other, which requires this condition.

72
00:09:55,900 --> 00:10:01,120
So some people went and expanded this, you know, sort of try to work this out, but there's a lot of extra gain there.

73
00:10:01,120 --> 00:10:07,330
But that's fine. But this is really the key point. I think most people got many questions at this point.

74
00:10:09,110 --> 00:10:15,769
Okay. The second one was a slightly trickier, which was basically asking you if this provide an unbiased estimate of all possible

75
00:10:15,770 --> 00:10:20,390
values of Tao compared to anywhere between zero and one minus one and one.

76
00:10:20,990 --> 00:10:29,680
If everybody's in this this case where they are one that her treatment of zero to prove and then Tao has one and as everybody's in this

77
00:10:29,690 --> 00:10:42,239
reverse case and Tao had the town three Tao is negative one value so so it's pretty clear that Tao had p can't be non-negative right?

78
00:10:42,240 --> 00:10:52,170
Pieces of count. So if Tao. I guess this is sorry.

79
00:10:53,130 --> 00:10:56,520
It must be biased, right? It's an error.

80
00:11:10,730 --> 00:11:17,630
So now the trickier thing to see is that if Tao had Tao P as equal to one,

81
00:11:18,410 --> 00:11:27,320
then that basically requires that in one zero equal n, which implies all the other possible cells are empty.

82
00:11:28,520 --> 00:11:35,600
Right. In this case, nobody is going to get assigned a control. So you're not going to ever be able to define Tao happy.

83
00:11:36,800 --> 00:11:41,030
But if Tao IP is less than one then some subject will get assigned to both treatment control.

84
00:11:41,030 --> 00:11:47,650
So that'll be can be estimated. So basically it provides an unbiased, potentially unbiased estimated right of this condition is.

85
00:11:47,660 --> 00:11:51,470
But if tao p lies in this speech here.

86
00:11:52,700 --> 00:11:59,089
So most people got the idea it had to be not negative.

87
00:11:59,090 --> 00:12:03,470
But to tell people one is a slightly more subtle point.

88
00:12:07,290 --> 00:12:14,830
But you. All right.

89
00:12:15,940 --> 00:12:27,250
And for part three, I basically just created some simulated data in a randomized trial with arbitrary idea from 1 to 1000,

90
00:12:27,550 --> 00:12:30,580
then a treatment assignment and an observed outcome.

91
00:12:32,380 --> 00:12:38,290
So assuming the data for a random population, I asked you to compute the sum of the population.

92
00:12:38,290 --> 00:12:44,020
Average causal factor. Really got that? You were just taking the means of the observed values under treatment and control.

93
00:12:46,300 --> 00:12:50,170
And then if we assume this correlation between the potential outcomes is 0.5.

94
00:12:50,950 --> 00:12:55,930
I asked you to complete the computer variance of tell tale happy and associated confidence interval.

95
00:12:57,100 --> 00:13:00,900
So here we have to recognize that are unbiased estimate or if we know row.

96
00:13:01,210 --> 00:13:02,890
The problem is we typically can't do that.

97
00:13:02,890 --> 00:13:09,880
But if we were to say some serious situation where we could, unusual situation we could or would just simply doing a sensitivity analysis,

98
00:13:11,020 --> 00:13:21,400
then then we're going to plug in our value here and all the other quantities can be our unbiased estimates of the, of the population values.

99
00:13:22,750 --> 00:13:30,639
So you kind of have to. Right. So I think I think this may be written as covariance of X and Y.

100
00:13:30,640 --> 00:13:38,530
So you just have a little identity there. So when you have that all together, you can compute all these various values,

101
00:13:38,530 --> 00:13:43,810
you get the associated variance and then the approximate 95% confidence interval accordingly.

102
00:13:44,440 --> 00:13:55,570
And I think most people do like that. Fine, do a little more work here kind of going through these results and in Section 6.5 with Rubin.

103
00:13:56,830 --> 00:14:04,150
And so they give you an estimate of how happy under the assumption of a constant treatment effect.

104
00:14:04,930 --> 00:14:11,169
In which case this quantity essentially disappears and you just can use this one squared over.

105
00:14:11,170 --> 00:14:23,260
It's not squared. If you assume heterogeneity of treatment then but not not assuming a particular value for p they suggest for per row.

106
00:14:23,860 --> 00:14:29,560
They suggest using this estimate here which assumed rho is equal to point but is slightly more efficient.

107
00:14:30,730 --> 00:14:37,050
So if. So if there's any if there's any difference between us one and a zero.

108
00:14:39,830 --> 00:14:43,520
So we get a socially competent recording.

109
00:14:43,880 --> 00:14:53,330
Accordingly. So finally, I asked to assume the draw from an infinite super population so that basically your point isn't always unchanged,

110
00:14:53,330 --> 00:14:58,760
whether it's population or super population. The variance estimate are basically this extra.

111
00:14:59,720 --> 00:15:06,980
The extra covariance term goes away and it's just equivalent to the assumption of the population of Spain and that constant treatment effect.

112
00:15:07,700 --> 00:15:12,590
And so we just basically get the result from per day in Part C one.

113
00:15:14,210 --> 00:15:19,430
So you can't you can see there's not a lot of difference here. Part of that is the sample size, which is pretty decent sized.

114
00:15:20,630 --> 00:15:26,630
So the difference between the population estimate and the super population estimate isn't that large because

115
00:15:26,630 --> 00:15:32,120
there's because essentially most of the uncertainty comes from the fact that you don't observe the counterfactuals.

116
00:15:33,100 --> 00:15:37,610
So so that extra extra pieces is not so big.

117
00:15:40,070 --> 00:15:43,790
And accordingly, the assumptions about sort of what's important.

118
00:15:43,790 --> 00:15:48,070
So that big difference is small ones, but not zero good.

119
00:15:50,710 --> 00:15:58,030
Any questions about that? There were some ups and downs in this one, but hopefully that sorted out for summer.

120
00:15:59,370 --> 00:16:02,400
All right.

121
00:16:03,090 --> 00:16:15,360
So the hardest one, of course, was I asked you to do a bit of coding to try to basically incorporate randomized cohorts in a randomized trial.

122
00:16:16,440 --> 00:16:19,559
So it's basically the same data we had in weeks, I should say.

123
00:16:19,560 --> 00:16:33,450
Three, four, one, three, four. But basically this extra column of X which is a pretreatment co-create.

124
00:16:34,980 --> 00:16:43,590
So, so for the first part, I just had you focus on the, the trial data.

125
00:16:44,920 --> 00:16:55,580
So, so my results here will be maybe a bit of variability in different results due to simulation variability.

126
00:16:56,550 --> 00:17:06,090
But generally, I think this is worked pretty well. So in terms of the actual coding for this, people have some different approaches.

127
00:17:06,990 --> 00:17:10,830
But my approach is basically to generate a vector in the treatment and control.

128
00:17:10,830 --> 00:17:18,780
We're assuming, by the way, independence is at the individual level.

129
00:17:18,780 --> 00:17:23,100
So there is essentially no the covariance between Y one and y010.

130
00:17:24,210 --> 00:17:34,020
To make this a little simpler. So, so again, I just needed some way to kind of kick off this, this sampler.

131
00:17:34,020 --> 00:17:39,570
So I just got linear regression of the treatment on the outcome.

132
00:17:40,230 --> 00:17:54,360
Adjusting for the excellently treated group, an excellent control group got my betas and estimates of my variances or standard deviations there.

133
00:17:56,070 --> 00:18:10,440
So. So then I created some design matrices because I actually wanted sort of you could fit all of this in different ways.

134
00:18:10,440 --> 00:18:16,680
But I decided I wanted to sort of really get down into the to the sort of under the hood here a little bit, so to speak,

135
00:18:17,310 --> 00:18:25,780
and and work directly with with the with the design matrices and the and the linear algebra approach to the Russian.

136
00:18:26,660 --> 00:18:34,649
So so it means I needed to include a which this is an issue I think some people might have forgotten.

137
00:18:34,650 --> 00:18:38,459
I mean, do include a random intercept here. Right.

138
00:18:38,460 --> 00:18:44,490
So if I'm going to do X, transpose x x transpose, it's transpose x inverse x, transpose Y,

139
00:18:44,880 --> 00:18:52,080
my x needs to include the intercept as well as the actual x to the single scalar covariant.

140
00:18:52,590 --> 00:18:59,180
I call this x one star to kind of distinguish that from x one in the same thing for the control group piece.

141
00:19:00,970 --> 00:19:08,580
So and then I generate an entire prediction values by combining my observed values with my,

142
00:19:09,810 --> 00:19:18,000
my predictions on the control group had they been assigned to treatment.

143
00:19:19,350 --> 00:19:26,130
And similarly for the predictions under the treatment group, had they been assigned to control,

144
00:19:26,790 --> 00:19:29,370
combine those with the observed treatments and observed controls.

145
00:19:33,530 --> 00:19:37,669
So now I'm going to kind of run this Gib sampler now and I'm going to put in the Xs and Ys together.

146
00:19:37,670 --> 00:19:49,460
So I kind of I created a little the four pitfall vector of Xs and arbitrarily I'm just assigning the first 500 observations

147
00:19:50,360 --> 00:19:54,860
to be the subjects that were assigned to treatment and then this 500 to be the subjects were assigned to control.

148
00:19:55,970 --> 00:19:59,150
They're going to need this x star to include my random intercept.

149
00:20:02,600 --> 00:20:05,629
Okay. So that's kind of just preparing everything.

150
00:20:05,630 --> 00:20:21,360
Are there questions about this? So then to do the actual run of the thing, then I needed to generate my draws of data.

151
00:20:22,560 --> 00:20:30,530
Right. So I just use x transpose x inverse x, transpose Y for the Y ones.

152
00:20:30,540 --> 00:20:35,430
Now the Y ones in parentheses, if you will, the the combination of observed and predicted.

153
00:20:41,100 --> 00:20:46,210
Outcomes that are treatment and the same thing for outcomes under control.

154
00:20:46,620 --> 00:20:53,250
To get my overall data now, I need to account for the uncertainty in the estimation of the data.

155
00:20:54,150 --> 00:21:05,549
And so I do that by doing a draw from the posterior knee, which we had worked out before as basically centered at these data.

156
00:21:05,550 --> 00:21:08,610
Has this variance covariance equal to.

157
00:21:11,130 --> 00:21:15,390
This one squared x transpose x and it's not squared x transposons.

158
00:21:19,430 --> 00:21:26,330
And then I do my growth as a sigma. So it basically said A and B here to zero.

159
00:21:27,230 --> 00:21:35,870
And so I'm just doing it's an inverse schema. So that means I can do a draw from the gamma distribution equal to N minus two and Y to.

160
00:21:46,470 --> 00:21:58,970
I know one or 300. That to represent a number of parameters.

161
00:21:59,480 --> 00:22:02,570
Right. The intercept and the slope from X.

162
00:22:03,230 --> 00:22:10,670
Right. So it's the number of columns you go on the X matrix and then then the residual sum squares.

163
00:22:12,380 --> 00:22:21,500
Same thing here. So when I take the reciprocal of that, because I'm doing this inverse, can we draw the square root?

164
00:22:21,500 --> 00:22:26,960
Because I'm just using S1 in a zero to be on in the standard deviation in the variance scale.

165
00:22:28,610 --> 00:22:33,800
So that's all my parameter draws and I'm not going to impute my my counterfactuals.

166
00:22:35,070 --> 00:22:35,320
Right.

167
00:22:35,330 --> 00:22:50,540
So again, I'm going to compute based on the predicted value, given my observed covariance in my most recent draw data and then adding an error term.

168
00:22:52,020 --> 00:22:55,190
Right. All right. No one uses the square root of the variance.

169
00:22:58,310 --> 00:23:01,640
So. Okay.

170
00:23:04,570 --> 00:23:10,830
All right. So then I have essentially my full imputations now.

171
00:23:12,150 --> 00:23:15,910
So then I just take the mean of that to get a draft of this town.

172
00:23:15,960 --> 00:23:25,820
What the difference in those means? And I just do that like 5000 times and it just gives me the results I have take.

173
00:23:26,870 --> 00:23:35,870
It took me I didn't ask you to give them the credible interval, but you got it anyway.

174
00:23:37,330 --> 00:23:45,550
That's right. So most folks, I think, did fine on this.

175
00:23:46,690 --> 00:23:53,160
There were some confusion, I think, about the need for the intercept and some of the things I think that kind of went off the rails.

176
00:23:53,170 --> 00:24:01,030
But again, you have questions, what we know now or in the future.

177
00:24:09,450 --> 00:24:17,250
Okay. So part B is where is where? I think things went a little off the rails and this was mostly my fault for most he was a problem, I think.

178
00:24:18,030 --> 00:24:18,470
So. Basically,

179
00:24:18,480 --> 00:24:27,930
I wanted you to include all of the available data in the population so these additional axis and you should end up getting different results.

180
00:24:28,620 --> 00:24:31,319
And I asked you to think a little bit about why that was so.

181
00:24:31,320 --> 00:24:40,900
And many of you plowed forward this almost always correctly as to why there would be different results even if sometimes you didn't get them, which.

182
00:24:40,930 --> 00:24:46,409
Okay, that's good. I guess so. So if you if you've done it correctly,

183
00:24:46,410 --> 00:24:50,190
you should get something sort of an arranged like this and a few people did because

184
00:24:50,190 --> 00:24:58,620
either by bye bye luck of how you where you started your draws or by doing it correctly,

185
00:24:58,620 --> 00:25:06,150
even though I was pretty vague about how to do that. So and of course, the reason it's higher is because there's two things going on.

186
00:25:08,010 --> 00:25:09,330
Well, I'm sorry.

187
00:25:09,480 --> 00:25:22,410
There's there's there's a difference in the mean of X in the sample 1000 people sample versus the 19,000 X rated individuals in the population.

188
00:25:23,310 --> 00:25:28,850
And also the relationship between Y and X is different in the treatment group too.

189
00:25:28,980 --> 00:25:35,730
So the outcome of the treatment is associated with X in a different way than the outcome of the controls.

190
00:25:36,990 --> 00:25:43,500
So the fact the slopes are different means the treatment effect itself interacts with X.

191
00:25:45,180 --> 00:25:55,050
So you have both these things going on. Then your estimate of the treatment effect using the trial, even if it's randomized,

192
00:25:55,620 --> 00:25:59,520
may not be unbiased estimator of the treatment effect in the population.

193
00:25:59,920 --> 00:26:09,719
Yeah. So so the the way to do this correctly though I think I was I was quite big on it has to do with

194
00:26:09,720 --> 00:26:14,400
when you do the draws of the of the betas in the segments you apply them to the population.

195
00:26:14,790 --> 00:26:22,440
They essentially need to sort of read you use those draws so you didn't forayed and use those to impute out to the rest of the axes.

196
00:26:22,950 --> 00:26:26,250
Or you could do what I did here, which is basically just restart it.

197
00:26:26,820 --> 00:26:35,880
And when you do the draws, the draws of the betas have to depend only on the at least partially observed outcomes.

198
00:26:36,690 --> 00:26:49,720
So the logic here can be a little. Mary.

199
00:26:52,820 --> 00:27:03,070
So. So basically you've got this computer data or predictive data.

200
00:27:06,580 --> 00:27:13,750
You want the distribution for that given user data so that space is going to be proportional to this.

201
00:27:28,900 --> 00:27:32,110
So here, basically, I've got no.

202
00:27:39,420 --> 00:27:47,440
One. So I have.

203
00:27:58,320 --> 00:28:04,620
I have partial data here. So when I'm having one kind of condition y0 and vice versa like this.

204
00:28:05,460 --> 00:28:11,280
But what I'm putting down here, I have really no information about.

205
00:28:11,610 --> 00:28:14,670
There's no way out, so to speak, of all of this excuse.

206
00:28:21,600 --> 00:28:25,320
So the basis of this becomes.

207
00:28:42,160 --> 00:28:53,070
So this you're basically getting from before aid, right?

208
00:28:53,080 --> 00:29:00,790
We sort of bounce back and forth between this sort of division part where we have partial information,

209
00:29:01,670 --> 00:29:05,290
where we don't we're going to use these to ask for data.

210
00:29:06,190 --> 00:29:11,950
And then we go in here, we just plug in.

211
00:29:25,620 --> 00:29:32,490
Just plug in under our feet and generate. So I should have made this clear.

212
00:29:37,060 --> 00:29:41,530
Again, I'm assuming you have a lot of previous background here.

213
00:29:42,730 --> 00:29:53,200
So. So again, you can kind of kick things off.

214
00:29:56,170 --> 00:29:58,270
You know, this isn't entirely necessary at this point.

215
00:29:59,050 --> 00:30:08,260
With your current estimates of Paternot and Beta, one of the previous draws are from this just initial regression.

216
00:30:09,730 --> 00:30:13,629
Why The Observer? Why is your treatment under index and observed?

217
00:30:13,630 --> 00:30:19,840
Why is it progress? But I'm going to impute now these other 19,000 observations.

218
00:30:24,840 --> 00:30:28,560
I'm sorry to kind of change this to selection, to politics.

219
00:30:34,700 --> 00:30:48,500
So I'm using this creating this additional 19,000 access point topics to refer to the population and then adding on to get this intercept term here.

220
00:30:49,400 --> 00:30:52,610
So, so when I do my.

221
00:30:58,030 --> 00:31:04,820
We can I compute my betas do my drawers operators and my asses just exactly they didn't for a.

222
00:31:05,030 --> 00:31:08,390
So that's getting that sort of draws of of betas and s is.

223
00:31:12,350 --> 00:31:22,430
But to impute the counterfactuals, then I use those draws of betas and essence, as I did before in the trial data.

224
00:31:23,180 --> 00:31:27,050
But then I also use them on.

225
00:31:32,270 --> 00:31:39,430
And the control data down here. So if you don't do that, it's going to really take a long time, if ever before it converges.

226
00:31:39,970 --> 00:31:41,280
It depends on where you start.

227
00:31:41,290 --> 00:31:48,340
If you sort of start at the right values for where beta one and beta zero from previous, you'll probably get away with it.

228
00:31:48,880 --> 00:31:54,190
Otherwise, some of you took like just the overall means and things like that, and then it's going to take a very long time to converge.

229
00:31:54,400 --> 00:31:59,110
The right thing to do is only do the draws of the color of the parameter values

230
00:32:00,040 --> 00:32:03,790
from the observations where you actually have information about those parameters.

231
00:32:03,790 --> 00:32:11,380
Right? In the rest of this population dataset, we don't have any information about why one and why zero.

232
00:32:13,660 --> 00:32:19,210
And so and so we can only we should only be strictly doing.

233
00:32:21,430 --> 00:32:29,530
We don't need that. We shouldn't be using those and trying to estimate my generic theta and more specifically, my betas and my my signals here.

234
00:32:30,940 --> 00:32:38,650
So, so that's the extra piece there. Of course, once I have all that, then then I can just do the same thing we did before.

235
00:32:41,760 --> 00:32:57,680
So that was the somewhat confusing part. Okay.

236
00:33:00,330 --> 00:33:16,580
So sure, that's a little bit. Each time we turn to finish up principal certification and go on to hopefully we can start an instrumental variable.

237
00:33:16,580 --> 00:33:20,660
So we have time today just before we start.

238
00:33:23,570 --> 00:33:36,290
Okay. So we talked about this is sort of a special was a particular application of this idea of princess stratification, so in non-compliant settings.

239
00:33:37,070 --> 00:33:48,799
So this is a situation where individuals, when their treatment assignment is, is attached to what their physician is, is getting,

240
00:33:48,800 --> 00:33:57,650
or then are they in an arm where the position is being encouraged to sort of do what's considered to be the current

241
00:33:57,650 --> 00:34:03,920
standard of care or they are an arm or are they position kind of doing whatever does whatever they're going to do,

242
00:34:03,920 --> 00:34:07,040
which may or may not be a full standard of care in this case,

243
00:34:07,280 --> 00:34:17,090
sort of additional cognitive behavioral therapy work and in addition to whatever medication they were doing in treatment of depression.

244
00:34:18,050 --> 00:34:21,440
So. So this.

245
00:34:29,770 --> 00:34:38,330
So in this case we were I'm sorry so we we've gotten into a specific example here and the of but.

246
00:34:46,070 --> 00:34:54,380
So basically this allows sort of of our four possible settings, right?

247
00:35:05,990 --> 00:35:10,260
So we basically can have never takers. Right.

248
00:35:10,280 --> 00:35:20,060
So these are situations where the patient was working with the doctor on on control and they didn't get the extra therapy.

249
00:35:21,400 --> 00:35:27,889
And we can have compliance. And if they're on treatment, for whatever reason,

250
00:35:27,890 --> 00:35:31,610
they're also not getting the extra therapy that they don't want to do it or the doctors are giving it to them.

251
00:35:32,940 --> 00:35:41,890
And in in even though they're getting this additional encouragement and we can have compliance.

252
00:35:41,900 --> 00:35:48,590
So. Right. If they're assigned if they're if they're working a physician assigned to control, they don't do the extra work, the therapy.

253
00:35:49,250 --> 00:35:54,110
If they're working with a physician that's been assigned to treatment, they do end up getting this cognitive behavioral therapy.

254
00:35:55,970 --> 00:35:57,340
Theoretically, we could have to fire.

255
00:35:57,360 --> 00:36:02,630
So maybe somebody who gets cognitive therapy or if they're working with a physician, that's not getting that additional work.

256
00:36:03,230 --> 00:36:12,350
But for some bizarre reason, maybe under that that possibly due to the physician behavior might be more than the

257
00:36:12,350 --> 00:36:15,950
patient behavior that they're not getting the therapy if they're under treatment.

258
00:36:17,210 --> 00:36:22,460
And finally, the always takers, right? So these would be subjects where they go ahead and get the treatment.

259
00:36:22,460 --> 00:36:27,920
If there are physicians being encouraged, maybe somehow they access it, either because the physician does it anyway,

260
00:36:27,920 --> 00:36:32,240
even the absence of encouragement, or they seek it out in some fashion.

261
00:36:32,810 --> 00:36:40,040
So they're sort of getting this extra, extra piece. So so those are sort of the four possible options.

262
00:36:40,790 --> 00:36:48,769
And of course, we could have different means associated with it, with an outcome with with each of these possible treatment arm.

263
00:36:48,770 --> 00:36:54,530
So really eight possible sort of possible means we could be working with here.

264
00:36:55,550 --> 00:37:01,970
But in practice, we're going to kind of do a couple of things, a couple of assumptions.

265
00:37:03,050 --> 00:37:15,990
One is we're going to assume. But assume this exclusion restriction, which basically says unless you're somehow getting this extra therapy,

266
00:37:16,590 --> 00:37:19,409
then you're not going to benefit from your physician being on that treatment.

267
00:37:19,410 --> 00:37:24,990
Or potentially you think in most cases that's a pretty reasonable assumption.

268
00:37:25,350 --> 00:37:33,420
You're actually somewhat iffy because it's possible that just the experience of sort of getting this extra push

269
00:37:34,290 --> 00:37:42,540
from from from the treatment may cause the the physician to interact somewhat differently with their patients.

270
00:37:43,920 --> 00:37:50,729
So you could imagine a scenario where there might be some some non-zero effect, but we think would probably be pretty small.

271
00:37:50,730 --> 00:37:54,660
And so we're going to assume it's zero and they're going to say,

272
00:37:54,660 --> 00:38:07,110
Mr. Fire assumption notifier is assumption basically that if you were going to get get the treatment when you're in the encouragement arm, then,

273
00:38:08,160 --> 00:38:12,540
all right, well, if you're not going to get the treatment and encouragement arm,

274
00:38:12,540 --> 00:38:18,900
that is not somehow you're going to get the treatment on the control or encouragement shouldn't cause you to not get the treatment.

275
00:38:20,010 --> 00:38:26,580
So probably again, more reasonable, although in a setting, possibly physicians sometimes don't like being told what to do.

276
00:38:27,840 --> 00:38:35,370
It's conceivable you could actually get some to fire behavior. But but we're we're going to make that assumption that doesn't occur here.

277
00:38:37,680 --> 00:38:48,780
Okay. So our actual application was actually it appears in a 24 paper coauthor,

278
00:38:50,400 --> 00:39:00,870
71 patients were randomized to either this usual care or to get this this telephone encouragement sort of standard of care treatment.

279
00:39:03,750 --> 00:39:11,040
So we're using S to basically, you know, indicate whether they were treated under here in the guidelines regardless of treatment arm.

280
00:39:12,450 --> 00:39:16,020
And our outcome is is continuous measure which is the number of depressive symptoms.

281
00:39:18,480 --> 00:39:23,700
So observed data again some of this before the sort of evidence of encouragement is not perfect.

282
00:39:24,480 --> 00:39:28,930
Right. There are some subjects that are not getting the cognitive therapy.

283
00:39:28,930 --> 00:39:30,750
You know, there are physicians being encouraged to give it.

284
00:39:31,770 --> 00:39:35,429
And then there are some subjects that are actually accessing it or getting it from their physician,

285
00:39:35,430 --> 00:39:37,350
even though they weren't getting the extra encouragement.

286
00:39:40,250 --> 00:39:48,829
So are outcome based little histograms in the treatment control here if we look at the intent to treat effect, right.

287
00:39:48,830 --> 00:39:58,540
So this is in some sense an overall measure of the treatment effect. Allowing for noncompliance suggest a sort of mild reduction.

288
00:39:58,540 --> 00:40:06,980
Not not really quite statistically significant. If we look at the people that actually got treatment and we do see a much more substantial reduction.

289
00:40:09,800 --> 00:40:17,040
So. So we could we could, of course, work with this.

290
00:40:18,570 --> 00:40:22,709
But can anyone express sort of what are possible concerns about the as treated

291
00:40:22,710 --> 00:40:28,380
group in terms of really being a measure of the effect among the players?

292
00:40:40,270 --> 00:40:48,550
So we're comparing the people that the treatment on the treatment arm with the people that were on the that control behavior under control arm.

293
00:40:49,870 --> 00:40:54,490
What is that? What is the issue with that for actually trying to estimate this counterfactual complaints?

294
00:41:05,120 --> 00:41:08,480
Affected where we bothering to? Not just stop here. Call it a day.

295
00:41:19,150 --> 00:41:24,160
So remember, we're looking at this counterfactual difference, that treatment. We want to get an estimate of.

296
00:41:27,300 --> 00:41:38,040
The outcome under treatment, minus the outcome under control for people that would have complied under treatment.

297
00:41:38,220 --> 00:41:43,330
Yes. Is a have to do with that. We don't know the proportion of always takers.

298
00:41:43,860 --> 00:41:47,300
Right. Part of the same. Right. Right.

299
00:41:47,310 --> 00:41:56,790
So we look at this, we're looking at a mixture. So the people that are observed to take the treatment are a mixture of of what we've always.

300
00:41:56,790 --> 00:42:01,530
And yes. And the people assigned a control or a mixture of what.

301
00:42:05,380 --> 00:42:09,330
Never. Yes. So.

302
00:42:10,630 --> 00:42:21,400
So we've got this sort of funny mix. We need to kind of try to drop those those other groups off and and get to that piece.

303
00:42:22,120 --> 00:42:25,239
So if we assume amount of tenacity and exclusion restriction. Right. You've only got this.

304
00:42:25,240 --> 00:42:32,020
Can players never takers. Always takers. And we're further going to assume this exclusion restriction.

305
00:42:32,020 --> 00:42:35,950
So that basically suggests that, you know, the never takers and the always takers,

306
00:42:35,950 --> 00:42:42,520
it doesn't matter what you're saying to you just have this common mean. But in the we are allowing for the possibility of a treatment effect.

307
00:42:44,800 --> 00:42:52,600
So we're going to go ahead and do this in a Bayesian format, basically assuming basically sort of flat priors on these means.

308
00:42:54,070 --> 00:43:04,390
So this directly is kind of like a multivariate extension of the of the beta distribution, I think, rather than get too much into the math behind it.

309
00:43:06,050 --> 00:43:11,290
Oh, I didn't mean to show on the I'll try to send a note of this and.

310
00:43:19,060 --> 00:43:27,860
It's basically so. So if you have a dichotomous outcome, a01 outcome with some probability theta,

311
00:43:28,550 --> 00:43:39,709
then basically the posterior disposition of theta follows a beta distribution where you sort of adjust the number of if you have a beta prior,

312
00:43:39,710 --> 00:43:45,770
you basically include the number of successes and failures on your on your sort of two parameters.

313
00:43:45,770 --> 00:43:51,380
And so the the mean then becomes close to the proportion of successes that you would like.

314
00:43:51,890 --> 00:43:55,070
And the variance is sort of a function of how many observations you have.

315
00:43:55,070 --> 00:43:59,959
So it gets tighter around that issue increase. So the duration is basically a multi normal extension of that.

316
00:43:59,960 --> 00:44:06,230
So essentially what's going to happen is you're going to get something that looks like the posteriors of these

317
00:44:06,230 --> 00:44:11,780
PI's are going to correspond to the fraction of individuals that are assigned to each of these classes.

318
00:44:13,160 --> 00:44:17,510
And if you use the sort of one on one, that's kind of corresponds to this idea of a flat prior,

319
00:44:18,230 --> 00:44:27,830
if you might want to think of it as one observation, will the number of the if you take,

320
00:44:29,810 --> 00:44:34,040
let's say, K observations where K is the number of categories and you're multi nominal,

321
00:44:34,520 --> 00:44:39,559
then one over K each, each of those observations is going to drop into one cell.

322
00:44:39,560 --> 00:44:44,540
So the prior probability is sort of one over K of being in any one of those cells.

323
00:44:45,500 --> 00:44:51,409
So and then your posterior basically gets inflated by the number of individuals in a

324
00:44:51,410 --> 00:44:58,810
given so group it counts of these C and now divided by the total sample size yet again.

325
00:44:58,820 --> 00:45:03,470
It's kind of flat prior on for any prior one number on the variance.

326
00:45:05,330 --> 00:45:16,370
So yeah, usually there's an assumption about like, you know, either the campfires, the navigators or the always right here.

327
00:45:16,700 --> 00:45:21,469
Yeah, you're right. And that one of the assumptions that one of the populations is the same is that we're

328
00:45:21,470 --> 00:45:25,160
assuming that the weather with the retreat or to control the mean is going to be the same.

329
00:45:25,730 --> 00:45:31,610
So there's no effect on the outcome. Right. So if you never if there's a mean no if no treatment effect.

330
00:45:32,600 --> 00:45:39,170
So if you're never take or basically whether your your physician was being encouraged to do that treatment or not,

331
00:45:39,770 --> 00:45:42,440
it didn't matter because you weren't getting an extra treatment.

332
00:45:42,560 --> 00:45:45,590
And the only way that you would get a difference in your depression would be through that.

333
00:45:45,590 --> 00:45:49,610
It's a treatment that's always takers is around because they always get it either way.

334
00:45:50,060 --> 00:45:53,660
So yeah, right. So that's why we're not using delay here.

335
00:45:54,860 --> 00:45:58,360
So we basically have this just for fun here.

336
00:45:58,360 --> 00:46:09,650
And one, two, three, four, five, six, seven, eight parameters really seven are for sort of three because this these these three pose a 2 to 1.

337
00:46:12,050 --> 00:46:26,870
So as we worked out previously. Well, again, this this is this mixture component, right?

338
00:46:26,870 --> 00:46:33,560
So if you're on if you're observed to to take the treatment have been assigned to it,

339
00:46:33,680 --> 00:46:41,360
you could be a compliant or you could be an always take her if you're observed not to take the treatment given the word assignment,

340
00:46:41,360 --> 00:46:44,690
you could be a compliant or never take her in.

341
00:46:47,060 --> 00:46:50,000
Now, what about subjects that are right, discordant, right?

342
00:46:50,000 --> 00:46:56,960
If they're encouraged to take the treatment and they don't, then we know they're never taken.

343
00:46:57,110 --> 00:47:00,140
Right. That's a kind of a definition in the same way, the other way around.

344
00:47:00,890 --> 00:47:05,540
If they somehow ended up on the treatment, even though they weren't assigned to it, then they're going to be always taken to.

345
00:47:05,540 --> 00:47:12,369
There's no uncertainty about these. So we can kind of go through and fit this model using the Gibbs sampler.

346
00:47:12,370 --> 00:47:18,849
So we're going to get a draw of the compliance class and then draw of the probabilities underneath those

347
00:47:18,850 --> 00:47:24,100
components classes and then a draw of the means and the variances given those those components classes.

348
00:47:25,180 --> 00:47:30,010
So I think maybe the most the easiest way to sort of see we have some initial estimates of the means.

349
00:47:30,880 --> 00:47:40,450
And so as we worked out before, the posterior of being a compliance class basically is the prior probability of being in a compliance class times

350
00:47:41,800 --> 00:47:49,270
the PDF of Y given you're in that compliance class summed over all the possible compliance classes you could be in.

351
00:47:50,680 --> 00:47:54,610
Right. So basically if.

352
00:48:08,510 --> 00:48:13,070
So we know if if he is not equal to s, there's no way they could be a team player.

353
00:48:13,310 --> 00:48:16,340
Right. Definition components, among other things, includes that.

354
00:48:16,620 --> 00:48:23,990
Right. So the only issue is if there are comply, if they're observed to take the treatment given assigned,

355
00:48:24,950 --> 00:48:31,700
then they could be either a player or a never take her or not always take her.

356
00:48:32,810 --> 00:48:41,630
So the probability of being supplier is basically this this first fraction I can tie time's pi times p of y given.

357
00:48:44,230 --> 00:48:51,520
Or P of being a problem that requires class times properly, observing the data, giving the complaints class,

358
00:48:52,570 --> 00:49:01,110
and then some do over the probability of the merger probably ought to be a always taker times.

359
00:49:02,620 --> 00:49:04,630
The pdf of the Y given you're always taken.

360
00:49:06,340 --> 00:49:16,809
And if you're in the situation where you're not taking the treatment and you were assigned to the control group,

361
00:49:16,810 --> 00:49:21,130
then you could be either comply or never take her. So the probability of compliance kind of works out the same way.

362
00:49:27,560 --> 00:49:34,790
And the probability of being a never taker. Well, if if you observe, take the treatment that has to be zero.

363
00:49:35,870 --> 00:49:39,140
And if you're driven to take the if you observe the to.

364
00:49:40,130 --> 00:49:43,310
I'm sorry. Right.

365
00:49:44,000 --> 00:49:48,890
So if you don't take the treatment and you were assigned to take it, then you know you're never taken.

366
00:49:49,250 --> 00:49:56,960
So that's going to be one. So the only uncertainty, again, comes in this situation where you're both both equals zero.

367
00:49:57,530 --> 00:50:01,880
In which case now this is kind of the flip side of this part here. Right.

368
00:50:02,960 --> 00:50:11,780
You're just using the probability of being never taker times pdf of being an ever taker given the observed data.

369
00:50:14,570 --> 00:50:20,780
And the same thing on the always side. Right. So if you don't take the treatment, you can't be and always take her.

370
00:50:21,620 --> 00:50:29,160
If you take the treatment given you're assigned to take control, then that you know, you're don't always take her in.

371
00:50:29,210 --> 00:50:33,230
Otherwise you're you're put a part of this probability, right?

372
00:50:33,230 --> 00:50:41,990
The probability of being and always take her versus a player given that you take the treatment and we're assigned to it.

373
00:50:45,590 --> 00:50:49,460
So. All right. So that kind of sorts out.

374
00:50:50,120 --> 00:50:53,600
We want to get everybody this is the observe day. We're trying to fill in all this unobserved data.

375
00:50:54,170 --> 00:50:59,030
So this gives us the compliance class. It's always going to be the hardest part of the problem.

376
00:51:02,790 --> 00:51:35,320
Questions about. So that you're going to go through and basically try to see, given these subjects level probabilities of being in compliance,

377
00:51:35,330 --> 00:51:43,010
never take or always take or some of these, you know, so you only have to draw it if it's going to be equal to that, to their right.

378
00:51:43,010 --> 00:51:51,380
If they're always take or never take her with the associated treatment it controls, it match that and you don't to do the draw.

379
00:51:51,860 --> 00:51:57,709
The issue is for the situations where a an S or or B which are in which case you'll be drawing

380
00:51:57,710 --> 00:52:02,060
from either a player and never take her probability for a player and always take a probability.

381
00:52:04,360 --> 00:52:10,960
Okay. So once you've got all that lined up, it's pretty easy then to get these these lambdas.

382
00:52:13,900 --> 00:52:23,290
You just draw from your own distribution centered at the mean and variance divided by the number of individuals and in the complaint player group.

383
00:52:24,700 --> 00:52:31,060
So you look at the estimate of the mean among subjects that are compliance, that are assigned to treatment,

384
00:52:32,320 --> 00:52:38,560
and the same thing for players that are assigned to control that the associated spheres,

385
00:52:39,640 --> 00:52:51,340
right because normal distribution, the draw of theta given the known sigma squared is going to be centered at the mean of

386
00:52:52,690 --> 00:52:58,750
those values with variance given by the variance divided by the number of observations.

387
00:53:00,250 --> 00:53:03,770
And the same thing on the never taken.

388
00:53:03,790 --> 00:53:07,419
Always taken. So here we don't we don't have to separate these, right.

389
00:53:07,420 --> 00:53:10,730
Because we assume it's equal regardless of whether you're.

390
00:53:16,990 --> 00:53:21,020
So then for the variance. Right.

391
00:53:21,220 --> 00:53:29,980
We can just compute. This some of squares tracking off whatever the appropriate mean is given their compliance class.

392
00:53:30,820 --> 00:53:42,410
And if they're compliant, their treatment stays. And then the last part is we can draw our pie from this delicious distribution.

393
00:53:43,040 --> 00:53:48,739
Again, I'll give you a little a little more background than I thought it was in the notes, but I guess is the Bayesian notes.

394
00:53:48,740 --> 00:53:58,970
So basically it's a it's this sort of extended autonomy, but now the parameters, you put the number of observations.

395
00:53:59,660 --> 00:54:05,630
So the number of players is sort of the number of observe and part of the players under the treatment of players with control.

396
00:54:06,470 --> 00:54:14,390
Never takers. Always takers. You just draw a pie from that and you keep going round.

397
00:54:14,720 --> 00:54:18,710
Go back up. Now you get new pies, got new lambdas and new segments.

398
00:54:18,920 --> 00:54:24,350
So you update your your services, the probabilities, murders, uncertainty.

399
00:54:25,610 --> 00:54:29,120
Then you with the lose, do those redo those assignments.

400
00:54:29,990 --> 00:54:33,980
Then we estimate those means. Redo the draw.

401
00:54:35,890 --> 00:54:41,910
So. So here is my sort of resulting change to pretty good convergence.

402
00:54:42,040 --> 00:54:48,130
It took a little while. I think there's a fair amount of uncertainty.

403
00:54:48,520 --> 00:54:57,520
We don't have a really big sample for the lambdas, particularly for the player groups, because of uncertainty there.

404
00:54:57,520 --> 00:55:01,300
There seems to be a little bit of bouncing around the map, but too bad.

405
00:55:02,890 --> 00:55:05,950
And my variance, which is to common, is pretty stable.

406
00:55:08,140 --> 00:55:13,480
So sorry my past here means then basically about 30% of the subject were compliance.

407
00:55:14,320 --> 00:55:21,100
About a quarter were never takers, and the residual, about four in ten were always takers.

408
00:55:22,420 --> 00:55:27,970
So. So some of this, again, is sort of driven by physician behavior.

409
00:55:28,840 --> 00:55:36,040
So there may be physicians for which they're going to sort of do this this appropriate standard of care.

410
00:55:38,480 --> 00:55:41,830
Regardless, it's going to be pretty good. We have a lot of subjects there.

411
00:55:44,530 --> 00:55:51,640
And then in terms of the overall means, so.

412
00:55:54,750 --> 00:55:58,620
Perhaps not surprisingly, remember, so high is sort of bad here.

413
00:55:59,590 --> 00:56:03,150
And these are these are measures of depression. So you kind of want those to be lower.

414
00:56:03,360 --> 00:56:10,800
That's low point of this. So the never takers actually had the highest mean always takers were a lot lower.

415
00:56:13,320 --> 00:56:16,469
The compliance group was a little was, but it did look like there was of course,

416
00:56:16,470 --> 00:56:19,770
some compliance effect for some effective treatment among that compliance.

417
00:56:19,770 --> 00:56:27,210
So that compliance that didn't have the treatment had higher CD scores than those that didn't.

418
00:56:27,810 --> 00:56:31,680
Although the magnitude of this was was such that there's still a fair amount of uncertainty.

419
00:56:31,770 --> 00:56:35,940
So it's far from conclusive proof that this was helpful.

420
00:56:38,010 --> 00:56:40,570
Yeah. Small sample. There are a few things that are out here.

421
00:56:42,010 --> 00:56:48,310
So in addition to this value, which is of course, kind of the main piece I think is sort of as you look at the that the sort of overall means here.

422
00:56:48,490 --> 00:56:55,150
So you can see the players are kind of a little bit different than these other other subjects in particular.

423
00:56:55,840 --> 00:57:03,460
Sort of the difference between the never takers and always takers is a lot broader than the difference among compliance groups here.

424
00:57:04,330 --> 00:57:12,850
So there's some sense of that. And if we compare this back to the has treated effect.

425
00:57:18,370 --> 00:57:24,040
And the intent to treat effect. Right. You can see that in centrifuge effect is is weaker.

426
00:57:24,940 --> 00:57:31,239
Right. Because that sort of blends in. I mean, it blends in all these compliance behaviors,

427
00:57:31,240 --> 00:57:37,120
sort of puts that into the treatment effect that we're looking at, which is arguably an important thing to look at.

428
00:57:37,960 --> 00:57:43,630
So from a sort of policy perspective, maybe in some ways are we going to implement this treatment?

429
00:57:44,470 --> 00:57:49,990
This is still an important value. But from the question of whether if we can get everybody to do it,

430
00:57:49,990 --> 00:58:00,250
if we sort of focus on the subsample for whom this there is this, there's a difference in their treatment scenario.

431
00:58:02,590 --> 00:58:09,550
And in that they would that they would be treated if given this extra encouragement to their doctor,

432
00:58:10,210 --> 00:58:14,070
then it appears that our estimate is a little bit weaker.

433
00:58:14,080 --> 00:58:18,550
And this in particular, both it's a little closer to zero.

434
00:58:18,910 --> 00:58:25,870
The confidence intervals is quite a bit wider in part because there's a lot of uncertainty about who's a never take her,

435
00:58:25,870 --> 00:58:30,190
who's boy taker, who's compliant and kind of gets built into all that through the give a sample.

436
00:58:31,590 --> 00:58:40,570
So so we're thinking about, you know, this is is this really helping among that subset of individuals that we're targeting?

437
00:58:43,060 --> 00:58:46,090
This is a better measure than just look using the as treated effect.

438
00:58:47,110 --> 00:58:52,600
So I think the short take on this is we can't say anything conclusive about that,

439
00:58:52,960 --> 00:58:56,380
whereas if we just look at the as treated would say, oh, yeah, it's working really makes a big difference.

440
00:58:57,890 --> 00:59:03,250
So so that's sort of the value of doing this extra.

441
00:59:05,830 --> 00:59:13,930
Extra work. The value of thinking about this causal perspective, at least in the way we define it here.

442
00:59:21,120 --> 00:59:28,710
Okay. So a lot you stop.

443
00:59:29,790 --> 00:59:36,110
People get stuck anywhere along the way. So we worked this out.

444
00:59:41,140 --> 00:59:46,820
Well, even the previous. Read his piece here.

445
00:59:51,950 --> 01:00:01,930
Well. So.

446
01:00:15,680 --> 01:00:26,280
We discussed this in the. Back to back to class notes five.

447
01:00:26,970 --> 01:00:27,230
Remember?

448
01:00:31,670 --> 01:00:40,430
And of course, once we feel each time we take one of these conditional draws, we can assume that whatever we do before we can treat is known.

449
01:00:41,720 --> 01:00:48,500
So that allows us to do so. See, given these past year, probabilities were poorly,

450
01:00:48,560 --> 01:00:54,860
we've now just dropped people into whatever class they were assigned to get their means and get the draws.

451
01:00:55,670 --> 01:01:01,250
And then we can also use that same assignment to figure out what the mean should be here for the sum of squares.

452
01:01:01,850 --> 01:01:07,790
Yes. So like, just like conceptually to draw the connection from what we did previously.

453
01:01:07,790 --> 01:01:14,209
And essentially we think of this as like the same kind of algorithm appropriate with the kind of uncertainties.

454
01:01:14,210 --> 01:01:17,430
And we don't know what right compliance tells us.

455
01:01:17,660 --> 01:01:22,010
Right. So before we we just did a right. You just conditioned on a and that's observed.

456
01:01:22,820 --> 01:01:28,610
Now we have this s and this and the other it give us some information about the true compliance status.

457
01:01:28,880 --> 01:01:32,300
And but it's not perfect. It is sometimes.

458
01:01:32,360 --> 01:01:36,830
Right. So we know that if they're not equal each other, they can be player.

459
01:01:37,700 --> 01:01:38,599
They are equal each other.

460
01:01:38,600 --> 01:01:44,960
We don't know if, depending on whether they're both one or both zero, they could be a comply or always take a comply or never take her.

461
01:01:46,700 --> 01:01:50,860
And the same thing down here, right? If there are never take her, then, you know, they can't they never take her.

462
01:01:50,870 --> 01:01:58,790
If they get pretty much conversely, if they are not getting the treatment given they were assigned,

463
01:01:59,390 --> 01:02:03,620
this was assigned then then we know they were never take her.

464
01:02:04,370 --> 01:02:11,329
So the uncertainty becomes if they weren't assigned to it and they don't take it, you can't tell if that's because they're player.

465
01:02:11,330 --> 01:02:18,050
They would have taken it how they gotten it or if they wouldn't. And so that's why we end up with this in this posture probability,

466
01:02:18,140 --> 01:02:22,610
which basically is the marginal probability of being a never take her given that you're a compliant never

467
01:02:22,610 --> 01:02:32,630
take her now weighted by the PDF of the data and we assume again we know these lambdas so we get that.

468
01:02:33,950 --> 01:02:37,430
Where do we always take her? This runs the other direction around.

469
01:02:39,680 --> 01:02:47,840
So once we get these posterior probabilities, then we just do this drive. C See, this is the goal of getting these, by the way, to get a drive of C.

470
01:02:48,770 --> 01:02:55,490
So again, some of we know, so this is essentially a degenerate graph and you're at zero one.

471
01:02:56,120 --> 01:03:00,560
But for the subjects that are in this, it these groups where we have, you know,

472
01:03:00,890 --> 01:03:08,570
some some uncertainty between meaning player and always take a real player never take her that we do the draw of that.

473
01:03:10,670 --> 01:03:22,370
And so then once we know the compliance status, then we can do the draws of the means and certainly the drawers of the variants.

474
01:03:23,900 --> 01:03:26,910
And finally, we have this uncertainty in the papers themselves. Right.

475
01:03:26,930 --> 01:03:32,060
We sort of assume that's marginal things upfront. Now we're going to go back and do a drawer of those.

476
01:03:33,020 --> 01:03:36,650
And we again, we assume we know, see, but we don't know pi here.

477
01:03:37,490 --> 01:03:40,550
So that allows us to basically drop in the counts for that.

478
01:03:42,170 --> 01:03:49,790
That's it. Then all the parameters PI's or lambdas are sigma's and we needed the C to do that,

479
01:03:49,790 --> 01:03:54,320
which meant we needed the PI's in the wise data to do that part to that part.

480
01:03:56,360 --> 01:04:00,430
So we seemed like a lot if we haven't approached this before,

481
01:04:00,440 --> 01:04:04,160
but if you kind of break it down piece by piece, we spend a lot of time with it, I think.

482
01:04:04,430 --> 01:04:05,720
You think it'll start to become clear?

483
01:04:13,740 --> 01:04:20,400
So I guess the last thing I wanted to say on this point is that all these models could be extended with the use of baseline covariance.

484
01:04:20,670 --> 01:04:24,420
It's important to see baseline because we don't have a causal world here.

485
01:04:25,380 --> 01:04:28,770
We need two conditional things that are observed before treatment occurs.

486
01:04:30,340 --> 01:04:33,480
Unless we want to do the extra extra work. We'll come to that later on.

487
01:04:33,600 --> 01:04:37,770
But to we could do like a linear regression on the outcomes.

488
01:04:38,580 --> 01:04:41,160
You could do like a logistic regression for the compliance classes.

489
01:04:42,720 --> 01:04:46,550
We could restrict effects of the curious the outcome models to be equal across the classes.

490
01:04:46,800 --> 01:04:54,300
So you could say, well, your risk of depression given your gender for the beat of your depression score,

491
01:04:54,300 --> 01:04:59,700
given your gender P isn't really going to affect whether you're a decent change, apply or never take her.

492
01:04:59,700 --> 01:05:02,990
That may or may not be reasonable, but you don't have a lot of data.

493
01:05:03,000 --> 01:05:11,700
Maybe you want to make that assumption that, you know, sort of keep things from getting too unstable and then you might even have

494
01:05:11,700 --> 01:05:15,210
interactions with this introductory effect within the place classes given covariates.

495
01:05:15,220 --> 01:05:20,430
So maybe the campfire effect is stronger and in both gender or the campfire effect

496
01:05:20,430 --> 01:05:23,880
is stronger in younger people versus older people were weaker or vice versa.

497
01:05:24,030 --> 01:05:30,600
So all these things could could be done. To be honest, you probably want more than one observation to do a lot here.

498
01:05:31,550 --> 01:05:34,950
So if you're really interested, you can take out the paper and take a look at it.

499
01:05:35,580 --> 01:05:39,450
You sort of did some various things here with that. So.

500
01:05:42,380 --> 01:05:42,810
Interesting.

501
01:05:42,810 --> 01:05:52,110
Also, the reason in general with compliance behaviors, it was actually hard to find a lot that predicts that in terms of cover it's really.

502
01:05:54,110 --> 01:06:00,550
I mean, maybe some more work's been done recently, but but some of the early stuff in this area, I believe, was showing much.

503
01:06:01,630 --> 01:06:07,240
So which is a little unusual, right? Because there's a lot of coverage to predict depression.

504
01:06:07,720 --> 01:06:10,520
So, yes, when we measure coverage,

505
01:06:10,600 --> 01:06:20,470
do we measure the various positions or for the subject could be the one really with that have interaction between those and this particular example,

506
01:06:21,940 --> 01:06:25,870
sort of similar situations in particular to just the subject to be dealt with.

507
01:06:25,870 --> 01:06:30,760
So people could vary, but it's important to be measured before the treatment is given.

508
01:06:36,700 --> 01:06:42,670
That could affect your recovery. All right.

509
01:06:43,960 --> 01:06:53,960
It sort of brings an end to principle stratification. Mm hmm.

510
01:06:54,620 --> 01:07:00,019
Gordon Turlington we're going to come back around to over with our next chapter, but for an exit notes.

511
01:07:00,020 --> 01:07:03,590
But. Just pronounced finished.

512
01:07:06,390 --> 01:07:16,860
Questions at this point. So here are a few minutes left here.

513
01:07:16,860 --> 01:07:23,750
I'm just going to jump on to. The next huge.

514
01:07:56,260 --> 01:08:01,150
Okay. So we're going to get to this idea of instrumental variables.

515
01:08:02,170 --> 01:08:08,410
And basically an instrumental variable is sort of an extension of the idea of a randomized treatment.

516
01:08:10,030 --> 01:08:20,079
So it's, it's, we'll see the way we define additional variable randomization, a randomized treatment assignment is perfect or the best instrument.

517
01:08:20,080 --> 01:08:30,340
You can have no complaints, but we may have other situations where it's usually a standard situation where you don't have randomized assignment,

518
01:08:30,700 --> 01:08:33,580
but you may have something that can act like randomized assignment.

519
01:08:34,720 --> 01:08:46,870
So we'll think about things like whether if you're looking at it at some kind of some sort of change that might be affected by weather and an outcome,

520
01:08:47,620 --> 01:08:51,800
you might think the absence of that weather changes are really going to have an effect.

521
01:08:52,780 --> 01:08:56,560
So this idea of sort of independence with respect to the potential outcome,

522
01:08:57,550 --> 01:09:03,100
changes in lives often are going to be treated as instruments in practice in various settings.

523
01:09:04,360 --> 01:09:08,200
But it's sort of a fun I think it's a fun little exercise and a matter of fact,

524
01:09:08,200 --> 01:09:14,620
I think it makes folks have to do that for a little bit of extra credit to kind of come up with here their own little instrumental variable.

525
01:09:15,310 --> 01:09:19,990
I mean, you can do it in your groups, so okay.

526
01:09:20,530 --> 01:09:27,880
So we're going to talk about some traditional methods of instrumental variable analysis that kind of pre-date the concept of of of potential outcomes.

527
01:09:29,720 --> 01:09:39,820
If you were getting burned down on a given thing, you'll be happy to know that this is entirely linear model of variance estimation.

528
01:09:39,820 --> 01:09:44,559
This is not trivial. But finally,

529
01:09:44,560 --> 01:09:50,800
we're going to kind of pick up of the link between and similar analysis of stratification to kind of show how these things do actually tie together.

530
01:09:51,550 --> 01:09:55,420
It really can be viewed as a special form of principal stratification.

531
01:09:55,930 --> 01:10:01,180
So, okay. So, all right, so here's our little picture.

532
01:10:01,180 --> 01:10:06,669
Maybe it's confounding. Is this again, this issue of always thinking about causal inference?

533
01:10:06,670 --> 01:10:10,240
It's really are nemesis, right? The thing that's that's always it's always after us.

534
01:10:12,370 --> 01:10:18,070
And so we've seen a couple of approaches to try to eliminate confounding.

535
01:10:18,850 --> 01:10:21,730
Right. The first kind of relies on design features like randomization.

536
01:10:24,010 --> 01:10:28,809
The second relies on the variability of pretreatment assignment coverage to explain the confounding, right.

537
01:10:28,810 --> 01:10:34,000
So this sort of propensity score approaches and so forth.

538
01:10:35,290 --> 01:10:39,690
So I think randomization is best really when.

539
01:10:41,700 --> 01:10:46,920
Sort of head y. So can you state why renovations better than using pretreatment fluids?

540
01:10:47,730 --> 01:10:51,030
Because it might be unknown. Precisely. Right.

541
01:10:51,810 --> 01:11:00,600
So the using the practice score approach will allow you to balance unknown potential confounders,

542
01:11:00,780 --> 01:11:10,080
but not an unknown potential confounders where both it wiped out transition but if can were expensive may not be practical for a variety of reasons.

543
01:11:10,710 --> 01:11:11,160
So that's, you know,

544
01:11:11,160 --> 01:11:17,430
variables are kind of a third way to kind of get some of the advantages of randomization without requiring a full control experiment.

545
01:11:20,530 --> 01:11:24,910
So what is an instrumental variable stated in words?

546
01:11:25,900 --> 01:11:31,300
It's the variables related to the treatment or the exposure of interest, but not related to the potential outcome.

547
01:11:31,470 --> 01:11:36,490
And you see potential here in quotes, because the way we're going to define this, we're not going to use potential outcomes.

548
01:11:36,490 --> 01:11:45,580
But the idea is that you're not. This is something that could change your exposure but isn't kind of related, you know,

549
01:11:46,180 --> 01:11:54,820
in advance of the the treatment the exposure or treatment effect with with the outcome.

550
01:11:55,930 --> 01:12:01,360
So indeed, randomization really is the ideal instrument, right? Because by definition, it's unrelated to the outcome.

551
01:12:01,780 --> 01:12:02,500
It's randomized.

552
01:12:02,510 --> 01:12:13,320
There's no way it is going to sort of be balanced on one or sort of two potential outcomes, but it's hopefully strong with it's frequent exposure.

553
01:12:13,330 --> 01:12:20,770
Right. We sort of looked at, right, you have noncompliance, you know, that that may diminish as an effect.

554
01:12:20,810 --> 01:12:26,470
In the end. When we pull this together, you sort of see how the deputy, for example, really fits in this idea of the control variable.

555
01:12:28,690 --> 01:12:33,850
So we'll sort of formalize this below, I guess, a couple quick things.

556
01:12:34,690 --> 01:12:45,250
These these two. These two things are in conflict with each other, typically outside of randomized trials.

557
01:12:46,300 --> 01:12:53,050
Right. So you might have a variable which is really unrelated the outcome, but often the same dynamics,

558
01:12:53,050 --> 01:12:56,440
unrelated outcome may make it only very weakly related to the treatment or exposure.

559
01:12:57,970 --> 01:13:03,970
Conversely, if you have something as really strongly related to treatment or exposure, it may also be related to the outcome of a priori.

560
01:13:04,840 --> 01:13:14,380
So so it's it you sort of have this, this issue of either instruments that aren't really instruments or very weak instruments.

561
01:13:15,190 --> 01:13:18,190
And the latter, in some sense, I think is preferable because it really does.

562
01:13:18,940 --> 01:13:23,469
You know, there's something you can sort of account for that in the estimation procedure.

563
01:13:23,470 --> 01:13:27,910
But if you really have something that's really, really the outcome, that you're going to end up with a bias dysthymia.

564
01:13:29,080 --> 01:13:32,770
So a causal treatment effect.

565
01:13:33,640 --> 01:13:38,020
Okay. So formalize this a little bit.

566
01:13:42,040 --> 01:13:51,730
So this idea kind of dates back to the early part of the 20th century or the first century.

567
01:13:52,840 --> 01:14:00,070
For you against the has been and it was really developed in the context of economics or econometrics.

568
01:14:00,970 --> 01:14:07,000
So it doesn't really use this potential outcomes paradigm, although we'll see how we can relate to it, as I've said before.

569
01:14:08,380 --> 01:14:16,660
So suppose we have an outcome of treatment or exposure that we want to relate to an outcome.

570
01:14:16,750 --> 01:14:19,620
Right. So we have a treatment or so you're actually going to relate to an outcome y.

571
01:14:21,610 --> 01:14:31,659
So if we want to get an unbiased estimate of beta using your usual ordinarily squares estimate, then remember call from your model theory.

572
01:14:31,660 --> 01:14:40,420
This, this, this residual error has to be not associated with X in particular if it were unknown confounders between X and Y.

573
01:14:40,790 --> 01:14:46,840
Right. So if we want to see if there is this association, as you might have other covariates and you're to kind of break that.

574
01:14:48,730 --> 01:14:58,120
But, but of course, if there are unknown confounders, then this this covariance here is going to be not equal zero in particular.

575
01:14:59,170 --> 01:15:05,240
Right. If we replace Y with its mean.

576
01:15:05,410 --> 01:15:12,370
Here I guess I'm using x now to be I'm always going to be included in The Intercept when I write x here.

577
01:15:12,370 --> 01:15:21,310
So the expected corresponds to beta, not plus beta one x plus my vector of their terms.

578
01:15:22,360 --> 01:15:31,300
So if I take the expected value of that balloon release squares estimate or we hit Y with extra x inverse x transpose,

579
01:15:32,200 --> 01:15:38,529
then I kind of get the nice little cancelation of my x's here.

580
01:15:38,530 --> 01:15:40,690
So I'm left with just beta. Well, that's great.

581
01:15:41,320 --> 01:15:48,250
But the problem is that because this covariance is non-zero, the x transpose e is going to be non-zero,

582
01:15:48,790 --> 01:15:52,180
and so x transpose x inverse x turns, Jose is going to be non-zero.

583
01:15:53,290 --> 01:16:02,290
Right? So this part essentially the ordinarily squares estimate is biased for beta one now still does a fine job at predicting y.

584
01:16:02,740 --> 01:16:05,830
Right as good as can be done with x let's say.

585
01:16:06,640 --> 01:16:18,850
But but this estimate of beta one hat is not going to be number have been on beta one collectively is not going be an unbiased estimate of beta.

586
01:16:19,000 --> 01:16:25,520
You have this misaligned correlation. So.

587
01:16:26,940 --> 01:16:32,099
So the trick with instrumental variable idea is to break X into two linear components.

588
01:16:32,100 --> 01:16:35,880
One, that's going to be uncorrelated with this turn and one that is correlated.

589
01:16:36,870 --> 01:16:43,290
So if we could just regress X on the uncorrelated component to get an unbiased estimate or of data, then we would be in good shape.

590
01:16:44,220 --> 01:16:49,570
So this results in what we termed the the two stage least squares estimated.

591
01:16:54,700 --> 01:17:04,150
So if we can find a variable Z such that Z is uncorrelated with this error that relates white x,

592
01:17:05,470 --> 01:17:10,960
but it is correlated with Z, so that alpha one is is non-zero.

593
01:17:11,650 --> 01:17:21,160
Then an unbiased estimate of beta can be obtained by fitting a linear regression of X on z to get this x hat,

594
01:17:21,910 --> 01:17:34,300
which is just a predictor of Z of x given z, and then instead of using x, use this x hand to regress beta to address, to rest on y.

595
01:17:34,690 --> 01:17:42,100
So you'll regress x head of y and this this to stage the squares estimate her beta has to be unbiased.

596
01:17:44,220 --> 01:17:48,510
For the beta. We want to estimate in this model.

597
01:17:55,950 --> 01:18:04,140
So I to stop there. You know, we're at the time for Dean, and that's basically the concept.

598
01:18:04,920 --> 01:18:13,770
The idea essentially is you're sort of tearing off a part of X that's independent of Epsilon and using that to predict.

599
01:18:15,270 --> 01:18:20,520
And that's. That's what we're getting here.

600
01:18:20,530 --> 01:18:21,610
And in order for that to work,

601
01:18:22,120 --> 01:18:30,280
there has to be some association between what we're using with Z to get that that sort of uncorrelated peace index itself.

602
01:18:32,080 --> 01:18:41,649
All right. So again, remember, always have the intercept just as a little trivial thing.

603
01:18:41,650 --> 01:18:52,000
I guess if there's no association in Alpha 120 and Alpha one here is zero, we're like for samples, then basically X is going to be constant.

604
01:18:52,930 --> 01:18:58,360
And if you have an intercept and a constant x, what happens when you try to do x transpose x equals.

605
01:19:02,650 --> 01:19:13,060
Right. If I ever seen the column of one's in the column of some other values that are all the same, what would happen if I tried to?

606
01:19:15,010 --> 01:19:19,840
Take the the product of the cross broken something inverted.

607
01:19:22,330 --> 01:19:25,899
When you look nonverbal, it's also not full rank, right?

608
01:19:25,900 --> 01:19:33,970
Because essentially that one column is equal to the column times, a constant whatever that whatever that constant value is in there.

609
01:19:34,660 --> 01:19:38,870
So. Doesn't work. That's what you have to have.

610
01:19:38,870 --> 01:19:44,509
This this this part here and this part here is my assumption, right?

611
01:19:44,510 --> 01:19:48,490
You can actually determine this. But this is going to have to be this is the assuming part.

612
01:19:48,500 --> 01:19:58,270
This corresponds to. All right.

613
01:20:04,560 --> 01:20:10,050
You know, to come to this part here.

614
01:20:14,120 --> 01:20:18,650
So. Okay. Thank you very much.

615
01:20:19,010 --> 01:20:23,330
I'll be around later this afternoon, as always and always see you.

