1
00:00:03,080 --> 00:00:07,700
All right. Well, people are chuckling. I'm just going to start with homework questions.

2
00:00:08,090 --> 00:00:15,620
So one being today is very happy that actually ten or 15 people showed up in office hour.

3
00:00:16,490 --> 00:00:25,459
At the same time, she feels, you know, she wants to give me some questions for me to address in class and because this lecture is videotaped.

4
00:00:25,460 --> 00:00:32,600
So I think that's the perfect time. This is a perfect time to do so. I will not answer all the questions because of time limit.

5
00:00:32,600 --> 00:00:37,940
We do have some formal lecture to cover, but I will go through a few major ones.

6
00:00:38,360 --> 00:00:43,280
Their first question How did we come up with the modified lecture function?

7
00:00:44,750 --> 00:00:46,130
How to derive it, basically?

8
00:00:46,820 --> 00:00:55,310
So my position on this is that if you are willing to start with the modifier like a function form, just plug in what you know based on the.

9
00:00:56,770 --> 00:01:01,960
Problem structure. See, the homework is good. So I do not require you to.

10
00:01:03,850 --> 00:01:10,480
Go from nothing to the modify likelihood if you can go from model the likelihood to the REMO, let's go then.

11
00:01:11,170 --> 00:01:14,410
But clearly I have to say that's in hand for B,

12
00:01:14,680 --> 00:01:25,390
we provided some extremely technical but optional A materials for you to go from the original log likelihood to the modified log likelihood.

13
00:01:25,900 --> 00:01:30,040
So some of you have been asking me that question during office hours before the class.

14
00:01:30,490 --> 00:01:35,020
I am confident that those are correct. Indeed, they are a bit more involved.

15
00:01:35,620 --> 00:01:44,140
So to summarize, you can your starting point can be just the modified log likelihood and you need to find out what are the.

16
00:01:46,030 --> 00:01:57,280
Quantities in the problem. The second question is that what is the general effect of ignoring the correlation?

17
00:01:57,310 --> 00:02:05,190
So this is, I believe, problem number. Question number three.

18
00:02:05,580 --> 00:02:13,280
So this is a simulation study. I asked you to fix the seed, so hopefully you will get the same similar data.

19
00:02:13,290 --> 00:02:18,960
But I do know that. So there are some weird reasons. Even if you said the same seed, you may still get different data set.

20
00:02:19,320 --> 00:02:24,170
But as long as you show that you committed to one particular seed, that's okay.

21
00:02:25,440 --> 00:02:28,500
A question that often get some people ask.

22
00:02:29,400 --> 00:02:35,860
General, what's the answer for three? I really cannot think about any. I prefer not to reveal that answer.

23
00:02:35,880 --> 00:02:44,760
I think if you consider what are the tests you have learned that considered pairing and that likely will be the right answer.

24
00:02:45,360 --> 00:02:48,530
And the second question is a hedge.

25
00:02:48,690 --> 00:02:54,340
And, you know, does the answer in three B differ from the answer in three C?

26
00:02:54,930 --> 00:02:58,830
Yes. So if you get the same answer that's assigned, that's you.

27
00:02:59,370 --> 00:03:03,569
Unless you don't care about that particular score, then that's okay.

28
00:03:03,570 --> 00:03:09,180
But if you do, make sure that you convince yourself the results are different in the right direction.

29
00:03:12,690 --> 00:03:19,370
So the third question is, I believe in question number two.

30
00:03:19,380 --> 00:03:26,780
So I'm just reading a lot of stuff. OMG this is so simple that I can't believe x, y and zx1 exits are the same.

31
00:03:28,110 --> 00:03:31,260
They can be the same if you convince yourself.

32
00:03:33,210 --> 00:03:37,140
Final question. I believe many people consider this most important.

33
00:03:38,610 --> 00:03:42,390
So the question is, do you need to work on this?

34
00:03:42,900 --> 00:03:49,020
These four questions as part of homework one. I would one as what do you mean?

35
00:03:49,050 --> 00:03:52,050
Do you mean do they count towards the scores in homework one?

36
00:03:52,500 --> 00:03:55,500
No, they do not. But do you need to work on them? Yes, you do.

37
00:03:55,890 --> 00:03:58,980
You want to get started with considering what data sets you want to use?

38
00:03:59,340 --> 00:04:09,900
So these are basically some kind of baby step you can take to initiate discussion or to dove deeper into the data set your team want to use, but.

39
00:04:11,120 --> 00:04:15,290
Why did I say that? We count towards homework three. Well, in homework three.

40
00:04:15,290 --> 00:04:18,319
There will also be another project related question there.

41
00:04:18,320 --> 00:04:25,399
We will assign some great there and clearly we will not be able to use what you submit homework.

42
00:04:25,400 --> 00:04:32,780
One, because I do not require one for homework, one to talk about projects, you know, homework.

43
00:04:32,780 --> 00:04:36,380
Three Just show these things in homework three report.

44
00:04:36,800 --> 00:04:46,040
So we would be just grading on that. So you do have the flexibility of determining data set and explore a little bit before homework three is due.

45
00:04:47,450 --> 00:04:52,160
I think that's all the kind of. Burning questions I received.

46
00:04:52,640 --> 00:04:56,990
Yes. Do you want us to show our vote for three or would you show.

47
00:04:59,330 --> 00:05:05,000
Oh, you mean for the project? Scatterplot.

48
00:05:05,000 --> 00:05:11,270
Yes, our code. So. If you can just provide them, I guess.

49
00:05:12,570 --> 00:05:21,720
I think the movie is extremely diligent that she will code. And also I think it probably is a good idea to supply the code if it's short but

50
00:05:21,730 --> 00:05:26,740
is too long that I would suggest to put them at the end of the summit document.

51
00:05:27,940 --> 00:05:32,830
Yeah, that's. Yeah, but the grading will only based on the plot though.

52
00:05:35,320 --> 00:05:44,360
Yes, we're very. But.

53
00:05:47,160 --> 00:05:54,070
We've asked. It's the same.

54
00:05:56,150 --> 00:06:08,559
Here it is. One. So let me try to rephrase the question.

55
00:06:08,560 --> 00:06:15,370
I do need to repeat the question for the people watching the video. So I think your question is a.

56
00:06:25,470 --> 00:06:30,390
Okay. I was trying really hard to, but I think to partially.

57
00:06:31,800 --> 00:06:38,709
Trying to recreate your question is. Maybe in general, how do we do?

58
00:06:38,710 --> 00:06:48,650
We have such kind of correspondence between a general need model and a kind of classical test that I cannot guarantee.

59
00:06:49,240 --> 00:06:55,170
And so. To be more specific,

60
00:06:55,170 --> 00:07:07,979
it is only in this particular model that you can find a correspondence between the models shown here and the classical test with a particular name,

61
00:07:07,980 --> 00:07:12,990
which I believe you just said. But I believe people who are watching the video may not have heard it.

62
00:07:12,990 --> 00:07:16,290
So that's okay, I think. Or any other questions.

63
00:07:21,570 --> 00:07:30,120
All right. Just feel free to hit us on the piazza or just, you know, come to the office hours or just stop me after class.

64
00:07:30,120 --> 00:07:35,820
I'll be happy to answer any questions. So I'm now going to switch gears.

65
00:07:36,780 --> 00:07:42,450
I'm going to open the handout. 06a.

66
00:08:01,360 --> 00:08:09,510
So today's goal essentially is trying to finish discussing the covariance modeling part last.

67
00:08:10,060 --> 00:08:17,740
In the last lecture we have stopped. It's around introducing, I believe, exponential models.

68
00:08:17,740 --> 00:08:22,300
And then we decided to talk about hybrid models today.

69
00:08:22,750 --> 00:08:29,230
But just for the sake of completeness and I know the weekend is, you know, hopefully took some break here.

70
00:08:29,260 --> 00:08:35,290
So I'm going to do a quick review just so you can get up to speed about why we're discussing this.

71
00:08:36,490 --> 00:08:40,750
Again, it is helpful to review the objectives.

72
00:08:40,750 --> 00:08:44,990
There are four objectives for this slide we have covered. Two.

73
00:08:45,900 --> 00:08:55,800
The first one is to describe the implication of correlation among longitudinal data, and particularly why it is important to consider correlation.

74
00:08:55,810 --> 00:09:01,980
What's the vintage? So by describe, I mean you describe OC.

75
00:09:02,040 --> 00:09:09,090
After you learn the lecture, you should be able to articulate in one sentence or two what's the implication?

76
00:09:09,480 --> 00:09:14,550
And I believe question number three in the homework one is getting directly at this point.

77
00:09:15,270 --> 00:09:24,390
Number two, after you have convinced yourselves that considering the correlations among that longitudinal outcomes, it's important.

78
00:09:24,720 --> 00:09:30,490
We do need to look at what I like to say. Items in the menu.

79
00:09:30,520 --> 00:09:33,730
What are available? There are a few options.

80
00:09:34,870 --> 00:09:39,819
These names are not exhaustive, so there will be other possibilities.

81
00:09:39,820 --> 00:09:47,980
But these are the most popular ones you see in any statistical package that fits these longitudinal data models.

82
00:09:48,220 --> 00:09:52,360
The first one is called compound symmetry. The second one is called Toeplitz.

83
00:09:52,870 --> 00:10:01,210
The third one is called auto regressive. And it is important that I make the claim that there are some nesting structures here.

84
00:10:02,290 --> 00:10:12,060
So if I abbreviate this to see us and the previous two are one and broom of this two, I don't need to repeat this.

85
00:10:12,070 --> 00:10:17,360
We just need to understand that compound symmetry is nested in Toeplitz.

86
00:10:18,850 --> 00:10:24,250
And again, one is also nested in Toeplitz.

87
00:10:26,200 --> 00:10:30,280
And then it is also nested in Toeplitz.

88
00:10:30,550 --> 00:10:40,720
So from a structural point of view, you ought to understand that Toeplitz is relatively flexible compared to these three model,

89
00:10:41,110 --> 00:10:50,050
three models for the convergence patterns. So when you are doing testing or rather you are trying to select which.

90
00:10:51,180 --> 00:10:55,440
Covariance model the use when they're nested. You can apply the like a racial test.

91
00:10:55,710 --> 00:11:01,560
We'll talk about them later. Then we moved on to talk about exponential.

92
00:11:01,560 --> 00:11:10,130
And this is often very effective for. Irregular, irregularly measured or spaced.

93
00:11:21,030 --> 00:11:27,190
Irregularly spaced timings. Okay.

94
00:11:28,540 --> 00:11:38,440
The final one I have not talked about is the hybrid model, and we need to at least provide a reason why hybrid models are needed.

95
00:11:47,660 --> 00:11:52,880
Because of weird limiting behavior.

96
00:12:00,080 --> 00:12:13,150
Of of exponential. Sometimes I have a motor fear of misspelling because that record I was remembering there.

97
00:12:13,360 --> 00:12:18,640
There is an episode in How I Met Your Mother that Ted was trying to spell his name on the.

98
00:12:19,670 --> 00:12:25,010
I'm the white board. And he was trying to spell professor and it does not know how many are there.

99
00:12:25,140 --> 00:12:28,190
I'm to sit there anyway. This is the view I have.

100
00:12:29,480 --> 00:12:33,050
Anyway, let's return to this. So we need to talk about hybrid models.

101
00:12:33,320 --> 00:12:39,560
And three. After you are presented with this menu, you want to choose among these models.

102
00:12:39,800 --> 00:12:45,500
Number four, we need an illustration. So you are seeing some model fitting option.

103
00:12:46,340 --> 00:12:56,480
Okay. So without further ado, I'm going to quickly scroll to the slide that corresponds to exponential and I will talk about limiting behavior.

104
00:12:57,110 --> 00:13:02,630
So the limiting behavior usually concerns about that gap between two measurements.

105
00:13:07,770 --> 00:13:18,240
Between two measurements. So if I call that Delta, we ask what would happen if Delta goes to zero or Delta goes to Infinite?

106
00:13:18,660 --> 00:13:24,180
Basically, if you pick two measurements that are extremely close, what will be the correlation?

107
00:13:24,300 --> 00:13:30,960
Is it going to be one or some other number? It turns out that exponential will have a correlation of one.

108
00:13:30,990 --> 00:13:36,240
Right? And what is that gap between two measurements are like?

109
00:13:36,240 --> 00:13:41,850
Infinite say they're very far away from each other. The what's the limiting value of the correlation?

110
00:13:41,850 --> 00:13:45,300
It turns out that an exponential model, that correlation will decay to zero.

111
00:13:45,780 --> 00:13:51,780
So scrolling to slide number.

112
00:13:54,270 --> 00:14:06,400
I believe 27. Let's see. Oh, there we go.

113
00:14:14,080 --> 00:14:22,840
So at 28, so on Onslow 28, it is basically making the two points I just made,

114
00:14:23,320 --> 00:14:33,970
plus the point in the first couple here showing that the exponential model is going to be invariant under a nine year transformation of the timing.

115
00:14:34,720 --> 00:14:36,910
So again, let's return to the main points we're trying to make.

116
00:14:38,320 --> 00:14:47,860
If you have the time gap between two measurements, Jay and Jay Prime from the same person, if that goes to zero, you have near one correlation.

117
00:14:47,860 --> 00:14:53,260
And if you have two measurements that are very far away from each other, that correlation is going to be zero.

118
00:14:53,920 --> 00:15:02,920
However, this kind of mathematical consequence of this exponential covariance pattern model is not often realistic in practical studies.

119
00:15:03,610 --> 00:15:10,560
For example, if I measure. One outcome at the same time twice.

120
00:15:10,920 --> 00:15:15,400
It is hard to expect there will be identical. There will be some measurement errors.

121
00:15:15,420 --> 00:15:18,060
There will be reasons where the numbers will disagree from each other.

122
00:15:18,390 --> 00:15:28,680
So it's unrealistic to assume that when you measure it, you know the outcome, say, at the same time, which means the gap between the two times a zero.

123
00:15:29,190 --> 00:15:31,350
You have a correlation one, which is unrealistic.

124
00:15:31,990 --> 00:15:38,760
And number two, if you have measurements that are four further away from each other, do they have correlation zero?

125
00:15:38,790 --> 00:15:47,850
Probably not. This is know, maybe there are some genetic factors within the person that dictates that the outcomes are very similar.

126
00:15:48,540 --> 00:15:52,110
Even if they are like 20 years apart. Right.

127
00:15:52,320 --> 00:16:03,780
Perhaps like Haidar Wade or of the outcomes in analysis of these longitudinal data, often you not always, often you don't have genetic data.

128
00:16:03,780 --> 00:16:12,060
So those unobserved or accounted for those those factors and accounted for or induce some correlation, even if two measurements are further apart.

129
00:16:12,720 --> 00:16:15,780
So this brings us to the question how should we fix this?

130
00:16:16,910 --> 00:16:17,920
But what's the goal, though?

131
00:16:18,530 --> 00:16:26,270
The goal is that when two measurements are extremely close to each other or measure at the same time, which then we call them duplicate measurements.

132
00:16:26,510 --> 00:16:33,050
We want the correlation to be less than 1/2 when the measurements are extremely far away from each other.

133
00:16:33,380 --> 00:16:40,010
We do not expect correlation to be kind of zero. So we want certain known zero coalition to be present.

134
00:16:40,550 --> 00:16:46,850
So how should we do this? It turns out that it's a very simple it's a very simple form.

135
00:16:47,360 --> 00:16:56,480
This is what we call hybrid models. So in this particular example, we are providing one solution, not all the possible solutions, but one solution.

136
00:16:56,930 --> 00:17:05,300
So we model that for one person's vector of outcomes, we assume the cover and structure can be decomposing, too.

137
00:17:06,080 --> 00:17:15,050
The first part is of a simple model. I believe you can recognize that this is of compound symmetry form which our abbreviate using see us.

138
00:17:16,070 --> 00:17:25,790
Number two, this is a little bit complicated, but as you can recall, this is exponential exponential covariance pattern model.

139
00:17:31,350 --> 00:17:39,840
A side note here, we indeed made some stronger assumptions relative to all the models you can see, especially in SAS packages.

140
00:17:40,830 --> 00:17:45,990
It is the assumption of the covariance being the same regardless of what timing it is.

141
00:17:46,260 --> 00:17:51,450
By that I mean only one sigma. One applies to all the occasions.

142
00:17:52,050 --> 00:17:57,330
So that's why we call this. At least in the language of says code manual.

143
00:17:57,830 --> 00:18:05,820
It's so homogeneous. Homogeneous compound symmetry and.

144
00:18:08,300 --> 00:18:14,530
If you one have sigma that are different across the occasions, you would call them heterogeneous.

145
00:18:14,620 --> 00:18:21,890
So so that's the just a side note if you use it says something here we call this head sorry homogeneous.

146
00:18:25,490 --> 00:18:31,370
Exponential. So all the examples you you've seen earlier are of a homogeneous nature.

147
00:18:33,080 --> 00:18:39,080
So now the claim is that we solve this problem by adding these two covariance patterns together.

148
00:18:40,220 --> 00:18:43,850
So what does that mean? Well, now let's look at.

149
00:18:46,480 --> 00:18:55,330
The following. So in that particular various covariance pattern model, you have two aspects to consider.

150
00:18:56,380 --> 00:19:04,900
Well, if I scroll back, you know, it is a sum. So let's consider diagonal values of the sum and also the off diagonal values of the sun.

151
00:19:05,830 --> 00:19:11,380
So for the diagonal values, it is going to be simple sigma one squared plus sigma two squared.

152
00:19:12,910 --> 00:19:16,030
So it is just the variance components from the two parts.

153
00:19:16,210 --> 00:19:22,930
Number two, it is the correlation between two measurements collected at occasions J and occasion k.

154
00:19:23,320 --> 00:19:28,390
What's that correlation? It is very simple. The correlation from the covariance.

155
00:19:28,570 --> 00:19:33,870
The covariance from the first. Part, right?

156
00:19:33,930 --> 00:19:40,190
Because we know the first part is a complete symmetry and that covariance is rho one times, sigma one squared.

157
00:19:40,490 --> 00:19:47,030
The second part essentially is the coherence from the exponential model.

158
00:19:47,390 --> 00:19:57,469
So if you put them together and apply the correlation formula, so correlation by definition it's a normalized version of covariance, right?

159
00:19:57,470 --> 00:20:05,660
By normalized, I mean you just divide by the standard errors of each of the variables you're going to be investigating.

160
00:20:05,660 --> 00:20:13,550
And here, regardless of whether you have wide, okay, they have the same variance always sigma one squared and sigma two squared.

161
00:20:14,420 --> 00:20:24,499
So this is what we have. If you're confused, I can just write down this, you know, should I say A and B, what's the correlation?

162
00:20:24,500 --> 00:20:34,760
And B is basically the covariance of A and B divided by the square root of variance of A and variance of B.

163
00:20:34,760 --> 00:20:39,470
What I just said is that in our case, the variance of an inverse of B are the same.

164
00:20:41,970 --> 00:20:45,090
So from this you will get a formula for the correlation.

165
00:20:47,080 --> 00:20:52,530
Now, this is a time where we're going to apply some simple calculus 1 to 1, right?

166
00:20:53,110 --> 00:21:06,440
What we're going to obtain if we let the gap between two measurements, which is say, okay, to be going to zero and go to infinite, what would happen?

167
00:21:06,460 --> 00:21:12,680
So let's write that down. The answer are in the next two slides.

168
00:21:14,210 --> 00:21:23,160
So. Just look at this. Just look at the script right now.

169
00:21:23,760 --> 00:21:29,970
So let's consider T.J. minus tick goes to zero.

170
00:21:31,030 --> 00:21:35,420
What's the what's the limiting value of. Correlation.

171
00:21:38,480 --> 00:21:45,590
Between these two measurements. What is tiger minus t ike?

172
00:21:46,160 --> 00:21:49,790
Goes to infinite. What's the correlation between these two things?

173
00:21:56,520 --> 00:22:00,180
I'll give you like 30 seconds to just quickly get a sense how to do it.

174
00:22:03,880 --> 00:22:38,430
Yeah. Okay.

175
00:22:38,610 --> 00:22:51,840
So. The important piece of figuring out the limit is basically asking what you want to get by letting the gap between the two time points go to zero.

176
00:22:51,990 --> 00:22:56,370
If it goes to zero this term, that's in blue circle goes to one, right?

177
00:22:56,370 --> 00:23:00,660
Because any number with a zero exponential is going to be one.

178
00:23:04,570 --> 00:23:11,620
I guess zero to the zero power is also one. So here you will get a limiting value of.

179
00:23:12,620 --> 00:23:19,100
Row one time sigma squared plus sigma two divided by sigma one squared plus sigma two squared.

180
00:23:21,860 --> 00:23:29,520
By noting that this term. Row one is going to be of absolute value at less than one.

181
00:23:30,420 --> 00:23:36,420
So the numerator will always be smaller than the denominator. So this whole term will be less than equal to one.

182
00:23:36,450 --> 00:23:42,090
Most likely less than one. If you consider a row greater than zero and smaller than one.

183
00:23:43,560 --> 00:23:53,490
So this achieves a goal that by having two measurements obtained at the same time, we do not have a perfect correlation, which is more realistic.

184
00:23:53,880 --> 00:23:59,340
On the other hand, if you let this time gap goes infinite this term in the.

185
00:24:00,400 --> 00:24:05,350
Blue Circle. Now, I emphasize here. This term is going to be zero.

186
00:24:06,340 --> 00:24:08,650
If you have a row that's greater than zero, less than one.

187
00:24:09,430 --> 00:24:16,270
So the term related to sigma two squared in the numerator will disappear along with that coefficient.

188
00:24:16,570 --> 00:24:19,840
So the limit will be rho one sigma.

189
00:24:19,840 --> 00:24:23,860
Sorry, sigma one squared divided by sigma one squared plus sigma two squared.

190
00:24:25,700 --> 00:24:35,600
So in contrast to the exponential model where this thing would go to zero, now we have a limit value that's not exactly zero.

191
00:24:36,260 --> 00:24:42,070
So I'm going to say this is going in zero. So in the next two slides, we basically.

192
00:24:43,180 --> 00:24:46,660
Just show you those two limits here and here.

193
00:24:47,740 --> 00:24:53,830
The take home message is that we can use items in the menu for the Korver inspired

194
00:24:53,920 --> 00:24:59,170
model to create real interesting patterns that are more empirically reasonable.

195
00:25:00,700 --> 00:25:08,290
So with that just we have concluded the discussion of some simple options to do covariance pattern modeling.

196
00:25:14,550 --> 00:25:22,110
Now with all these options, we need to talk about covariance pattern model choices.

197
00:25:22,290 --> 00:25:23,640
How do we choose among them?

198
00:25:24,810 --> 00:25:34,200
So we need to start with some high level conceptual understanding of how this ought to be done, and then we will illustrate by some example.

199
00:25:34,650 --> 00:25:42,240
So in the slides that to follow a of these are like words again, they are there for the sake of completeness of the slides.

200
00:25:43,020 --> 00:25:52,800
What I will do is just go through these points one by one and to explain why they are reasonable suggestions at a high level.

201
00:25:55,530 --> 00:26:03,330
So first is the overview. First, the choice of models for the covers and for the meeting are interdependent.

202
00:26:03,360 --> 00:26:13,080
If you recall from the last handout where we have grossly misfit the mean patterns for the blood level in the treated group,

203
00:26:13,470 --> 00:26:16,560
we saw inflations of the Darren's estimates.

204
00:26:17,490 --> 00:26:21,360
Those are not because they truly have high, highly variable residuals,

205
00:26:21,360 --> 00:26:26,519
but rather is because we have done such a poor job by fitting a straight line to

206
00:26:26,520 --> 00:26:32,580
the otherwise curved like trajectory for the blood level in the treated group.

207
00:26:33,210 --> 00:26:40,650
So that example illustrates the importance of making the ME model roughly okay.

208
00:26:40,890 --> 00:26:59,900
Before you can choose the Covance model. So again, just as I said, any model for the covariance depends on the assumed me model.

209
00:27:00,440 --> 00:27:06,530
And to do so often, we would need to consider a maximum model.

210
00:27:06,980 --> 00:27:10,970
So this term may be very study dependent, context dependent.

211
00:27:12,440 --> 00:27:19,849
As I have said, the reason for using maximal model is trying to alleviate the concern or remove

212
00:27:19,850 --> 00:27:25,610
the concern that the main model for the longitudinal outcome is grossly wrong.

213
00:27:26,120 --> 00:27:36,530
If you can recall that in analysis of response profiles, we just use occasion indicators and also group by occasion indicator interactions, right?

214
00:27:37,100 --> 00:27:43,280
We did not specify the mean model. Those are doable because we had a balanced design.

215
00:27:43,790 --> 00:27:47,990
So there we were already using a maximal model or other.

216
00:27:48,290 --> 00:27:54,980
Correct me model. You cannot get wrong by using the analysis response profile approach,

217
00:27:55,580 --> 00:28:03,020
but things will become more complicated if you have continuous covariance or if you have irregular timings.

218
00:28:03,350 --> 00:28:08,270
And that's where you can never expect to have a saturated model.

219
00:28:08,300 --> 00:28:12,820
You have to consider what's maximal. We will explain what this word maximal means here.

220
00:28:22,120 --> 00:28:26,860
So by the word maximal, we will need to discuss separately the first one instead.

221
00:28:27,340 --> 00:28:32,410
As we have just alluded to in the analysis profile approach.

222
00:28:32,830 --> 00:28:41,800
When you have discrete groups, when you have balanced design in general, you can just specify all the interactions you want.

223
00:28:42,130 --> 00:28:47,350
And those are called saturated models and there is no MIS specification.

224
00:28:47,350 --> 00:28:50,770
And the mean response profile analysis is one example.

225
00:28:51,280 --> 00:29:02,320
So to understand why I challenge you to give me an example under balanced design that the analysis of main profile approach is going to be.

226
00:29:03,490 --> 00:29:09,370
Wrong. So that's a challenge. And I think the my prediction is that you cannot find such an example.

227
00:29:09,610 --> 00:29:13,060
So this indicates that you can never specify that mean model.

228
00:29:13,720 --> 00:29:18,150
And when that is the case, we call that saturated model. Okay.

229
00:29:22,460 --> 00:29:33,260
Second when you have, you know, continuous covariates, then in general, you cannot perform analysis of response profiles.

230
00:29:34,250 --> 00:29:39,890
So you will need to basically consider the scientific context.

231
00:29:39,980 --> 00:29:43,940
What is the mean model that the investigators are willing to play with?

232
00:29:44,450 --> 00:29:50,660
And you you go one level above that. So this brings us to this slide.

233
00:29:52,460 --> 00:29:53,570
Mm hmm. Let me see.

234
00:30:06,020 --> 00:30:18,260
So the first step is that if your study involves treatment groups and other covariate information, you want to distinguish them first.

235
00:30:18,770 --> 00:30:25,120
Because often, you know, the million dollar question is whether the treatment worked right.

236
00:30:25,670 --> 00:30:31,480
And the reason why other covers might have been collected is for fear of confounding.

237
00:30:31,490 --> 00:30:36,320
So you want to use them for adjustment, especially if you're not doing randomized trials.

238
00:30:38,040 --> 00:30:45,120
So distinguishing the treatment, covariates and other keywords are important you want to focus on.

239
00:30:46,880 --> 00:30:52,100
You know. The factor of the most interest, mostly the treatment.

240
00:30:53,390 --> 00:30:58,790
So this maximum model will often include first the May effects of treatment.

241
00:31:01,570 --> 00:31:03,700
And second, their interruption with time.

242
00:31:05,520 --> 00:31:17,040
And the reason is because you do want to have the flexibility to let the trajectories of the outcomes to vary by group.

243
00:31:17,280 --> 00:31:26,430
For example, if a treatment group, what the curve is, you know, level, you know, not many not much change across time,

244
00:31:27,600 --> 00:31:32,910
maybe for the other treatment group, the mean response trajectory will decrease over time.

245
00:31:33,360 --> 00:31:43,650
So having interaction term between the treatment and group and the time will be capturing that potential pattern in data.

246
00:31:44,280 --> 00:31:51,300
And remember, all these models often needs to be pre-specified before you see the data.

247
00:31:52,200 --> 00:31:59,310
Okay. So this often would appear in what we call CEP statistical analysis plans.

248
00:31:59,730 --> 00:32:07,980
You would need to provide the funding agency or your supervising investigator that what is the analysis plan?

249
00:32:08,010 --> 00:32:11,040
So you want to be articulating what are the terms when put in there?

250
00:32:13,430 --> 00:32:20,690
Finally, additional interaction terms. For example, if two variables are not treatment, not time,

251
00:32:21,080 --> 00:32:26,330
do you want to include only the main effects or do you want to include the interaction terms?

252
00:32:26,840 --> 00:32:29,600
So I don't have a general answer for that,

253
00:32:29,930 --> 00:32:35,960
but that totally depends on whether the collaborating scientists would think that's a reasonable thing to do.

254
00:32:36,830 --> 00:32:42,710
So that involves a certain level of communication between the staff, statisticians and the collaborating scientists.

255
00:32:57,680 --> 00:33:04,610
So how do we actually what tools do we have to choose among the covariance models?

256
00:33:05,780 --> 00:33:11,450
So we need to fit the models to each possible cover and spread model.

257
00:33:12,180 --> 00:33:20,090
Right. Using the data right now, we have seen the data. So if you have to cover in some models that are nested within each other.

258
00:33:20,660 --> 00:33:29,629
So we can call the bigger model or the more flexible cover pattern model to be the full model and the smaller one to be the reduced model.

259
00:33:29,630 --> 00:33:33,230
So this is the setting, as you have learned in your study inference 1 to 1.

260
00:33:34,580 --> 00:33:42,590
What we do is to calculate this term where it is just two times the maximized.

261
00:33:43,760 --> 00:33:50,930
Last likelihood under the bigger model minus the maximized log likelihood obtained under the reduced model.

262
00:33:52,520 --> 00:33:56,960
So this must be a non-active number, right?

263
00:33:56,990 --> 00:34:01,940
Because the four model has a more flexible clearance pattern.

264
00:34:02,630 --> 00:34:10,640
So whatever can be maximized within that reduced set of models can be achieved via a more flexible model.

265
00:34:11,090 --> 00:34:15,080
So if you get a number that indicates some calculation was well.

266
00:34:16,090 --> 00:34:24,280
And naturally we would want to consider what's the reference distribution of this statistic.

267
00:34:25,030 --> 00:34:29,700
So there are lots of. Exceptions to what I'm going to say.

268
00:34:30,660 --> 00:34:38,970
But in general, still, we are going to consider Chi Square with the degree of freedom between the four minus the degree of freedom.

269
00:34:40,090 --> 00:34:43,570
Of the reduce. So already means reduced.

270
00:34:45,570 --> 00:34:56,730
And we basically ask, okay, so for the reference distribution, this is the density of the statistic we are going to expect before we see the data.

271
00:34:57,300 --> 00:35:05,220
Now we have seen the data and you ask, Hey, this test statistic ends up at the vertical line.

272
00:35:05,250 --> 00:35:08,520
Is that considered extreme?

273
00:35:09,270 --> 00:35:18,450
If so, then we declare that. The four models are needed because the reduced model is not doing its job here.

274
00:35:18,450 --> 00:35:22,170
Then of course we need to figure out what level of significance.

275
00:35:22,320 --> 00:35:36,290
For example, if this is the threshold where the TELLE area is .05, then this indicates that we probably can be okay with the reduced model.

276
00:35:37,110 --> 00:35:47,940
But if alternatively you have a statistic that's here, then you can say that hey, the reduced model is not good enough, we have to reject it.

277
00:35:48,870 --> 00:35:56,909
So that's how we use it. Now, as I have alluded to, this is a general setting and there are weird cases,

278
00:35:56,910 --> 00:36:02,280
really weird cases that the reference distribution is not this beautiful.

279
00:36:02,580 --> 00:36:07,980
It will be a mixture of a degenerate chi square and the non degenerate chi square.

280
00:36:08,430 --> 00:36:14,280
So I will refrain myself from discussing more about this kind of peculiarities.

281
00:36:14,880 --> 00:36:18,720
But. But that does not mean you should not know it. You will need to know it.

282
00:36:19,680 --> 00:36:24,720
I'm just going to provide a one reason why those exceptions would occur.

283
00:36:26,340 --> 00:36:33,900
Okay. You guys have learned that the maximum likelihood estimate is consistent and sometime normal, right?

284
00:36:36,570 --> 00:36:40,330
I so I assume so. In which class, though?

285
00:36:41,300 --> 00:36:47,590
If I vote. Something. I have no idea what these numbers correspond to.

286
00:36:48,610 --> 00:36:53,590
So I guess some influence class where you have learned the maximum likelihood for the first time.

287
00:36:54,010 --> 00:36:59,030
And I believe that the instructor should have taught some theory about it, you know.

288
00:36:59,860 --> 00:37:07,900
And have you guys ever try to remember the regularity conditions of the maximum, like a function?

289
00:37:12,920 --> 00:37:21,479
Okay. So the regulatory condition is that. Suppose the capsule theta, the Greek letter caps.

290
00:37:21,480 --> 00:37:25,380
It represents all the possible parameters you can take.

291
00:37:25,740 --> 00:37:31,170
Suppose this is the parameter and suppose your data.

292
00:37:33,270 --> 00:37:37,200
Has a density that is of. Theta.

293
00:37:37,800 --> 00:37:45,240
Sorry. Let me rewrite this. Of data.

294
00:37:46,310 --> 00:37:55,270
And theta. Right. And then what Fisher taught you is that, hey, let's ride this thing out and treat this as true data,

295
00:37:55,270 --> 00:38:00,700
as fix it as a known and maximize this whole thing. You got theta hat and that theta hat is good.

296
00:38:01,100 --> 00:38:06,100
Yeah, but it is only good when the following is is true.

297
00:38:06,820 --> 00:38:12,680
So suppose the truth is here. By truth, I mean zero zero.

298
00:38:15,540 --> 00:38:24,509
I have used the Theater Zero to represent it. The regular condition says you are able to draw a circle around it and everything.

299
00:38:24,510 --> 00:38:32,540
That circle belongs to the parameter space. Okay, so if you have ever done topology, this means that.

300
00:38:33,820 --> 00:38:35,320
The truth is a terror point.

301
00:38:37,040 --> 00:38:46,730
But if you have topology, it means that you can just draw a ball around the truth and that entire ball is going to be within the parameter space.

302
00:38:48,720 --> 00:38:54,310
This is one of many requirements for maximum liquid estimate to be consistent.

303
00:38:54,310 --> 00:38:58,410
The symmetry normal. Now, what's the peculiarity?

304
00:38:58,920 --> 00:39:04,640
Well, let's enter. Entertain. With the possibility that, hey, suppose the truth is here.

305
00:39:06,480 --> 00:39:09,570
Question. Can you draw? It's on the boundary. On the boundary.

306
00:39:14,290 --> 00:39:16,630
So BD means boundary. I'm lazy.

307
00:39:17,050 --> 00:39:26,650
So can you draw a ball around theta zero where everything that ball still falls into the big red circle which represents the premier space?

308
00:39:28,280 --> 00:39:32,810
I think intuitively. No. Intuitive. No. Immediately, no.

309
00:39:32,850 --> 00:39:39,280
Okay. So this is a. A situation where only these party is going to be falling to the prime of space.

310
00:39:40,000 --> 00:39:45,070
So this means that, you know, the maxim legal theory will fall apart.

311
00:39:46,470 --> 00:39:52,680
Okay. So there people have worked hard to figure out what is the syntactic distribution

312
00:39:52,680 --> 00:39:59,130
for the maps and like the rest of it and in particular for the like racial test.

313
00:39:59,310 --> 00:40:02,910
Right. Now, how does it correspond to the setting here?

314
00:40:04,050 --> 00:40:18,760
Let me give you one example. Basically an example.

315
00:40:20,520 --> 00:40:24,910
Where? The reduced model.

316
00:40:29,520 --> 00:40:34,540
Is Alma the boundary. Of the four model.

317
00:41:08,360 --> 00:41:14,170
So what's the one example? Hmm.

318
00:41:21,470 --> 00:41:27,540
Let's consider this one. So you have the hybrid model.

319
00:41:32,080 --> 00:41:38,320
Where is the what clearly contains the exponential model or the.

320
00:41:41,140 --> 00:41:48,490
Compound signature marvel. Then say where is? If you're going to draw a dot somewhere to represent a.

321
00:41:50,020 --> 00:41:58,900
Compound Cemetery. Mount it where you will be. Well, you can obtain that by setting Sigma Square equals zero.

322
00:42:01,690 --> 00:42:04,780
So by setting sigma two square to zero, you get.

323
00:42:07,110 --> 00:42:10,379
The compound signature model. Because that was a hybrid.

324
00:42:10,380 --> 00:42:19,300
And these basically remove everything from the second part. So setting sigma to square to zero, this whole thing will be gone, right?

325
00:42:22,800 --> 00:42:26,340
But what's the value sigma to square can take?

326
00:42:33,180 --> 00:42:36,810
Negative one? No. So it's going to belong to what?

327
00:42:36,840 --> 00:42:41,430
Sorry. Zero and infinite. So this.

328
00:42:42,560 --> 00:42:50,450
Smaller model of. Compound symmetry is going to lie on the boundary of the prime of space.

329
00:42:51,860 --> 00:42:55,520
Because sigma to square equals zero correspond to compound submission model.

330
00:42:56,120 --> 00:43:02,930
And it's right here. Visually, I'm going to be a little bit hand-waving somewhere here, say, you know, but you see what I mean?

331
00:43:04,160 --> 00:43:09,230
So by this very specific example, I'm going to return back to this slide.

332
00:43:10,630 --> 00:43:21,300
To. To say that in general the reference distribution is going to be Chi Square distribution, but you do have to be a little more careful.

333
00:43:21,990 --> 00:43:26,600
We will have some. Midterm exam.

334
00:43:26,600 --> 00:43:36,370
Questions on this. Basically this date is to hammer home the message that you really need to look at the.

335
00:43:37,230 --> 00:43:43,440
Reduce the model in the full model and ask yourself whether the reduced model is on the boundary of the parameter space.

336
00:43:44,640 --> 00:43:50,970
I will not talk more about this. So this is for comparing to nested models, right?

337
00:43:51,210 --> 00:43:56,400
And examples. What are the models that are nested at the beginning of the class?

338
00:43:56,850 --> 00:44:03,180
We have discussed that the complete symmetry is nested within Toeplitz,

339
00:44:03,600 --> 00:44:10,370
although regressive with first order as nested within Toeplitz and Bendit is also nested within Toeplitz.

340
00:44:12,700 --> 00:44:17,550
Right. So for those comparisons, you can use like a racial test.

341
00:44:18,720 --> 00:44:22,800
What about to cover powdered models that have the same number of parameters?

342
00:44:26,640 --> 00:44:29,580
What can we do? Examples.

343
00:44:29,670 --> 00:44:36,870
What are the two covariance panel models that have the same number of parameters so they can in no way be nested within one another?

344
00:44:45,960 --> 00:44:49,770
How about ar1 and exponential?

345
00:44:52,450 --> 00:44:56,499
So it has sigma squared. It has row by.

346
00:44:56,500 --> 00:45:01,320
The correlation is going to be. Sorry I won.

347
00:45:01,570 --> 00:45:06,430
Say one or two. It's going to be what? Going to be, bro.

348
00:45:08,100 --> 00:45:17,880
And for the exponential is going to be. Row ti1 minus two.

349
00:45:18,250 --> 00:45:22,090
I only did the first and second occasion, but for all the pairs it's the same.

350
00:45:23,800 --> 00:45:27,250
So not really nested. Yeah. With the same number of parameters.

351
00:45:27,910 --> 00:45:31,030
But are we going to stop there?

352
00:45:31,450 --> 00:45:38,080
We are going to have other criteria. So the first one is called IQ key information criteria.

353
00:45:38,110 --> 00:45:45,149
It has another name. I think this is just to just have one name.

354
00:45:45,150 --> 00:45:50,880
Sorry. Just as. Okay. So its definition is that it has two terms.

355
00:45:51,600 --> 00:45:55,860
The first one is the minus two times the maximized restricted log likelihood.

356
00:45:57,470 --> 00:46:01,910
You know what love like that is? You should know at least after you have done homework.

357
00:46:01,910 --> 00:46:05,570
One what the restricted log likelihood is basically the modify likelihood.

358
00:46:07,280 --> 00:46:10,460
The second part is the number of parameters.

359
00:46:11,640 --> 00:46:17,580
Right. So the more flexible your current model is, the higher the second term is.

360
00:46:22,050 --> 00:46:25,410
So in short, for him, it's going to be written out as minus two times.

361
00:46:25,740 --> 00:46:30,309
I'll have minus C. Okay.

362
00:46:30,310 --> 00:46:39,790
But what does this entire term mean? So let's consider a simple dial it can you can use.

363
00:46:40,480 --> 00:46:45,810
So on the dial. You're going to have a bunch of different models.

364
00:46:46,560 --> 00:46:53,610
This is conceptual. Okay. Not real number. So we have a dollar here. So on the right hand side that it's very flexible.

365
00:46:56,200 --> 00:47:00,920
On the left hand side is very. Parsimonious.

366
00:47:07,040 --> 00:47:11,060
Or less flexible. So my question is that.

367
00:47:13,030 --> 00:47:17,770
If you go to the right direction or go towards the right.

368
00:47:21,140 --> 00:47:24,410
How would the first time change?

369
00:47:24,860 --> 00:47:28,839
How would the second term change? Increasing or decreasing?

370
00:47:28,840 --> 00:47:40,390
Increasing or decreasing. That's the first question. The second question is that if you dial to the left, does the first term increase or decrease?

371
00:47:41,050 --> 00:47:45,630
Does the second term increase or decrease? So.

372
00:47:47,720 --> 00:47:52,840
Let's use, say, 30 seconds.

373
00:47:52,850 --> 00:47:56,900
It's my favorite number to consider these changes.

374
00:48:09,820 --> 00:48:12,550
So the assumption is that if you have a more flexible model,

375
00:48:12,910 --> 00:48:17,470
the maximized log lives will be always bigger and the number of parameters will be bigger.

376
00:48:30,570 --> 00:48:35,160
All right, so let's work on the first arrow. If the dial goes towards the left.

377
00:48:35,370 --> 00:48:41,770
Towards the right. Clearly the number of parameters is going to increase.

378
00:48:43,870 --> 00:48:46,990
What about the maximum maximized log likelihood?

379
00:48:47,800 --> 00:48:51,550
This thing is going to increase, but because we multiply by minus two.

380
00:48:51,670 --> 00:49:00,930
So the first term is going to be smaller. On the other hand, if you move the dial to left.

381
00:49:02,940 --> 00:49:08,040
Because the model is more parsimonious. You did not put a lot of parameters in there.

382
00:49:08,130 --> 00:49:18,750
So the number of parameters will decrease. And same thing for the maximized log life because the model space is smaller now you will have a

383
00:49:18,750 --> 00:49:24,930
smaller value to to obtain because you only have limited options within this very parsimonious space.

384
00:49:26,140 --> 00:49:30,640
So the term in the blue parentheses will be smaller.

385
00:49:30,640 --> 00:49:33,670
But because we multiplied by minus two, this whole thing will be bigger.

386
00:49:35,560 --> 00:49:41,050
So as you can see that either go left or right, you will have two competing forces.

387
00:49:41,740 --> 00:49:49,360
The flexibility, the model which is supposed to be, you know, if Adel characterization of the coherence pattern.

388
00:49:50,710 --> 00:49:56,610
On the other hand, that pays the price of, you know, stretch the data to say, right.

389
00:49:57,570 --> 00:50:03,660
What does that mean? Well, you may make them all extreme flexible, but the gain in the maximized likelihood is so small.

390
00:50:04,110 --> 00:50:10,350
You probably don't want a flexible model, you just use a smaller model. So it is somewhere in between.

391
00:50:10,530 --> 00:50:17,400
We will we will strike a balance. All right. And it is the design principle of this particular criteria.

392
00:50:18,300 --> 00:50:27,090
And the remarks here basically is going to be summarizing what I just said first minus, well, how to measure the model fit.

393
00:50:27,960 --> 00:50:36,180
The smaller, the better because of the negative side number to see it is a penalty for the estimation of each additional covariance parameter.

394
00:50:36,450 --> 00:50:48,660
Basically, C is a number of parameters and you can use these are used this AIC, which is a combined criteria to choose the coherence model.

395
00:50:48,780 --> 00:50:52,800
But again, you've got to fix them all the same with with the same structure.

396
00:50:55,010 --> 00:50:59,150
Question is ace is smaller, better or bigger, the better.

397
00:51:38,400 --> 00:51:43,020
Okay. Let's take 5 minutes break and let's return at 357 and continue discussion.

398
00:52:27,390 --> 00:52:35,240
So we have not had a lot to. Just be careful in those situations, of course.

399
00:52:36,230 --> 00:52:59,260
The recommendation should not. The press will be replaced by something.

400
00:53:20,430 --> 00:53:30,250
Considering the small. Yeah.

401
00:53:30,350 --> 00:53:34,560
We to send. Just.

402
00:53:34,580 --> 00:53:41,760
I will just show you one. I have not.

403
00:53:43,660 --> 00:53:56,580
Is that what they're. So obviously I think that is the time during.

404
00:53:58,570 --> 00:54:02,010
Now I decided to make the. At the time of.

405
00:54:03,090 --> 00:54:08,300
Where and when exactly? Las Vegas on the dance floor when you start.

406
00:54:15,310 --> 00:54:18,980
I think what I would. So.

407
00:54:22,110 --> 00:54:26,980
You have only one time. The U.S. You can choose whether to way.

408
00:54:29,860 --> 00:54:33,360
One of the worst. But you can certainly do this before there.

409
00:54:37,930 --> 00:54:42,320
So you have some people trying. How do.

410
00:54:45,770 --> 00:54:51,740
I'm not saying he's always. Don.

411
00:54:54,290 --> 00:56:56,220
But we will not have. All right, let's get back to work.

412
00:56:56,230 --> 00:56:59,800
So again, the question is, is smaller, better or bigger, better?

413
00:57:04,840 --> 00:57:07,870
Smaller Schnauzer had. It's not about the way.

414
00:57:07,870 --> 00:57:12,150
The thing about is very easy, actually. So consider two models with the same number of parameters.

415
00:57:12,160 --> 00:57:17,180
Do you want a lot like her to be bigger or smaller? Bigger.

416
00:57:17,210 --> 00:57:22,760
Right. So that means where the money's in front of it, the whole thing will be smaller.

417
00:57:23,150 --> 00:57:30,410
So I see. The smaller, the better. She's a balance between the model fit and the flexibility.

418
00:57:31,490 --> 00:57:37,720
A second criteria is called, Yeah, I see Bayesian information criteria.

419
00:57:37,730 --> 00:57:41,630
I do think this as a second name. It's called the Schwartz Information Criteria.

420
00:57:48,730 --> 00:57:51,549
The reason why I mentioned this is not because I really like these different names.

421
00:57:51,550 --> 00:57:54,280
I think that statisticians probably have made a mess in naming things,

422
00:57:55,090 --> 00:58:00,790
but it's just because sometimes you run into these names and I hope that you will not blame me for not teaching this.

423
00:58:01,150 --> 00:58:05,050
So this is B.S. and this structure is pretty much the same.

424
00:58:05,140 --> 00:58:09,450
So I'm going to point you towards this particular formula again, one and two.

425
00:58:10,540 --> 00:58:14,190
So the first time again is the maximized log likelihood here.

426
00:58:14,200 --> 00:58:23,260
As you can see, we do have the option of using maximized original log likelihood or the modified log like this.

427
00:58:23,270 --> 00:58:31,120
So you can do either the second term. What's the difference between this one and the previous one?

428
00:58:32,900 --> 00:58:38,930
It is essentially a log in factor. The log n factor.

429
00:58:39,920 --> 00:58:47,720
So this log in factor is going to be placing the heavy penalty on models that are too extreme.

430
00:58:51,830 --> 00:59:00,730
Okay. So essentially the if you use the dial analogy there, the dynamics will be the same.

431
00:59:00,740 --> 00:59:08,219
It is just. You know, the the degree of distaste for more flexible models,

432
00:59:08,220 --> 00:59:17,450
Bayesian information criteria is going to be very unhappy if the model fits roughly the same, but has a lot more parameters up.

433
00:59:17,550 --> 00:59:23,790
So and start here in the second part is going to be representing number of subjects.

434
00:59:24,180 --> 00:59:27,300
If you use the original likelihood, it's going to be the number of people.

435
00:59:27,750 --> 00:59:31,840
And if you use the. You know.

436
00:59:33,310 --> 00:59:38,950
The Remo. Then you will need to do in minus p where P is a number of covariates in your me model.

437
00:59:42,310 --> 00:59:49,540
So again, I think should not be surprising that I see a smart model with a smaller Bisi will be better.

438
00:59:59,770 --> 01:00:07,000
So we have so far talked about three approaches to choose coherence models.

439
01:00:07,960 --> 01:00:12,760
The first one all requires you have to fix the model at the maximum model.

440
01:00:13,180 --> 01:00:17,470
The first one is for pairs of nested covariance models.

441
01:00:18,340 --> 01:00:23,620
You can use a like a racial test with a caveat that if the smaller model is on the boundary of the bigger model,

442
01:00:24,880 --> 01:00:28,570
you will need to consider some peculiar reference distribution.

443
01:00:28,570 --> 01:00:36,100
Not exactly Chi Square. I have not talk about what those will be because we will discuss them in the mixed model component of this class.

444
01:00:36,520 --> 01:00:39,280
But just remember that it's a case by case situation.

445
01:00:39,940 --> 01:00:45,720
So you have to convince yourself that for a particular comparison, the smaller model is not on boundary.

446
01:00:45,730 --> 01:00:49,090
If so, then you have to say wait until the next model part.

447
01:00:50,410 --> 01:00:55,720
The second approach is based on see. Well, I can say the third approach is based on BRC.

448
01:00:56,740 --> 01:01:02,180
Both approaches are going to be useful regardless of whether models are nested or not.

449
01:01:02,200 --> 01:01:08,259
But it's mostly useful for models that are not necessarily nested because you can have two models for the

450
01:01:08,260 --> 01:01:12,970
coherence that are have the same number of parameters and like a ratio test is not going to be an option.

451
01:01:13,630 --> 01:01:20,350
So you're going to calculate AIC or BRC and then choose the model with the smaller criteria.

452
01:01:21,310 --> 01:01:28,540
And the difference between the CBC essentially is going to be the distaste for more parameters.

453
01:01:29,020 --> 01:01:33,790
Now what are the other aspects of coherence model?

454
01:01:34,300 --> 01:01:46,420
So for the. The most important thing to remember is that oftentimes by modeling the coverage structure, well,

455
01:01:46,420 --> 01:01:57,160
you can have some gain in terms of estimating the coefficient of interest usually represent a beta with smaller or smaller standard error.

456
01:01:57,580 --> 01:02:01,310
And in general, that means you can use the data better, right?

457
01:02:01,390 --> 01:02:10,360
For example, you've got two people, the first guy use the same dataset, estimate the ten errors to the second person, estimates Jenner is one.

458
01:02:10,660 --> 01:02:12,370
Both produced the same estimate.

459
01:02:13,120 --> 01:02:22,210
I would say that if both have their method valid, I would prefer the second person because the same estimate with much higher precision.

460
01:02:22,720 --> 01:02:31,570
So this is same thing here. You know, by doing a good model of the occurrence structure, you can gain something in terms of the.

461
01:02:33,830 --> 01:02:36,920
Senator. Finally,

462
01:02:38,450 --> 01:02:46,850
we will be talking about an extremely popular approach called a generalized estimating equation

463
01:02:47,000 --> 01:02:55,730
g e and there we will be able to overcome the possibility of a wrong variance coherence model.

464
01:02:56,420 --> 01:03:00,740
And that is again left to be discussed later in the class.

465
01:03:01,250 --> 01:03:05,730
But I just want to present it here as a remark to say that, you know.

466
01:03:07,190 --> 01:03:13,690
Many. Often times we fear that Vern's core instruction model is incorrect, and that may ruin analyzes.

467
01:03:14,420 --> 01:03:20,209
And the point number three here is trying to say there are some situations those

468
01:03:20,210 --> 01:03:25,910
misses specified square current model can be rescued to produce valid inference.

469
01:03:27,630 --> 01:03:32,560
Again, the price we pay is that although we have rescued in France, we still pay in terms of the center.

470
01:03:33,480 --> 01:03:37,530
I will not talk too much here, but just to give you a prelude.

471
01:03:41,130 --> 01:03:46,860
Okay. So let's see some examples. So this example is based on an exercise therapy trial.

472
01:03:46,890 --> 01:03:52,170
So essentially subjects were assigned to one or two weightlifting programs to increase muscle strength.

473
01:03:52,620 --> 01:03:57,210
As many of you know, you know, you want to be do repetitions.

474
01:03:59,170 --> 01:04:06,079
With, you know. Heavier heavy weights because that breaks the muscle tissue.

475
01:04:06,080 --> 01:04:10,640
And when it regrows, it adapts to the strength you require to lift those weights.

476
01:04:11,120 --> 01:04:19,490
So this trial was conducted to, you know, to investigate whether that common wisdom is true or traditional wisdom is true.

477
01:04:19,520 --> 01:04:27,110
So the treatment one is the is representing a situation where a number repetition of exercises was increase, the subject became stronger.

478
01:04:27,110 --> 01:04:34,669
So the hopefully ideal one or the more painful one, the tremendous to the number repetitions was how constant.

479
01:04:34,670 --> 01:04:38,540
But the amount of weight was increased as the subject became stronger.

480
01:04:41,620 --> 01:04:48,669
Oh, I misspoke. So I guess that Truman one is about just number repetition, but which I guess remains the same.

481
01:04:48,670 --> 01:04:51,910
But the second one is the same number repetition, but more weights.

482
01:04:52,270 --> 01:04:55,510
I apologize. I was thinking about something else.

483
01:04:57,010 --> 01:05:10,210
So for this dataset, in the original dataset, we have a few different measurements collected at baseline, day to day for day six, eight, ten, 12.

484
01:05:10,630 --> 01:05:15,670
So in the original dataset, you do have the measurements obtained at evenly spaced timings, right?

485
01:05:15,700 --> 01:05:20,919
02468 ten, 12. So we're going to do a little bit modification.

486
01:05:20,920 --> 01:05:28,120
We're going to take the baseline. We're going to pick day for the six days.

487
01:05:29,830 --> 01:05:34,160
Not ten, but de 12. The reason to do so is very clear.

488
01:05:34,180 --> 01:05:40,600
We're trying to create a situation where the type that gaps between the measurements, measurement, time is not going to be the same.

489
01:05:41,170 --> 01:05:44,800
Right. So if you go from the baseline to the first measurement, that's four days.

490
01:05:45,370 --> 01:05:52,070
If you go from the fourth to today's, the gaps are two days, but you go from the eighth day to the 12th, the gap is four days.

491
01:05:52,090 --> 01:05:58,480
So this creates a situation where exponential variance coherence pattern model will be useful.

492
01:05:59,260 --> 01:06:07,480
So let's just use this toy example to investigate or to illustrate how the variance currents pattern model can be selected.

493
01:06:09,260 --> 01:06:12,350
First we need to consider what's the maximum model.

494
01:06:12,950 --> 01:06:20,690
This is a very simple situation, although the timings were only evenly spaced, but the timings were common to everybody.

495
01:06:21,530 --> 01:06:29,900
Baseline day for six, eight day 12. So we just fit a we just assumed that at each time for each group we have a mean.

496
01:06:29,960 --> 01:06:35,870
Right. So that's what we have called been response profile analyzes.

497
01:06:36,590 --> 01:06:39,800
So how many parameters for the mean profile?

498
01:06:40,850 --> 01:06:45,150
So we have treatment one treatment, two, we have DASA.

499
01:06:45,200 --> 01:06:50,030
We have j equals 1 to 3, four, five.

500
01:06:50,660 --> 01:06:56,900
And the actual timing will be day zero, day four.

501
01:06:57,770 --> 01:07:02,470
Day 810 and. Sorry.

502
01:07:03,370 --> 01:07:09,370
I need to raise this. I was literally looking for an eraser and realize it's electronic.

503
01:07:09,760 --> 01:07:13,930
So let's do six, eight and 12.

504
01:07:14,110 --> 01:07:20,340
Right. So we need to have how many parameters you want one?

505
01:07:20,490 --> 01:07:23,600
Me one to be one. Three. You know where I'm going?

506
01:07:23,610 --> 01:07:28,590
So just bear with me now. Me to one. You two to you to three.

507
01:07:28,980 --> 01:07:33,930
You two. For me to five, ten. Parameters fully saturated, you can never be wrong.

508
01:07:34,080 --> 01:07:37,860
Okay, so the ME model is maximal and in this case saturated.

509
01:07:39,510 --> 01:07:45,360
So it's different from saturated. But in this particular case in general.

510
01:07:49,160 --> 01:07:54,560
Sorry. Let me make this clear. I apologize. So in general.

511
01:07:57,460 --> 01:08:02,860
A maximal. Does not equal saturated.

512
01:08:05,580 --> 01:08:10,550
But here. It's the same because we use the.

513
01:08:13,910 --> 01:08:18,069
Because we used. Analysis.

514
01:08:18,070 --> 01:08:34,220
I mean, profiles. Where you have ten parameters and that's the maximal thing you can do and that's saturated.

515
01:08:35,840 --> 01:08:47,240
Moving on, we need to fit the model. So the first model we're going to fit is basically the structure variance covariance model for five time points.

516
01:08:47,360 --> 01:08:50,600
I hope that this formula is now imprinted on your brain.

517
01:08:50,720 --> 01:08:57,590
So it's basically and times implies one divided by two, which is five times six divide by two, which should be a.

518
01:08:59,870 --> 01:09:06,880
You know. Uh, 15 parameters for variance covariance model because it's on strip unstructured.

519
01:09:08,620 --> 01:09:16,060
You can see that the variance is going to get larger from, say, 9.7 to almost 14.

520
01:09:16,990 --> 01:09:25,570
In the end of the study. Okay. So there seems to be some evidence that you don't want to fit a homogeneous model for the marginal variances.

521
01:09:27,280 --> 01:09:31,870
What what is the second observation? The second observation concerns the correlation.

522
01:09:32,290 --> 01:09:39,790
So if you look at correlation, you do see that there seems to be certain level of a decrease for the correlations.

523
01:09:40,240 --> 01:09:46,540
So, for example, this number point nine, 2.9237 is the correlation between the baseline.

524
01:09:47,230 --> 01:09:50,320
Um, I believe muscle weight and the.

525
01:09:52,360 --> 01:09:53,950
Must wait at the fourth day.

526
01:09:54,760 --> 01:10:03,580
So that coalition point 92 accounting for all the main patents and that correlation between the baseline and the final measurement on day 12,

527
01:10:04,540 --> 01:10:08,500
roughly like two weeks of. Insane weight training, I guess.

528
01:10:08,830 --> 01:10:16,600
So that correlation becomes 0.81. So that shows some decay in the pairwise correlation in terms of the outcome.

529
01:10:17,920 --> 01:10:22,000
So that's the second, second observation.

530
01:10:22,510 --> 01:10:27,459
And all I did between these two slides is to converse the covariance matrix to the correlation.

531
01:10:27,460 --> 01:10:32,740
So you can see that here. The correlation are ones indicating this is correlation.

532
01:10:36,410 --> 01:10:41,510
Once for the diagonal elements. So this is on structure, currents model.

533
01:10:41,780 --> 01:10:45,709
Again, very flexible, but it eats up a lot of degree of freedom.

534
01:10:45,710 --> 01:10:53,300
15 parameters. We want to entertain with more restricted models.

535
01:10:55,630 --> 01:10:58,780
The first model we're going to consider is auto regressive occurrence model.

536
01:10:59,680 --> 01:11:04,630
Remember that these gaps are like four days, two days, two days and four days.

537
01:11:06,880 --> 01:11:15,070
So the regression model basically says we don't. We don't we just disregard this.

538
01:11:15,940 --> 01:11:32,799
And the correlation will be. It does not care whether Jake was one close to or Jake was.

539
01:11:32,800 --> 01:11:37,790
One came close. Sorry, Jake was. Eight.

540
01:11:39,050 --> 01:11:47,720
Three. Four. Five. Right. So in the first pair of JK, it represents the baseline and day four measurements.

541
01:11:48,230 --> 01:11:51,680
The second pair, Jake, was four kilos, five measures, the final two pairs.

542
01:11:51,710 --> 01:11:56,090
Right? For both cases you will have J minus K equals one.

543
01:11:57,690 --> 01:12:03,390
Right. But. Okay. So I need to provide a better example here.

544
01:12:03,630 --> 01:12:07,620
Jacobs Three and cables four for all three cases. That's.

545
01:12:07,950 --> 01:12:14,580
Jacobs Two minus cables one. However, the actual time gaps are going to be quite different.

546
01:12:15,150 --> 01:12:25,500
The time gaps are going to be. T.

547
01:12:25,510 --> 01:12:35,900
I. J. T like that difference is going to be four for two, hence the reason I need to draw the final pair.

548
01:12:36,230 --> 01:12:39,590
So the actual timings are different. Right. But this mod does not care.

549
01:12:39,740 --> 01:12:47,750
So we fit the model using the goals function from the LME package.

550
01:12:48,770 --> 01:12:54,440
The margin of error assessment will be 11.87 and row is 0.94.

551
01:12:55,520 --> 01:12:57,889
So look for the auto regression model.

552
01:12:57,890 --> 01:13:05,330
We only have one margin of error as parameter and that's going to be just single number and as you can see from earlier observation.

553
01:13:06,960 --> 01:13:10,910
Based on the on structure events core. As a matter of fact, this might be, you know.

554
01:13:12,760 --> 01:13:17,090
Maybe not a good fit. The second model.

555
01:13:17,480 --> 01:13:21,340
Sorry, the third model is going to be exponential covariance model.

556
01:13:21,350 --> 01:13:29,330
In this case, we're going to account for the actual measurement timings of these for the for these outcomes.

557
01:13:29,340 --> 01:13:37,100
So in this case, we estimate the variance parameter 11.7 and the correlation to be .98.

558
01:13:40,890 --> 01:13:46,860
So let's do the comparison. How about we comparing the aggressive one on the structured?

559
01:13:47,310 --> 01:13:52,230
So this is the full model now. This is a reduced model.

560
01:13:53,980 --> 01:13:57,850
And air one is nestled within unstructured right.

561
01:13:57,850 --> 01:14:03,540
So. Tell me which number?

562
01:14:03,750 --> 01:14:07,850
Six 21.1 or 597.3.

563
01:14:07,890 --> 01:14:19,430
Which one comes from the full model? So essentially we need to recognize that this whole thing can be written as.

564
01:14:22,170 --> 01:14:33,390
Reduced, right? So basically you got to have the six 21.1 is basically two times a maximized log, restricted log likelihood there.

565
01:14:34,230 --> 01:14:41,459
So that difference is 23.8. And because in this case, the smaller model is not on the boundary.

566
01:14:41,460 --> 01:14:44,850
And for model again, you have to do this checking of case by case.

567
01:14:45,300 --> 01:14:48,570
Um, the degree of freedom is 13 y well.

568
01:14:48,750 --> 01:14:54,910
R one has. A number of parameters.

569
01:14:56,820 --> 01:15:05,930
So you have the Covance part. You have the mean part, right? For the commons you have to parameters for the main party of ten for the unstructured.

570
01:15:08,600 --> 01:15:11,930
For the covariance. You have 15 for the mean you have ten.

571
01:15:12,770 --> 01:15:16,490
So the difference is 13, right? The only the difference occurred.

572
01:15:16,610 --> 01:15:26,780
The number of difference occurred in the difference model. So the degree of freedom for this test is 13 and with 13 degrees freedom.

573
01:15:27,230 --> 01:15:30,320
The Chi Square distribution has an average of like 13.

574
01:15:30,320 --> 01:15:36,590
And if we check the table, this p value is going to be less 10.05, which is to say that.

575
01:15:37,760 --> 01:15:42,530
Hey, you know, the smaller model needs to be rejected. You know, you need to use some structured.

576
01:15:44,660 --> 01:15:50,060
In this particular comparison. Now, if we switch to the second pair comparison, same story.

577
01:15:51,800 --> 01:16:00,620
We found that in this case. The statistic is just going to be producing a P value that's greater than 1.5.

578
01:16:00,830 --> 01:16:06,950
So at that point of five significance, the null of the reduced model be inadequate, cannot be rejected.

579
01:16:07,370 --> 01:16:13,820
Cannot be rejected. So you can see that exponential model provides provides an adequate fit to the data at hand.

580
01:16:19,910 --> 01:16:30,290
Next slide. So if we're going to use AIC here so we can ask whether this AIC aligns with our story that we told earlier.

581
01:16:31,140 --> 01:16:37,250
So let's look at the first comparison. Unstructured versus ultra aggressive.

582
01:16:37,270 --> 01:16:43,060
If you look at EIC, which one is smaller? So I see a smaller.

583
01:16:54,540 --> 01:17:01,140
So that's also a lesson. You know, if you are doing this comparison, I would often prefer like a racial test.

584
01:17:03,890 --> 01:17:10,940
The second comparison. See this one? One versus exponential.

585
01:17:11,100 --> 01:17:14,750
They both have the same number of parameters. Which one is smaller?

586
01:17:20,270 --> 01:17:28,800
Exponentially smaller. Right. So between auto regressive expansion, you're going to choose exponential.

587
01:17:30,170 --> 01:17:37,640
We also have a comparison that's unstructured versus exponential, I guess in that case, exponential wins, right?

588
01:17:38,060 --> 01:17:44,870
But usually we just do not do pairwise comparisons. If we're using a C, we just fit a bunch of models and then choose the one with a smaller CAC.

589
01:17:45,260 --> 01:17:48,490
In this case, the exponential wins.

590
01:17:48,500 --> 01:17:54,800
So overall, you know, you would want to use exponential model for this particular dataset.

591
01:18:01,760 --> 01:18:07,790
So that concludes hand out. 068 And I do have like one and 2 minutes left.

592
01:18:08,450 --> 01:18:14,299
So in the rest of the analysis be these are just code to reproduce the results.

593
01:18:14,300 --> 01:18:17,900
We just seen the maximum maximize log likelihood.

594
01:18:18,590 --> 01:18:26,900
I see, I see. And so on and so forth. So there's really this is really just a self-contained set of slides for you to review for code.

595
01:18:27,230 --> 01:18:29,510
So I'm going to scroll down here. All these codes are.

596
01:18:29,810 --> 01:18:34,969
Hopefully I'll be able and I think I have put the, our code and data on the canvas so you can try them.

597
01:18:34,970 --> 01:18:38,960
I highly encourage you to try them because you will use them. So.

598
01:18:40,140 --> 01:18:43,440
A parting thoughts here about a function.

599
01:18:46,930 --> 01:18:51,220
Did you pay anything when you downloading art? I hope hopefully did not.

600
01:18:52,200 --> 01:18:56,350
So that's the. Price you pay for free software.

601
01:18:57,250 --> 01:19:01,240
We found the bug in that particular in one particular function.

602
01:19:03,810 --> 01:19:14,350
Which is called get var. So this function with a name in blue is a real function in a package.

603
01:19:15,580 --> 01:19:20,920
But unfortunately so the purpose is trying to get the fitted various programs.

604
01:19:22,210 --> 01:19:29,170
But unfortunately it is not coded. Right. So I wrote a function that's called get var revised goals.

605
01:19:29,770 --> 01:19:44,060
So that is going to be the. Correct function to use and this code is available on the canvas corresponding to hand or six p.

606
01:19:45,430 --> 01:19:51,860
And the reason why I discover this is because when I was trying to write the code, I would say, Hey, what's going on?

607
01:19:51,880 --> 01:19:57,220
This is not the result. And I look through line by line of code, they're just code it wrong again.

608
01:19:57,400 --> 01:20:01,900
The price we pay for free software. But you know, that's an overall good thing.

609
01:20:02,950 --> 01:20:06,160
So that concludes the lecture.

610
01:20:06,970 --> 01:20:11,890
I do want to quickly chat where we are so we can have a mentor, know where we are here.

611
01:20:12,850 --> 01:20:18,370
We are going to move on to exciting stuff or rather more exciting stuff.

612
01:20:18,940 --> 01:20:21,610
So we have concluded, talking about general models,

613
01:20:22,690 --> 01:20:31,660
we will be talking about mixed models and hopefully I think we'll be having lots of fun because there will be lots of new terminologies.

614
01:20:33,070 --> 01:20:39,160
But for the sake of completeness, we will be trying to.

615
01:20:39,190 --> 01:20:45,510
I think that this needs to be modified a little bit. We will be starting from the introduction and moving forward.

616
01:20:45,520 --> 01:20:49,930
After this time, we are going to talk about generalized mixed models. So that's the plan.

617
01:20:50,380 --> 01:20:54,070
Okay. So the first module is done. That's it.

618
01:20:54,070 --> 01:20:54,940
And have a good day.

