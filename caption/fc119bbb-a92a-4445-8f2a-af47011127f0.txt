1
00:00:02,630 --> 00:00:11,020
I think there's good. So last week was Halloween.

2
00:00:11,380 --> 00:00:14,440
Today it's election. No linkage between means.

3
00:00:14,440 --> 00:00:17,560
I hope you we'll see what happens.

4
00:00:18,340 --> 00:00:26,850
I'm not going to tune into anything until tomorrow, I think, because I just need to have a little calm in my life somehow, you know?

5
00:00:28,120 --> 00:00:32,290
What do you guys think if you voted? Are you talking about.

6
00:00:33,190 --> 00:00:37,360
Yeah, I got to go. It's a long ballot to be voting in Michigan.

7
00:00:38,110 --> 00:00:44,510
Yeah, there's a lot of things to vote for. And so I stayed up researching all the down ballot stuff.

8
00:00:45,190 --> 00:00:50,650
So it's really interesting. I mean, the first one about term limits and disclosure.

9
00:00:51,040 --> 00:00:54,370
I wish they didn't put these together. No, that was my.

10
00:00:55,150 --> 00:01:04,750
Yeah. I don't know a lot about that one, do you? Uh, all I can figure out is that it seems to have bipartisan support in the state legislature,

11
00:01:05,080 --> 00:01:10,520
but then they have, like, some private, like, organizations like Gone Against It.

12
00:01:11,110 --> 00:01:20,819
And so that makes me really sad for the reason they say that it has it actually extended for how long people have.

13
00:01:20,820 --> 00:01:24,430
Because what we have right now is six and eight.

14
00:01:24,580 --> 00:01:27,970
Right. Four. And this will change it for 12. 12.

15
00:01:28,450 --> 00:01:31,550
Cumulative what that used to mean that people could do 14.

16
00:01:32,590 --> 00:01:36,549
Right? Right. So, I mean,

17
00:01:36,550 --> 00:01:40,629
what we have is and I know legislators who have switched from the House to the Senate

18
00:01:40,630 --> 00:01:45,130
because they can only serve three years in the House and then they flip over to the Senate.

19
00:01:46,150 --> 00:01:50,100
And this would allow them to stay in the House for 12 years.

20
00:01:50,420 --> 00:01:54,400
Yeah, but then they won't be able to be a senator. Yeah.

21
00:01:54,980 --> 00:02:04,660
So, in fact, my wife worked on this for like 15 years ago because before we had the term limits,

22
00:02:04,660 --> 00:02:12,790
because term limits is really interesting because, you know, you do develop expertise of your constituents, request you.

23
00:02:13,270 --> 00:02:19,300
You know, if you don't have term limits and there are some really good politicians out there.

24
00:02:21,000 --> 00:02:30,380
And so that's one number. Number two is voting regulation, right?

25
00:02:31,260 --> 00:02:41,250
That seems to me a little more clear cut. Number three, of course, has to do with abortion and other reproductive number is very important.

26
00:02:41,850 --> 00:02:45,089
There's an interesting one. Yeah, sustainability.

27
00:02:45,090 --> 00:02:51,270
When I was in if you live north, there is one on farmland preservation.

28
00:02:53,240 --> 00:02:54,620
Oh, I don't know so many things.

29
00:02:54,620 --> 00:03:08,380
And then, you know, what's really unusual for me is we get to vote on the different colleges here as well and judges as well.

30
00:03:09,800 --> 00:03:15,830
Of course, you know, the national the governor did.

31
00:03:17,980 --> 00:03:31,360
Feels like I don't like them. Yeah, well, you know, I mean, in many jurisdictions, like a lot of Detroit, if you win the primary, then you're off.

32
00:03:32,720 --> 00:03:36,290
So it's just so solidly Democratic. But I.

33
00:03:36,290 --> 00:03:41,320
This is Georgia. Yeah. It's it's a lot.

34
00:03:42,670 --> 00:03:49,659
And, I mean, I think a lot of people just have to spend a lot of time to figure out what's going on.

35
00:03:49,660 --> 00:03:56,110
It's really hard. Anyway.

36
00:03:57,430 --> 00:04:03,310
Be interesting to see what happens. Uh. So, um.

37
00:04:06,090 --> 00:04:10,310
This is the article that I sent you guys. I hope you had a chance to take a look at it.

38
00:04:10,320 --> 00:04:20,880
Here's the. You know, I'm always scouring for materials for this class, and sometimes they just fall into my lap,

39
00:04:21,300 --> 00:04:25,620
which is kind of the case when I open the daily paper and, you know, see this.

40
00:04:26,070 --> 00:04:32,790
And this was the one that just popped up last week on the front page.

41
00:04:33,180 --> 00:04:36,450
Actually, it was the old school.

42
00:04:36,450 --> 00:04:40,110
Forgive me for cutting down, you know, I get the paper and stuff,

43
00:04:40,560 --> 00:04:47,040
but it is her opinion piece in The New York Times or national newspaper or, you know, from the East Coast.

44
00:04:48,450 --> 00:04:55,220
And it's such a nasty photoshopping of everybody knows what I mean, right?

45
00:04:55,260 --> 00:05:01,350
You take your photograph and you alter it to get the results that you're like, What's the problem that they're talking about here?

46
00:05:06,970 --> 00:05:19,459
We've got my. So in fact, in here it's hard to see, but they have like the Western law thing.

47
00:05:19,460 --> 00:05:31,520
You know how that works. You're looking for markers, assays and a lot of microscopy looking at bacteria under a microscope.

48
00:05:31,970 --> 00:05:36,709
And the if you get on to The New York Times, it's an interactive page.

49
00:05:36,710 --> 00:05:39,740
It's a lot better than what I can do as a PDF.

50
00:05:40,420 --> 00:05:49,530
Um, but they're showing, uh, you know, the expression of different, uh, uh,

51
00:05:50,270 --> 00:05:57,380
genes with respect to exposure to different chemicals and what's the Photoshopping that?

52
00:06:00,430 --> 00:06:08,739
This woman who's been who said she recognized that she had a really good eye for recognizing repeated

53
00:06:08,740 --> 00:06:14,900
patterns as a scientist worked for many years and then she just started looking literature.

54
00:06:14,950 --> 00:06:18,340
She saw folks who had been flipping these things around.

55
00:06:18,740 --> 00:06:22,070
Or manipulating the images with Photoshop.

56
00:06:22,820 --> 00:06:29,059
And she's set up a she she does this herself.

57
00:06:29,060 --> 00:06:42,010
She's looked at like 100,000 scientific articles and found that people had manipulated the images and like, I think 8000 of them, something amazing.

58
00:06:44,660 --> 00:06:52,160
And and I mean, this is just taking an image and flipping it or changing its orientation and claiming

59
00:06:52,160 --> 00:06:57,530
that you saw something in a new publication and she found the image in your publication.

60
00:06:59,090 --> 00:07:03,860
I mean, a couple of things are amazing to me about the person that somebody would do this.

61
00:07:05,100 --> 00:07:09,560
Okay. And second, that she could remember all these different images. And third.

62
00:07:09,800 --> 00:07:15,020
Like, why didn't she use an aid program to do it? You know, because I mean, this is like ad hoc.

63
00:07:15,050 --> 00:07:18,860
You know, I'm just looking at what I remember crossing my desk or something like that.

64
00:07:19,380 --> 00:07:31,730
But, I mean, the key thing for us is if she found like 8000 of these in looking at 100,000 pieces or something, just every one of these cases is from.

65
00:07:33,880 --> 00:07:37,240
If she's right. Amazing.

66
00:07:40,320 --> 00:07:45,360
Well it looks like this one within the paper. Yeah, even within the same paper.

67
00:07:47,010 --> 00:07:55,260
So, I mean, there's other examples in here they didn't really print out in my pdf, I don't think.

68
00:07:57,680 --> 00:08:01,900
Unfortunately. Actually, I don't like interactive.

69
00:08:02,720 --> 00:08:09,440
Stuff. It doesn't read that well, either. But this is just image manipulation and.

70
00:08:12,340 --> 00:08:22,480
This is published some of her work in saying, you know, problematic images are no fewer than one and 25 papers or something like that.

71
00:08:22,930 --> 00:08:27,600
Rotated, flipped, stretched or otherwise photoshopped. Hmm.

72
00:08:28,920 --> 00:08:37,830
So she quit her job in 2019, devoted herself to finding and reporting these cases of scientific fraud,

73
00:08:38,460 --> 00:08:44,820
and then using my pattern matching eyes and lots of caffeine, 100,000 papers.

74
00:08:44,820 --> 00:08:51,930
And she found 4800 and other issues in these reported them to the journals and.

75
00:08:55,060 --> 00:09:01,480
You know, has a website called Public Cure. I haven't been able to look at it yet.

76
00:09:01,480 --> 00:09:13,480
But, you know, this is important because some of these are lying basically about finding all of the Voltaren specific, so many people.

77
00:09:16,830 --> 00:09:22,770
And other researchers have been unable to reproduce the study results.

78
00:09:24,390 --> 00:09:29,670
And here's a case with Harvard falsified date in 31 papers.

79
00:09:31,440 --> 00:09:39,450
And I've spent at least $588 million pursuing that research that was apparently falsified.

80
00:09:42,140 --> 00:09:48,890
Nobel Prize winning geneticist retracted four of his papers because they had manipulated images.

81
00:09:53,110 --> 00:09:57,800
You know, she's a great detective. She's gotten hate mail and so forth.

82
00:10:02,020 --> 00:10:06,220
You know, the incentive to publish is so strong.

83
00:10:07,810 --> 00:10:12,160
People apparently Nobel Prize winners for taking shortcuts.

84
00:10:12,910 --> 00:10:22,600
How can this happen? So what's our system to ensure that this doesn't happen to police forces?

85
00:10:27,120 --> 00:10:31,500
Why we depend on peer review breaking down.

86
00:10:32,820 --> 00:10:36,650
We don't have a traffic cop for peer review. Right.

87
00:10:37,380 --> 00:10:41,530
Do we need them? We talked a little bit about peer review.

88
00:10:41,550 --> 00:10:45,740
I mean, this is the idea that it's a system of peers.

89
00:10:45,750 --> 00:10:52,880
It's self-correcting. It will catch these things. It will serve our purpose in the long run.

90
00:10:54,890 --> 00:10:59,030
In the article, though, she talks about that peer review is underpaid,

91
00:10:59,030 --> 00:11:03,560
undervalued and the system is based on a trustworthy, non adversarial relationship.

92
00:11:03,930 --> 00:11:07,190
Therefore, peer review is not set up to detect fraud.

93
00:11:10,500 --> 00:11:14,220
So she goes and talks about trying to use A.I. to do it.

94
00:11:15,800 --> 00:11:23,070
But then they're also saying that you can use A.I. to actually just falsify your data before and.

95
00:11:23,760 --> 00:11:30,630
Yeah, yeah, that's another issue now.

96
00:11:30,720 --> 00:11:35,040
Absolutely. I mean, it's a bad industry. So what do we do?

97
00:11:35,220 --> 00:11:40,410
How can we make sure we do better? It's not p, it's a labor of love.

98
00:11:40,710 --> 00:11:55,780
Right. Why do people do things? You. You know, it's interesting, I think that I told you last weekend that I was reviewing Fulbright applications.

99
00:11:56,620 --> 00:11:57,050
Okay.

100
00:11:57,100 --> 00:12:06,459
And one of the things that was an option was to generate like a letter that I could give to my boss or whatever, saying, you know, I did this work.

101
00:12:06,460 --> 00:12:12,580
It took so many hours of service just to give me, like, a rationale to say, you.

102
00:12:15,300 --> 00:12:19,780
I never had that chance. I just. Doesn't matter for me.

103
00:12:20,170 --> 00:12:24,510
But I mean, for some people, it might be useful. Why do people keep.

104
00:12:27,130 --> 00:12:33,460
Participate in the process. They're at the pole.

105
00:12:37,110 --> 00:12:40,740
I mean, I have no altruistic argument. I think.

106
00:12:42,740 --> 00:12:48,070
Who should be. Sure.

107
00:12:48,730 --> 00:12:56,440
Yeah, I'm not sure either. And. You know, the peer review process is highly variable.

108
00:12:57,040 --> 00:13:07,549
And I think I mentioned to you that some journals are starting to use a technique to know whether they get into the level of detail with respect to,

109
00:13:07,550 --> 00:13:10,690
you know. It's quite interesting.

110
00:13:12,250 --> 00:13:20,840
Well, you know, images that are presented, you know, tend to be a fairly small part of the people in many.

111
00:13:22,170 --> 00:13:26,880
Of the analysis that could be fabricated or falsified don't have images.

112
00:13:29,660 --> 00:13:46,880
How can we ensure the veracity of what's published before left and right for earning credit for replicating already published study?

113
00:13:48,020 --> 00:14:00,550
I think you definitely have already. I continue to here.

114
00:14:04,320 --> 00:14:09,760
Yeah. We have a huge reproducibility issue outside of the company.

115
00:14:10,180 --> 00:14:14,860
Oh, absolutely. So, I mean, I like to give doctoral students, teachers for peer review.

116
00:14:15,550 --> 00:14:21,190
And I do that because I if I published in the journal, I feel like all the reviews for the papers are true.

117
00:14:21,880 --> 00:14:27,430
Okay. But, you know, once you published in the Journal, you're on the list.

118
00:14:27,460 --> 00:14:31,840
And every week or every month you're going to get a request from that editor.

119
00:14:33,670 --> 00:14:37,750
Okay. I can't possibly do it all. So I do try to form some of them now.

120
00:14:38,170 --> 00:14:40,810
But then I have to look at what the student wrote.

121
00:14:41,470 --> 00:14:49,810
We have a very nice discussion about that and it's super informative for everybody because you get to see what people are thinking about.

122
00:14:50,170 --> 00:14:57,489
You get to learn some of the newer methods where the field is going to get on citations

123
00:14:57,490 --> 00:15:01,900
that you ought to know about that you haven't seen because they have to be covered.

124
00:15:03,280 --> 00:15:06,790
But all this takes a while. And for students.

125
00:15:07,450 --> 00:15:12,060
So typically you ask them to look to see if they falsified their data,

126
00:15:12,080 --> 00:15:18,370
if they Photoshop, you know, because that's like a little beyond what we look at.

127
00:15:18,370 --> 00:15:21,660
We look to see is it a good methodology, you know?

128
00:15:22,490 --> 00:15:27,480
Samples. So it won't catch fabrication.

129
00:15:28,200 --> 00:15:32,550
Well, and this isn't about period. This is about reproducing studies that have already been published.

130
00:15:35,350 --> 00:15:38,350
Okay. That's a really good point.

131
00:15:40,010 --> 00:15:43,940
Ideas and thoughts about how to make this a better process.

132
00:15:52,020 --> 00:15:59,170
I'm sure that some of the studies do this, but is there any. Within the study is very.

133
00:16:05,290 --> 00:16:15,490
Yeah. That's a great point, too. You know who we we talk about cognitive biases, and I need to come back to the PowerPoint where I talk about that.

134
00:16:15,850 --> 00:16:19,810
So I expect to see certain things. I expect not to see other things.

135
00:16:20,230 --> 00:16:28,720
And these days, you know, there's so many statistical techniques out there, there's so many outcomes, there's so many adjustments that we can make.

136
00:16:29,530 --> 00:16:37,000
It really gets complicated. And I you guys know about multiple comparisons.

137
00:16:38,420 --> 00:16:51,100
For example, are you familiar with this? So for example, I mean we're this comes up very frequently is oh, gosh, I'm looking to see whether there's a.

138
00:16:53,500 --> 00:16:59,770
Hormonal deficit. Okay. Among your cohort as a result of exposures to chemicals.

139
00:17:01,490 --> 00:17:10,190
And in my untargeted chemical analysis of your blood, I have a million different chemical markers.

140
00:17:13,080 --> 00:17:18,810
And I'm saying if you have a deficit of some form, whatever your estimate,

141
00:17:20,220 --> 00:17:29,400
and if I have a million markers just by chance with the People's .05, I'm going to have 50,000 of them.

142
00:17:29,640 --> 00:17:34,680
Yes. Can't be right. But that's the result I'm going to do.

143
00:17:35,390 --> 00:17:40,740
So I can just do a T test or something like that with a million different chemicals.

144
00:17:40,740 --> 00:17:48,370
So I have to do an analysis that accounts for multiple comparisons in a sophisticated way, because otherwise I'm just giving you randomness.

145
00:17:50,350 --> 00:18:00,580
That's the kind of extreme extremism. But. My hypothesis is that APC being will do something in my on target analysis.

146
00:18:00,590 --> 00:18:05,700
I have a lot of hits with the PCBs, so I think I've got positive.

147
00:18:07,440 --> 00:18:13,020
But it still is. Or it could be. Mr. Just make sense.

148
00:18:14,080 --> 00:18:23,120
You don't talk about. Okay. I mean, that's just one issue. Well, I don't know.

149
00:18:23,300 --> 00:18:36,380
I mean, I think these suggestions here are really important, and it's very much in line with the themes of today's talk.

150
00:18:36,980 --> 00:18:52,610
And, you know, I had talked last week with you briefly about these cognitive fallacies and, you know, recognizing what's wrong with the system.

151
00:18:52,610 --> 00:18:57,710
So I want to, in the next half hour or so, talk about ways to try to make it better.

152
00:18:58,490 --> 00:19:06,320
Okay. So we can move forward in a constructive way. So, you know, we we certainly understand bias.

153
00:19:07,690 --> 00:19:16,410
Probably. We've heard of two people fabricating.

154
00:19:18,450 --> 00:19:24,430
You talked a little bit about peer review. And some other things that would help.

155
00:19:26,390 --> 00:19:35,180
And I think I talked about this last week through devising techniques and ways to sort of improve the situation.

156
00:19:35,720 --> 00:19:38,260
You know, consider the devil's advocate position.

157
00:19:39,230 --> 00:19:46,629
Do a data collection analysis plan for the experiment so that you set up rules that you think that make sense.

158
00:19:46,630 --> 00:19:49,100
So you're not just driven by data.

159
00:19:50,240 --> 00:19:58,460
You know what's really interesting to me, having been involved in various types of studies, like epidemiological studies for 35 years.

160
00:19:58,550 --> 00:20:09,110
In the old days, you're a very strict hypothesis and tested only within your data analysis and collection.

161
00:20:10,290 --> 00:20:15,690
Now we have so much more data. We can't even think of all the hypotheses.

162
00:20:15,990 --> 00:20:23,940
We just let the data generate hypotheses for us. And so things that we would never have done in the past are routinely done.

163
00:20:23,940 --> 00:20:27,090
Now we've just become much more flexible.

164
00:20:28,170 --> 00:20:31,379
This is really. Teams of rivals.

165
00:20:31,380 --> 00:20:38,490
Very interesting approach. Get your antagonists here, your adversaries, your competitors.

166
00:20:39,510 --> 00:20:41,910
I don't know very many people that are able to do that.

167
00:20:42,120 --> 00:20:49,450
That's an interesting approach for your team employing data analysis to understand data analysis.

168
00:20:51,470 --> 00:21:00,770
If your professor gives you a data set and says, analyze this and you get an answer, you get an answer, everybody gets different answers.

169
00:21:01,520 --> 00:21:07,040
Who's right? But I was right.

170
00:21:07,640 --> 00:21:17,480
But if I give you a data set and I manipulated it and I have put in or generated it to test your approach and I give it to each one of you,

171
00:21:17,480 --> 00:21:20,600
you should all come up with the same answer. Okay.

172
00:21:20,720 --> 00:21:27,440
And so that's like a shootout to use a bad acronym, you know, to see who's coming up with the right answer with their technique.

173
00:21:28,010 --> 00:21:43,570
This is sometimes stuff. So some suggestions to move forward here in a good way is to first ask the right question.

174
00:21:45,440 --> 00:21:49,480
This is a Nobel Prize winner, but it's common for.

175
00:21:51,020 --> 00:21:56,870
Sorry answer, but answering the question, sir, how do you ask the question?

176
00:21:57,680 --> 00:22:03,560
Well, you might have some observations if you can try and understand why is this happening?

177
00:22:03,860 --> 00:22:07,430
Where is it happening? When is it happening? Why?

178
00:22:07,820 --> 00:22:14,490
What's the mechanism for it? These are. Know, these are all, in a sense to me, very specific.

179
00:22:15,570 --> 00:22:18,750
So it's not in general, it should be testable.

180
00:22:18,810 --> 00:22:23,160
And this comes really to the scientific method. You guys all understand the scientific method.

181
00:22:23,500 --> 00:22:27,660
But you control those parameters. You change one thing, you test it.

182
00:22:28,020 --> 00:22:32,370
You see whether the change is meaningful, whether it's statistical or whether it's reproducible,

183
00:22:33,120 --> 00:22:36,330
should learn to, you know, should lead to new knowledge.

184
00:22:38,760 --> 00:22:43,650
But do a scientific experiment. You have to have good experimental design.

185
00:22:44,360 --> 00:22:50,550
You guys take in, of course, some experimental design. Probably not.

186
00:22:50,570 --> 00:22:55,639
So, you know, there's a lot of ways to think about it,

187
00:22:55,640 --> 00:23:04,040
but the overall goal is that good experimental design should be to something which is robust, which is rigorous and reproducible.

188
00:23:05,110 --> 00:23:12,550
So forth. There's a number of requirements. Ideally as a result.

189
00:23:13,520 --> 00:23:18,980
So it should apply in a lot of different situations.

190
00:23:20,450 --> 00:23:27,970
Talk more about that. It should be simple, ideally elegant somehow, and you should be able to calculate.

191
00:23:28,280 --> 00:23:33,049
Certainly there's other things that go on in randomization.

192
00:23:33,050 --> 00:23:39,160
So. The cartoon is one example of longer experimental design.

193
00:23:39,760 --> 00:23:43,210
All the mice are scorpions. What does he mean by that?

194
00:23:51,000 --> 00:23:54,840
They're not sagittarians right now. They're all.

195
00:23:55,230 --> 00:24:00,370
Yeah, they're all born at the same time. Maybe they're all the same age and they should be looking at different ages, of course.

196
00:24:00,990 --> 00:24:11,830
Okay. Or maybe different, strange, whatever. And if you're only testing such a subset, how do you know those results are reproducible?

197
00:24:11,830 --> 00:24:14,920
They're going to apply to a different population. There's no idea.

198
00:24:16,090 --> 00:24:27,790
So even if you exclude, you know, males or females. So again, we want to use the scientific method with good experimental design.

199
00:24:28,330 --> 00:24:32,500
Part of the scientific method is a systematic. Study literature.

200
00:24:33,430 --> 00:24:37,420
And so you understand where we're starting from and then moving forward from that.

201
00:24:38,530 --> 00:24:41,409
And, you know, in my role as professor,

202
00:24:41,410 --> 00:24:49,510
I have to be really patient because you have to develop the background to understand the scientific literature that's going to take you from time.

203
00:24:50,530 --> 00:24:57,340
Okay. And a lot of times we see people who are less familiar with literature repeating the things that were already done.

204
00:24:57,350 --> 00:25:02,350
That's not really advancing literature. So you have to push them up a little bit higher.

205
00:25:04,030 --> 00:25:10,200
Peer review. We talked about transparency when we're starting to talk about transparency.

206
00:25:11,870 --> 00:25:20,630
So providing all the information, the design, the execution of the experiment, the data that you got.

207
00:25:21,990 --> 00:25:28,620
The analysis, how you did your analysis, making all this stuff accessible and understandable to people.

208
00:25:31,750 --> 00:25:36,020
You know, in a journal. How many words is a typical.

209
00:25:39,730 --> 00:25:45,770
If you have any idea. When Jim Baker gives you his homework, how many words to see allow you to write?

210
00:25:48,570 --> 00:26:01,840
It doesn't really matter because journals are typically 4 to 7000 words, which is about a hundred more,

211
00:26:02,100 --> 00:26:10,860
seven single spaced pages, something like that, maybe less, but not very much to describe a complicated experiment.

212
00:26:11,550 --> 00:26:17,790
Okay. And this is everything. Intro hypothesis, the results of discussion.

213
00:26:17,790 --> 00:26:23,550
The conclusion doesn't include the references to the more textbook.

214
00:26:25,760 --> 00:26:29,420
Not much space. No way.

215
00:26:29,750 --> 00:26:40,160
I can describe a complicated measurement system or CMS analysis or satellite data processing or anything.

216
00:26:40,430 --> 00:26:45,260
I can't do that in 4000 words, but click a button.

217
00:26:45,410 --> 00:26:53,290
You get the supplemental information. You guys are now approaching a hundred pages, in some cases, no limit.

218
00:26:54,100 --> 00:27:03,460
Okay. So you can put it there. All those images that this person found that were Photoshop should all be listed at all available.

219
00:27:05,770 --> 00:27:12,390
So you can do that now. So that's a really positive aspect in terms of transparency, you know,

220
00:27:12,430 --> 00:27:16,450
with electronic journals is to put all that information out there that will solve that problem.

221
00:27:17,740 --> 00:27:22,350
So that's really good. Data presentation.

222
00:27:22,830 --> 00:27:27,450
We'll talk about that briefly. And so conference system.

223
00:27:28,200 --> 00:27:34,290
And by that, you know what I mean? Or if it's publish or perish, my daughter published 35 articles to get promoted.

224
00:27:34,860 --> 00:27:39,840
Oh, my gosh. You know, that's got to be really challenging and that's part of the culture.

225
00:27:40,290 --> 00:27:44,440
Can we change that or. Peer review?

226
00:27:44,670 --> 00:27:49,350
Why should I do an interview? I did nothing back and it takes a long time.

227
00:27:50,040 --> 00:27:54,510
We need to build in incentives to get people to do that in a serious way.

228
00:27:55,610 --> 00:28:03,110
Can we do that? Sure. Part of your evaluation of the job each year for merit or promotion,

229
00:28:03,470 --> 00:28:08,210
salary increase and say what do you do in terms of helping the professional community?

230
00:28:08,690 --> 00:28:17,960
So you have a service component or professional development component, then you get to sort of wait for your evaluation so we can do that.

231
00:28:21,910 --> 00:28:31,090
You guys think of others incentives or ways to change the culture a little bit to make this more feasible?

232
00:28:38,960 --> 00:28:42,260
You know that accessibility, you know?

233
00:28:42,820 --> 00:28:50,140
Yeah. Also like publishing. Or like not rejecting the.

234
00:28:53,550 --> 00:28:56,670
Actually, I feel like that might only happen if people are like.

235
00:29:01,190 --> 00:29:03,319
Yeah. You know, another thing that comes to mind, too,

236
00:29:03,320 --> 00:29:09,410
is that there are many topics that we have probably published a lot of papers on and now journals don't want to see.

237
00:29:10,550 --> 00:29:21,110
Okay. And if those older studies aren't that good or, you know, there's new techniques or things, maybe we should reconsider that, too.

238
00:29:21,560 --> 00:29:27,320
So these are good talks. I wish I had a few more slides here.

239
00:29:28,510 --> 00:29:35,030
Oh, so, you know, in, oh, about eight or nine years ago, you know.

240
00:29:37,980 --> 00:29:45,230
Reviewed. With. Coming from this story, the Duke story,

241
00:29:45,280 --> 00:29:53,740
the other story that we talk about and really fond of the emphasis on this reproducibility and response and rigor and transparency.

242
00:29:54,550 --> 00:30:05,620
And to clarify for researchers the expectations regarding this and how an audience wants to see this described in applications,

243
00:30:05,620 --> 00:30:12,820
and what I mean applications and what this slide is talking about is that if you're a scientist, you're trying to get money from NIH, you're applying.

244
00:30:13,180 --> 00:30:16,770
So you put together a research proposal or an application to Emily.

245
00:30:17,020 --> 00:30:27,730
That's what we're talking about. And they are working through their grants mechanism to try to bolster greater transparency in scientific research.

246
00:30:28,970 --> 00:30:40,280
But recognize, of course, NIH is spending and NSF and other agencies on the order of $100 billion or something like that in research each year.

247
00:30:40,850 --> 00:30:46,340
So it's fairly influential, but. Industry is spending a lot.

248
00:30:48,740 --> 00:30:54,080
Okay. So this is a most of the research money. It's just the money that NIH has control over.

249
00:30:54,860 --> 00:31:04,590
So we still have this other problem. So anyway, you know, it is time to change what they have in their system.

250
00:31:04,740 --> 00:31:07,850
They want to begin this cultural shift. Culture shift.

251
00:31:10,220 --> 00:31:17,290
They want applicants to consider all the issues that we talk about bias, reproducibility, replicability.

252
00:31:18,770 --> 00:31:26,790
So that's good. Push. It seems to me that industry would even want more accurate results and that there would be lot.

253
00:31:28,290 --> 00:31:31,800
From industry itself, not from the research.

254
00:31:34,160 --> 00:31:38,330
0111.

255
00:31:38,330 --> 00:31:41,350
Just like they're not going to produce that drug. Well, maybe they will.

256
00:31:45,950 --> 00:31:49,070
Well, I mean, the drug, you know, bringing a drug to market.

257
00:31:51,470 --> 00:31:56,420
Some indications of efficacy. Efficacy can be hugely profitable.

258
00:31:57,320 --> 00:32:03,980
Whether it works or not in the long term. We have lots of examples of this and in fact, you know,

259
00:32:04,010 --> 00:32:10,639
there's tremendous pressure on the Food and Drug Administration to relax the testing

260
00:32:10,640 --> 00:32:16,370
requirements for drugs that are used in situations where patients are going to die anyway.

261
00:32:17,390 --> 00:32:21,350
Very soon. So this is really interesting.

262
00:32:21,440 --> 00:32:25,370
I mean, this affects me in the work that I do on hate list, which is typically.

263
00:32:25,700 --> 00:32:32,840
For three years. Yeah, right. In a community of people with this disease or friends that they were searching for.

264
00:32:35,630 --> 00:32:46,520
So now there are some sort of therapies, things out there which are not going through all the different phases of testing the FDA typically wants.

265
00:32:48,120 --> 00:32:55,669
But you know, if you're a patient, you're searching. Back.

266
00:32:55,670 --> 00:32:58,760
Then I answered again and I just tried what they can do.

267
00:32:59,780 --> 00:33:13,540
They want to demonstrate to stakeholders. But it also wonders for the Congress that they're doing the right thing and investing in science and.

268
00:33:16,000 --> 00:33:24,580
So, you know, transparency has been adopted as key priorities for NIH groups.

269
00:33:25,140 --> 00:33:31,959
They go going. So back in 2016, they put in all these requirements.

270
00:33:31,960 --> 00:33:40,060
And that's also added to this course because prior to 2016, we didn't teach rigor and reproducibility in five terms.

271
00:33:40,660 --> 00:33:46,600
Okay. So the ninth module that was added and rigor reproducibility.

272
00:33:46,960 --> 00:33:52,300
I have not defined the difference between what I work with, which is pretty simple.

273
00:33:52,630 --> 00:34:02,290
Reproducibility is something that measures the closeness of the agreement between the results of the test results under different circumstances.

274
00:34:03,040 --> 00:34:08,050
Okay. So might be different populations, different settings, different times.

275
00:34:08,530 --> 00:34:17,440
It should give similar sort of results. So this accounts for the use of different measurement methods, analysis by people, different locations.

276
00:34:18,070 --> 00:34:25,030
Replication is just doing the same thing over again and getting the same results, not testing a different method,

277
00:34:25,390 --> 00:34:30,160
not testing a different population, not even testing it with a different investigator.

278
00:34:30,460 --> 00:34:40,440
It could be just you in the lab. This is not enough to get high quality research, but it's essentially okay.

279
00:34:40,500 --> 00:34:43,750
It has to be reproducible, has to be replicable.

280
00:34:44,320 --> 00:34:50,230
We want that to be relevant, so important for transparency and validity.

281
00:34:50,650 --> 00:34:56,400
And so some of these things is considered quite quality.

282
00:34:59,800 --> 00:35:05,260
So to improve this, we need good experimental design.

283
00:35:06,520 --> 00:35:10,180
Lots of issues involved in that experimental design.

284
00:35:11,200 --> 00:35:14,310
You know, classically then we have an experiment with change.

285
00:35:14,320 --> 00:35:19,870
One factor. You see, if there is an effect due to that factor with an appropriate statistical test,

286
00:35:19,870 --> 00:35:24,850
the classical, mandolin and types of exercises that you have been exposed to.

287
00:35:25,600 --> 00:35:34,210
But, you know, and there's so many issues involved, I can go through all the list, the right control.

288
00:35:35,220 --> 00:35:44,490
This is an epidemiological studies are and drug studies a huge challenge because I can find all the people that have the disease.

289
00:35:45,240 --> 00:35:49,830
But if I'm looking for like an age, gender, match control, population.

290
00:35:50,820 --> 00:36:01,540
Where do I find those people? Do you guys ever respond to a request to be a study subject and role in the study?

291
00:36:02,710 --> 00:36:09,570
Have you just. But some surveys.

292
00:36:10,780 --> 00:36:20,780
But this is sometimes really problematic because people that enroll in these studies typically tend to be more urban and

293
00:36:20,780 --> 00:36:30,679
younger and educated compared to people with a disease that could be affecting the folks who live in the countryside,

294
00:36:30,680 --> 00:36:38,230
don't even have Internet, you know, wouldn't be having the same lifestyle, diet, whatever you.

295
00:36:41,390 --> 00:36:47,209
You know, we used to just do random digit daily, but now everybody is hard to find.

296
00:36:47,210 --> 00:36:50,790
So this doesn't work at all. You know, they recruit people.

297
00:36:51,220 --> 00:36:54,890
It's really a problem. We have people walking the streets sometimes.

298
00:36:55,040 --> 00:37:02,720
But again, how do you get out in the country? I don't have enough time to go through all these things.

299
00:37:03,260 --> 00:37:11,900
You don't understand statistical power when you're supposed to sometimes done sort of an ecological study where.

300
00:37:14,060 --> 00:37:22,320
Well, here, here, you know, I talked about matching funds, cases and controls as an example of.

301
00:37:22,860 --> 00:37:27,720
Another example might be to do a longitudinal study and I sample,

302
00:37:28,080 --> 00:37:36,489
I don't know your blood and urine and do all these different tests on you and they might catch up with you in another five years, another ten years.

303
00:37:36,490 --> 00:37:41,790
And I just look at you compared to yourself over time. Potentially very powerful operation.

304
00:37:41,820 --> 00:37:51,730
Okay. Paired measurements or. Certainly lots of different types of experimental design, observational studies.

305
00:37:53,040 --> 00:38:00,690
And interventional studies. These are becoming really, really powerful if we're able to do the interventions,

306
00:38:01,560 --> 00:38:06,960
which is something that we like to do in the environmental justice community a lot because,

307
00:38:07,080 --> 00:38:16,650
you know, we can change some things and see what effectiveness randomized controlled trials to progress or multiple.

308
00:38:18,390 --> 00:38:24,330
Choose a good study design really critical elements for research application.

309
00:38:28,440 --> 00:38:33,090
So all in all, we want to study design, which is now.

310
00:38:34,420 --> 00:38:42,220
If you. Only so far.

311
00:38:42,550 --> 00:38:47,650
What about a faulty design? Total waste of time.

312
00:38:48,760 --> 00:38:54,500
Don't even go there. Okay. Just wasting resources.

313
00:38:55,850 --> 00:39:02,540
Okay, I have a couple of more concepts to get across. Some of these things will be familiar to you, but maybe in a slightly different way.

314
00:39:03,080 --> 00:39:12,500
For example, the notion of validity okay, is really a powerful, comprehensive statement to say that something is valid.

315
00:39:13,220 --> 00:39:16,490
A lot of times we're actually not quite there.

316
00:39:16,940 --> 00:39:21,710
Instead, we might say, Oh, my study was evaluated and it looks good.

317
00:39:22,700 --> 00:39:23,380
Validation.

318
00:39:23,900 --> 00:39:33,710
Another step up where I have the test or the drug or whatever I'm looking at applied to multiple contexts to see that in fact it's reproducible.

319
00:39:33,830 --> 00:39:38,120
It's not just a single study that would show me that a drug test works.

320
00:39:39,770 --> 00:39:44,270
Last week, another COVID. Booster.

321
00:39:45,220 --> 00:39:47,170
Was described in the literature.

322
00:39:47,170 --> 00:40:00,430
This included a formulation designed to be against two or three of COVID variants, and it was tested on like 40 people,

323
00:40:01,030 --> 00:40:07,570
and they looked at the antibody levels and they were like four or five times higher than that for every other booster.

324
00:40:08,380 --> 00:40:12,520
And they said, Oh, fantastic news. We've got a great new formulation.

325
00:40:13,720 --> 00:40:17,990
40 people. Validity, in my opinion.

326
00:40:18,280 --> 00:40:29,320
Okay, it's a test. We need more. And often we look at validation as sort of formal recognition by many.

327
00:40:30,960 --> 00:40:35,400
So it's not just, you know, the drug companies say it works great.

328
00:40:37,860 --> 00:40:44,070
These other terms are important sometimes for users or people.

329
00:40:45,860 --> 00:40:49,550
This is the hierarchy that I use for something that.

330
00:40:51,620 --> 00:40:55,790
Gender calibration. A valuation of just.

331
00:40:59,460 --> 00:41:05,760
Some other techniques that I didn't describe much when we were talking about experimental design was blinding and randomization.

332
00:41:06,210 --> 00:41:09,540
So everybody know what these things are. So I need to talk about them.

333
00:41:10,290 --> 00:41:14,970
Why are they important? They can eliminate potential biases.

334
00:41:16,640 --> 00:41:22,890
You know, if I am running a chemical analysis and I say, okay,

335
00:41:22,890 --> 00:41:30,450
let's do all the kids first and lots of middle schoolers and all the high schoolers and my instrument drips over time.

336
00:41:31,320 --> 00:41:36,900
Then I have an instant bias in my results. Right. So I should randomize things that eliminate several.

337
00:41:38,100 --> 00:41:49,280
Large. Uh, on the website, there's some links to the YouTube videos that you can watch.

338
00:41:51,130 --> 00:41:56,800
Okay. We've talked about replication and I've got 4 minutes.

339
00:41:56,860 --> 00:42:01,840
Oh, my gosh. So we're going to talk more about the sample size outliers.

340
00:42:03,160 --> 00:42:10,180
What I want to get to. And then the materials that you use in the tests should be authenticated.

341
00:42:10,570 --> 00:42:14,430
They should be, you know, not ten years old after expiration.

342
00:42:15,520 --> 00:42:20,110
Basic stuff like that. Data presentation.

343
00:42:20,680 --> 00:42:28,050
This is a really simple example dataset to another dataset we want to compare,

344
00:42:28,090 --> 00:42:33,760
see if there's a difference in quality and a standard error of the same, right?

345
00:42:35,350 --> 00:42:36,790
There's a different group of vision.

346
00:42:37,120 --> 00:42:46,720
And what's the point of you being totally told we did that first presentation didn't show any of those differences.

347
00:42:47,140 --> 00:42:50,800
Very misleading. There's something strange.

348
00:42:53,140 --> 00:43:00,330
These are the same. So how you presented it is really important in interpreting it.

349
00:43:01,530 --> 00:43:07,020
This is where I want to end up with this rigor and reproducibility in this notion of open science.

350
00:43:07,740 --> 00:43:10,770
You all know about open access journals. Okay. All of them.

351
00:43:11,570 --> 00:43:17,610
Okay. You can click on it. You get that page right away that you want. Don't have to go through the library system or anything like that.

352
00:43:17,610 --> 00:43:25,290
So that's hugely convenient. But there's more to the open science.

353
00:43:25,290 --> 00:43:28,950
People want to have transparency.

354
00:43:29,660 --> 00:43:35,780
Three years of data participation and accountability in practices that all promote

355
00:43:36,050 --> 00:43:42,260
data sharing and common data analysis to improve the credibility of the research.

356
00:43:43,340 --> 00:43:50,690
And they have this taxonomy of open science and the open access journals for just one part of.

357
00:43:52,010 --> 00:43:59,120
We want to have open data. We want to have metrics that are open.

358
00:43:59,120 --> 00:44:05,360
So you evaluate, for example, an outcome using the same clinical or at least defined clinical terms.

359
00:44:05,990 --> 00:44:09,740
Okay. And up and down this hierarchy really complicated.

360
00:44:09,770 --> 00:44:17,720
I don't understand most of this. Okay. This is just the concept that there's so much more than simply open access to.

361
00:44:20,810 --> 00:44:29,050
To promote this. We have now begun to see this evolution.

362
00:44:29,680 --> 00:44:36,760
Of their guiding principles for scientific theory, management and stewardship.

363
00:44:37,210 --> 00:44:40,390
There is an acronym findable accessible for all.

364
00:44:41,120 --> 00:44:53,469
And use. It's a standard basically for archiving data so that you could look at it 20 years from now and see if what I did was work with my data.

365
00:44:53,470 --> 00:44:56,760
So. And so we're starting to see.

366
00:44:58,710 --> 00:45:04,080
Journalists request that you put your data in a secure location.

367
00:45:05,070 --> 00:45:08,729
That people can access. You can actually support this.

368
00:45:08,730 --> 00:45:10,260
We have something called Deep Blue.

369
00:45:11,130 --> 00:45:19,530
And I think somewhere in the south of town, in some subterranean place, there are servers that are supposed to be intact forever.

370
00:45:20,310 --> 00:45:22,350
Okay. That you can pull out of that data.

371
00:45:22,820 --> 00:45:29,850
Data to put in there has to have better data describing what it is in enough detail so that others can use it.

372
00:45:31,290 --> 00:45:40,950
But more broadly, this is part of this. We want to help you understand how to use it to get where you from and so forth.

373
00:45:41,340 --> 00:45:46,020
And so this fair standard is something that's been evolving.

374
00:45:46,770 --> 00:45:51,219
You guys heard of it? So it's pretty interesting concept.

375
00:45:51,220 --> 00:45:58,750
It's a lot of work to do this. Who's going to pay me to put my data?

376
00:46:00,570 --> 00:46:08,240
Comply with fair principles and then put it in the. I don't.

377
00:46:09,860 --> 00:46:13,820
It's another another challenge. Okay.

378
00:46:13,910 --> 00:46:23,090
The very last topic in my last minute here is that despite all of the steps that we can take to improve rigor,

379
00:46:23,090 --> 00:46:28,970
reproducibility and transparency and so forth, it's still going to be uncertainty, right?

380
00:46:29,630 --> 00:46:35,390
I mean, you never know if this pill that you've just been prescribed is going to do the trick.

381
00:46:37,160 --> 00:46:39,500
So we have to make decisions after uncertainty.

382
00:46:40,280 --> 00:46:52,130
And the last article that I gave you, I believe, was from one of the founders, I guess you could say, of of decision science or risk assessment.

383
00:46:53,660 --> 00:46:57,620
And the idea here is that the future is uncertain.

384
00:46:58,280 --> 00:47:05,330
We still have to make decisions. How do we do that? And so, you know, as an example.

385
00:47:06,550 --> 00:47:18,040
Two strains of the disease. We have a vaccine that protects it protects you for one vaccine only does a partial job against both.

386
00:47:18,670 --> 00:47:22,600
Which do you take? What's your call?

387
00:47:22,720 --> 00:47:25,930
Who takes a. Raise your hand.

388
00:47:25,940 --> 00:47:29,390
He takes me. Can you make a decision?

389
00:47:30,080 --> 00:47:34,820
You have to make a decision. Okay. Which one do you think they'll be or not?

390
00:47:37,550 --> 00:47:49,550
You could be anybody today. You know, so this is interesting to me because the other section of the class, everybody went along with the nobody today.

391
00:47:51,350 --> 00:47:54,980
So you're kind of diversifying your portfolio, right?

392
00:47:55,720 --> 00:48:03,780
All right. That kind of makes sense. But also like. Like the COVID vaccine reduces your reaction rate.

393
00:48:03,790 --> 00:48:12,530
So. Knocked down by the other three.

394
00:48:14,500 --> 00:48:24,480
Yeah. No, I mean, there's no right answer here. Subjected to what Bernard Christoff has done is to get you to understand her your views.

395
00:48:24,760 --> 00:48:27,920
You have to understand the nature of the risk and your perception of.

396
00:48:28,520 --> 00:48:32,290
So these are all risk perception factors that influence how we perceive risk.

397
00:48:32,650 --> 00:48:38,800
And, you know, it can be whether you're going to take a vaccine, A or B or do something else.

398
00:48:39,610 --> 00:48:43,750
And so we should just be aware of our personal factors that influence.

399
00:48:45,960 --> 00:48:53,080
Yeah. Okay. That's where I wanted to end up. You know, we're not in a perfect world, but the notion of rigor and reproducibility,

400
00:48:53,080 --> 00:48:58,310
making things transparent that could help us with our outcomes are critical.

401
00:48:59,110 --> 00:49:03,070
Thank you, guys. You have a good week next week.

402
00:49:03,580 --> 00:49:06,970
I forget who's up in this class.

403
00:49:07,450 --> 00:49:15,460
I think you. It's not. I call on.

404
00:49:16,750 --> 00:49:20,590
That's what I was going to say. Yeah. So that should be good. Okay. So thank you so much.

