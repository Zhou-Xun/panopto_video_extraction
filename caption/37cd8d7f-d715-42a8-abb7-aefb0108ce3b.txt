1
00:00:00,660 --> 00:00:05,020
One of my daughter's past clothes.

2
00:00:05,040 --> 00:00:13,950
There was I'm going to throw up $11 to this interview because we're going to get started here.

3
00:00:14,270 --> 00:00:18,030
I want to make a few just kind of general class announcements.

4
00:00:18,060 --> 00:00:21,950
First of all, I thank you for trying to project preferences.

5
00:00:21,960 --> 00:00:26,580
I think I use 5% of you actually turn your party preferences.

6
00:00:27,030 --> 00:00:28,290
So I've assigned you to groups.

7
00:00:28,290 --> 00:00:33,900
I just emailed you in the last half an hour or so for your project group, so you should know which groups you're engaged on.

8
00:00:33,910 --> 00:00:41,670
This is the course website. We've got groups looking at bike lanes, cigaret bands, colon cancer screening,

9
00:00:41,680 --> 00:00:46,049
drugs for diseases, end of life care, genetic screening, Medicare, substance abuse care.

10
00:00:46,050 --> 00:00:50,130
People have certain lists for delivery and sky raising for Crohn's disease.

11
00:00:51,420 --> 00:00:58,230
And I tried to give you your top preference and try to make sure you're with people you like and not people you didn't like to work with.

12
00:00:58,530 --> 00:01:02,400
But there are no guarantees here. So I apologize if you didn't get exactly what you're looking for,

13
00:01:03,570 --> 00:01:11,130
but if you if you have any challenges with your groups or meeting with your family or other members, please let me know if you're having challenges.

14
00:01:13,650 --> 00:01:21,360
And also, I hope today at the end of class was some time for you to talk with your group mates and get to meet them.

15
00:01:21,370 --> 00:01:25,679
And so that's kicking off.

16
00:01:25,680 --> 00:01:32,759
Also, I wanted to make an announcement. Too many of you may have noticed the links here to the Munich book, if you click on it.

17
00:01:32,760 --> 00:01:39,569
If I said the book is not available and it was available last week and I'm not sure why the Michigan Library is working there,

18
00:01:39,570 --> 00:01:42,990
so I'm going to put a second link at the top of page actually.

19
00:01:42,990 --> 00:01:49,020
I'll try to maybe correct all the links here. So if you click on this link, it shows you the second edition,

20
00:01:49,020 --> 00:01:55,320
which we do for some reason still have online access to try to figure this out with the library, with what happened here in the last week.

21
00:01:56,010 --> 00:01:59,100
But you again, the second edition has basically exactly the same content.

22
00:01:59,100 --> 00:02:02,700
So so don't worry about that if you're reading Syracuse University.

23
00:02:02,850 --> 00:02:10,110
In addition, I let you know if I can get that resolved with this definition, which is very important stuff online.

24
00:02:10,110 --> 00:02:16,860
I think it's hopefully it's handy for you guys to be able to read this online rather than get more them all around.

25
00:02:17,010 --> 00:02:20,790
You've heard it works personalization more than anything.

26
00:02:21,120 --> 00:02:27,269
So that link at the top of our page has this and I'll try to actually I'll try to update, I'll give a lecture.

27
00:02:27,270 --> 00:02:32,270
So you have the links to the online book readings.

28
00:02:32,820 --> 00:02:37,780
Any questions? So.

29
00:02:38,560 --> 00:02:41,200
Well, one other thing, too, with your project, Bruce,

30
00:02:41,920 --> 00:02:49,060
I'd like you to turn in the kind of at the topic description of project plan and document a week from today.

31
00:02:49,390 --> 00:02:54,190
So I would like you to meet with your groups here soon to kind of start working around.

32
00:02:54,490 --> 00:03:02,740
A lot of these project topics are kind of broad, you know, bike lanes or, you know, drugs for rare diseases.

33
00:03:02,740 --> 00:03:10,150
Those are very broad categories. And that's okay to help you find people you want to work with and a topic you find interesting.

34
00:03:10,870 --> 00:03:18,250
But in the end, your cost effectiveness project will probably have to be something a little more narrow than just drugs for our diseases, for example.

35
00:03:19,150 --> 00:03:22,389
So that's one of the things that you want to do in the next week.

36
00:03:22,390 --> 00:03:29,740
Here is your group and try to solidify a little more precisely what your project topic will look at.

37
00:03:30,220 --> 00:03:36,600
And so I'd like you to be thinking about, you know, what are the potential interventions you want to look at.

38
00:03:36,610 --> 00:03:41,049
So, for example, drugs for rare diseases, you know, what type of rare diseases are you thinking?

39
00:03:41,050 --> 00:03:44,620
You know, maybe pick a specific pregnancy, maybe a drug.

40
00:03:44,920 --> 00:03:48,870
And that drug, what are you comparing it to? Is is is no treatment at all?

41
00:03:48,910 --> 00:03:52,150
Is it drug versus drug B, you know, biplanes.

42
00:03:52,930 --> 00:03:56,740
How are you thinking about bike lanes? Bike lanes in what locations and and so forth?

43
00:03:57,310 --> 00:04:02,740
To try to think through a few more specifics, you don't have to have every last specific nail down in the next week,

44
00:04:03,040 --> 00:04:06,159
but I'd like you to start moving along that process. Throughout the semester.

45
00:04:06,160 --> 00:04:12,880
You will be kind of refining your project topics, and that's okay. But you know, the sooner you can kind of get things narrowed down, the better.

46
00:04:13,210 --> 00:04:18,430
So that's one thing is the project topic is precisely what topic are you going to be looking at,

47
00:04:18,430 --> 00:04:24,580
what population, what type of intervention interventions are you going to compare with each other?

48
00:04:25,000 --> 00:04:36,310
So those are some of the kind of key things you'll be thinking about here for that first assignment for the project, in fact, up here.

49
00:04:37,980 --> 00:04:50,400
The. So a little bit of kind of background on what what you're thinking about here,

50
00:04:51,060 --> 00:04:54,330
the alternative interventions, what information you think you might have.

51
00:04:54,360 --> 00:04:58,740
So if it's biplanes, you say, hey, I got a screen study that says bike lanes or, you know,

52
00:04:58,920 --> 00:05:05,790
increase cardiovascular exercise by X and reduce traffic deaths by Y, you know.

53
00:05:06,210 --> 00:05:07,590
What information have you found so far?

54
00:05:07,590 --> 00:05:14,010
You don't have to have found everything yet in the next week, but the definite average, the amount of important outcomes to be analyzed.

55
00:05:14,010 --> 00:05:18,180
So again, going to bike lanes, you know, what are the key outcomes you think are important?

56
00:05:18,510 --> 00:05:21,810
Is it cardiovascular exercise? Is it traffic deaths? Is that pollution?

57
00:05:22,180 --> 00:05:28,589
You know, what things do you think you want to look at? And then remarks on why?

58
00:05:28,590 --> 00:05:33,810
It's an important and interesting question to why. Why do you think bike lanes are is an interesting question.

59
00:05:34,530 --> 00:05:40,979
It's not obvious that, hey, we should build bike lanes every ten feet, you know, are there trade offs here?

60
00:05:40,980 --> 00:05:45,560
Okay. It cost money to build bike lanes versus the health benefits or potential health risks.

61
00:05:45,570 --> 00:05:47,820
You know, why is this an interesting question to analyze?

62
00:05:49,440 --> 00:05:54,960
And then the project plan is really I just want you to start thinking about how you want to work together as a team.

63
00:05:55,650 --> 00:06:01,440
Every last detail, but maybe a simple yes. Chart or something about how you want to be doing things.

64
00:06:01,800 --> 00:06:07,500
I don't. I designed this project to have a lot of deliverables. So think about, you know, when are you going to be doing certain things.

65
00:06:07,740 --> 00:06:12,420
I think it's helpful for you to start talking to as a group about how you want to work together.

66
00:06:12,870 --> 00:06:17,460
So, you know, typically, often you've probably had a lot of experiences like this.

67
00:06:18,270 --> 00:06:21,300
Students will, you know, do things the very, very last minute.

68
00:06:21,390 --> 00:06:26,129
So, you know, that's that works for some groups, but maybe that's not the best in other situations.

69
00:06:26,130 --> 00:06:32,520
And so if you can have an honest conversation with each other about timelines and when you what your expectations are, I think that can be helpful.

70
00:06:34,410 --> 00:06:39,120
So if you say, hey, look, we need to get a draft together and we're going to review at least 24 hours in advance.

71
00:06:39,360 --> 00:06:44,130
You know, those are things you can think about before your your project plan or a plan for how you're to work together.

72
00:06:45,360 --> 00:06:48,929
Yeah. And then you don't have to have every single thing answer it.

73
00:06:48,930 --> 00:06:56,610
So if you have questions for me also, you put that in this, in this document, you any questions about this first project?

74
00:06:56,790 --> 00:07:00,690
Yes. Do we do this as a group or turn it in individually?

75
00:07:00,930 --> 00:07:09,690
Yes. Great question. So all of the well, not all almost all of the project assignments are group assignments.

76
00:07:09,690 --> 00:07:15,120
So you'll turn them in as a group. There's one project assignment that is not a group.

77
00:07:15,570 --> 00:07:19,890
And that assignment that is the last one, the bonus allocation.

78
00:07:21,450 --> 00:07:23,710
Well, actually, let me go through all these all these questions.

79
00:07:23,790 --> 00:07:34,170
So you'll turn in your project description or project plan in a week, and then you'll turn in your introduction section a week after that.

80
00:07:35,120 --> 00:07:40,229
So that kind of sets the stage for what you're looking at, and then you'll present your method.

81
00:07:40,230 --> 00:07:42,660
So how do you think you're going to analyze the problem?

82
00:07:42,670 --> 00:07:49,800
You give a presentation, a group presentation in front of the class here, but how do you think you're going to do your analysis?

83
00:07:50,070 --> 00:07:53,990
That is ungraded, but it's opportunity to get feedback.

84
00:07:54,000 --> 00:07:55,290
Myself with the rest of the class.

85
00:07:56,010 --> 00:08:03,960
A week or so later, you will turn in a draft of the method section of paper describing how you're going to analyze the problem.

86
00:08:03,990 --> 00:08:10,530
You know what data sources you have? Are you going to use a markup model decision tree, whatever it is taught me to write that out explicitly.

87
00:08:10,530 --> 00:08:16,229
How you're going to analyze the problem that is graded and a model review is ungraded.

88
00:08:16,230 --> 00:08:24,930
That's a meeting with your groups and meet to talk about how you're progressing and I'm here to help you get through any roadblocks you might have.

89
00:08:26,490 --> 00:08:33,560
And then at the very end of the semester, you have your final presentations where you present your final results and a final paper that you write.

90
00:08:33,570 --> 00:08:39,150
You turn that in. Those are all group assignments except for this last one, the bonus allocation.

91
00:08:40,930 --> 00:08:48,760
The bonus allocation is individual thing. You'll turn into me assigning a fictitious $10,000 bonus to your team.

92
00:08:49,030 --> 00:08:55,060
So if you were the one to allocate a $10,000 to your team, how would you allocate it and why?

93
00:08:55,570 --> 00:09:05,320
So I want you to to describe that in this scene, this final message, and that's that's helped to help for me to help me with writing.

94
00:09:06,910 --> 00:09:10,970
And also, it's an opportunity and this is you think about kind of carrots and sticks.

95
00:09:11,230 --> 00:09:19,090
This is can be a characteristic. So if someone has done a great job going above and beyond the call of duty, you can say, hey, Tina did a great job.

96
00:09:19,270 --> 00:09:22,270
She deserves a $5,000 bonus because she was super.

97
00:09:23,860 --> 00:09:27,879
Or you can say Fred was terrible. He never turned in assignments.

98
00:09:27,880 --> 00:09:31,600
He never came in the meetings. And when he did, he had to watch typos.

99
00:09:31,600 --> 00:09:39,370
And, you know, if that's a problem, then you can mention that here and the bonus and say Fred gets a $0 bonus because he was a terrible teammate.

100
00:09:40,090 --> 00:09:43,900
And so that's an I mean, of course, if you're having those issues in the middle of semester, you know,

101
00:09:44,680 --> 00:09:48,670
try to work them out with your team and talk to me and happy to help you work through those things.

102
00:09:48,940 --> 00:09:53,080
But this is also an opportunity for you to give me feedback about how the team is working.

103
00:09:53,320 --> 00:09:57,580
And hopefully this is also kind of a step if there are students who feel like they can

104
00:09:57,580 --> 00:10:02,470
kind of skirt by and hide out in the back and not participate in your group projects.

105
00:10:02,740 --> 00:10:07,150
You know, I will be aware of what's going on, both just in general and then through this.

106
00:10:07,690 --> 00:10:12,040
So please make sure you're putting in a good effort with your teammates.

107
00:10:13,660 --> 00:10:19,500
Any questions? All right.

108
00:10:19,500 --> 00:10:29,159
So let's start start talking about mathematical modeling. So as I mentioned, I think the first day of class, I have this background in engineering.

109
00:10:29,160 --> 00:10:33,959
So I really like using mathematical modeling, I like putting numbers on things.

110
00:10:33,960 --> 00:10:39,240
And so this is, I think, an important approach that's used in public health.

111
00:10:39,240 --> 00:10:43,559
It's mathematical modeling, both for cost effective analysis or other analyzes.

112
00:10:43,560 --> 00:10:49,170
So I think it's important to understand this. And so I want to talk about kind of today and just talk about models in general.

113
00:10:50,280 --> 00:10:55,590
We'll talk about these five axioms just kind of a little bit silly, but and it's so, so the examples.

114
00:10:55,950 --> 00:11:01,469
But in the end, a couple of these kind of silly examples will show you the power that these tools

115
00:11:01,470 --> 00:11:07,200
have to give you insight and clarity in making decisions under uncertainty.

116
00:11:07,200 --> 00:11:15,089
And oftentimes that's really what we're facing all time to these public health and health decisions is a lot of uncertainty and you need clarity.

117
00:11:15,090 --> 00:11:18,360
So talk with these fine maxims and we'll talk a little bit of decision trees today.

118
00:11:18,630 --> 00:11:22,490
We'll do some more decision trees and Markov models next week as well.

119
00:11:22,590 --> 00:11:29,129
So more details of that. So all models are wrong, but some of them are useful.

120
00:11:29,130 --> 00:11:39,600
And so we're using these mathematical models to give us some information, aggregate some information, put it all together to help us make decisions.

121
00:11:39,600 --> 00:11:47,880
And in cost effective analysis, we're specifically looking at costs and health outcomes as well as the outputs that you care about from these models.

122
00:11:49,200 --> 00:11:56,460
I really think it's helpful. It's really nice that these mathematical models have explicit assumptions and value judgments in them.

123
00:11:56,970 --> 00:12:06,900
What do I mean by explicit assumptions and value judgments? Well, explicit assumptions like this vaccine reduces the risk of infection by 82%.

124
00:12:07,680 --> 00:12:14,459
Vaccine effectiveness is 82%. That would be an explicit assumption that you put into the model and it helps you calculate costs and health

125
00:12:14,460 --> 00:12:22,980
outcomes are the explicit value judgments that is putting dollar values and quality of life on things.

126
00:12:23,280 --> 00:12:34,470
So when you say this hospitalization cost of $82,642, that is explicitly valuing that hospitalization and that specific dollar amount.

127
00:12:35,070 --> 00:12:42,180
When you say having advanced liver cancer gives you a quality of life of 0.42,

128
00:12:42,810 --> 00:12:48,690
that's put in a very explicit value judgment on quality of life, living with that specific health condition.

129
00:12:50,600 --> 00:12:52,790
And those are going to be explicit in the motion.

130
00:12:52,910 --> 00:12:58,910
Actually, you might disagree that that cost amount or that quality of life amount should be those numbers.

131
00:12:59,360 --> 00:13:03,470
But at least those numbers are explicit in these models. And actually oftentimes we'll see later on.

132
00:13:03,860 --> 00:13:12,950
You can change those numbers and see how does that change my results here at the end, if I say a hospital version only costs $42,000, some $80,000.

133
00:13:13,370 --> 00:13:15,810
How does that how much does that change our cost?

134
00:13:16,040 --> 00:13:21,590
If I say quality rate for liver cancer is 0.6 and 7.4 for one, how does it change the health outcomes?

135
00:13:21,740 --> 00:13:27,860
And would that change my decision about, you know, vaccinating or treatment A versus treatment B?

136
00:13:28,310 --> 00:13:33,110
How does that affect my decision in the end, if at all?

137
00:13:34,830 --> 00:13:40,200
And those are nice things that we can use models to help us do or help us answer things.

138
00:13:40,860 --> 00:13:45,300
So, you know, why can we even use these models in the first place?

139
00:13:45,330 --> 00:13:53,370
Well, we have this these five rules with the mnemonic pose choice, the five rules.

140
00:13:53,370 --> 00:13:58,800
Here are the probability rule quarter rule, the equivalence rule, the substitution rule and the choice rule.

141
00:13:59,310 --> 00:14:06,150
Now, these are rules. You don't have to agree with the rules, but if you do agree that these are reasonable rules,

142
00:14:07,200 --> 00:14:11,100
then these mathematical models, decision trees in particular,

143
00:14:11,400 --> 00:14:17,040
can help you solve problems and can help give you insight, help you answer questions,

144
00:14:17,040 --> 00:14:21,750
giving insight into what is your best action to take in the face of uncertainty.

145
00:14:22,800 --> 00:14:33,270
So if these if you think these rules are reasonable, then decision trees can help you answer questions with clarity in the face of uncertainty.

146
00:14:33,990 --> 00:14:39,900
It helps you put together the information and preferences that you have to help you get clarity about what the right decisions.

147
00:14:40,470 --> 00:14:44,130
If you if you want to follow these five rules, you don't have to follow these five rules.

148
00:14:44,340 --> 00:14:48,420
But if you do follow these five rules, then decision trees can help give you that kind of clarity.

149
00:14:48,700 --> 00:14:49,950
So let's walk through what those rules are.

150
00:14:52,000 --> 00:14:58,120
The first rule is the probability rule that says you can assign probabilities to quantify uncertainty in outcomes.

151
00:14:58,120 --> 00:15:05,100
So you can say there's a 30% chance of rainfall. Any questions about this probability?

152
00:15:07,650 --> 00:15:16,060
Against what? 30% chance of rain, like against the historical data or so?

153
00:15:16,070 --> 00:15:21,100
That's a great question. So this probability rule, it might be informed by data.

154
00:15:22,710 --> 00:15:26,670
But there is not there's not really a right answer to what is the right probability.

155
00:15:26,670 --> 00:15:30,970
The right probability is the probability that you, the decision maker, think is reasonable.

156
00:15:31,470 --> 00:15:35,550
Now, you could that could be informed by experts. So, hey, I don't know what the chance of rain is tomorrow.

157
00:15:35,790 --> 00:15:37,440
Let me check some sort of weather app.

158
00:15:37,620 --> 00:15:45,840
And I trust that the meteorologists who said the information and the models that they used to create those estimates are reasonable and good.

159
00:15:46,080 --> 00:15:50,160
And so I think there's only, you know, so let's check here.

160
00:15:50,460 --> 00:15:54,540
You know, weather here is telling me it's not going to rain today or tomorrow.

161
00:15:55,380 --> 00:16:00,210
You know what? I could check and maybe it say is like, so maybe I'll say 2% chance of rain or something tomorrow.

162
00:16:00,360 --> 00:16:08,460
So I think it's really low given all these forecasts are saying if I could look at pictures of clouds and stuff, but that is that is my own belief.

163
00:16:08,670 --> 00:16:13,110
The probability this is a this is a probability in this framework is a degree of belief.

164
00:16:13,440 --> 00:16:21,360
There's no right or wrong answer that we roll dice 18,000 times and found it had be heads, you know X number of times.

165
00:16:21,570 --> 00:16:25,320
This is a degree of belief and it can be about really anything.

166
00:16:25,320 --> 00:16:28,860
It doesn't have to be about coin flips and and rolls of dice.

167
00:16:29,040 --> 00:16:32,960
It can be about the probability of arrangements. Okay.

168
00:16:33,420 --> 00:16:37,160
So we can assign probabilities to to anything. It could be probably rain tomorrow.

169
00:16:37,170 --> 00:16:40,490
Could be probably someone will die who has liver cancer.

170
00:16:40,500 --> 00:16:45,630
It could be the probability that I get into a car accident in the next 24 hours.

171
00:16:47,400 --> 00:16:50,640
It could be for any kind of event.

172
00:16:52,200 --> 00:16:55,529
And so the many rules as we can assign probabilities to those outcomes and they

173
00:16:55,530 --> 00:17:02,790
might be really small like probably I'll be in an accident the next 24 hours. Maybe that's, you know, one in 20,000 or 100,000 or something like that.

174
00:17:04,080 --> 00:17:12,070
But that there is a probability that that might happen to me. Other actually know the other.

175
00:17:12,520 --> 00:17:15,640
But this this can be a controversial rule for that.

176
00:17:16,330 --> 00:17:22,930
For some people, they say just I can't. They have difficulty putting your numbers and covering something.

177
00:17:23,170 --> 00:17:26,979
So that if that's a challenge, then then using decision trees can be a challenge.

178
00:17:26,980 --> 00:17:33,310
But I guess I would argue often that we actually we can put probabilities on things again.

179
00:17:33,550 --> 00:17:39,520
They might be really very small or really, really large abilities and they might be difficult to make precise,

180
00:17:39,520 --> 00:17:43,240
but you can put probabilities and chances of things happening.

181
00:17:46,720 --> 00:17:48,250
So the next one is the order rule,

182
00:17:49,000 --> 00:17:57,850
and that says that we can order our preferred outcomes in terms of our preferences so we can define an outcome precisely enough or different outcomes,

183
00:17:57,850 --> 00:18:06,410
precisely enough that we can order them. So would you prefer to have Donald Trump as president or Joe Biden as president?

184
00:18:06,860 --> 00:18:09,830
You can order those in preferences about who you who is better.

185
00:18:10,430 --> 00:18:15,570
And then you could add Hillary Clinton and you could order those three and you would have a strict ordering of preferences.

186
00:18:15,710 --> 00:18:21,470
Okay. Hillary would be better than Biden would be better than Trump or Trump would be better than Biden or better than him.

187
00:18:21,950 --> 00:18:30,320
You know, whenever you can order those things in a strict order, the thing is, those order, those preferences can't be cyclical.

188
00:18:30,620 --> 00:18:38,780
So you can't say, I prefer Hillary to Joe Biden to Trump, but I prefer Trump to Hillary Clinton and have kind of a cycle of these preferences.

189
00:18:39,110 --> 00:18:45,739
We have to be able to strictly order all these preferences in order. And again, in the video, you show me, watch chocolate ice cream.

190
00:18:45,740 --> 00:18:49,160
So I could say I prefer chocolate to vanilla strawberry ice cream or something like that.

191
00:18:49,160 --> 00:18:52,790
So it can be for political candidates, it can be for things and for money.

192
00:18:52,790 --> 00:18:57,110
You can say I prefer to have $1,000,000 versus $900,000 versus $1.

193
00:18:57,230 --> 00:19:01,400
And and typically cost factors were in assume would prefer more money to less.

194
00:19:04,260 --> 00:19:08,300
And more body just like you. But.

195
00:19:08,430 --> 00:19:16,080
But it wouldn't necessarily have to be the. So we we can order outcomes and these outcomes could be.

196
00:19:17,160 --> 00:19:22,650
I prefer living in perfect health to living with diabetes, to living with liver cancer.

197
00:19:24,660 --> 00:19:31,530
Or you could say I prefer living 100 years with diabetes to living one year in perfect health.

198
00:19:32,940 --> 00:19:37,470
So you could you know, these could be very more complex combinations of what is and what is be worse.

199
00:19:40,540 --> 00:19:44,680
Any any questions about the approach or sorry, the order. Okay.

200
00:19:45,130 --> 00:19:49,270
So hopefully these rules seem reasonable here. I can assign probabilities to things.

201
00:19:49,270 --> 00:19:51,280
I can order my preferences about what I like.

202
00:19:52,510 --> 00:20:01,149
The equivalence rule then kind of combines sort of thinking about these things and, and it says if you prefer A to B to C.

203
00:20:01,150 --> 00:20:04,840
So we have to if you believe that the order rule is reasonable.

204
00:20:05,350 --> 00:20:08,500
Now we've ordered things, I prefer A to B to C.

205
00:20:09,760 --> 00:20:12,700
So A is the best, C is the worst, and B is somewhere in the middle.

206
00:20:13,600 --> 00:20:18,640
The equivalence rule says we can find a probability p where you're indifferent between.

207
00:20:20,110 --> 00:20:27,130
Receiving A or C with some sort of probability, probability P of a one minus P of getting C,

208
00:20:28,210 --> 00:20:34,210
and this uncertain deal is equivalent to that thing in the middle B.

209
00:20:37,790 --> 00:20:41,179
So B is equivalent to this and I can it says I can find some sort of probability.

210
00:20:41,180 --> 00:20:51,050
P So again, let's say, let's say I really love Hillary Clinton and Hillary Clinton is better than Joe Biden is better than Donald Trump.

211
00:20:52,310 --> 00:20:57,690
Let's say that's what I say. I prefer says I can find a probability here of.

212
00:20:58,160 --> 00:21:07,400
So let's say there's an uncertain chance that we'll get Joe Biden or Donald Trump or I could add or sorry, making this up here.

213
00:21:08,150 --> 00:21:13,670
Hillary was the best here. So I said Hillary Clinton and Joe Biden.

214
00:21:14,750 --> 00:21:17,990
So if I prefer Hillary to Joe to Donald,

215
00:21:19,100 --> 00:21:26,879
I can find some sort of probability P here and one minus me here where this uncertain outcome is equivalent to Joe Biden.

216
00:21:26,880 --> 00:21:30,200
Now, let's say let's say I really like Hillary Clinton.

217
00:21:30,770 --> 00:21:34,579
I kind of like Joe Biden and I really hate Donald Trump. There are probably a few of you in this country.

218
00:21:34,580 --> 00:21:35,540
Would they feel that way?

219
00:21:37,340 --> 00:21:43,850
But it says here, if you really do prefer Hillary to Joe, you would be willing to to have this deal with some sort of probability.

220
00:21:43,850 --> 00:21:49,819
P You'd be equivalent in different you having Joe Biden for sure where this uncertain

221
00:21:49,820 --> 00:21:54,980
outcome here now if you really love Hillary Clinton and you're a really also like Joe Biden,

222
00:21:55,190 --> 00:21:56,510
you really hate Donald Trump.

223
00:21:56,810 --> 00:22:03,469
This PE would have to be really big, might have to be a 99% chance or something or 98 point no, no, no, no, no, not in chance or whatever it is.

224
00:22:03,470 --> 00:22:12,830
If you really, really hate Donald Trump. Right. But you could find some sort of probability here where you would be indifferent between Joe Biden,

225
00:22:12,830 --> 00:22:15,950
for sure, and this uncertain outcome of Hillary Clinton versus Donald Trump.

226
00:22:17,800 --> 00:22:22,030
Okay. So that's what the equivalence rule is.

227
00:22:22,030 --> 00:22:24,579
And you could think of all of this as as political candidates.

228
00:22:24,580 --> 00:22:31,210
You could also think about this as, you know, your chocolate ice cream cone and try this terribly and in your.

229
00:22:32,940 --> 00:22:40,259
And your chocolate ice cream cone gets brown and then your vanilla ice cream cone cones,

230
00:22:40,260 --> 00:22:44,250
and then you're maybe some sort of orange flavored ice cream cone.

231
00:22:44,850 --> 00:22:52,470
You know, you can think about I'm happy with vanilla and some sort of, you know, where I randomly get chocolate or orange ice cream.

232
00:22:52,680 --> 00:23:00,630
But you can also think about this in terms of dollars and health outcomes. So let's say you prefer $1,000,000 to $100000 to $0.

233
00:23:00,900 --> 00:23:09,240
We could find some sort of deal where you'd say, I'm indifferent between a deal where I might get $1,000,000 or zero versus having $1,000 for sure.

234
00:23:11,040 --> 00:23:14,760
We could find some sort of holiday party where these deals are equivalent.

235
00:23:16,640 --> 00:23:21,770
Cross for health states. So you can say, what if I had perfect health or maybe death?

236
00:23:22,490 --> 00:23:28,640
And then something in the middle is living with diabetes. And so you say, I need to live with diabetes for sure.

237
00:23:29,780 --> 00:23:37,639
Or maybe there's an uncertain deal where I could have perfect health or be dead and there might be some sort of probability.

238
00:23:37,640 --> 00:23:42,680
P where you'd be indifferent between the uncertain deal and the certain health, this other sort.

239
00:23:46,020 --> 00:23:49,790
Actually face this in real life. I mean, people do.

240
00:23:50,030 --> 00:23:54,680
They might live with some health condition that's less than perfect health.

241
00:23:55,370 --> 00:24:02,660
And they actually do maybe take a surgery to be free of that bad health outcome.

242
00:24:02,810 --> 00:24:09,480
But they do have a small risk of dying in the during the surgery. So there are you know, people oftentimes will do an uncertain, you know,

243
00:24:09,530 --> 00:24:15,559
take that uncertainty over in comparison to a certain health state that is that is worse where

244
00:24:15,560 --> 00:24:19,640
that uncertain outcome is either perfect health or death or something better or something worse.

245
00:24:21,880 --> 00:24:27,640
Anyway. The comments are also as we can find a probability p were indifferent between the better the lottery,

246
00:24:27,640 --> 00:24:33,290
between the better and worse outcomes, and then something in the middle for sure. Any questions about this equivalence rule?

247
00:24:38,750 --> 00:24:41,830
So again, you know, I've been a lot maybe, as we can say,

248
00:24:41,850 --> 00:24:48,430
been on for sure is we can find a public we're not sure is equivalent to a deal where you might get chocolate or might get strawberry.

249
00:24:50,960 --> 00:24:53,240
The substitution rule is that do you really mean it rule?

250
00:24:54,320 --> 00:25:00,140
So if you read if you said that B is equivalent to probably P of A and when I speak of getting C,

251
00:25:00,500 --> 00:25:07,670
then B really is equivalent to game on a C and if you actually faced B,

252
00:25:08,060 --> 00:25:12,800
you really would be willing to substitute this uncertain outcome for B instead of B for sure.

253
00:25:15,750 --> 00:25:18,860
That's a substation. It's really meaningful.

254
00:25:19,040 --> 00:25:30,890
And then the final rule is the choice rule says if you add equivalent to deals that just have interest are binary outcomes either any or C.

255
00:25:31,550 --> 00:25:36,650
So it's not a really confusing tree, it's just two potential outcomes. IAC and IAC are equivalent here.

256
00:25:37,100 --> 00:25:41,180
So you have two deals, A versus C here and A versus C here.

257
00:25:41,540 --> 00:25:52,580
You prefer a, C, and if P if you have better chance of getting the thing you prefer the most here P is greater than Q and you prefer a deal.

258
00:25:53,060 --> 00:25:56,840
This deal to this deal. So let's say is $1,000,000.

259
00:25:56,960 --> 00:26:02,480
See a $0. You'd prefer the one that has a better chance of getting you the million dollars.

260
00:26:03,200 --> 00:26:09,620
You think about this as life and death. Life and death. If this P is greater than Q, you prefer this deal here and this.

261
00:26:10,580 --> 00:26:18,770
So those are the five rules probability rule order, rule equivalents, rule substitution rule and choice rule.

262
00:26:19,580 --> 00:26:29,479
And if you think those are reasonable rules, then we can actually kind of and decision trees are a tool that can help us give clarity

263
00:26:29,480 --> 00:26:33,260
into making the right decisions that are in line with our information and preferences.

264
00:26:34,370 --> 00:26:39,440
Any questions about these rules? Yeah. So this can all make sense logically.

265
00:26:39,540 --> 00:26:46,669
I'm just thinking if you're doing this practically, this is the kind of first step to just assign kind of probabilities to the choices you're facing,

266
00:26:46,670 --> 00:26:51,280
because that's not always so straightforward, obviously, when you're comparing apples.

267
00:26:51,710 --> 00:26:54,970
Yeah. So see here, if I cannot.

268
00:27:00,740 --> 00:27:05,090
I guess the first. So let me know if we hear that.

269
00:27:14,530 --> 00:27:20,530
The first thing I would do is I'd kind of try to structure what is my decision problem I'm facing.

270
00:27:20,860 --> 00:27:25,420
So and an injury, I think can be useful to to think about that.

271
00:27:25,660 --> 00:27:31,330
So I think about. Well, if I. Well, first of all, what's my decision and what are different outcomes?

272
00:27:31,660 --> 00:27:40,060
My sort of choices I have. But then if I make a certain choice, what are the the branches of the outcomes that might happen?

273
00:27:41,360 --> 00:27:47,620
Kind of the first step is kind of structuring what might potentially happen and things that I the things that I'm uncertain about.

274
00:27:47,800 --> 00:27:54,730
Actions I can take. And then I think about what are those probabilities of those things happening?

275
00:27:55,150 --> 00:27:58,180
And you might have to spend some time gathering information about that.

276
00:27:58,180 --> 00:28:01,390
What are those things? And then at the end.

277
00:28:04,840 --> 00:28:10,389
What are those outcomes? And then how do I value them? How do I value them versus each other?

278
00:28:10,390 --> 00:28:15,010
So how do I think about A, B versus C versus D? Which ones do I prefer?

279
00:28:15,250 --> 00:28:22,810
So you see, you think about one of those probabilities in the tree and then how would I rank those things?

280
00:28:26,150 --> 00:28:35,310
And so then let's say this is E and F, let's say this is our decision tree here, A, B, C, D and E and F.

281
00:28:38,280 --> 00:28:45,810
In the end with this one, if we can figure out probabilities here and rank the outcomes,

282
00:28:47,610 --> 00:29:01,050
we should be able to using these rules, turn this complex decision here between X and Y into a simple decision.

283
00:29:03,380 --> 00:29:13,700
It looks like this and what we can do is if we can figure out so ABCD if.

284
00:29:16,180 --> 00:29:29,590
Let's say here he is better than a is better than be better than C, better than D, better than F.

285
00:29:30,130 --> 00:29:34,210
So he's the best. F is the worst. And these things are all kind of in between.

286
00:29:37,590 --> 00:29:43,860
We should be able to turn this tree converted into an equivalent E versus house.

287
00:29:45,780 --> 00:29:57,960
And how do we do that? Well, hey, we have to find out at what point is a, we can compare that using the commons rule.

288
00:29:58,680 --> 00:30:04,920
How is equivalent to E versus F? Well, I have to find I'd have to, you know, carefully think this through,

289
00:30:05,100 --> 00:30:13,980
but figure out what is this probability that a is a committed this E versus F uncertain outcome.

290
00:30:15,870 --> 00:30:21,930
The substitution rule then says, if I really find out that A is equivalent to E versus at some sort of probability X,

291
00:30:22,860 --> 00:30:33,089
then I can erase your race and the tree here and just replace this with E versus F and then I can do the same for B E versus F.

292
00:30:33,090 --> 00:30:36,330
I can replace that with some other different probability.

293
00:30:38,490 --> 00:30:46,980
And I say this next one, which is x2. So I can replace C, D and E, all these things with E versus F.

294
00:30:52,270 --> 00:30:56,829
And then I can just use simple probability calculations or times x one plus one.

295
00:30:56,830 --> 00:31:00,160
I started x two and we can roll this tree back.

296
00:31:00,170 --> 00:31:03,310
We'll talk about that later, probably next week.

297
00:31:03,700 --> 00:31:08,680
And in turn, this whole complicated branch into a simple versus F decision.

298
00:31:08,980 --> 00:31:16,240
And then the question then is just so this is this is P one mrp2 I calculate and then I can just say,

299
00:31:16,240 --> 00:31:20,139
okay, which, which one has a better chance of E, which is what I prefer.

300
00:31:20,140 --> 00:31:24,340
So is P2 greater than P1? If so, this is my best choice of action.

301
00:31:25,000 --> 00:31:29,080
If P2 or P1 is greater than this is my best action here.

302
00:31:31,150 --> 00:31:41,230
So if you think these rules are reasonable, then you can use decision trees to help give you clarity of which action is best.

303
00:31:41,350 --> 00:31:46,680
Even in the face of kind of a nasty, complicated decision tree here, it's you know,

304
00:31:46,750 --> 00:31:49,840
you have this nasty, complicated decision tree, a lot of confusing things.

305
00:31:51,340 --> 00:31:53,320
There's a lot of complexity there.

306
00:31:53,710 --> 00:32:01,000
But if I can systematically go through this process of assigning probabilities, weighing my preferences about which things are,

307
00:32:01,330 --> 00:32:09,820
what's my ranking order, how do these relate to each other with the equivalence rule that I can solve the decision tree.

308
00:32:10,840 --> 00:32:16,630
Now, in this class, we're not going to do too much monkeying around about what the court's rules and so forth.

309
00:32:16,640 --> 00:32:26,680
We're going to make some other additional assumptions about dollars that we're just going to multiply probabilities and so forth,

310
00:32:27,160 --> 00:32:31,389
probably subsequent adjusted life years. But in essence, though,

311
00:32:31,390 --> 00:32:41,110
these five rules are the ones underpinning why decision trees can help you get clarity in making your decisions in the face of complex uncertainty.

312
00:32:46,670 --> 00:32:49,820
Other questions. It's three questions.

313
00:32:53,080 --> 00:32:56,319
Okay. So decision trees.

314
00:32:56,320 --> 00:32:58,780
And actually, before we get into this basic example,

315
00:32:59,020 --> 00:33:13,450
I just want to quickly walk through or see if any raised questions about this kind of party problem video that you guys watched for for today.

316
00:33:15,340 --> 00:33:20,500
Anyone had any questions as they were looking through this this party problem example?

317
00:33:22,870 --> 00:33:28,749
Remember what this party problem was about? Inside or outside their party.

318
00:33:28,750 --> 00:33:32,739
Inside or outside. Exactly. So the idea is you have it's kind of a simple question.

319
00:33:32,740 --> 00:33:37,210
You have a party inside or outside. We're uncertain about rain or sunshine.

320
00:33:37,660 --> 00:33:43,690
And so we have four potential outcomes. Right. If we had the party outside, it might be rainy or or sunny.

321
00:33:44,200 --> 00:33:46,450
And so that's those are couple possibilities.

322
00:33:46,450 --> 00:33:53,409
And that and that if we had a party inside, that party inside to have be sunny, you've got party inside it.

323
00:33:53,410 --> 00:33:57,830
Be rainy. And so we could describe our decision.

324
00:33:57,860 --> 00:34:00,690
The first thing here is kind of structuring the decision tree when it looks like.

325
00:34:00,700 --> 00:34:09,740
So first I make the decision about the party and then I observe the rain or sun and that I might have some information about what's my chance of rain.

326
00:34:10,100 --> 00:34:15,860
I spent a lot of time gathering that information. Then I can rank things, you know, maybe outdoor party.

327
00:34:15,860 --> 00:34:20,750
The sun is the best outdoor party. The rain is the worst. So I could rank those different outcomes.

328
00:34:21,410 --> 00:34:25,280
By the way, in this simple example, at an indoor party when it's raining outside,

329
00:34:25,280 --> 00:34:29,140
might be preferred to an indoor party with a sunny outside or something. That's okay.

330
00:34:29,150 --> 00:34:41,090
These are just your preferences. Then the equivalence rule helps us make those parties comparable in this rank order with these probabilities.

331
00:34:41,090 --> 00:34:55,160
So one might say, you know, party rain might be equivalent to 80% chance of outdoor party in the sun and 20% chance of an outdoor party the rain.

332
00:34:55,640 --> 00:35:00,910
So we're making things equivalent. And this is this is a this is probability here, 80%.

333
00:35:01,340 --> 00:35:04,579
That is a preference of whoever the decision maker is.

334
00:35:04,580 --> 00:35:10,840
There's no no one is actually spinning a wheel and then magically snapping their fingers and going after party in the rain.

335
00:35:10,850 --> 00:35:14,509
And so you can you can use that as a thought experiment about some sort of

336
00:35:14,510 --> 00:35:18,020
wizard who might spin wheel and give you the after party in the sun and rain.

337
00:35:18,200 --> 00:35:24,770
But that's not what's actually happening. But this 80%, that's your preference.

338
00:35:24,980 --> 00:35:30,620
It's your beliefs and how you how strongly you feel about parties in the sun versus

339
00:35:30,620 --> 00:35:36,200
parties in the rain versus outdoor parties in some rain versus in your party in the rain.

340
00:35:37,310 --> 00:35:41,120
So we're making these things equivalent kind of quantitatively with that probability.

341
00:35:42,320 --> 00:35:49,070
And so then we can kind of earlier we had these things ranked in terms of just a simple ranking.

342
00:35:49,340 --> 00:35:56,120
And then once we figure out those probabilities between the best and worst outcomes, we can also almost call that preference probability.

343
00:35:56,450 --> 00:36:02,930
So a party outside in the sun is equivalent to 100% chance of a party in the sun.

344
00:36:04,190 --> 00:36:09,890
Either party rain is equal to 0% chance of sun, or conversely, 100% chance of an after party thing.

345
00:36:10,490 --> 00:36:16,930
And then these ones here are preference probabilities for indoor parties in the rain in the sun later on.

346
00:36:16,970 --> 00:36:21,470
Keep this in mind in a few weeks when we start talking about quality of life,

347
00:36:21,860 --> 00:36:31,459
because we're going to be using kind of similar questions to kind of think about what is your how do you rank and think precisely

348
00:36:31,460 --> 00:36:37,850
about how you would value quality of life with various health conditions of the using this later on things like health conditions.

349
00:36:37,850 --> 00:36:43,190
But in this simple example, it's about how do I rank or think about preferences for different parties?

350
00:36:44,330 --> 00:36:48,860
And then we can put this in our decision tree. We can substitute things.

351
00:36:49,210 --> 00:36:58,130
So like an indoor party in the rain. We can substitute that with our outdoor parties.

352
00:37:01,960 --> 00:37:06,700
And turn this branch in the tree instead of indoor parties,

353
00:37:06,700 --> 00:37:14,110
we're bringing it equivalent to when we substituted Caucasians equivalent to outdoor parties in the sun and rain.

354
00:37:14,290 --> 00:37:19,950
And then we can say, Well, this outdoor party gives us a 60% chance of an outdoor party and sun.

355
00:37:20,500 --> 00:37:28,300
And this indoor party gives us something that is equivalent to a 62% chance of an outdoor party in the sun.

356
00:37:30,100 --> 00:37:37,810
And a 38% chance of an after party the rain. So now it gives us clarity that are in line with our values and information.

357
00:37:38,710 --> 00:37:48,860
The indoor party is preferred to the outdoor party. Now, if you changed your values and preferences or information, then the results might change.

358
00:37:48,860 --> 00:37:56,810
So if you said, Oops, it's not a 40% chance of rain, it's a 60% chance of rain or something that might change your.

359
00:37:57,980 --> 00:38:03,440
Your rankings here are 10% chance of rain. If you change your preference probabilities, you're.

360
00:38:06,710 --> 00:38:09,800
You said actually giving your party of rain isn't so great.

361
00:38:10,010 --> 00:38:15,230
If you change these numbers, that also might change your your your ratings as well.

362
00:38:16,070 --> 00:38:25,310
But given the information you had, once you plug it in and this isn't trade, it gives you clarity that this is your preferred choice.

363
00:38:26,360 --> 00:38:28,250
And so in this simple example here,

364
00:38:28,850 --> 00:38:36,020
we're walking through something kind of silly about someone choosing a party in the outdoors or indoors and just thinking about the rain or sunshine.

365
00:38:36,620 --> 00:38:45,440
But in the end, will use these tools, these decision tree tools to help us answer much more complex and policy questions about,

366
00:38:45,860 --> 00:38:51,200
hey, should we do bike lanes or should we tree with cancer, tree trimming acts and training kids?

367
00:38:51,560 --> 00:38:54,980
Why and how do we think about COVID vaccinations?

368
00:38:55,110 --> 00:38:58,579
When I did that injection, my arm, my arm is sore. That is a bad thing.

369
00:38:58,580 --> 00:39:05,510
I don't like that. But how to weigh that with and actually maybe I have a chance of getting myocarditis and that's bad also.

370
00:39:05,510 --> 00:39:14,500
I don't like that but but also have the chance of not getting coronavirus and adverse events from that that outcomes.

371
00:39:14,510 --> 00:39:21,920
You know, how do I weigh all those things together? Well, I can think about the probabilities of those things happening,

372
00:39:22,160 --> 00:39:25,459
and I can think about my preferences for how do I feel about soreness in my

373
00:39:25,460 --> 00:39:29,570
arm versus long COVID or ending up in the ICU on a ventilator or something.

374
00:39:30,620 --> 00:39:32,480
So those are all things that I need to weigh.

375
00:39:32,660 --> 00:39:42,230
And these decision tree tools help us put that all together and give us clarity about what is the right decision in the face of all that.

376
00:39:48,910 --> 00:39:54,500
Any other questions? Okay.

377
00:39:54,570 --> 00:39:57,590
So let's walk through something that's a little more closely related to health care.

378
00:39:57,600 --> 00:40:04,560
So let's talk about Lasik eye surgery and full disclosure.

379
00:40:04,650 --> 00:40:09,510
About 15 years ago, I had Lasik surgery. I used to wear contacts and busses.

380
00:40:09,510 --> 00:40:12,549
That was fine, but I thought it'd be nice not to have to deal with that.

381
00:40:12,550 --> 00:40:15,900
And later, a surgery. Day after surgery, I looked at the window.

382
00:40:15,900 --> 00:40:24,900
Everything was really clear. It's really cool. But there was a risk when I did that that maybe, you know, I would go blind or something like that.

383
00:40:24,900 --> 00:40:27,240
That's, you know, very, very rare outcome.

384
00:40:27,240 --> 00:40:33,930
But that's a bad outcome that you need to think about when think about, hey, better vision versus worse vision versus maybe blindness or something.

385
00:40:34,530 --> 00:40:39,420
So I needed, you know, consider those things when you're making that kind of decision.

386
00:40:41,170 --> 00:40:47,350
So your decision might look something like this. I have no surgery, and I'm sick with my current vision.

387
00:40:47,740 --> 00:40:52,660
I might have the surgery, and there could be some complications with the surgery or no complications.

388
00:40:53,020 --> 00:40:58,840
And then in the end, you could think about what your your vision might be and let's say with no complications.

389
00:40:59,740 --> 00:41:02,800
You either write that 2020 vision or you have a different vision.

390
00:41:03,160 --> 00:41:07,720
And if you could complications, they might be minor. Or maybe you totally lose all of your vision or something like that.

391
00:41:08,170 --> 00:41:13,870
Now, the first no, this is kind of a simple this entry. There might be more branches you might put in here, right?

392
00:41:14,090 --> 00:41:17,680
Maybe you maybe your 2020 vision. You have 2030 vision.

393
00:41:17,920 --> 00:41:23,270
2040 vision. You mean you could you could make this more complex, but for today's possible, which make it kind of simpler.

394
00:41:24,760 --> 00:41:30,640
So this is taking into account a few things here complications and and the end vision outcomes with laser kaiser.

395
00:41:31,630 --> 00:41:40,510
So how does the probability rule fit in here? So just again, the first thing here is structuring the tree, though.

396
00:41:40,540 --> 00:41:45,549
Keep in mind that that's also important that I'm kind of glossing over a little bit, you know, what are the things that I'm including in this for?

397
00:41:45,550 --> 00:41:50,410
What are the things I'm not including in this tree. Yeah.

398
00:41:50,710 --> 00:41:53,930
Did you have are you are you asking for an answer to.

399
00:41:54,110 --> 00:41:59,810
Yes. Yes. Okay. So you have the probability of you doing the surgery's surgery, right?

400
00:41:59,830 --> 00:42:02,350
Or is that just a so that's a great question.

401
00:42:02,650 --> 00:42:10,479
So in this case, this square here, the decision that there's no probability that I'm going to do that I will either do this or this.

402
00:42:10,480 --> 00:42:13,920
That's not really a probability. Just a wire now and then the surgery.

403
00:42:13,930 --> 00:42:18,579
Then you have the complications and no complication P and then one minus P,

404
00:42:18,580 --> 00:42:29,559
and then that goes into the 2020 vision versus current vision, which would be minus or.

405
00:42:29,560 --> 00:42:34,120
Q Yeah, if you're going to denote it differently or minus. Q And then you just do the same thing.

406
00:42:35,380 --> 00:42:38,500
Complications and no complications with the different letter of the alphabet.

407
00:42:38,740 --> 00:42:45,120
Yes, you may have to teach you an art, but yeah, you have abilities at each branch in the tree.

408
00:42:45,130 --> 00:42:46,860
Here, we're going to have some sort of problems.

409
00:42:46,930 --> 00:42:54,280
And all the trees you were trying to today here we just every every uncertainty node has just a single kind of binary branch.

410
00:42:54,430 --> 00:42:59,970
You can have more than that. You could have three things or four things or 17 things branching off into poverty.

411
00:43:01,180 --> 00:43:04,240
Sorry, the uncertainty node here, chance node.

412
00:43:05,620 --> 00:43:14,830
But keep in mind that anytime you have multiple branches here, this is equivalent to breaking up the tree into something like this.

413
00:43:14,920 --> 00:43:21,700
You guys do that if you want to. Can make something with 17 branches into a bunch of different binary trees as well.

414
00:43:23,650 --> 00:43:27,670
So it's, you know, most of the time it doesn't make a huge difference with where you go.

415
00:43:28,750 --> 00:43:33,819
But this is kind of simpler here. So, yeah, we have probably at every uncertainty or chance node here.

416
00:43:33,820 --> 00:43:37,030
We have probabilities we need to need to define.

417
00:43:38,620 --> 00:43:42,729
Okay. So but the probably rule says that we can assign probabilities here.

418
00:43:42,730 --> 00:43:53,139
So I can go look in the medical literature and see in the last five years, you know, 3% of surgeries have complications and and so forth.

419
00:43:53,140 --> 00:43:59,690
So I can look at that. I can also talk to my roommate who got the surgery a year ago and say, hey, how did you feel about it?

420
00:43:59,690 --> 00:44:02,710
And what do you think the risks were? You know, how satisfied are you?

421
00:44:02,890 --> 00:44:07,900
So it doesn't I mean, I can use the medical literature. I can use all of those kind of information at my disposal.

422
00:44:08,950 --> 00:44:15,130
You know, I can say, hey, I want to choose Doctor X versus Dr. Y because doctor access to the surgery 5000 times.

423
00:44:15,140 --> 00:44:20,469
Dr. Y, I'm not sure about him. So and that might also affect my beliefs about what these properties are.

424
00:44:20,470 --> 00:44:29,180
These properties are probably informed by evidence, hopefully, but also they are a degree of belief and influenced by what we think about them.

425
00:44:29,200 --> 00:44:32,150
So again, going back to your project, you might find, you know,

426
00:44:32,170 --> 00:44:38,050
some sort of study of bike lanes that say bike lanes increase cardiovascular activity by 36%.

427
00:44:39,280 --> 00:44:47,169
But then you read the study, you say, well, this was done in a series of, you know, in Memphis and San Diego and Washington, DC.

428
00:44:47,170 --> 00:44:48,670
They study this and all these outcomes.

429
00:44:48,670 --> 00:44:58,360
And you might say, well, but I think here in Michigan, because of the cold weather, the bike lanes can't be used as often as they can in Memphis.

430
00:44:58,780 --> 00:45:04,419
So maybe you will only get 80% of the benefit because the bike lanes are snowed over in December.

431
00:45:04,420 --> 00:45:14,049
And so you can add your own judgment to things even, you know, look at the evidence, of course, and use that best of your ability.

432
00:45:14,050 --> 00:45:20,330
But in the end, these properties are our beliefs informed by judge probability.

433
00:45:21,880 --> 00:45:24,880
Next is the order rule. How would the order rule fit in here?

434
00:45:33,790 --> 00:45:42,670
Sing for the past. Would it be the 2020 version and then would be the current situation?

435
00:45:44,830 --> 00:45:49,600
Then it's a minor forest resort subdivision.

436
00:45:52,590 --> 00:45:55,920
Yeah. So the best one was probably 2020 vision, the worst, probably loss of vision.

437
00:45:56,160 --> 00:46:02,930
And then here current vision minor, you know, those ones are probably somewhere in the middle.

438
00:46:02,940 --> 00:46:26,980
Yeah, this is probably second best. So order will just as I can order these outcomes in terms of how I feel about them.

439
00:46:27,240 --> 00:46:32,280
Actually, I would actually say, actually, this is probably this is probably number three.

440
00:46:32,670 --> 00:46:36,510
This is probably number four. This is probably number two.

441
00:46:39,360 --> 00:46:42,329
Just because I didn't have to. Here, I got my commission.

442
00:46:42,330 --> 00:46:49,140
I didn't have to go to the doctor, didn't have to spend an hour in the waiting room and our get my eyes sliced open and whatever.

443
00:46:50,820 --> 00:46:55,530
And I didn't have to pay $3,000 or whatever. So this outcome probably better than this one.

444
00:46:55,860 --> 00:46:58,890
Anyway, you can. The idea here is that we can rank these outcomes.

445
00:47:02,040 --> 00:47:05,250
Then what? How does the equivalence rule fit in here?

446
00:47:16,780 --> 00:47:22,660
Yeah. You could assign probabilities to each inch, and at some point they'll be equivalent to you.

447
00:47:23,020 --> 00:47:27,630
So, like, if there's a high chance of 2020 vision and low chances of complications and.

448
00:47:29,570 --> 00:47:32,060
Yeah. So this one was the best. And we said, this is the worst.

449
00:47:33,840 --> 00:47:40,110
And so let's say current vision, it says, I can find something, I can find a probability here.

450
00:47:40,170 --> 00:47:45,060
P I'm not sure what it is yet, but I can find a. P That's it.

451
00:47:45,630 --> 00:47:51,690
So that current vision is equivalent to some sort of chance of the best versus the worst.

452
00:47:53,050 --> 00:47:57,550
So I will pick on someone that way.

453
00:47:57,580 --> 00:48:03,130
You're wearing glasses. So think about your life with your current vision and ignoring glasses or contacts.

454
00:48:04,180 --> 00:48:07,570
What if I said I can snap? I can give you a procedure. I snap my fingers.

455
00:48:07,720 --> 00:48:14,410
All of a listen. You have perfect vision. 100% chance and 0% chance of any loss of vision.

456
00:48:15,070 --> 00:48:18,550
Which would you. Would you prefer to live with your glasses or have. Perfect.

457
00:48:22,090 --> 00:48:34,960
Perfect vision. Okay. Now, what if it was 99.999999999% chance of perfect vision and 0.0000001% chance of loss of vision.

458
00:48:35,740 --> 00:48:42,940
A still a 2020 vision. Now, now you'd prefer that even though with that small chance of philosophizing, you still prefer that procedure.

459
00:48:43,580 --> 00:48:48,220
Yeah. And so. And then. But what if it was 50% chance of perfect vision, 50% of blindness?

460
00:48:48,550 --> 00:48:54,220
Okay, so, so there's some but somewhere in between that, you know, 99.99% chance, a 50% chance.

461
00:48:54,460 --> 00:49:00,210
There's probably some sort of probability where it's very difficult for you to decide which one you prefer to listen to,

462
00:49:00,490 --> 00:49:10,240
to live with you are with with glasses, or have this chance at perfect vision versus a loss of vision, somewhere between 50% and 90%.

463
00:49:11,230 --> 00:49:19,690
So you can kind of but this is the one experiment here is at what point is it really difficult to make that choice or decide

464
00:49:19,690 --> 00:49:28,210
which is better to stick with your current vision or have this uncertain outcome of best vision versus loss of vision.

465
00:49:30,580 --> 00:49:35,140
So, you know, besides, it can take time to kind of think through that and figure that out.

466
00:49:35,350 --> 00:49:43,360
But the control says we can find a probability where we're equipped, where current vision is equivalent to 2020, vision versus loss of vision.

467
00:49:46,320 --> 00:49:51,450
And we can also do that for minor complications and and so forth.

468
00:49:51,570 --> 00:49:59,880
So, yeah. Any questions about this? So that's the equivalence rule and then the substitution rule,

469
00:50:00,240 --> 00:50:06,390
which says that we really meant it when we when we answer those questions about, you know,

470
00:50:06,450 --> 00:50:13,200
P's and Q's about what what are those preference probabilities to make current vision and

471
00:50:13,200 --> 00:50:18,480
minor complications equivalent to some sort of uncertain outcome about best or worst vision.

472
00:50:19,000 --> 00:50:24,270
We can replace that in our tree here so that instead of having career vision here,

473
00:50:24,270 --> 00:50:30,030
I can kind of replace this with a lottery or chance of 2020 version responsive vision minor.

474
00:50:30,030 --> 00:50:33,450
I could replace that with the chance of 2020 versus loss of vision and current vision.

475
00:50:33,450 --> 00:50:38,219
I can replace that with that uncertain lottery with my preference probabilities

476
00:50:38,220 --> 00:50:44,670
in there because I said they were equivalent and then we can roll back the tree.

477
00:50:45,600 --> 00:50:52,200
So multiply the properties and the different paths and we should be able to turn this node here into something.

478
00:50:52,200 --> 00:51:02,700
It's equivalent to 2020 vision compared to loss of vision and multiply by this probability times,

479
00:51:02,700 --> 00:51:12,390
this probability plus this probability times this probability times this probability and this probability and this probability times this probability.

480
00:51:12,750 --> 00:51:17,850
I add that all together, and that's my overall probability of 2020 vision on that branch.

481
00:51:17,850 --> 00:51:20,370
And the remainder is the probability of loss of vision.

482
00:51:20,880 --> 00:51:28,830
And then I can compare those and say, well, which one gives me a better chance of 2020 vision versus loss of vision?

483
00:51:30,060 --> 00:51:40,620
Is this better or is this one that? So now I should be facing a decent year with equivalent outcomes.

484
00:51:42,030 --> 00:51:45,990
Just pick the one with a higher probability of the of in this case, 2020 vision.

485
00:51:51,700 --> 00:51:56,800
Any questions about that? Yeah, well, the equivalent of the one.

486
00:51:57,310 --> 00:52:05,980
Oh, we shouldn't. It is not necessary for us to choose the best and worst one to make as we could.

487
00:52:08,400 --> 00:52:20,040
To make comparable. So I suppose you I mean, you could also ask the question about what's the probability of 2020 vision versus minor complication.

488
00:52:20,940 --> 00:52:26,790
And and so, you know, minor complication is worse than current vision.

489
00:52:26,790 --> 00:52:33,840
And 2020 vision is better. So you could ask the question of current vision compared to 2020 versus minor complications,

490
00:52:34,290 --> 00:52:39,240
but typically it's easiest to compare versus best and best and worst because in the end,

491
00:52:40,440 --> 00:52:48,010
our tree should have, you know, best versus worst, because then we can always find anything.

492
00:52:48,030 --> 00:52:51,810
The middle is always going to be in the middle of the best, in the worst, or always going to be comparing the best.

493
00:52:52,250 --> 00:52:56,780
Does that answer your question? Yeah.

494
00:52:56,880 --> 00:53:01,170
And there are ways. Yeah. Typically you want to compare best versus worst.

495
00:53:11,580 --> 00:53:14,010
Other persons have questions.

496
00:53:14,220 --> 00:53:28,290
So in the Kuhns part, so we can equivalent the confirmation based like the 2020 on the last operation and also the manner we do the same thing.

497
00:53:28,320 --> 00:53:35,210
So is to increase it. So we just assign the same probability for the 2020 region.

498
00:53:37,410 --> 00:53:42,060
So the let me actually it's.

499
00:53:51,140 --> 00:53:58,310
So we're going to have our preference probabilities, which is how I feel about 2020 version vision versus loss of vision.

500
00:53:58,970 --> 00:54:12,740
And so that's going to be these probabilities here. Then the other probabilities here, the probability of complications.

501
00:54:14,070 --> 00:54:23,800
I call that a. And probably a 2020 vision with no complications is B and let's say probably minor complications.

502
00:54:24,340 --> 00:54:30,219
Given that have a complication is is C, so A, B and C are my beliefs again,

503
00:54:30,220 --> 00:54:33,280
maybe based on reading a lot of evidence about what what are the probabilities of

504
00:54:33,280 --> 00:54:39,490
these things happening and going to the doctor is and how good they are and so forth.

505
00:54:39,760 --> 00:54:44,320
I think this doctor is better than average. So he's going to get better outcomes than what I read in the medical literature.

506
00:54:45,400 --> 00:54:51,100
So in the end, though, to calculate the the value here at surgery.

507
00:54:53,540 --> 00:54:57,170
What's the to make this equivalent to best versus worst?

508
00:54:57,380 --> 00:55:02,030
I'm going to multiply the A's, B's and C's times, the PS and Qs and one minus PS, one minus Q's.

509
00:55:03,260 --> 00:55:09,200
And the the substitution rule says basically says that's a legitimate comparison

510
00:55:09,200 --> 00:55:13,849
to our multiplication activity to multiply the green and the red numbers

511
00:55:13,850 --> 00:55:21,680
together because I said they were equivalent and that I really can't substitute them in my trees and I really believe it that those are equivalent,

512
00:55:21,920 --> 00:55:29,300
that the minor complications really is equivalent to a two chance of transmission to one minus Q Chance of loss of vision.

513
00:55:30,200 --> 00:55:34,580
So in the end, I'm going to be multiplying the green and the red numbers together.

514
00:55:36,230 --> 00:55:42,830
So the chance of 20 years is going to be eight times one minus B times P plus four, sorry,

515
00:55:43,280 --> 00:55:50,450
eight times B plus eight times one minus B times P plus one minus eight times C times.

516
00:55:50,450 --> 00:55:58,340
Q That's my overall probability of 2020 vision on the upper part of it under the surgery range.

517
00:55:58,880 --> 00:56:03,600
Does that answer your question? Is there something else? And also, I want us to further write the latter.

518
00:56:04,100 --> 00:56:07,669
What you've got is a preference. Yeah.

519
00:56:07,670 --> 00:56:10,579
So A, B and C, the green numbers are things I'd probably, you know,

520
00:56:10,580 --> 00:56:15,590
look at the medical literature and that's my belief about how likely those things are to happen.

521
00:56:15,890 --> 00:56:18,290
And the red numbers here are my preferences.

522
00:56:18,590 --> 00:56:26,510
So like my question, Dan Way about how do you feel about living with glasses versus living with perfect vision versus living with loss of vision?

523
00:56:26,930 --> 00:56:34,190
That is your preference. And Dan Ray's answer about her answer for what PS and Qs are might be different for your answers.

524
00:56:34,350 --> 00:56:38,930
I mean, you might really enjoy wearing glasses and you might say, I would need to have,

525
00:56:39,140 --> 00:56:45,110
you know, 99.99% chance of of 2020 vision to give up living with glasses.

526
00:56:45,860 --> 00:56:51,979
But Hamlet maybe he hates glasses and he says, I would only need a 98% chance of getting a vision.

527
00:56:51,980 --> 00:56:57,370
I would risk a 2% chance of going blind to get rid of these terrible glasses that I hate so different.

528
00:56:57,440 --> 00:57:01,579
Those are different preferences for current vision versus 2020.

529
00:57:01,580 --> 00:57:08,180
Vision versus loss of vision. And that's that's your personal preference is how you feel about those outcomes compared to each other.

530
00:57:08,660 --> 00:57:14,930
That's the red numbers. So it could be different because under pressure conditions, yeah, it could do.

531
00:57:15,110 --> 00:57:22,760
Yeah. And so. Q The red numbers are more based on your preferences and how you feel about these outcomes compared to each other.

532
00:57:23,000 --> 00:57:29,870
And the green is based on your information about how much you how likely it is you think these outcomes will happen.

533
00:57:31,040 --> 00:57:34,210
Thank you. Yeah. Yeah.

534
00:57:34,510 --> 00:57:39,249
Wonder if there is a better way to establish the preference structure.

535
00:57:39,250 --> 00:57:44,380
Because suppose I have like a four layer tree that will be like 16 outcomes in total.

536
00:57:44,680 --> 00:57:49,470
Then probably the decision maker will have to make hundreds of, you know,

537
00:57:49,570 --> 00:57:54,220
comparison between those possible outcomes in order to establish a full tree.

538
00:57:54,780 --> 00:57:58,870
I wonder if there's a way to avoid that manual process.

539
00:57:59,350 --> 00:58:04,870
Yeah. You know, when the outcomes. A lot of the process will be really tedious.

540
00:58:05,650 --> 00:58:13,150
Yeah. And there are some shortcuts we'll use. So one example here is that for money.

541
00:58:13,990 --> 00:58:22,720
So we'll say, what if you have, you know, $100,000 versus $0?

542
00:58:23,770 --> 00:58:28,480
And how do I feel about that compared to having $1,000?

543
00:58:34,900 --> 00:58:41,200
And what about AP? Am I indifferent between having a chance of $100,000 versus $0?

544
00:58:43,990 --> 00:58:47,440
So how do you think about this? So what if you had a chance of getting a hundred?

545
00:58:47,680 --> 00:58:53,500
You could have 1000 for sure. Where you have this uncertain deal of getting $100,000 or nothing.

546
00:58:54,040 --> 00:58:58,900
I'll make sure that $101,000 won't be the average of the two possible outcome.

547
00:58:59,110 --> 00:59:05,560
Yeah. So what we'll what we'll do is we'll assume that you have linear preferences for dollars.

548
00:59:05,890 --> 00:59:10,900
And so if we do that, we say P is going to be equal to 1%.

549
00:59:11,530 --> 00:59:16,420
Right. So that would be the kind of expected value here, as we say.

550
00:59:16,450 --> 00:59:21,790
Well, P is, you know, 1% times $100,000 gives us $1,000.

551
00:59:21,790 --> 00:59:25,000
And that's going to be a 99% of times zero gives a zero.

552
00:59:25,360 --> 00:59:30,430
So the expected value of this part of the tree is $1,000, and that's equivalent to $1,000 here.

553
00:59:32,080 --> 00:59:35,260
So that is one thing we're going to do in cost effectiveness analysis.

554
00:59:35,440 --> 00:59:43,570
We are going to assume that we have this type of preferences where we can just multiply probabilities times the dollars.

555
00:59:44,260 --> 00:59:51,400
And and I guess I would argue that is totally appropriate for a lot of public policy decisions.

556
00:59:51,730 --> 00:59:59,860
When Joe Biden is making decisions or Rochelle Walensky at CDC is making decisions or the CEO of Trust Michelle making decisions.

557
01:00:00,610 --> 01:00:06,970
I think they probably should use this and just say, look, if we have a 1% chance of 100000 hours, that's worth $1,000 to us.

558
01:00:07,660 --> 01:00:14,920
That's equivalent to $1,000 to us. Now, for you in this room, you might not say.

559
01:00:15,670 --> 01:00:17,590
You might not say. These are equivalent here.

560
01:00:17,590 --> 01:00:23,080
If I have a 1% chance of getting 100000 hours or having 1000 hours in my pocket, who would prefer 1000 hours in their pocket?

561
01:00:25,040 --> 01:00:28,579
Who would prefer the deal where they get 100,000? 1% chance.

562
01:00:28,580 --> 01:00:33,680
$400,000. Go for it. Who is in difference between the two?

563
01:00:36,260 --> 01:00:46,579
So most of you had said they they prefer this kind of lower risk outcome here, which we say you're you're you're risk averse.

564
01:00:46,580 --> 01:00:51,530
If you chose this great risk called risk seeking, if you choose this, it might be more fun to have that chance.

565
01:00:51,540 --> 01:00:56,060
And I was right. And then you'd say we you're risk neutral if you say the equivalent.

566
01:00:56,240 --> 01:00:59,450
And so in this class, we're going to kind of big public policy decisions.

567
01:00:59,490 --> 01:01:03,320
We're going to assume that we're risk neutral. So that makes a lot of this a lot easier to calculate.

568
01:01:03,950 --> 01:01:08,350
We don't have to we don't have to do these questions and ask people much

569
01:01:08,360 --> 01:01:11,630
different questions about how do you feel about $100,000 for zero versus $1,000?

570
01:01:12,650 --> 01:01:17,240
We can just if we assume you're risk neutral, that makes the calculations a lot easier once we have dollars in here.

571
01:01:18,620 --> 01:01:27,410
Now, with that said, again, in the real world, especially for individuals making high stakes decisions, you might be risk averse.

572
01:01:28,880 --> 01:01:33,620
And so if you are risk averse, then it might make sense for you to go through the preference probabilities and say,

573
01:01:33,620 --> 01:01:39,770
look, for me to be indifferent between this and this, I would need a 10% chance, 100,000 or something like that.

574
01:01:40,430 --> 01:01:47,570
And if you are if you are really risk averse, then then maybe it does make sense to go through all those equivalence rule assessments.

575
01:01:48,050 --> 01:01:55,430
But for like big scale public policy decisions for the federal government, spending trillions of dollars like this is really, really small potatoes.

576
01:01:55,430 --> 01:02:00,770
And they should, in my view, and a lot of countries view they should treat this as being risk neutral and,

577
01:02:01,370 --> 01:02:05,750
you know, and just assume that a 1% chance of $100 equivalent $1,000.

578
01:02:08,810 --> 01:02:14,750
But for, you know, for students, this might be a bigger deal. So you might actually be risk averse in this range of money.

579
01:02:15,150 --> 01:02:18,830
And so for you, you know, maybe it makes sense to go.

580
01:02:19,190 --> 01:02:23,420
You can even if you are risk averse, you can still use these these approaches here.

581
01:02:24,530 --> 01:02:31,910
You just need to go through all the equivalence rule and substitution rule assessments.

582
01:02:32,270 --> 01:02:39,469
Actually, there's a few workarounds from that to me, which will see me across if you have questions about that concept.

583
01:02:39,470 --> 01:02:43,730
But yeah, so we're going to assume that we're risk neutral and that helps.

584
01:02:43,910 --> 01:02:48,150
That makes the calculations a lot easier. Just so you don't.

585
01:02:48,840 --> 01:02:55,350
We will do some more. But when we talk about quality of life, we will do some more of this equivalence rule assessment.

586
01:02:59,750 --> 01:03:04,129
But for the most part, we're going to assume that we're risk neutral.

587
01:03:04,130 --> 01:03:08,060
And so you're just going to you're actually just going to put your dollar values at the end here.

588
01:03:08,060 --> 01:03:12,600
And this looks like probably $7. Other questions.

589
01:03:12,930 --> 01:03:20,240
Good questions. Okay.

590
01:03:22,750 --> 01:03:33,160
So it's been a long hours. So hear me trying to convince you that using these five rules of probability rule or the rule equivalents, rule,

591
01:03:34,510 --> 01:03:44,559
substitution rule and choice rule mean if you think those are reasonable decision criteria or rules that you agree to abide by,

592
01:03:44,560 --> 01:03:45,580
if you think those are reasonable,

593
01:03:46,540 --> 01:03:53,590
then using decision trees can help you make again decisions give you clarity in your decision making under uncertainty.

594
01:03:53,710 --> 01:03:59,170
And I think, you know, I think for a lot of public policy decisions and for many personal decisions actually as well,

595
01:03:59,950 --> 01:04:05,800
these rules are reasonable and decision trees can help us, you know, give clarity in the decisions.

596
01:04:06,400 --> 01:04:13,600
So, again, in a kind of a complex question here, we say, how should I think about Lasik eye surgery versus basic eye surgery?

597
01:04:14,080 --> 01:04:20,770
If you go through this process, you should be able to make it a kind of clear answer of, okay, I should do this one or this one.

598
01:04:20,770 --> 01:04:27,160
And it's, you know, whatever the properties add up to me that I have clarity of action like this is the right decision for me,

599
01:04:27,250 --> 01:04:28,750
or maybe this is the right decision for me.

600
01:04:30,290 --> 01:04:40,600
Depending on how I what those probabilities are of the BS and C's and my beliefs about how I feel about those rocks outside of the Qs in the piece.

601
01:04:42,260 --> 01:04:51,160
Okay. Okay. So now I'd like to stop talking and give you guys the opportunity if you haven't already.

602
01:04:51,190 --> 01:04:57,940
You can check your emails or website. Find your groups and get to talk to your groups.

603
01:04:58,450 --> 01:05:02,380
And so I'll just kind of give you some free time here for the next 15 minutes or so.

604
01:05:02,650 --> 01:05:12,790
And next week we'll support some trees in our homes. This is what I find this year.

605
01:05:15,670 --> 01:05:24,170
I like my religion. It is not everything shapes, but it works like this.

606
01:05:25,870 --> 01:05:29,770
You are doing something different.

607
01:05:30,110 --> 01:05:37,480
Yeah. I was wondering why you guys are trying to do it before.

608
01:05:37,550 --> 01:05:41,260
Yeah, I don't know. That's just part of his philosophy.

609
01:05:42,480 --> 01:05:47,850
That's what I mean.

610
01:05:54,490 --> 01:05:57,880
Yeah, yeah, yeah, yeah. I was thinking.

611
01:05:58,070 --> 01:06:09,960
Oh, I don't know. He has to be right about something from.

612
01:06:10,510 --> 01:06:15,540
I do not want to talk about.

613
01:06:16,180 --> 01:06:37,450
Just do. But I think it's disrespectful.

614
01:06:39,550 --> 01:06:55,880
Yeah. No, that's right. I don't know anybody here, I guess, and or something like that.

615
01:06:56,110 --> 01:07:25,180
I mean, this situation is very specifically about like like have a specific disease or discomfort.

616
01:07:25,680 --> 01:07:28,900
I just had to go to Harvard.

617
01:07:29,230 --> 01:07:59,750
So, yeah, you know, I'll say to our doctor, I think cancer would have a lot to say about that right now.

618
01:08:01,990 --> 01:08:06,940
I was like, what? That's good. I like, I like, I like.

619
01:08:06,960 --> 01:08:26,170
I don't really have to worry about something like that.

620
01:08:28,300 --> 01:08:51,700
Okay, cool. Like, it's sort of like, for example, like, you're just, like, magic to Olivia, I guess.

621
01:08:51,700 --> 01:08:58,029
Emotional issues. I don't care. Yeah, let's do.

622
01:08:58,030 --> 01:09:01,930
Nothing is supposed to be done just like that.

623
01:09:03,730 --> 01:09:07,510
It's like. Yeah, yeah, yeah, yeah.

624
01:09:07,510 --> 01:09:16,240
Let me. I'm. I'm not sure I need to do that in cities.

625
01:09:16,870 --> 01:09:21,700
It definitely confirmed that. Yeah.

626
01:09:22,330 --> 01:09:25,600
Yeah, we did that like too much.

627
01:09:25,620 --> 01:09:34,740
Like six years later, we got together and then you get through this.

628
01:09:34,780 --> 01:09:37,000
Your email usually is so unique.

629
01:09:41,940 --> 01:09:57,419
Part of the company should respond to to email about Apple, because I think that is to subvert the whole notion of Scott.

630
01:09:57,420 --> 01:10:31,050
And so I'm trying to think Dexter and I have to say I got here because I think it's only the second

631
01:10:32,880 --> 01:10:48,780
reading and possibly I probably should have prepared for that third class program last month,

632
01:10:50,160 --> 01:11:15,600
which is a major win because to have to find out, we want to make sure that it doesn't go into everything.

633
01:11:18,420 --> 01:11:28,999
So you think you're doing well?

634
01:11:29,000 --> 01:11:58,010
That's right. That's right. Yeah. But there are like I think that is far too much, but I don't think she's just like two years ago the way to do it.

635
01:11:58,170 --> 01:12:11,170
So we have class always talking. So I thought that everybody is around.

636
01:12:11,170 --> 01:12:27,840
So that's class. So she's not the only person that is considering what I wish I was.

637
01:12:28,570 --> 01:12:40,150
So yeah, I think an article today that otherwise would be interesting to see if that's the

638
01:12:40,230 --> 01:12:50,010
case that this is a rare diseases group in here somewhere before we want to do

639
01:12:50,010 --> 01:12:56,920
like a group of people and we can all this group research I really don't know

640
01:12:57,870 --> 01:13:08,910
maybe they're having meetings so any time that he's going to listen to all that,

641
01:13:10,740 --> 01:13:18,389
that's not a rare disease. But I knew that he made for you to keep in mind, has no concern.

642
01:13:18,390 --> 01:13:26,160
And I thought it was totally the wrong person.

643
01:13:26,160 --> 01:13:38,819
I just I just like I guess, I mean, I've never had to be.

644
01:13:38,820 --> 01:13:45,990
So I'm just saying it like, you know, I just thought I didn't have the time.

645
01:13:45,990 --> 01:13:59,340
So I was like, maybe I'll put my number. And I mean, like, I genuinely like, I know you all go, Oh yeah, what's up?

646
01:14:01,590 --> 01:14:07,380
So I what's up? Shouldn't be doing anything.

647
01:14:07,410 --> 01:14:12,990
I just learned much stuff. Like, I don't know exactly.

648
01:14:14,500 --> 01:14:28,040
It's like procedure because my grandmother and then like, it's 810900 274.

649
01:14:28,290 --> 01:14:33,989
Okay. So it does help to be more specific.

650
01:14:33,990 --> 01:14:39,440
I think that will help you, especially when you're trying to think about these things for my phone number.

651
01:14:39,550 --> 01:14:48,360
So. We have a more precise idea of what I feel like, precise population pressures.

652
01:14:48,870 --> 01:15:00,210
And so we're doing it now. So should I think that any activity going on today which is shaped by your name.

653
01:15:00,390 --> 01:15:06,690
Now it is better to have zero tolerance and we're going to do on Friday, it's going to be like Friday.

654
01:15:06,690 --> 01:15:12,840
We were going to think about top ten, which means we want to focus on, I think, bearing down.

655
01:15:13,520 --> 01:15:17,130
But you there's actually easier, right?

656
01:15:17,460 --> 01:15:22,260
More narrow is rare. So a rare service here.

657
01:15:23,280 --> 01:15:36,580
I want to show you something and I want to get anything if anyone has an error,

658
01:15:36,690 --> 01:15:49,130
like I think it might be something that I would say it's not really a question 790 seconds or so.

659
01:15:49,410 --> 01:15:53,410
So there is interest, but that's not a big part of this.

660
01:15:53,700 --> 01:16:15,290
I'd be very appreciative because like I said, it's interesting in different countries I was thinking on how much about extending your life.

661
01:16:15,850 --> 01:16:19,680
This is not the afterlife and thank God that is different.

662
01:16:19,690 --> 01:16:35,750
There's another kind of connection with the business market.

663
01:16:40,500 --> 01:16:46,470
Option that options to increase.

664
01:16:49,350 --> 01:16:53,590
And to do that it has a delivery instrument.

665
01:16:54,950 --> 01:16:57,989
I was like it was a disease.

666
01:16:57,990 --> 01:17:14,580
When I think about something like this treatment for this drug treatment.

667
01:17:15,000 --> 01:17:18,360
They are talking about a war on terrorism.

668
01:17:20,310 --> 01:17:24,480
I have sort of stories for that sort of thing.

669
01:17:25,140 --> 01:17:31,950
But they want their care to be directed at is what, one yesterday.

670
01:17:37,470 --> 01:17:55,320
So we have to about whether or not the country is able to make sure that whatever I

671
01:17:55,320 --> 01:18:04,040
definitely don't want or need to say going to have to research for a number of reasons.

672
01:18:04,230 --> 01:18:12,209
So you need the great linkage that's like a separate it's not like you could emphasize

673
01:18:12,210 --> 01:18:19,740
this person because I don't really like talking about each patient for 60 to 90 days,

674
01:18:21,090 --> 01:18:25,980
but we can incorporate that into the case.

675
01:18:28,080 --> 01:18:30,120
I mean,

676
01:18:30,240 --> 01:18:50,450
you can make a really great generating probably that kind of thing that I would like to do it because it looks like a survey because like I said,

677
01:18:51,120 --> 01:18:54,240
I can definitely be okay.

678
01:18:55,020 --> 01:18:59,310
I can only do one. I have to be five years, many times.

679
01:19:00,120 --> 01:19:08,030
I don't know how long it's going to be there.

680
01:19:09,080 --> 01:19:24,930
Sure. And yeah, so we just like for that, but it's like, okay, this is something that I have to try to figure out how we got to plan.

681
01:19:25,130 --> 01:19:40,950
Like everyone has access to everything. So I should just tell you, I definitely don't know what I.

682
01:19:45,170 --> 01:19:51,969
Well, I was nervous because it wasn't the outcome we think is I don't think either.

683
01:19:51,970 --> 01:20:03,280
I can think of anyone that I think we're going to have this thing that going to just have.

