1
00:00:00,180 --> 00:00:09,270
You don't need to go to Mars and you're probably seeming like that is really cool.

2
00:00:09,690 --> 00:00:14,930
The first question is based on online right now, it's amazing.

3
00:00:15,270 --> 00:00:18,960
It was before this quote already happened. And you guys finished?

4
00:00:19,170 --> 00:00:29,240
Yeah. Oh, wow. Okay, cool. Usually I heard rumors that maybe there was COVID floating around, but it seems perfect.

5
00:00:30,060 --> 00:00:40,260
All right. Happy Wednesday. I had one of those days where my soul keeps getting steps down on days.

6
00:00:40,320 --> 00:00:46,649
I feel like I should have stayed in bed today, so I'm trying my best here to end on a good note.

7
00:00:46,650 --> 00:00:52,290
So let's see. Farkle, it seems like everybody's still playing.

8
00:00:52,290 --> 00:00:56,340
Everybody is still interested. I hear the word Farkle in your conversation.

9
00:00:56,350 --> 00:01:00,380
So we're so barely about 50% above the computer.

10
00:01:00,390 --> 00:01:06,600
So, again, not surprising. No one's trying to skew their results winning all the time.

11
00:01:08,280 --> 00:01:12,170
Let's see. Homework number one, the grades I released this morning.

12
00:01:12,180 --> 00:01:20,130
At some point, I expect grades to be fairly high near the end, with very few points taken off.

13
00:01:20,940 --> 00:01:24,569
I give you time to work in class, to work with each other, to talk about it.

14
00:01:24,570 --> 00:01:28,530
So I don't expect there to be major surprises for you and homework assignments.

15
00:01:30,630 --> 00:01:35,790
I told some you mostly grade on completeness and legibility and understanding.

16
00:01:35,790 --> 00:01:43,620
He said if he took points off, it was mostly because you said something that he couldn't find any evidence for in a plot, for example.

17
00:01:44,460 --> 00:01:50,160
So if you don't like your grade or you're concerned or questioning, what do you got taken off?

18
00:01:51,570 --> 00:02:00,870
I would ask you to reach out to him. And if he feels like you need to have your grade augmented, we can figure that out again.

19
00:02:00,870 --> 00:02:06,660
If you come to me, I'm happy to talk about it. In the end I might say, Well, we have to figure out what he was thinking, what he was at.

20
00:02:08,460 --> 00:02:12,060
So don't check your grade. If you don't like it, we can always discuss.

21
00:02:15,120 --> 00:02:26,439
Number two is due a week from today. And I spent about 5 hours on my homework assignment to yesterday because I was

22
00:02:26,440 --> 00:02:30,100
so baffled by what I was trying to do and what I was asking you guys to do.

23
00:02:32,210 --> 00:02:37,660
I don't know if you guys are baffled or everything is straightforward, but it's always interesting.

24
00:02:37,660 --> 00:02:41,140
When you do your homework assignments, you realize that you weren't quite clear.

25
00:02:41,170 --> 00:02:45,910
Anyway, so if there are questions, we need to hit those before the test.

26
00:02:47,080 --> 00:02:55,400
The test is and then it gets set. They say, I don't want to say the word test. The test is a week from Friday opens on Friday.

27
00:02:55,420 --> 00:03:00,480
There is no class. It's open from Friday till Tuesday.

28
00:03:01,050 --> 00:03:05,940
It is 90 minutes long once you open it. It right now has 14 questions.

29
00:03:07,110 --> 00:03:11,470
The questions are a range of multiple choice. There's one computation question.

30
00:03:11,490 --> 00:03:20,700
There's some matching type ideas. There are only two questions that cover material we have not yet talked about in this class.

31
00:03:21,180 --> 00:03:26,640
So if you feel like there's still a massive amount of material you need to cram for the exam, that is not the case.

32
00:03:27,360 --> 00:03:30,750
You have a majority of the information in already covered in class.

33
00:03:30,760 --> 00:03:34,620
The rest of it we're going to talk about as soon as I shut up and get to the lectures.

34
00:03:34,860 --> 00:03:38,850
All right. So we need to talk about remote restrict of maximum likelihood.

35
00:03:39,570 --> 00:03:42,750
And just how do you select a correlation structure using the data?

36
00:03:42,750 --> 00:03:44,760
What's the best correlation structure?

37
00:03:45,690 --> 00:03:54,450
Once we get through those two concepts, we're through the material on the exam for next week on other organizational issues.

38
00:03:55,500 --> 00:03:58,980
Yes, sir. Looks like the next exams material.

39
00:03:59,010 --> 00:04:05,840
Can you make a powerpoint a PDF like have both the powerpoint and the picture in order to have the PDF versions of the slides on canvas.

40
00:04:05,910 --> 00:04:09,240
You like PDF versions of the slides so you can write on them.

41
00:04:10,050 --> 00:04:22,400
Got it. When you're 54 years old, you'll know that if you don't write something down as soon as someone tells you to do it.

42
00:04:23,260 --> 00:04:31,719
And I don't have a pen. So that means that the person lives on.

43
00:04:31,720 --> 00:04:38,710
The candidate's got other things topics, issues, concerns.

44
00:04:40,000 --> 00:04:42,160
Again, please don't go to the test with a lot of panic.

45
00:04:43,330 --> 00:04:50,680
It's it's just to force you to learn some of the material and show me that outside of analyzing data,

46
00:04:50,680 --> 00:04:56,200
you understand some of the things that I'm talking about in class. All right.

47
00:04:57,970 --> 00:05:02,340
Before we go there. The trend is the way forward.

48
00:05:15,540 --> 00:05:20,670
Now, most importantly, let's see, say longitudinal. Longitudinal data.

49
00:05:30,670 --> 00:05:36,190
So at this point, I've really thrown out two different ideas that you could use when you're analyzing longitudinal data.

50
00:05:37,970 --> 00:05:41,910
The first one says. The correlation.

51
00:05:46,900 --> 00:05:55,450
It's a wellness or an hour of continuous data set as well as if it's binary data, you can just use a global economy, just a progression, for example.

52
00:05:55,870 --> 00:05:59,350
So you could ignore the correlation just fine. Ordinarily squares.

53
00:06:00,780 --> 00:06:05,450
Then get a sandwich. Standard errors.

54
00:06:14,470 --> 00:06:22,660
The second approach is to model the correlation. So then you're going to set a generalized three squares model and you're going to get the model.

55
00:06:24,250 --> 00:06:27,270
It's standard errors.

56
00:06:29,450 --> 00:06:34,250
And these two approaches are going to come up more when we talk about generalized estimating equations,

57
00:06:34,700 --> 00:06:37,930
when we talk about sandwich versus model based standard errors.

58
00:06:39,320 --> 00:06:44,780
I don't think one is any better than the other. I think number one is easier.

59
00:06:45,500 --> 00:06:53,030
One doesn't you know, some people have sort of paraphrasing, they've come up to me and said to me is, why do we need this class?

60
00:06:54,500 --> 00:06:57,780
Why can't I just use what I did in 650 and get a sandwich? Standard error.

61
00:06:57,800 --> 00:07:04,040
Thank you. I'm done with the semester. Right. That's one approach.

62
00:07:06,560 --> 00:07:09,830
I don't want to tell you. I don't want you guys coming away.

63
00:07:09,830 --> 00:07:14,360
You folks coming away with the idea that called modeling correlation is a waste of time.

64
00:07:16,100 --> 00:07:19,580
So the question is, is do I need.

65
00:07:21,690 --> 00:07:30,610
To model relations. Yes.

66
00:07:30,610 --> 00:07:33,910
If you're interested in it, maybe you want to know what the correlation coefficient is.

67
00:07:35,970 --> 00:07:40,830
If that's of interest, is it point to is a point for if that's of interest, then you should try and model it, right.

68
00:07:42,590 --> 00:07:45,649
If it's a nuisance parameter, if you're using an approach, number one,

69
00:07:45,650 --> 00:07:49,280
you're simply saying correlation is just a problem that I'm trying to get rid of.

70
00:07:50,030 --> 00:07:54,890
It gets in the way of my inference. So the what senator is going to help me deal with that?

71
00:07:55,180 --> 00:08:04,460
All right. So correlation might be a nuisance. Remember that sandwich standard errors are trying to estimate a matrix.

72
00:08:13,470 --> 00:08:17,640
It was the variance of why I is a four by four matrix.

73
00:08:18,300 --> 00:08:26,820
And so there's four variance and a bunch of covariance. So you're trying to use the residuals to estimate everything in this matrix.

74
00:08:28,380 --> 00:08:38,430
Well, remember that the residuals in large samples, the formula of the residuals times themselves, it goes to the variance, right?

75
00:08:40,180 --> 00:08:49,110
But in small samples, this might be a big mess. So if you have 20 people, I'm not sure a sandwich standard air is a great way to go.

76
00:08:50,260 --> 00:08:55,200
Because it's going to be noisy to begin with. That's why we have models.

77
00:08:56,480 --> 00:09:00,530
Models helped me try to get to an answer. They make assumptions.

78
00:09:00,530 --> 00:09:09,540
They require me to assume things. Those assumptions get me an answer that is plausible based upon the assumptions.

79
00:09:10,130 --> 00:09:13,670
Okay. So sandwich standard errors are a large sample concept.

80
00:09:13,670 --> 00:09:19,010
If you have enough data to estimate this matrix, then you might be okay.

81
00:09:19,310 --> 00:09:23,990
So I don't mean to tell you that number one is the best way to go all the time.

82
00:09:26,240 --> 00:09:30,110
I use number the number one approach quite often in my own analysis.

83
00:09:31,780 --> 00:09:37,850
As I told you folks, some folks when I teach 699 and I teach this correlated data stuff as a review,

84
00:09:37,880 --> 00:09:42,290
you'll get a review of this next semester again, which you won't need because it's going to be crystal clear.

85
00:09:44,990 --> 00:09:49,910
I will talk about number seven. Number one approach. Jeremy Taylor will say, well, that's dumb.

86
00:09:50,390 --> 00:09:56,360
You can say it's dumb, but he's like, number two, you would ask me would always model something when you feel like you have an idea of what it is,

87
00:09:57,110 --> 00:10:03,140
you can always gain some effect on bias or precision when you try to model things correctly.

88
00:10:04,310 --> 00:10:10,430
So either way is acceptable in certain situations, right?

89
00:10:10,540 --> 00:10:18,320
You don't need to do both. I know folks like to do as many things as they possibly can to make people happy, you know?

90
00:10:18,410 --> 00:10:21,560
So I'm going to show you an examples and I hope you're saying this in your own analyzes.

91
00:10:22,070 --> 00:10:28,220
Please never show multiple approaches to the analysis to an investigator who isn't statistically smart.

92
00:10:29,600 --> 00:10:34,610
Because which analysis are they going to pick the women as the smallest p value?

93
00:10:35,150 --> 00:10:43,340
Right. It's your job to figure out what model is best and then to present that evidence to your audience.

94
00:10:43,340 --> 00:10:49,910
Right. Don't let them choose unless they have a degree from the University of Michigan.

95
00:10:51,630 --> 00:10:55,390
All right. So keep that in mind that I'd show you these approaches.

96
00:10:55,400 --> 00:10:59,690
I don't mean to say one is better than the other. They both have utility and do.

97
00:10:59,690 --> 00:11:06,800
What's your wish? Right. The questions, their concerns.

98
00:11:07,070 --> 00:11:11,540
Confusion. Because they told Junko Woo,

99
00:11:11,540 --> 00:11:18,770
who teaches the other section of this class that I was covering sandwich standard errors already and you tell her what you do.

100
00:11:20,360 --> 00:11:26,240
So anyways, we'll see you next year and we'll be all right.

101
00:11:28,080 --> 00:11:30,899
So those are the ideas I wanted you to start exploring and homework.

102
00:11:30,900 --> 00:11:35,549
Number two, I only asked you to look at two correlation structures out of aggressive one.

103
00:11:35,550 --> 00:11:43,080
And how about symmetry? I didn't mean to say that others were weren't useful, but I didn't want you to spend forever on the homework assignment.

104
00:11:43,080 --> 00:11:49,470
Like I said, I spent many hours on only one of my datasets scratching my head as to what I was actually doing.

105
00:11:49,770 --> 00:11:54,809
So let's talk about the rest. So it looks like I have a lot of lecture slides here, right?

106
00:11:54,810 --> 00:12:02,760
I've got to. And I actually added up to hide because I wanted to go through these concepts specifically with the labor pain data.

107
00:12:02,790 --> 00:12:07,860
And I realize although I gave you our code, I never really showed you what I did with that our code.

108
00:12:08,160 --> 00:12:11,430
So I think we can cover all these today. Some of them. I only have a few slides.

109
00:12:13,410 --> 00:12:32,320
What's that? I guess I'd never one of those days to see where it had to go as I closed it in you.

110
00:12:39,970 --> 00:12:45,790
This is why never, ever watch my video recordings. They were just fabulous.

111
00:12:47,740 --> 00:12:56,950
There we go. All right. And we reviewed ideas of ordinarily squares and correlation structures and goals.

112
00:12:57,760 --> 00:13:04,660
And now I'm asking you to do that. And, ah, I just want to focus on everything else we do in a generalized these squares models.

113
00:13:05,080 --> 00:13:10,000
A lot of this you have seen already extended now to the fact that there's this weight matrix,

114
00:13:10,000 --> 00:13:14,410
the sigma that goes into the estimation and the inference for regression parameters.

115
00:13:16,060 --> 00:13:23,080
So again, reviewing the notation, we've got an outcome that's continuous and we put a normal distribution upon.

116
00:13:23,920 --> 00:13:32,770
I represent subject or person participant and a j represents a time point and every person can have a different number of time points.

117
00:13:34,330 --> 00:13:37,420
So each person has a vector of outcomes.

118
00:13:38,410 --> 00:13:43,840
Each person has a vector of outcomes that can be modeled as a mean x data.

119
00:13:44,900 --> 00:13:50,750
Plus this vector, this vector of errors, and the errors are positively correlated with each other.

120
00:13:51,200 --> 00:13:54,230
And again, beta is a vector of regression parameters.

121
00:13:54,500 --> 00:13:59,630
And everybody has a design matrix. So every row corresponds to a time point.

122
00:14:00,320 --> 00:14:08,990
And very often each row is the same. If we just have a bunch of baseline covariates, age, sex income and a short term status.

123
00:14:09,530 --> 00:14:14,000
We can have access so we can have a column of X's that change over time.

124
00:14:14,370 --> 00:14:21,200
That's a time varying covariant. Most often we have baseline covariance counts over time.

125
00:14:22,020 --> 00:14:26,150
And then we have a vector of residuals, each one for each outcome.

126
00:14:27,410 --> 00:14:37,430
Those residual effects each have a variance, and they each have a covariance with each of the other residuals in its most unstructured form.

127
00:14:39,150 --> 00:14:45,420
Right. That is just way too much unstructured for my comfort level with most datasets.

128
00:14:45,840 --> 00:14:50,310
And so we try to put some structure around this correlation matrix or this covariance matrix.

129
00:14:51,000 --> 00:14:57,130
For example, with compound symmetry, we take this high dimensional matrix and we turn it into a heavy,

130
00:14:57,510 --> 00:15:03,059
many dimensional matrix and make it a matrix with only two parameters the variance

131
00:15:03,060 --> 00:15:08,970
of each observation that's constant and a correlation parameter to parameters.

132
00:15:13,710 --> 00:15:20,690
So again, to get more into now everybody, we have a vector of all the outcomes in the study.

133
00:15:20,700 --> 00:15:24,329
We take each vector of each person and put them all on top of each other.

134
00:15:24,330 --> 00:15:34,300
Right? Big long vector. And that vector has a mean structure and an error structure and same regression parameters.

135
00:15:34,350 --> 00:15:39,480
This design matrix is now every person's designed matrix placed on top of each other.

136
00:15:41,230 --> 00:15:45,130
The errors are now everybody's vector of airspace and on top of each other.

137
00:15:45,940 --> 00:15:52,750
And the overall variance of all of the observations is a black diagonal matrix.

138
00:15:53,440 --> 00:15:57,820
Sigma one is the variance covariance structure of person one's observations.

139
00:15:58,210 --> 00:16:03,130
Sigma two is the variance covariance of the second person's observations and so forth.

140
00:16:03,910 --> 00:16:11,500
And then although my observations are correlated with each other, they are independent or uncorrelated with everybody else's observations.

141
00:16:11,500 --> 00:16:15,590
So there are zeros everywhere else in this block, a diagonal matrix, right?

142
00:16:15,610 --> 00:16:18,660
The diagonal has a matrix for each element that's so.

143
00:16:22,570 --> 00:16:27,219
By doing that, then, if everything is normally distributed,

144
00:16:27,220 --> 00:16:33,760
we use the law of likelihood to take the derivative and we end up with way to least squares that the regression parameters have.

145
00:16:33,760 --> 00:16:42,010
The form that we're used to there, right? That transposed segment reverse x inverse x sigma y inverse next segment versus y.

146
00:16:42,580 --> 00:16:49,390
So again, the inverse of the variance covariance matrix is a set of weights that we apply in the regression model.

147
00:16:50,680 --> 00:16:56,259
And the variance of my regression parameter estimates is simply the part in front of the

148
00:16:56,260 --> 00:17:03,940
usual x transpose sigma inverse x inverse sigma z identity matrix times sigma squared.

149
00:17:04,330 --> 00:17:07,330
Then we get x transpose x inverse times sigma squared.

150
00:17:07,810 --> 00:17:12,160
We get know this now because everybody is independent of each other.

151
00:17:12,370 --> 00:17:21,639
The more common form you might see is that I don't rate beta hat as that overall it's just a bunch of matrix multiplication.

152
00:17:21,640 --> 00:17:27,490
It's the sum of each person's matrix multiplication because everybody's independent.

153
00:17:28,710 --> 00:17:37,840
So again, if I take X transpose Times Sigma, the zero is help me get rid of a lot of things.

154
00:17:38,140 --> 00:17:44,860
So essentially I'm doing this individually for every person and then adding them up and therefore the variance

155
00:17:44,860 --> 00:17:49,390
of the regression parameter estimates looks like that it's the sum of each individual component there.

156
00:17:51,300 --> 00:17:56,370
The problem is, is I don't know. Omega omega is the set of parameters in sigma.

157
00:17:56,520 --> 00:17:59,520
It's usually a variance parameter and a correlation parameter.

158
00:18:00,390 --> 00:18:06,820
You have to figure out what those. And of course, what do we do?

159
00:18:06,850 --> 00:18:11,050
We do our old idea of maximizing the log likelihood.

160
00:18:11,650 --> 00:18:18,280
So we've got a log likelihood there. This is just the log of a multi a very normal PDF and there it is.

161
00:18:20,340 --> 00:18:27,270
And so we require solutions such that the derivative of that with respect to sigma squared and the derivative

162
00:18:27,270 --> 00:18:33,990
with respect to rho is zero and maximizes the likelihood we know the likelihood is a gain and so forth.

163
00:18:35,920 --> 00:18:44,470
One of the problems is that in so in order to get data estimate, we need an estimate of Omega to get an estimate of Omega.

164
00:18:44,560 --> 00:18:47,950
We need an estimate for beta. Vice versa. Right.

165
00:18:48,610 --> 00:19:00,490
So the simplest thing to do is to simply take an omega value estimate data and then stick that in and get an updated value of my parameters.

166
00:19:01,210 --> 00:19:07,420
That's like the best approach because there's some bias that goes on.

167
00:19:07,420 --> 00:19:10,720
So trying to surmise, again, a homework problem would never be.

168
00:19:10,840 --> 00:19:18,010
Please write down the close form expressions for rho hat and sigma squared for this model because we don't exist.

169
00:19:18,880 --> 00:19:26,040
Maximizing these likelihoods is some people have made their careers out of figuring out how to tell computers to do this efficiently.

170
00:19:26,050 --> 00:19:34,630
There's lots of ways to do it. Expectation, maximization on rhythm M and lots of other approaches that one spent time in this class talking about.

171
00:19:35,320 --> 00:19:46,210
Except for right now. But regardless of how you decide to estimate the variance covariance matrix, they're biased.

172
00:19:47,050 --> 00:19:52,390
And that's because we assume when we plug in beta that we assume that that's known.

173
00:19:52,510 --> 00:20:01,050
But again, it's an estimate. It has its own noise about it. And it's the same concept you saw in ordinary squares to get sigma squared het.

174
00:20:01,620 --> 00:20:06,660
You don't divide by n james the sum of the red residuals or divide by n.

175
00:20:06,750 --> 00:20:11,280
You take the square residuals and you divide by and minus P for the number of parameters you estimated.

176
00:20:12,270 --> 00:20:15,690
That's a Rummel. That's a restricted maximum likelihood kind of concept.

177
00:20:16,170 --> 00:20:19,440
It's to get rid of bias in the variance parameter.

178
00:20:20,400 --> 00:20:28,120
But of course, in this situation, estimation of beta didn't rely on sigma, it was x, transpose x, x transpose y was an inversion.

179
00:20:28,710 --> 00:20:34,410
But so here it's a little more complicated, but the same concept applies.

180
00:20:35,370 --> 00:20:44,490
So in order to get variance covariance parameters that are unbiased, we use something called REMO restricted maximum likelihood.

181
00:20:45,450 --> 00:20:52,500
So you'll hear a lot about Remo estimates in this in longitudinal data and I think you hear about it a little too much.

182
00:20:54,150 --> 00:20:59,610
There's several ways to develop the idea of what restrictive maximum likelihood looks like.

183
00:21:01,260 --> 00:21:01,800
Excuse me.

184
00:21:02,580 --> 00:21:10,830
But essentially, they all come down to this resulting approach here that the log likelihoods with respect to Omega, the variance parameters.

185
00:21:11,670 --> 00:21:15,360
So what's the likelihood of the variance parameters conditional on beta hat?

186
00:21:15,390 --> 00:21:20,520
That's what I'm trying to maximize, not conditional on beta or conditional on beta hat.

187
00:21:21,120 --> 00:21:32,349
And it looks like that there are three terms. In the first two terms are the usual log likelihood of a normal multivariate normal distribution.

188
00:21:32,350 --> 00:21:38,800
But we plugged made ahead and for data but there's a third term this right here.

189
00:21:40,480 --> 00:21:46,540
And hopefully that looks familiar. It's X transpose sigma inverse x.

190
00:21:48,210 --> 00:21:54,720
Has something to do with the variants of beta, right? The variants of beta have x transpose segment versus x.

191
00:21:56,100 --> 00:22:08,250
So we're trying to find values of Omega that also keep us from having beaten that hitchhiking data coefficient estimates that are too noisy.

192
00:22:09,060 --> 00:22:17,400
So we're penalizing how we find Omega to make sure that we don't get an omega that makes the variance of beta hat explode.

193
00:22:18,150 --> 00:22:25,560
So if the variance of beta ahead is too big, right, this likelihood is penalized for that because we're trying to minimize or try to minimize this.

194
00:22:26,220 --> 00:22:33,000
So again, that term I just showed you, as I just told, you can look like that, which then, you know, you move to minus one,

195
00:22:33,450 --> 00:22:40,120
then you move the one half up into the exponent and that's the variance of beta like this in the last.

196
00:22:40,290 --> 00:22:48,720
Right. So we try to find values of our variance covariance parameters that try to keep the variability of beta had as small as possible.

197
00:22:48,930 --> 00:22:54,480
I try to balance the needs of these parameters with each other.

198
00:22:57,180 --> 00:23:03,360
So we're going to use this likelihood right here to find value once we estimate at.

199
00:23:04,700 --> 00:23:09,320
Now. Again, remember, in order to get better hat, I had to pick a valuable automaker to start with.

200
00:23:10,940 --> 00:23:16,130
And then I get that data hat and then I maximize the likelihood and I get a new Omega.

201
00:23:16,400 --> 00:23:20,660
We call that the rental estimate instead of the maximum likelihood estimate.

202
00:23:22,790 --> 00:23:30,020
And then once I have an Omega from Rummel from research on maximum likelihood, I plug about in to get a new bay and a hat.

203
00:23:31,010 --> 00:23:36,230
I got to use that to get a new estimate of Omega. This is a little bit of an iteration, but it goes very quickly.

204
00:23:37,790 --> 00:23:41,330
Not even know how many steps it actually requires to get a nice convergence.

205
00:23:41,900 --> 00:23:46,219
So Remo is an iterative process because each time we computer a new estimate of the various parameters,

206
00:23:46,220 --> 00:23:52,459
we get to go back to the regression mean parameters and so forth. So again,

207
00:23:52,460 --> 00:23:56,510
if I go on Stack Exchange or one of these wonderful websites we all use when we're scratching our

208
00:23:56,510 --> 00:24:01,790
head on what to do and you may have already gone there this year in question like this will appear.

209
00:24:02,900 --> 00:24:07,140
Should I use maximum likelihood around the. I don't really care.

210
00:24:07,830 --> 00:24:17,490
No, this is bronze opinion. I think we spend a lot of time worrying about minutia in theory sometimes when we actually are trying to analyze data.

211
00:24:17,520 --> 00:24:20,640
I'm not saying to say that it's trivial, but it kind of is.

212
00:24:22,050 --> 00:24:29,250
So there's no right method. Remember that both of these approaches are going to give me consistent estimation for me.

213
00:24:30,780 --> 00:24:35,730
So remember, speed ahead is consistent regardless of what's inside the bag, the weight matrix.

214
00:24:37,310 --> 00:24:40,880
Right. So it's regardless of the structure and what actually Omega is.

215
00:24:42,140 --> 00:24:47,120
So why do we care in this whole class? Why do we care about the variance covariance matrix?

216
00:24:48,260 --> 00:24:51,860
Because in small samples, the bias might be enough to be an issue.

217
00:24:53,030 --> 00:25:00,720
Right. That n minus p in ordinarily squares and minus p might be enough to worry about versus n that you might want to account for.

218
00:25:01,070 --> 00:25:05,780
But in large samples, we generally don't need to worry about this too much.

219
00:25:05,870 --> 00:25:09,410
So again, what is a small sample? I have no idea.

220
00:25:10,910 --> 00:25:17,819
But you kind of know it when you see it. Right. And I throw this last line here.

221
00:25:17,820 --> 00:25:20,639
But remember, in the end, you can always fix stuff with a sandwich estimate.

222
00:25:20,640 --> 00:25:28,710
Or if you get the model wrong for for the sigma matrix, you can always throw in a sandwich estimate or to try and cover your tracks a little bit.

223
00:25:31,700 --> 00:25:36,310
There's are some theoretic underpinnings. Yeah, go ahead. So far around, when do we stop?

224
00:25:36,320 --> 00:25:40,380
Like what? Iterations. If we use the ram master.

225
00:25:40,490 --> 00:25:44,810
When do we stop? When do you stop? Yeah, the computer is again.

226
00:25:45,260 --> 00:25:48,800
Ah, we'll have a certain decimal points of convergence.

227
00:25:49,250 --> 00:25:56,420
That's the default. So within the goals machinery, for example, if you specify rebel instead of maximum likelihood,

228
00:25:57,530 --> 00:26:02,000
there is a default that says, well, once you're within four decimal places of both parameters, you stop.

229
00:26:02,300 --> 00:26:07,930
So you don't define that necessarily. Okay. The iterations happen within the machinery of our.

230
00:26:08,600 --> 00:26:16,010
So for you up front, there has to be a starting point to remember, to get better, to have the first time you need an omega out.

231
00:26:16,670 --> 00:26:23,060
I don't know what our starts with, whether it's zero independence or it does something else.

232
00:26:23,150 --> 00:26:26,930
But again, that's all in the background. You don't have to worry about any of that.

233
00:26:27,290 --> 00:26:33,590
All you get are the REMO estimates. You will notice that in all of the code that I have done in over two, I don't say Remo.

234
00:26:33,620 --> 00:26:39,860
I just use maximum likelihood. So that tells you where I'm at in terms of this question.

235
00:26:43,520 --> 00:26:50,749
Again, you do need to think about how much data you have relative to how complex your regression model P is,

236
00:26:50,750 --> 00:26:53,450
the number of mean parameters in this notation here.

237
00:26:54,140 --> 00:27:02,010
So again, recall that mean squared error of something is how noisy it is, plus the square of how biased is right.

238
00:27:02,030 --> 00:27:09,620
We try to balance both those things. Many times in statistics we have a method that reduces the variance but adds a little bit of bias.

239
00:27:10,370 --> 00:27:17,509
So we balance these two things with something called Mean Square there. So in general, in general, this is not a law or a rule or anything.

240
00:27:17,510 --> 00:27:25,310
I want you to quote to anybody. But in general, if you have four or fewer parameters, they mean square error.

241
00:27:25,340 --> 00:27:29,720
Using maximum likelihood for the variance parameters is less than that from random.

242
00:27:30,710 --> 00:27:35,500
If you have more than four parameters, then maybe it's the other way around.

243
00:27:36,080 --> 00:27:44,900
Do with that information what you will. I'm not saying that every time you have more than four regression parameters you must use Remo.

244
00:27:45,770 --> 00:27:48,290
I'm not saying that right.

245
00:27:49,340 --> 00:27:56,330
It's just some rule of thumb that somebody came up with and we teach it in our classes, but it should be nothing more than that.

246
00:27:56,360 --> 00:28:01,250
It's just some some guidance here. One of the issues with Rimmel, however.

247
00:28:02,350 --> 00:28:09,130
Is that, remember, a lot of what we do is founded in likelihood theory, the original likelihood, not the restricted likelihood.

248
00:28:10,210 --> 00:28:15,610
So likelihood ratio tests for deciding whether a beta parameter should be in the model or not.

249
00:28:16,570 --> 00:28:22,320
SS For comparing models. A.B.C. These things we're going to review shortly if you haven't already seen them and.

250
00:28:22,320 --> 00:28:29,410
Six 5651. Those all require a likelihood in the computation, not a restricted likelihood.

251
00:28:29,410 --> 00:28:35,020
Right. So again, you might be using parameters that weren't strictly based in likelihood theory.

252
00:28:36,120 --> 00:28:48,870
And do. Is that what you wish? So again, Beta had Remo and Omega had Remo not strictly justified in theory when using them in these methods.

253
00:28:48,870 --> 00:28:53,370
But again, large samples, if you've got enough data, let's not worry about all these things.

254
00:28:54,240 --> 00:28:58,500
The effect is small, but these these issues do exist.

255
00:29:00,180 --> 00:29:10,380
Right. So that's Rummel. We'll never talk about it again this semester except a week from Friday when you're taking the test.

256
00:29:11,820 --> 00:29:18,300
It is important to know again, I don't I don't want to diminish its importance, but it's maybe not worry about it so much.

257
00:29:18,450 --> 00:29:24,430
Just be aware of what that concept is. I guess I could close the.

258
00:29:25,750 --> 00:29:34,760
All right to. So again, I apologize for the number of slide decks,

259
00:29:35,240 --> 00:29:39,590
but I think it's better for at least when I'm formulating this class to think of

260
00:29:39,590 --> 00:29:44,030
chunks of topics and then put the slides together rather than having 100 slides.

261
00:29:44,540 --> 00:29:47,779
And that's like remembering where I was supposed to be taking a new idea.

262
00:29:47,780 --> 00:29:52,380
So. How can you choose a correlation structure?

263
00:29:53,010 --> 00:30:01,469
Right. So I've asked you to do this in homework number two. But as you just said, independence on a regressive and compound symmetric.

264
00:30:01,470 --> 00:30:05,100
And then ask you to pick one of those. Right.

265
00:30:05,520 --> 00:30:09,510
And if you looked ahead or you've done some stuff, maybe you've already know what to do.

266
00:30:09,520 --> 00:30:12,600
But let's make it too concrete here. Right.

267
00:30:13,290 --> 00:30:18,030
And I've only asked you to come to select among three correlation structures.

268
00:30:18,390 --> 00:30:26,190
When I get to my last stack of slides, I'm going to look at nine correlation matrices and try to decide which one I should use.

269
00:30:27,150 --> 00:30:31,200
So how do you decide? Right. Three approaches to consider.

270
00:30:32,070 --> 00:30:36,150
MAN Is it hot in here? All right, it must be me.

271
00:30:37,980 --> 00:30:44,160
And this rule applies to everything you do and statistics that cannot phrase this enough.

272
00:30:45,000 --> 00:30:48,000
Your number one rule is common sense.

273
00:30:48,960 --> 00:30:54,690
Nothing drives me more crazy than when I ask a student how they came up with their final model.

274
00:30:54,690 --> 00:30:58,800
And they say, Well, the AIC is the smallest one among all these models.

275
00:30:59,850 --> 00:31:04,350
Well, that model has 16 parameters in it. Right.

276
00:31:04,590 --> 00:31:08,040
I don't like that model. It's too confusing for me.

277
00:31:08,050 --> 00:31:11,300
I don't care what the computer told you. What is your bringing up here?

278
00:31:11,310 --> 00:31:16,830
Tell you the model should be. So use common sense. The structure is okay.

279
00:31:16,980 --> 00:31:24,690
Assumptions are okay. So again, folks like to come out with their correlated data and use an unstructured correlation matrix.

280
00:31:24,690 --> 00:31:31,320
Because I didn't make any assumptions. My. It's probably going to be very messy and noisy and inaccurate.

281
00:31:32,010 --> 00:31:39,060
So structure is okay. If you're in a longitudinal setting, many times you're going to have either a one or compound symmetric.

282
00:31:39,510 --> 00:31:43,350
That's why I've asked you to use those right now.

283
00:31:43,350 --> 00:31:48,840
But some tree may not be plausible because we often believe that correlation decays over time.

284
00:31:49,650 --> 00:31:59,850
And the second approach is to assume independence and examine the residuals to get an idea for what the correlation structure looks like.

285
00:31:59,850 --> 00:32:01,680
And I see that in some our code eventually.

286
00:32:03,750 --> 00:32:11,690
Again, these ideas are most useful when everybody has the same number of observations measured at the same point in time, right?

287
00:32:11,750 --> 00:32:19,410
Visit, one, two, three, four, five to get a side by side matrix. What happens in a longitudinal setting where people come in at random points in time?

288
00:32:19,650 --> 00:32:23,639
How do you lump them back together into into a matrix of observations?

289
00:32:23,640 --> 00:32:31,770
So sometimes it gets a little complicated, right? But you use your common sense fit independence to see what's again, the residuals.

290
00:32:32,220 --> 00:32:38,910
Residuals are useful. They tell you what's missing from what you've already done, both in the mean and in the correlate.

291
00:32:38,910 --> 00:32:47,050
The variance structure. Correlation structure. You can also find several different correlation structures to the data and try to pick among them.

292
00:32:47,440 --> 00:32:53,620
And the metrics we use are AIC, ACC information and basically the Bayesian information criteria.

293
00:32:54,580 --> 00:32:57,700
I don't care which one you use, one isn't better than the other.

294
00:32:57,700 --> 00:33:04,569
I mean, some people will argue one is better than the other. Depending upon their favorite, their likelihood based methods,

295
00:33:04,570 --> 00:33:11,139
they require a likelihood and then they adjust their log likelihood for a model based upon its complexity.

296
00:33:11,140 --> 00:33:15,490
And it penalizes models that are too complex because they have too many parameters.

297
00:33:15,910 --> 00:33:21,770
That's how we measure complexity. There are no P values for picking correlation structures.

298
00:33:23,310 --> 00:33:28,280
I want to make that clear here. There is no inference. This is one of these wonderful personal judgment.

299
00:33:28,280 --> 00:33:35,750
Things that you have to decide is one model have a distinctly different AC from a different model.

300
00:33:37,640 --> 00:33:40,880
So there's the formulas in our confuses for you.

301
00:33:40,890 --> 00:33:44,840
There's an AC ABC function that you can apply to a model already.

302
00:33:45,320 --> 00:33:48,800
There they are. It's the negative to log likelihood. Right.

303
00:33:48,920 --> 00:33:52,220
But we also penalize for p the number of parameters in the model,

304
00:33:52,610 --> 00:33:57,440
the number of data parameters and Q the number of parameters in the variance covariance model.

305
00:33:58,520 --> 00:34:06,590
So as those grow in complexity, the AC is penalized more and more and likewise by now by Bisi.

306
00:34:07,250 --> 00:34:16,490
Again, I don't remember the Bayesian underpinnings of this criteria function here, but it is very similar to AC.

307
00:34:16,700 --> 00:34:24,410
But instead of multiplying by two times the number of parameters total, it multiplies by the total number of observations.

308
00:34:26,730 --> 00:34:38,760
So again, it puts greater penalty on complex models and diseases and smaller values of either of these indicate a preferred model.

309
00:34:38,880 --> 00:34:43,260
But again, how much difference in AIC or PSC is enough?

310
00:34:44,340 --> 00:34:50,940
Don't just pick the model with the smallest bike because it's one unit lower than the next model.

311
00:34:52,140 --> 00:34:55,370
Think about the complexity between those two models. Right.

312
00:34:56,100 --> 00:34:59,510
There are no key values. And use personal judgment. No.

313
00:34:59,520 --> 00:35:05,400
Even in judgment, remember that parsimony is highly valued by your collaborators.

314
00:35:07,140 --> 00:35:11,490
Can't stress that enough. We are not going to find the right model here.

315
00:35:12,120 --> 00:35:17,910
We're going to find a good model that tries to answer the questions and is justified in our assumptions.

316
00:35:18,480 --> 00:35:25,440
Right. The ICBC. There is one other quantity, and that's the coefficient of determination.

317
00:35:26,490 --> 00:35:33,570
And I want to get to that in a second. Right. So again, in the homework assignment, you should be fitting three models to each of two sets.

318
00:35:33,600 --> 00:35:41,070
Each of them you should be producing a season, because if they don't give you some direction as to what the best correlation structure again is.

319
00:35:42,150 --> 00:35:45,719
There's no reason why you can't look at the actual correlation structure of the residuals.

320
00:35:45,720 --> 00:35:52,410
Remember the lag one, the lag to kind of idea and see if lag one or two, are they just close to each other or do they tend to decrease?

321
00:35:52,980 --> 00:35:59,060
That might give you some idea as to what direct correlation structure might be supported by more than just the AC or the AC.

322
00:36:00,790 --> 00:36:04,989
This only from the equation wouldn't be icy.

323
00:36:04,990 --> 00:36:08,980
Get smaller with higher levels of oh, there's a minus there.

324
00:36:10,990 --> 00:36:14,350
You're penalizing by you're multiplying by negative total data.

325
00:36:14,600 --> 00:36:17,800
Right. So more data helps you, right?

326
00:36:19,690 --> 00:36:23,080
Sorry. Yes, sorry. Let's make that clear in the back.

327
00:36:26,960 --> 00:36:31,490
Currents. There we go. Right. So here motivated by a positive, too.

328
00:36:31,520 --> 00:36:40,850
So as this grow, this number gets bigger. That's bad. In B.C. this gets smaller as the total amount of data gets bigger.

329
00:36:41,270 --> 00:36:45,320
So more data helps you. Thank you. That makes more sense.

330
00:36:45,500 --> 00:36:49,460
Our data should be not a penalty. It should help you out.

331
00:36:54,660 --> 00:37:00,390
All rights to Jean. The material of.

332
00:37:05,290 --> 00:37:09,730
To which she. Here we go. Okay.

333
00:37:10,630 --> 00:37:13,630
Correlation structures. I not think of correlation structure.

334
00:37:16,240 --> 00:37:19,600
Last step. Is How do I decide?

335
00:37:19,630 --> 00:37:23,740
Again, I asked you to tell me, are there group differences, time differences?

336
00:37:23,980 --> 00:37:28,210
And do the time differences vary between the groups? That's inference.

337
00:37:28,630 --> 00:37:36,520
The mean model. So let's talk about inference and model selection in ordinarily squares like

338
00:37:36,520 --> 00:37:40,180
I've been doing so far and extend that to what you should be doing with ACL.

339
00:37:41,350 --> 00:37:45,790
Not a big exception here. Right. We've talked a lot about estimation.

340
00:37:45,860 --> 00:37:48,910
I've told you how to estimate the parameters as well as their variance.

341
00:37:49,960 --> 00:37:55,930
We want to do things like confidence intervals, hypothesis tests and again, try to figure out what the best model is traps.

342
00:37:56,260 --> 00:38:00,730
If we have a lot of parameters for a lot of covariates, we're trying to to win it down.

343
00:38:01,450 --> 00:38:04,270
So let's start with what you learned last year.

344
00:38:06,220 --> 00:38:15,370
So again, back to one observation per person, there's an outcome and there are a bunch of covariates measured at one point in time for everybody.

345
00:38:17,590 --> 00:38:22,210
And we have a regression model, ranking errors, normal distribution and constant variance.

346
00:38:23,350 --> 00:38:31,090
So we assumed that in fact each way has a mean that is a function of covariance and has a variance, right?

347
00:38:31,120 --> 00:38:35,170
Everything is normally distributed. They're independent. Each of the ways is independent.

348
00:38:36,040 --> 00:38:40,330
And then we criminology matrix notation that you've seen before.

349
00:38:41,470 --> 00:38:48,190
So we have a vector of outcomes. We have a vector of covariates for every person that are then put into a design matrix.

350
00:38:49,120 --> 00:38:52,390
And so we have a model that has lean parameters.

351
00:38:53,200 --> 00:38:59,110
The mean is then it's theta and there's a vector of residuals and we end up with a multivariate normal distribution, right?

352
00:38:59,830 --> 00:39:10,540
So again, in all else, we have a vector of observations that have a multivariate normal distribution that there's no correlation in that multivariate.

353
00:39:10,540 --> 00:39:20,200
No, it's a product of independent normals. Right. So again, you got to know these formulas from linear, linear regression.

354
00:39:20,740 --> 00:39:22,870
The parameter estimates are expenses, inverse X,

355
00:39:22,870 --> 00:39:29,710
transpose Y when we have constant variance in independence and therefore the variance of weighted parameters looks like that.

356
00:39:30,520 --> 00:39:35,440
And Sigma Square is estimated from the squared residuals scale by N minus p.

357
00:39:37,240 --> 00:39:42,880
So again, because Beat Ahead is a linear combination of normal random variables,

358
00:39:43,900 --> 00:39:48,720
the regression parameters are also normally distributed and they have meaning data.

359
00:39:48,730 --> 00:39:54,300
They're consistent, right? And they have a variance that comes from that model.

360
00:39:54,450 --> 00:39:56,820
That is a model based variance formula. Right.

361
00:39:57,150 --> 00:40:02,790
And up until a couple of weeks ago, you never thought about anything other than a model based variance, because that's all you ever had.

362
00:40:03,960 --> 00:40:07,190
We can use the diagonal of this variance covariance research.

363
00:40:07,200 --> 00:40:10,200
Remember, your regression parameters are correlated. Right.

364
00:40:10,230 --> 00:40:13,950
We often believe that an intercept that's high is correlated with a negative slope.

365
00:40:14,370 --> 00:40:17,940
They're negatively correlated. We use the diagonal.

366
00:40:18,240 --> 00:40:22,500
Those are the marginal variances for each of the regression parameters.

367
00:40:23,190 --> 00:40:30,510
And if you want to do inference on more than one pair on prem, at the same time, you have to use the variance covariance of those parameters.

368
00:40:34,190 --> 00:40:42,650
So and hopefully this is mostly review. If you want to examine one parameter in the model, leaving all the others in the model.

369
00:40:43,370 --> 00:40:50,990
Right. So beta zero versus not being zero. The easiest thing to do is a wald test, which is typically what you get from RS.

370
00:40:51,590 --> 00:40:58,160
So you take the estimate minus the null value. How different are they from each other relative to sampling variability?

371
00:40:59,200 --> 00:41:05,020
And we want that number to be bigger than two because that's the critical value in a normal distribution.

372
00:41:07,070 --> 00:41:13,010
And you can get a confidence interval. The confidence interval rate is the estimate plus or minus about two standard errors.

373
00:41:15,910 --> 00:41:18,140
So again, this assumes a large sample.

374
00:41:19,850 --> 00:41:26,270
If you have a small sample that the number of observations is small relative to how many regression parameters you've estimated,

375
00:41:27,050 --> 00:41:31,610
then your critical value for the world test isn't necessarily from interval zero one.

376
00:41:32,000 --> 00:41:34,940
It's from a T distribution with minus X freedom.

377
00:41:35,570 --> 00:41:43,670
But again, if N is very large relative to P, the T distribution gets closer and closer to a normal distribution.

378
00:41:43,670 --> 00:41:48,680
The tails get less and less heavy. So rarely do I split hairs in degrees of freedom.

379
00:41:49,460 --> 00:41:55,040
And when I teach this stuff to non-biased folks, I don't even get into degrees of freedom.

380
00:41:55,460 --> 00:42:01,610
Can't say I quite got it when I first saw myself. And why are they called Degrees of Freedom anyway?

381
00:42:02,030 --> 00:42:09,650
So if you just want to stick with what's normal critical values, most of the time you're right in large samples.

382
00:42:12,090 --> 00:42:14,940
If you have multiple parameters. Right.

383
00:42:15,450 --> 00:42:25,350
So again, one of the models I have has four interaction terms and I want to know if I can throw out all of the interaction terms simultaneously.

384
00:42:25,890 --> 00:42:28,950
So I'm going to do inference on all four beta parameters at once.

385
00:42:29,430 --> 00:42:35,940
So there is a contrast there. Al-Bayda is zero. I've got a vector of combinations of beta that are zero.

386
00:42:37,170 --> 00:42:45,030
And so therefore, for example, if I want to say that beta one and beta two are both zero versus not zero,

387
00:42:45,450 --> 00:42:52,680
then the matrix is one times beta one and then one times beta to get these two things at the same time.

388
00:42:53,190 --> 00:43:00,010
Or if you want to say that the difference is again, often I don't look at the difference of two regression parameters, but you can do it.

389
00:43:00,420 --> 00:43:08,060
If these two things have a difference of zero, then the L matrix is one minus one zero theta one minus two.

390
00:43:09,870 --> 00:43:17,050
So again, there are hypothesis as al-bayda zero. I loved linear models when I was in grad school.

391
00:43:17,610 --> 00:43:23,500
Just a dark. All right. Since Beta Hat has an asymptotic neural distribution.

392
00:43:24,130 --> 00:43:28,780
Right. Then L Times Beta also has a known distribution.

393
00:43:29,110 --> 00:43:34,900
So L beta half has an asymptotic normal distribution with mean l data and variance.

394
00:43:35,230 --> 00:43:37,180
Again, something times a variance.

395
00:43:37,180 --> 00:43:44,620
The variance of the variance of something times a random variable is the variance, times that thing squared the multiplier.

396
00:43:44,950 --> 00:43:50,260
And we do that in matrix this way, right. L sigma beta l transpose.

397
00:43:50,980 --> 00:43:56,070
And then we get again a world test. But we square everything.

398
00:43:58,550 --> 00:44:04,310
So we take the estimated values for the contrast minus the normal value of

399
00:44:05,090 --> 00:44:09,860
the square that and we divide essentially by the variance of those contrasts.

400
00:44:10,550 --> 00:44:17,390
And that has a Chi Square distribution under the null hypothesis, again, which is the number of contrast we're looking at.

401
00:44:19,350 --> 00:44:28,140
And that's a large sample. There are degrees of freedom floating around the analog of going from normal to t es five square to f.

402
00:44:28,200 --> 00:44:35,819
You've heard enough tests read. This is the old f test idea that this test statistic has enough distribution with h and and

403
00:44:35,820 --> 00:44:41,400
minus p degrees of freedom that some topically both of those distributions look the same.

404
00:44:43,940 --> 00:44:53,420
The actual degrees of freedom in this class congeals the actual small sample degrees of freedom is unknown.

405
00:44:54,800 --> 00:44:56,150
The best degrees of freedom.

406
00:44:56,870 --> 00:45:05,720
And so you will hear when I was in grad school, I had to learn about Mr. Sanders weight and his name and how his degrees of freedom were computed.

407
00:45:06,260 --> 00:45:09,560
There's something called the Kenward Rogers Degrees of Freedom.

408
00:45:11,450 --> 00:45:16,940
And if you have enough data. How many is a chi square with each degrees of freedom?

409
00:45:19,130 --> 00:45:23,050
I don't even know if all these are programed. Someone's written library.

410
00:45:23,060 --> 00:45:27,050
You guys are in a world in which there's a library for every darn thing that's ever been thought of.

411
00:45:27,650 --> 00:45:33,049
Which isn't to say you have a good life. There are too many libraries in Irish as far as I'm concerned.

412
00:45:33,050 --> 00:45:41,900
But anyway, this is why in girls I don't think they give you p values when you ask for the coefficient table.

413
00:45:43,010 --> 00:45:47,090
It gives you estimates. Standard error and then the ratio.

414
00:45:47,720 --> 00:45:54,720
But it doesn't look up the p values. And that's because it wants you to decide what's the appropriate node distribution to compare to.

415
00:45:57,260 --> 00:46:02,630
So maybe Ingolstadt asks you if you want to take your degrees of freedom.

416
00:46:03,470 --> 00:46:07,040
Anyway, I wouldn't worry about it unless you've got a really small sample.

417
00:46:08,060 --> 00:46:12,440
Oh. All that bothers tests are really a comparison of two nested models.

418
00:46:12,440 --> 00:46:15,950
Right. If I want to throw out a parameter, I'm comparing two models.

419
00:46:17,060 --> 00:46:21,830
So model selection is the topic I hate teaching the most.

420
00:46:23,220 --> 00:46:26,150
Because there is no right model. Right.

421
00:46:26,160 --> 00:46:31,740
There is backward selection, there's forward selection, there is stepwise selection, there is other selections.

422
00:46:33,390 --> 00:46:37,170
And people come to me and say, Well, what's the right way to do it right there?

423
00:46:37,170 --> 00:46:40,680
It's pick your favorite model selection process.

424
00:46:41,070 --> 00:46:46,650
Right. But again, if you're just trying to throw out a collection of parameters, you usually have two nested models.

425
00:46:47,190 --> 00:46:50,849
Right. And so we set both models to the data.

426
00:46:50,850 --> 00:46:54,360
We have my estimates under the null model,

427
00:46:54,360 --> 00:46:59,850
where some of the parameters are forced to be zero versus the alternative, where they're all estimated freely.

428
00:47:02,220 --> 00:47:10,230
And we compare nested models in our test and we saw this in a lot less likely a ratio test.

429
00:47:11,150 --> 00:47:16,250
There are lots of things we can describe as a the number of degrees of freedom, our d the number of parameters, right?

430
00:47:16,260 --> 00:47:22,620
The difference in how many parameters are removed or the number of parameters that differ between the two models.

431
00:47:22,860 --> 00:47:26,090
That's the degrees of freedom and that's a chi square.

432
00:47:26,100 --> 00:47:31,860
But again, in small samples, we'll use an F and minus feelings of freedom, perhaps.

433
00:47:32,460 --> 00:47:38,190
And that's what you get from the ANOVA table. Let's say that you saw at 650.

434
00:47:40,880 --> 00:47:43,100
Often we're going to have non nested models.

435
00:47:44,210 --> 00:47:51,830
And again, I'm sure you learned that there is no likelihood theory for comparing non nested models and that's perhaps if you saw AIC,

436
00:47:51,830 --> 00:48:02,200
obviously that's where you saw them first. This is why with correlation matrices we have to use AC because they're usually not nested with each other.

437
00:48:03,070 --> 00:48:05,889
So we've already talked about AC. Now there's no queues.

438
00:48:05,890 --> 00:48:11,830
Now I'm talking about ordinarily squares, there's no Q in the AC DC because it's the analog in ordinary squares.

439
00:48:12,200 --> 00:48:16,690
She was zero. R-squared. You probably haven't seen R-squared return like this.

440
00:48:16,720 --> 00:48:24,010
Least I didn't know when I was in grad school. Remember, R-squared is the percentage of variability explained by your regression model.

441
00:48:24,880 --> 00:48:31,000
So again, the numerator up there in the first equation, we've got the Y minus X.

442
00:48:31,000 --> 00:48:38,620
I made a hat. So that's the square of the residuals relative to the variability in the data from the outset.

443
00:48:38,740 --> 00:48:45,700
Y minus. Right. That's just the sample variance. So it's one minus that how much variability?

444
00:48:46,060 --> 00:48:53,740
Again, that ratio tells me how much leftover variability there still is in the residuals relative to before the regression model was fit.

445
00:48:54,280 --> 00:48:57,760
One minus that tells me the value of the model for explaining variability.

446
00:48:58,420 --> 00:49:03,580
So that's R squared. So again, r squared is related to a ratio of two variances.

447
00:49:04,420 --> 00:49:13,120
It's the variability in the data before the model. And it's the variability after I fit my model to paying the ratio of those two things.

448
00:49:13,510 --> 00:49:17,800
One of the points of regression is to explain why everybody has a different Y value.

449
00:49:18,220 --> 00:49:23,440
Why is there variability in something? Well, they're in a model like covariance of to explain it.

450
00:49:25,120 --> 00:49:33,880
And then I give you this last equation here. So it is also based on likelihood you can rate R squared as a ratio of likelihoods.

451
00:49:34,420 --> 00:49:43,570
And there it is. That lovely equation there. We're going to use this equation to come up with an R squared analog for generalized these squares

452
00:49:44,260 --> 00:49:50,230
because we'll get a beta here from my again with the correlation matrix I fit as a compound symmetry.

453
00:49:51,270 --> 00:49:53,070
And I get a bit ahead and there's a likelihood.

454
00:49:53,910 --> 00:50:02,520
I compare that to a no model, which in ordinarily squares was the intercept only model, no covariates in the model.

455
00:50:04,860 --> 00:50:08,670
Woops. When do I must be in the next set of slides.

456
00:50:09,120 --> 00:50:16,080
Maybe should have broken. Is that correct. So this is a well let's say you should be familiar with the last two terms.

457
00:50:16,320 --> 00:50:21,030
You should be familiar with R-squared. And I've written it in a new way for you.

458
00:50:22,300 --> 00:50:27,180
And now we're going to take these three ideas and we're going to extend them to us.

459
00:50:28,480 --> 00:50:35,770
And it is 5 to 4. I ain't thrown a lot of information at you guys.

460
00:50:37,540 --> 00:50:42,850
I apologize. Let me down whenever you need me to see this slide for two slides in this one.

461
00:50:43,600 --> 00:50:46,690
Oh, maybe I can put those together in future years. All right.

462
00:50:50,140 --> 00:50:56,680
All the inferential ideas you learned nested models f tests while tests for a single parameter.

463
00:50:57,190 --> 00:50:58,810
All of that stuff carries over here.

464
00:50:59,170 --> 00:51:07,090
The only thing we're doing different is putting a weight matrix in estimation and the variance that is used in all the inference.

465
00:51:08,680 --> 00:51:14,740
But remember that inference from data is now conditional on both what we assume the correlation structure to be.

466
00:51:16,410 --> 00:51:24,420
And the estimate we use for those those parameters in the covariance matrix and sigma theta, that could be maximum likelihood or rebel.

467
00:51:26,820 --> 00:51:33,870
Again, everything is consistent. My data here gets to the right thing on average.

468
00:51:34,590 --> 00:51:38,010
That doesn't mean your beta is going to be closer than mine, right?

469
00:51:38,250 --> 00:51:46,140
For your individual data set. Who knows how close you are. But the distribution of all possible data hats is centered around the truth.

470
00:51:47,160 --> 00:51:51,959
So there is I don't mean to say you're going to get the same answer regardless of what you do, you'll get slightly different answers.

471
00:51:51,960 --> 00:51:58,830
But they're all consistent for whatever that does for you. Right. All of the inferential theories based on that sum likelihood.

472
00:51:58,920 --> 00:52:07,590
So again, if you're going to use Remo, just remember that you're plugging in estimates into a likelihood and you didn't get likelihood based,

473
00:52:07,650 --> 00:52:15,600
excellent likelihood based parameters. So the theories are a little fuzzy, but probably close enough that we don't need to talk about it further.

474
00:52:16,980 --> 00:52:21,660
So I showed you AIC and Vicky for selecting correlation structures.

475
00:52:23,310 --> 00:52:26,640
You can also start talking about looking at an R squared value.

476
00:52:28,110 --> 00:52:32,160
R squared is a wonderful concept and multiple linear regression.

477
00:52:33,560 --> 00:52:37,910
That everybody thinks exists for every other regression approach.

478
00:52:39,290 --> 00:52:42,290
So maybe you will saw an R-squared value for logistic regression.

479
00:52:42,290 --> 00:52:46,370
Did you see anything like that in 651? You remember?

480
00:52:49,050 --> 00:52:52,770
Again, investigators will still ask me what you just did was logistic regression for me.

481
00:52:52,770 --> 00:52:58,230
Time what's our square? Not have one that doesn't fit here.

482
00:52:59,070 --> 00:53:03,750
But there is a there is a pseudo or square. We do the same thing for glass.

483
00:53:03,750 --> 00:53:11,580
We use that last concept that a glass model has an R-squared value, and it's important for that last formula there.

484
00:53:12,000 --> 00:53:21,389
Again, it's a ratio of two likelihoods. It's the likelihood of my fitted model relative to a no model in ordinarily squares.

485
00:53:21,390 --> 00:53:24,960
The null model was clear. It was the model with no covariance.

486
00:53:25,740 --> 00:53:32,820
What's the null model here? Is it getting rid of all the regression parameters and assuming independence?

487
00:53:34,230 --> 00:53:41,940
Is it? Well, I know the correlation structure, so I'm going to stick with that and only make the no model in the mean structure.

488
00:53:42,460 --> 00:53:47,880
Okay. There's a little bit of fuzziness here. And what's the non model that I'm going to use for the R-squared here?

489
00:53:49,620 --> 00:53:53,790
So less care about the absolute value of our square that you come up with.

490
00:53:54,450 --> 00:54:02,729
What is important is that if you're going to use R-squared to compare goals models, you should use the same no model, right?

491
00:54:02,730 --> 00:54:07,440
The same reference for everybody so that they're all being compared to the same reference,

492
00:54:07,920 --> 00:54:11,580
so that when you're comparing them, you don't want R-squared. It makes sense.

493
00:54:14,640 --> 00:54:19,380
Model selection for generalized least squares can be a never ending loop.

494
00:54:21,000 --> 00:54:25,049
And in this class, we're not typically doing model selection.

495
00:54:25,050 --> 00:54:31,920
I'm asking you about a group effect and a time effect, and you can think of that as model selection by including or excluding variables.

496
00:54:32,760 --> 00:54:38,730
But we're not really going to do model selection in this class. I'm going to give you ten covariates and ask you to come up with a best model.

497
00:54:39,860 --> 00:54:45,890
But as you're doing longitudinal data analysis, there is there's an extra model here.

498
00:54:45,900 --> 00:54:49,460
I've got the mean structure and I've got the correlation structure, model models, those two models.

499
00:54:50,870 --> 00:54:55,580
What do you do here? Suppose you have a total of capital P covariates that could be or not be in your main model.

500
00:54:57,620 --> 00:55:02,960
Okay. So I'm going to say, well, I think I've got a lot of progressive, so I'm going to stick with that.

501
00:55:04,220 --> 00:55:08,360
And now you fit a bunch of goals models and you do some inference and you say, Oh yeah,

502
00:55:08,360 --> 00:55:12,740
that variables should come out and I should have an interaction with this term. And I like that model.

503
00:55:14,090 --> 00:55:17,690
Now what about your correlation? Do you want to go back and check that?

504
00:55:18,710 --> 00:55:22,610
Because now it's conditional on what's in the mean model. Oh, God.

505
00:55:22,760 --> 00:55:26,600
Do I then have to go back and fix my model and. Oh, God, then I need to go Six Sigma.

506
00:55:27,290 --> 00:55:32,780
Right. And you can do this forever. So don't get caught up in this trying to perfect model stuff.

507
00:55:35,090 --> 00:55:41,600
Just, again, don't try to optimize this process. Use your common sense and personal.

508
00:55:42,650 --> 00:55:46,410
I have a longitudinal model I like on symmetry. I'm going to stick with that.

509
00:55:46,430 --> 00:55:48,560
The residuals seem to tell me that that's a good idea.

510
00:55:49,040 --> 00:55:54,860
Then I'll pare down the main structure and probably won't go back and try and look at the correlation structure again.

511
00:55:56,880 --> 00:56:00,030
You're not trying to find the right model. We're trying to find a good model.

512
00:56:01,050 --> 00:56:05,700
So don't get caught up in this concept infinite loop of ten different correlation

513
00:56:05,700 --> 00:56:09,990
structures and three way interactions in the main model and so forth.

514
00:56:11,460 --> 00:56:15,950
You're not going to get much out of that exercise except ill health.

515
00:56:17,520 --> 00:56:26,610
All right. So I'm asking much for you in homework one. But as you move along, for instance, in 699, you'll probably get a much more complicated.

516
00:56:26,740 --> 00:56:28,500
You will get a much more complicated problem.

517
00:56:29,160 --> 00:56:36,000
And they're going to give you a longitudinal study with 150 people, ten covariates, missing data all over the place.

518
00:56:37,050 --> 00:56:38,340
They're going to ask you to assign a model.

519
00:56:40,070 --> 00:56:48,050
Think about how you're going to do that in a smart way that is feasible in the week and a half that you get to analyze the data.

520
00:56:49,730 --> 00:56:52,280
It's a fun class. It really is.

521
00:56:52,700 --> 00:57:02,810
All right, Rasmussen, I do want to do a poll in work, but just to give you some more comfort about homework to some more comfort.

522
00:57:03,790 --> 00:57:09,780
Maybe less uncomfortable. I'm going to show you what I did with glass with the labor pin data.

523
00:57:09,890 --> 00:57:14,830
Just to give you some ideas, this again, everything I show you is my approach.

524
00:57:15,250 --> 00:57:20,770
It is not the best approach. It's a pretty good approach, but I'm not going to say it's the best.

525
00:57:21,670 --> 00:57:28,890
So recall these sets of data here, the set of data. I have two groups of women who are being measured for a level of pain during childbirth.

526
00:57:28,900 --> 00:57:34,150
Some got a treatment and some didn't. And we saw some very distinct differences between the two groups.

527
00:57:34,960 --> 00:57:39,310
The intervention seemed to curb the pain over time to eliminate it.

528
00:57:40,390 --> 00:57:45,460
Excuse me. While the placebo, the pain just kept getting worse and worse over time.

529
00:57:46,060 --> 00:57:51,310
So any model I would sit here, I would expect to maybe not see a group difference the baseline.

530
00:57:52,630 --> 00:57:58,210
But then certainly there is a time effects and the time effect is different between the two groups.

531
00:57:58,330 --> 00:58:06,460
That's what I see in the picture. And I did some leg, one leg, two correlations on the observations, separate for placebo and intervention.

532
00:58:07,330 --> 00:58:15,190
And, you know, there's a good indication here that correlation is taking over time between observations as they move further apart.

533
00:58:16,950 --> 00:58:22,780
Everything we're doing right now, we're assuming the same correlation structure for every person at the same correlation.

534
00:58:22,800 --> 00:58:28,920
If I do a leg one correlation, it's the same in my model for every person, regardless of their treatment group,

535
00:58:29,790 --> 00:58:33,030
you might say, Well, there's distinctly more correlation in the placebo group.

536
00:58:34,650 --> 00:58:39,719
We're going to talk about ways that you could actually model separate correlation parameters for the placebo group.

537
00:58:39,720 --> 00:58:43,680
And the intervention group outside of you could fit two Jill's models, right?

538
00:58:43,680 --> 00:58:48,180
You could do a jealous model for the placebo women in a jealous model for the intervention women.

539
00:58:48,990 --> 00:58:53,910
But we don't like to do that right. We like to analyze everybody at once and gain some efficiency.

540
00:58:55,170 --> 00:58:59,459
So we're not going to worry about the fact that the correlation coefficients are higher placebo.

541
00:58:59,460 --> 00:59:03,630
I just wanted to see the trend that probably they are. One is probably the way to go.

542
00:59:05,410 --> 00:59:11,800
But based upon what I just saw here, how do we choose the models to further maintain scores over time?

543
00:59:12,100 --> 00:59:18,570
And are there differences among the groups? And so I did a regression model, right?

544
00:59:19,090 --> 00:59:23,549
Dummy variables for group. A treated time is continuous.

545
00:59:23,550 --> 00:59:29,760
In this model. It seemed like the line was sufficient for this set of data and interaction to.

546
00:59:30,600 --> 00:59:38,730
And this is what I got here. I got coefficient estimates, I got model based standard errors based upon independence.

547
00:59:39,630 --> 00:59:48,170
I know those are wrong. I don't ever want to use those. There's got to be some correlation to these data and the sandwich standard errors.

548
00:59:51,990 --> 00:59:54,660
Don't go anywhere. I'm not going to get into that again.

549
00:59:56,040 --> 01:00:03,720
Regardless of the standard errors, remember, significance is measured by the ratio of the estimate to its sampling variability.

550
01:00:04,470 --> 01:00:09,510
All of those coefficients are bigger than the standard errors.

551
01:00:10,830 --> 01:00:19,890
And again, to make things clear, you might say, Well, Tom, the growth coefficient of 3.4 is much smaller than its variability.

552
01:00:21,020 --> 01:00:27,110
They can't do inference on a group looking at that one alone because group is in the interaction.

553
01:00:28,250 --> 01:00:31,550
Please. When you. This is why interaction models are a pain in the butt.

554
01:00:32,510 --> 01:00:39,020
They're really good. But I can't do inference on the group variable because it's in both.

555
01:00:39,260 --> 01:00:46,850
Right. Don't tell me there aren't group differences because 3.4 over 6.9.7 or seven is really small.

556
01:00:48,100 --> 01:00:51,310
Well, there is a group effect. It's in the interaction.

557
01:00:51,520 --> 01:00:55,540
The interaction is wildly significant. Right, 17 over three.

558
01:00:57,310 --> 01:01:00,520
So anyway, it looks like there are group differences over time.

559
01:01:04,290 --> 01:01:08,820
So that's the first approach. Like I said, oh, well, this sandwich estimate, we're done.

560
01:01:13,620 --> 01:01:18,870
I could play structure on the correlation and use the standard errors that are model based from that assumption.

561
01:01:20,310 --> 01:01:25,440
So as an academic exercise, I want to make this clear because we're in a classroom right now.

562
01:01:25,860 --> 01:01:28,110
I'm going to try nine different correlation structures.

563
01:01:29,250 --> 01:01:34,860
Please don't ever do that unless you have a lot of time and you like to do this in your spare time.

564
01:01:35,430 --> 01:01:39,060
Right. Don't think about every possible correlation structure there is.

565
01:01:39,300 --> 01:01:47,280
Use some common sense. Right. I can already tell from my idea that some sort of became correlation structure is probably going to win out.

566
01:01:47,790 --> 01:01:55,860
And I want to point out something here that I was scratching my head at, and it took me a while to figure out, but I'm going to try nine main things.

567
01:01:56,220 --> 01:02:01,020
Unstructured correlation. Can every place off the diagonal it can have its own value.

568
01:02:02,430 --> 01:02:07,170
Constant variance versus non-cancer variance. So the diagonal could have different values on it.

569
01:02:07,590 --> 01:02:11,450
Lots of parameters there. Compound symmetric or exchangeable.

570
01:02:11,760 --> 01:02:17,850
Every observation has the same correlation with cousin variance or knockouts of variance auto regressive,

571
01:02:17,850 --> 01:02:21,090
decaying correlation structure with constant variance or not in constant rates.

572
01:02:22,140 --> 01:02:30,540
Again, a continuous error. One model where the exponent isn't just 1 to 3 or four, it's the actual measure of time span in the exponent.

573
01:02:31,500 --> 01:02:36,090
Exponential correlation and Gaussian correlation. I showed you those formulas in the slides.

574
01:02:36,090 --> 01:02:40,290
They're all decay type approaches and I just took Wisconsin variance with those.

575
01:02:41,310 --> 01:02:47,640
All of that is in my our code for the labor pain data. I have a file called examining correlation structures, I think.

576
01:02:49,800 --> 01:02:56,340
And so what I got or the issues and bases of all nine of those models and there they are once again,

577
01:02:56,340 --> 01:03:02,940
models one or two have a lot of parameters, unstructured, unstructured.

578
01:03:03,240 --> 01:03:07,200
And then I allow for all these different variances. That's a lot of parameters.

579
01:03:08,940 --> 01:03:12,600
Oh, perfect. All right.

580
01:03:12,750 --> 01:03:18,970
Compound symmetry has among the highest bases and it's high enough, right?

581
01:03:18,990 --> 01:03:22,470
We're talking about 3940 something versus some of them at 3800.

582
01:03:23,400 --> 01:03:27,780
Seems like a big enough difference that compound symmetry might not be my choice.

583
01:03:31,830 --> 01:03:37,950
These five right here all have among the lowest, and those are the R one versions.

584
01:03:37,950 --> 01:03:41,400
And this idea of exponential decay.

585
01:03:42,760 --> 01:03:45,819
And you're going to see that three of them have exactly the same age.

586
01:03:45,820 --> 01:03:49,660
I see. And I see. I just even kind of close. Exactly.

587
01:03:50,860 --> 01:03:55,530
And when I saw that, I went, what should I do?

588
01:03:56,020 --> 01:04:02,379
And that is because. All right. So I'm going to remove them and remove the other ones, too, because I'm going to remove sorry.

589
01:04:02,380 --> 01:04:11,180
Number six. Number six is pretty close, but why would I model one constant variance versus constant variance when I get three points of eight.

590
01:04:11,200 --> 01:04:18,670
I see. Yeah, that's right. Even if it were three points lower, if number six were three points lower than all the others, it's still throw it out.

591
01:04:19,180 --> 01:04:22,720
It's just not worth it relative to the number of the parameters that I've added into the model.

592
01:04:24,670 --> 01:04:31,030
So all three of the models I showed you that I could choose from, they all have identical correlation structures.

593
01:04:31,030 --> 01:04:39,580
They're just parameters differently. So again, remember, we have a correlation parameter relative to the difference between the visit numbers.

594
01:04:40,090 --> 01:04:43,780
So that's a one, two, three or four model.

595
01:04:43,780 --> 01:04:51,580
Seven takes the actual time points and looks at the difference between the time points and then this exponential is this crazy looking thing.

596
01:04:52,660 --> 01:04:57,970
But you can look at this and see that row seven is just a function of real eight.

597
01:04:59,530 --> 01:05:03,550
So again, then I am like, why did I ever bother teaching this to you guys?

598
01:05:05,200 --> 01:05:09,849
I can imagine there's a world in which people like this exponential function decaying over time.

599
01:05:09,850 --> 01:05:14,620
There's lots of things that have exponential decay. So maybe somebody said, I like this better in my world.

600
01:05:15,310 --> 01:05:23,930
But those two models are the same, and Model five is equivalent to model seven only because of the design of the study.

601
01:05:24,010 --> 01:05:29,260
And in general, those two approaches will not give the same result. It's just that time.

602
01:05:29,260 --> 01:05:33,489
One is point five times two is one, and three is 1.5.

603
01:05:33,490 --> 01:05:38,319
Everything's multiplied by .05. So that comes out of everything.

604
01:05:38,320 --> 01:05:46,149
There's the square root. There's a square root in there. That's what is it is be that seven square root.

605
01:05:46,150 --> 01:05:49,990
It is real five anyway. So those three models that are being the same.

606
01:05:50,320 --> 01:05:54,940
So pick one and then I get some results here.

607
01:05:57,040 --> 01:06:02,170
So here are the results I got with air. One correlation I get model based standard errors.

608
01:06:02,440 --> 01:06:12,100
Of course, I can also go ahead and get additional sandwich to see if anything left over is still there, but don't really see any leftovers here.

609
01:06:14,240 --> 01:06:18,830
I get the same conclusion that I got from independents with sandwich senators.

610
01:06:19,430 --> 01:06:25,390
However. The group coefficient with air one is pretty close to zero.

611
01:06:26,980 --> 01:06:31,030
So efficient using independence plus standard air sandwich sodas is three.

612
01:06:32,500 --> 01:06:36,200
If you're interested in parameter estimates. Right.

613
01:06:37,340 --> 01:06:40,639
They're a little bit different from the other ones.

614
01:06:40,640 --> 01:06:48,320
Maybe not so much so. So, again, they're both consistent, but it doesn't mean they're going to be identical from the same data.

615
01:06:48,950 --> 01:06:57,230
And then you have to kind of go back to the the plots. Right. And say, you know, again, look at the where the two means line up.

616
01:06:59,570 --> 01:07:08,770
Is there a difference at time zero for a time to right size. So and this is where you have to use your own personal judgment.

617
01:07:08,780 --> 01:07:12,709
I'm not saying either one of those is the right answer. An exam question.

618
01:07:12,710 --> 01:07:17,010
I would never an exam question that says which of those is the right model?

619
01:07:17,060 --> 01:07:20,640
It's a trick question because they're both right. Right. No.

620
01:07:21,530 --> 01:07:24,560
So this is where personal judgment comes in. This is why we have jobs.

621
01:07:26,150 --> 01:07:36,720
Think about which one of those is better. So I typically would go with the modeling, the correlation structure, right?

622
01:07:37,780 --> 01:07:43,540
But all of these results, everything I have on these slides is in my R code applied to the library data.

623
01:07:43,540 --> 01:07:50,560
If you want to see how I got those numbers and so forth, things get a lot more complicated when you have.

624
01:07:51,640 --> 01:07:56,230
So I have a dataset in which I have a piecewise linear time effects that pipe data.

625
01:07:57,100 --> 01:08:01,060
So I've got two time parameters and then I have an interaction with group.

626
01:08:01,960 --> 01:08:06,340
So the number of parameters gets more complicated, trying to figure out what to keep and what to throw out.

627
01:08:07,010 --> 01:08:16,180
I've got pretty, pretty here anyways. So yes, you are running into datasets where these answers are not immediately obvious.

628
01:08:16,600 --> 01:08:21,430
Some of these datasets, it's going to be obvious. Things are changing over time or that there's a group difference.

629
01:08:21,970 --> 01:08:25,600
But if you're running into an asset where things are not clear now, please let me know.

630
01:08:26,860 --> 01:08:30,009
My goal is to look at homework number one submissions.

631
01:08:30,010 --> 01:08:38,020
So I have an idea of what these assets are doing. So again, hopefully homework one is already informed you what homework two is going to look like.

632
01:08:38,950 --> 01:08:42,520
That's my goal here. That's.

633
01:08:44,710 --> 01:08:55,990
Everyone's excited. A lot of goals. Um, my goal Friday again is to work more on homework two with you everywhere.

634
01:08:58,420 --> 01:09:01,690
Next week. I don't know what Wednesday is going to look like.

635
01:09:02,470 --> 01:09:05,830
We really should start covering new stuff, but you guys slow me down.

636
01:09:06,280 --> 01:09:09,460
I'm happy to review stuff before the test on Friday. So.

637
01:09:10,270 --> 01:09:14,950
And then us October. Yeah.

638
01:09:15,730 --> 01:09:19,060
All right, let me start Panopto.

