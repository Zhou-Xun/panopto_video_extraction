1
00:00:00,420 --> 00:00:05,400
Your conversation will be recorded. Oh, it's just so wordy.

2
00:00:07,560 --> 00:00:11,880
Okay, I'm just telling you that. Let's just start recording this lecture.

3
00:01:56,260 --> 00:02:01,540
Yeah. There are some general questions about the classified benefit, everyone knowing that.

4
00:02:01,810 --> 00:02:10,540
So probably the Kaptur project were looking for county level data, either weekly, monthly, whatever you can get for your specific robberies.

5
00:02:10,690 --> 00:02:16,000
Yeah. Now, once you got into the second half of the project, were you doing like data analysis?

6
00:02:16,300 --> 00:02:23,620
Do we have like vaccine data at the county level because it or like I was able to find

7
00:02:24,040 --> 00:02:30,730
infection data and we're talking about at the county level weekly through John Hopkins,

8
00:02:31,060 --> 00:02:39,580
but not like subdivided by age or sex. So I'm just curious, like, will we be will we have county level data to work with?

9
00:02:39,610 --> 00:02:42,610
Yeah, that's the one that I got from CDC website.

10
00:02:44,050 --> 00:02:50,230
The data that you have for the for the the homework.

11
00:02:50,290 --> 00:02:54,790
Right. So that's state level data. Oh, oh.

12
00:02:55,120 --> 00:02:58,780
Okay. I saw. Okay. Yeah, that's data level. Yes, of course.

13
00:02:58,820 --> 00:03:03,400
Yeah. So would we assume that each county in the state has the same vaccine performance?

14
00:03:04,050 --> 00:03:08,440
Yeah. And then how would we get the mortality data at the county level?

15
00:03:08,680 --> 00:03:11,950
Because the CDC data is for the entire United States.

16
00:03:13,660 --> 00:03:19,210
Yeah. So you if you want to get county level data, then you need to make a special requests, I guess.

17
00:03:19,270 --> 00:03:26,679
Okay. So would it be okay to use county so county level data that isn't based by age from like

18
00:03:26,680 --> 00:03:32,800
John Hopkins and then the vaccine data from the CDC by combining data sets like that with.

19
00:03:33,070 --> 00:03:36,130
Does that make sense? Yeah.

20
00:03:37,270 --> 00:03:49,240
So what I did last year is that we come up with some, uh, how to see the factors to.

21
00:03:55,630 --> 00:04:01,150
To create like age groups specific the sort of vaccination rate.

22
00:04:01,840 --> 00:04:07,000
I can go back to see how those factors have been created.

23
00:04:07,750 --> 00:04:11,760
So we did we we have done that over vaccine performer.

24
00:04:12,110 --> 00:04:14,500
It one time. Okay.

25
00:04:14,650 --> 00:04:23,680
Because we don't have mortality data from the CDC at the county level and then the Johns Hopkins mortality data is just at the county level,

26
00:04:23,680 --> 00:04:34,900
not subdivided by age or sex. That's why I'm saying that the there was a some publication to indicate that the age distribution about the mentality.

27
00:04:35,740 --> 00:04:41,140
So that could be the factor we can use the county level to distribute.

28
00:04:41,950 --> 00:04:48,810
So so there is a. So, of course, that's a estimate.

29
00:04:48,970 --> 00:04:54,010
It's sort of the distribution of the mentality across different age groups.

30
00:04:54,090 --> 00:05:03,220
Yes. So that's something we can use to create age groups sort of specific mortality data.

31
00:05:03,420 --> 00:05:12,879
Yes. But I think my worry there is, are you not creating some type of underlying distribution or data work for every single county?

32
00:05:12,880 --> 00:05:16,630
The distribution of mortality and age is the exact same.

33
00:05:17,110 --> 00:05:20,680
Does that not lead to any modeling problems?

34
00:05:21,220 --> 00:05:28,570
If you're assuming like two thirds of the mortality is that individual 65 plus across all counties in the US,

35
00:05:28,690 --> 00:05:33,870
doesn't that create like a David dependent structure on mortality and age?

36
00:05:33,880 --> 00:05:37,880
Like, I'm just I don't actually have an answer. Yes, but that's kind of what I mean.

37
00:05:37,920 --> 00:05:43,690
Yeah, I think that your worries are fine. I, I, I think that.

38
00:05:46,270 --> 00:05:49,299
I have to go back to the calculation we did last year.

39
00:05:49,300 --> 00:05:57,850
I think this distribution is time varying is not that constant for all the time and.

40
00:06:02,090 --> 00:06:10,219
I think we still have time to make a request because CBC allows you to request specific

41
00:06:10,220 --> 00:06:16,920
data because the way they publish on the website is under some kind of file size control.

42
00:06:16,940 --> 00:06:21,290
They don't want take a big data set that if you want to have the data,

43
00:06:21,950 --> 00:06:33,970
there is a place that we can request what type of the data you want and then you can submit it and then they can process that for you.

44
00:06:33,980 --> 00:06:38,480
It takes maybe two weeks or so full in process that.

45
00:06:38,660 --> 00:06:43,280
Yeah. And yeah I yes.

46
00:06:43,280 --> 00:06:46,860
Because that doesn't give us a lot of time considering it. So. Right, exactly.

47
00:06:46,880 --> 00:06:55,310
Yes. So the for the age group, I have to go back to look at how that was created last year.

48
00:06:55,310 --> 00:07:05,420
We did do this of each group specific sort of calculation to get the sort of mortality rate across the entire country.

49
00:07:05,490 --> 00:07:16,040
Yeah. So there were some like data that people can use for that to divide the total pilot, but some kind of called deconvolution.

50
00:07:16,460 --> 00:07:23,720
So you sort of deconvolution the aggregate into subgroups by age and stuff like that.

51
00:07:23,720 --> 00:07:33,020
Yeah. Okay. And so for the midterm project, the group project for just to be the capture, we have to incorporate the mortality data.

52
00:07:33,440 --> 00:07:37,110
No, no, you just report whatever data you capture.

53
00:07:37,610 --> 00:07:44,990
Right. And so the part in this memo that says run within binary comparison analysis if necessary.

54
00:07:45,080 --> 00:07:48,560
Yeah. What does that entail exactly. Because I thought that meant using mortality.

55
00:07:48,770 --> 00:07:56,420
Yeah. Yeah. So I, I was, you know, if you have your data, I don't know what data you've got, unemployment rate or something like that.

56
00:07:56,930 --> 00:08:03,170
So you can demonstrate the, uh, the unemployment rate distribution across.

57
00:08:03,410 --> 00:08:10,220
You have monthly data right across the entire country and then you can have the mortality data from

58
00:08:10,870 --> 00:08:17,710
the entire country now across different age groups and you save this to things are somehow similar.

59
00:08:17,720 --> 00:08:24,340
You can give to maps them and you can identify where are the highly expressed

60
00:08:24,740 --> 00:08:30,200
sort so things cross the map and or you can do state has the highest or lower.

61
00:08:30,200 --> 00:08:40,909
Yeah you can you can see if your is right then you can have vaccine and you can see that or you can basically plotted this scatterplot, right.

62
00:08:40,910 --> 00:08:52,340
So, so that you know, so you can see that the x axis will be, you know, the the the county level monthly unemployment rate.

63
00:08:52,340 --> 00:08:59,600
And this is sort of monthly vaccination or no, that would require us to have monthly vaccination,

64
00:08:59,600 --> 00:09:03,020
which you don't really have to think of necessarily for each.

65
00:09:03,310 --> 00:09:06,680
But this is where I'm running into that issue. Right.

66
00:09:07,130 --> 00:09:12,140
So I can't really make that analysis without that data this specific.

67
00:09:12,440 --> 00:09:15,650
Yeah, but you do have mortality rate, right?

68
00:09:16,220 --> 00:09:19,460
You don't have mortality rate among county level. No.

69
00:09:21,470 --> 00:09:27,950
Well, at least there's no data that we have on campus that I think is available in the CDC website.

70
00:09:29,090 --> 00:09:34,730
They are not divided into age groups, but they don't see the county level mortality rate.

71
00:09:35,180 --> 00:09:41,210
I don't know exactly. Okay. I can go to find out for you because that's the data we used.

72
00:09:41,420 --> 00:09:47,749
So and so does it say like this dude, there is a link to each state, like the link.

73
00:09:47,750 --> 00:09:53,510
So for each state we have like different data sets for more data. And for some states it's split up into counties.

74
00:09:54,260 --> 00:09:58,730
But some states do not have this I mean, county level mortality there.

75
00:09:59,930 --> 00:10:06,110
I mean, maybe they do, but it's like 50 I think we call the data directly from Johns Hopkins website.

76
00:10:06,650 --> 00:10:10,910
Yes. John Hopkins has it not subdivided by age groups.

77
00:10:11,420 --> 00:10:15,649
Yeah, but you have the county level. Yes. County level mortality.

78
00:10:15,650 --> 00:10:22,610
That that's what we use. Yeah. Okay. Yeah. So yeah, that's, that's one place I think you can have it.

79
00:10:22,970 --> 00:10:26,450
Okay. So what do we do?

80
00:10:26,450 --> 00:10:31,670
Last year was I think we there are some publication to tell that what's the the relative

81
00:10:32,150 --> 00:10:38,750
distribution of the mortality across different age groups that some of the factors

82
00:10:38,750 --> 00:10:45,920
that you can use and I think there is a time varying distribution like in a different

83
00:10:45,920 --> 00:10:53,749
period of the pandemic associative is different types of like the variance right.

84
00:10:53,750 --> 00:11:00,690
So Delta and Omnicom and after that there are different type you look in the distribution of the.

85
00:11:01,190 --> 00:11:04,250
The talent is cross the age groups. That's the factor we use.

86
00:11:04,450 --> 00:11:10,609
I say, yeah, yeah, yeah, yeah.

87
00:11:10,610 --> 00:11:13,970
We just allies like available, right.

88
00:11:14,210 --> 00:11:19,340
So probably Johns Hopkins stayed up to call to do this sort of association.

89
00:11:20,210 --> 00:11:25,640
100 groups anybody else find the better data source that for your classes.

90
00:11:29,720 --> 00:11:35,410
We were mainly looking at the CDC. They later said, you know, maybe we should look at the Johns Hopkins data.

91
00:11:35,440 --> 00:11:40,680
I think that we used the Johns Hopkins data. Yeah, we were looking for a needle stick which has going to the fore.

92
00:11:41,130 --> 00:11:46,150
Also, we use the data from Harvard. They do worse, data worse.

93
00:11:46,170 --> 00:11:52,770
That's the place I can go to see. I know the guy who is doing the maintenance of database at Harvard.

94
00:11:52,800 --> 00:11:57,840
I could ask the guy that whether or not they have some better resolution data out there.

95
00:11:58,530 --> 00:12:10,020
And yeah, two years ago we did use the Johns Hopkins data and the Harvard data was yeah.

96
00:12:10,530 --> 00:12:16,259
To to get that data. At that time, the CDC database was not well constructed.

97
00:12:16,260 --> 00:12:20,030
So everybody somehow really. Johns Hopkins database.

98
00:12:20,070 --> 00:12:25,540
Yeah. Yeah.

99
00:12:25,540 --> 00:12:30,100
I can find some data source and to see for that.

100
00:12:30,140 --> 00:12:40,090
Yeah. You know, no questions. So we were thinking like for the data service find we do the all the counties and like all the states,

101
00:12:40,090 --> 00:12:44,440
but like what we were thinking is obviously if it's fine with you,

102
00:12:44,440 --> 00:12:46,540
then we were thinking of paying for the final project,

103
00:12:46,540 --> 00:12:52,130
like restricting our focus to like one or two states and then making that more valid information.

104
00:12:52,810 --> 00:13:01,879
Maybe that's that could be another way to analyze data and you can have a sort of case control.

105
00:13:01,880 --> 00:13:05,180
Design is not really case control but is like two states.

106
00:13:05,210 --> 00:13:13,620
You believe they have a lot of history optionality. Then you can make a comparative study using the data from these two particular states.

107
00:13:13,630 --> 00:13:18,940
Right. So they may have very different, you know, control policy.

108
00:13:19,120 --> 00:13:22,239
They may have different political separation distribution.

109
00:13:22,240 --> 00:13:27,010
They may have different weather, they may have different economic status.

110
00:13:27,310 --> 00:13:35,230
You can sort of look at this to sort of different states where you, you know,

111
00:13:36,580 --> 00:13:43,750
make a comparison on that to see whether or not some specific risk factors are important to

112
00:13:44,950 --> 00:13:51,390
be associate with some of mentality or of vaccination hesitation or something like that.

113
00:13:51,530 --> 00:13:55,350
Right. I'm surprised that.

114
00:13:55,500 --> 00:13:59,980
Okay, the. Okay.

115
00:13:59,990 --> 00:14:04,010
So that's all I need. Any other questions?

116
00:14:06,310 --> 00:14:09,940
How about homework? Homework three. Do you have any question about homework three?

117
00:14:10,000 --> 00:14:14,560
Yeah. Yeah. I have a question about question three.

118
00:14:14,740 --> 00:14:22,180
Yeah. When you're forecasting the last part of question three asks you to forecast for.

119
00:14:23,530 --> 00:14:27,770
And I was looking at it.

120
00:14:27,790 --> 00:14:31,990
So do you need four weeks of vaccine data?

121
00:14:32,330 --> 00:14:37,270
A vaccine and data. You don't need tested data.

122
00:14:37,270 --> 00:14:41,160
That's data is something you're trying to predict. Is that right? Right.

123
00:14:41,170 --> 00:14:45,400
Right. So so vaccine data, it's somebody already asked this.

124
00:14:45,420 --> 00:14:51,910
I'm thinking that, of course, CDC has the data. If you don't want to go back to look at a data, that's fine.

125
00:14:51,940 --> 00:15:01,870
Because I think in October, the you know, that probably the vaccination has somewhat reached the situation.

126
00:15:01,870 --> 00:15:05,050
I mean, I think that increment, the change will be very minimal.

127
00:15:06,070 --> 00:15:10,750
So you can simply do this loss observation carried forward.

128
00:15:11,140 --> 00:15:15,010
So I sort of, you know, I'll believe that this one will now be lower than that.

129
00:15:15,580 --> 00:15:19,840
But when I think about this question, somebody else emailed me about the vaccination.

130
00:15:19,840 --> 00:15:28,420
I was thinking about this sort of protection sort of window for vaccination.

131
00:15:28,490 --> 00:15:36,910
And so so we can see that when we analyze data, that nurse can reverse sort of association.

132
00:15:36,910 --> 00:15:44,010
When you see there are more and more people got vaccinated, but the mortality rate goes up, i,

133
00:15:44,700 --> 00:15:53,379
I thought about that was probably due to the reason that the expiration of the vaccination because people think that

134
00:15:53,380 --> 00:16:00,520
if you get vaccinated the protection period is just roughly three months people have documented in the publication.

135
00:16:01,090 --> 00:16:07,190
So I think that the vaccination rates should not be really think about as the

136
00:16:07,210 --> 00:16:13,840
we we've sort of used that in the models thinking about always a small and

137
00:16:13,840 --> 00:16:22,659
tonic increasing pattern there is some kind of expiration like if you believe

138
00:16:22,660 --> 00:16:29,830
that vaccination would have its effectiveness for three months or six months

139
00:16:30,520 --> 00:16:44,050
and I don't know what it is some some publications said that the you know this did very little that like vaccination sort of teeters or something like

140
00:16:44,320 --> 00:16:51,910
in biology that remains that as the peak for periods three months after that

141
00:16:51,910 --> 00:17:02,110
you can see the decrease of that in the protection of of the in infection.

142
00:17:02,620 --> 00:17:13,870
So so so I think that maybe there should be some kind of decreasing in terms of this vaccination, but I don't know how to calculate that.

143
00:17:14,500 --> 00:17:23,890
Okay. So I mean, by the data itself, you see there's a cumulative increasing percentage of of being vaccinated.

144
00:17:24,370 --> 00:17:28,300
But what matters the most is actually the level of protection.

145
00:17:28,960 --> 00:17:39,250
But given that this protection is going to be good only for three months and start to decay of sorts through months,

146
00:17:39,640 --> 00:17:51,640
presumably then this cumulative increase in pattern may not be very accurate too to be to be used in the model for the actual video.

147
00:17:53,140 --> 00:18:00,040
So so you I mean for this for homework number of three per that you could assume that the situation

148
00:18:00,040 --> 00:18:07,479
is there and in the next 3 to 4 weeks that it probably almost the same not much change much.

149
00:18:07,480 --> 00:18:16,150
Yeah just do a last observation cover for imputation if you don't want to go back to look at the data.

150
00:18:17,320 --> 00:18:26,620
Yeah. Yeah. So sort of know what to like for you to to see these led to study like we are doing.

151
00:18:26,620 --> 00:18:31,209
Like if you don't need to be don't read the times study minus one does if indeed so

152
00:18:31,210 --> 00:18:35,570
in your slides like you know things are three you know the margin leads to follow up,

153
00:18:36,190 --> 00:18:42,220
etc. This would put a marginal distortion in the mean one and variance one very or dense lambda.

154
00:18:42,760 --> 00:18:48,040
But that's not virus over dispersion this over dispersion parameter.

155
00:18:48,040 --> 00:18:52,330
Yes. Okay. So yeah, it should the mean should be one for 70.

156
00:18:52,870 --> 00:18:59,379
Yeah. So I use this very special notation. So this is very special notation.

157
00:18:59,380 --> 00:19:04,210
Right. So A is the mean.

158
00:19:04,800 --> 00:19:13,870
How much GDP is the index parameter. So so in this parameter means B is the dispersion parameter.

159
00:19:14,740 --> 00:19:18,100
So the portions of the variance are diverse.

160
00:19:18,400 --> 00:19:22,990
The variance will be the one over b b square.

161
00:19:23,530 --> 00:19:30,249
Okay. That's the variance. Okay. So because the convolution full gamma is just that you have to have the same mean.

162
00:19:30,250 --> 00:19:37,260
So Gamma Star A plus B one plus government star and.

163
00:19:37,570 --> 00:19:47,950
Q So you have this convolution closure only and the index parameter equals Gamma Star eight and V

164
00:19:47,950 --> 00:19:56,590
one plus de to write that that's the the property of convolution closure for gamma distribution.

165
00:19:58,440 --> 00:20:02,880
Yes. Is. This is an index parameter.

166
00:20:02,970 --> 00:20:08,580
Yeah. That's a special notation that just will come in as a yes or no problem.

167
00:20:08,700 --> 00:20:14,550
But if you'd like to comment, you don't got to separate your data.

168
00:20:15,330 --> 00:20:22,050
Yeah. Okay. And when we're forecasting leg length, we are probably forecast to y by just the if.

169
00:20:22,740 --> 00:20:26,930
Yeah. I think this app right here, the f t is the forecast.

170
00:20:27,390 --> 00:20:37,830
So you have this ft00 last day of, you know, the observation so that you can full pass to ft0 plus one.

171
00:20:37,980 --> 00:20:48,580
Right. You have the form in earth and you treat that F 80 plus one as actually observation to continue to forecast that you will see for next year.

172
00:20:49,740 --> 00:20:58,139
So so this will be something as what you what you you could guess where you can get the best rate for next day.

173
00:20:58,140 --> 00:21:06,680
Then like going forward, then you will be tweaked that something like you might observe that you go to T plus two.

174
00:21:07,170 --> 00:21:17,210
Yeah. Get it. And for three days concurrent, it's got to a lot smoother to the Latin series of like,

175
00:21:17,220 --> 00:21:21,600
little Latin or like you don't have live and you don't have to break.

176
00:21:21,780 --> 00:21:28,979
No. So. So you only have the estimate, but you can log the observed a y t, but the observer.

177
00:21:28,980 --> 00:21:32,640
This scale is really large compared to the lengthy process. Oh, really?

178
00:21:33,030 --> 00:21:42,030
Yeah, of course you can have log like. Like I was planning to like predict like from using the like in process predicting y did like eight times.

179
00:21:42,340 --> 00:21:49,290
Eight times. Does that feel that we can get a white text and then got in my head from like that's predictive backwards.

180
00:21:49,530 --> 00:21:51,450
Yeah. Yeah. That's yeah.

181
00:21:51,660 --> 00:22:02,550
But if you think that life itself is tortured and killed because what you want is to really capture the variability of this scaling is not critical.

182
00:22:02,600 --> 00:22:06,719
It is the variability of the pattern that matter most.

183
00:22:06,720 --> 00:22:18,750
To. Of. And so when we're making these grabs, the common field transfer here look like they're almost like exactly on top of one another.

184
00:22:18,750 --> 00:22:22,680
So when you are plotting them, you can't you can only really see one color you can't see.

185
00:22:23,250 --> 00:22:28,030
Oh, really? Yeah. It's just overlap almost perfectly overnight.

186
00:22:28,090 --> 00:22:34,560
Yeah, well, that perfect building at the same principles and the obvious things are also.

187
00:22:34,890 --> 00:22:38,280
Yeah. Not misses are similar to the four decimal places.

188
00:22:38,790 --> 00:22:45,420
I see. So. Well. Well then there you make them separate plot.

189
00:22:45,520 --> 00:22:57,850
Right. So. Okay. Okay. So. So you can target the computer and symbols are the same as X by statements like 1.211.211.

190
00:22:58,440 --> 00:23:06,389
So I never say that. But you know, I would say the first part of the common filter somehow the smoother.

191
00:23:06,390 --> 00:23:11,500
That's probably the one that use more data than your how filter.

192
00:23:11,980 --> 00:23:19,770
Uh, yeah. If you follow them together or something else, you can probably see the difference.

193
00:23:20,120 --> 00:23:24,660
But the difference is very, very minor. Yeah. You can have a separate plot to plug both.

194
00:23:25,170 --> 00:23:30,600
Okay. Okay. Okay. So you are able to code that, right?

195
00:23:30,620 --> 00:23:39,180
So good. Yeah. Hmm. I still want to ask for the distribution for yourself here, because, uh.

196
00:23:39,540 --> 00:23:47,720
Yeah, this should be the mean is equal to one, so. And the dispersion is one minus one minus through multiple of them.

197
00:23:48,220 --> 00:23:59,880
Yeah. But in the, in the other slides let me get the derivation for the expectation of data here conditional on the T minus one.

198
00:23:59,980 --> 00:24:10,060
We actually use the expected of efficiency. And in a derivation they said expectation of efficiency is a one -0 one zero.

199
00:24:10,080 --> 00:24:13,740
Yeah. This is the rule and variance is one meters railway limit or something.

200
00:24:13,770 --> 00:24:23,010
Yeah. So it's is not. Oh I said maybe that's the title but what if I use this one to determine is eight and the variance is a square.

201
00:24:23,020 --> 00:24:25,259
Maybe then like here in this case.

202
00:24:25,260 --> 00:24:32,430
So basically that's shaped by damaging the force, by really coming doing one minus two times, they're going to say whatever is one minus rule by now.

203
00:24:32,640 --> 00:24:35,870
I don't think that will get me no one. Yeah, right.

204
00:24:36,020 --> 00:24:43,440
Yeah. Then it's going to be one because we need something has to be getting to be one and variance is also one.

205
00:24:44,700 --> 00:24:48,059
I don't think I'm getting that so, so marginally.

206
00:24:48,060 --> 00:24:56,610
I need to about equal to one on that and the and index from like right you draw the two that goes on come a one.

207
00:24:58,080 --> 00:25:01,680
Just. Just slow down. Just sleep. So, so, so.

208
00:25:01,680 --> 00:25:09,780
So I did I set out here to become a mean one and very similar in that to trying to mom.

209
00:25:10,020 --> 00:25:13,770
Right. So now I want to split this into two parts.

210
00:25:14,490 --> 00:25:18,330
Right. So the first part, I wanted me to be role of mom.

211
00:25:19,020 --> 00:25:22,740
Role. Role, yeah.

212
00:25:25,490 --> 00:25:33,010
Yeah the old times the mean because I think I have this they are one process right.

213
00:25:34,750 --> 00:25:42,330
So see that t v t t minus one plus it's on t, right?

214
00:25:42,370 --> 00:25:48,220
Yeah. Okay. Okay. So this one marginal because this narrow, you have to keep that.

215
00:25:48,940 --> 00:25:53,440
They see that you will have marginal distribution like this fixed.

216
00:25:53,650 --> 00:25:57,220
It doesn't depend on time according to stationary.

217
00:25:57,550 --> 00:26:05,500
So this one has this distribution as well. Also the meaning of this is RO and this meaning of this is.

218
00:26:10,430 --> 00:26:13,430
Should be. Okay.

219
00:26:14,510 --> 00:26:18,170
So, um, it is independent or independent?

220
00:26:18,290 --> 00:26:22,610
No. Are BP is like BP one and meeting the two are independent producers.

221
00:26:23,200 --> 00:26:35,510
Yeah. Yeah, they're independent here. So, so, so I had a rule and global warming rule for Lambda.

222
00:26:35,540 --> 00:26:43,700
Is that like beta distribution. I don't remember. Yeah. So that means you do you roll over through a line, right.

223
00:26:43,830 --> 00:26:47,050
Ralph Lauren, I can give you the rule.

224
00:26:47,160 --> 00:26:50,750
I hope so. So this this one should be rope.

225
00:26:50,750 --> 00:26:56,540
And this one, the being of this episode should be one mine.

226
00:26:57,530 --> 00:27:02,790
Yeah. So that was that. It said these would have been one minute through where I where I.

227
00:27:02,980 --> 00:27:07,570
Okay, so this should be Gama, star of one minus role.

228
00:27:08,390 --> 00:27:12,960
Yeah. Okay, sure. As I. You have this person from or long time.

229
00:27:13,100 --> 00:27:16,790
Yes. Okay, that's. Yeah, that's correct.

230
00:27:16,940 --> 00:27:28,600
Yeah, I think that's correct. But anyway, I, I think you should make sure that this whole thing will give you a gold star.

231
00:27:29,660 --> 00:27:33,889
What? Right. Yeah.

232
00:27:33,890 --> 00:27:37,670
Yeah, I like that still.

233
00:27:40,240 --> 00:27:46,110
So you one minus central square root.

234
00:27:46,760 --> 00:27:56,640
Oh yeah, yeah. Oh, yeah, yeah, yeah, yeah, yeah, yeah.

235
00:27:57,170 --> 00:28:02,720
It's more like one minus Rolando, you know, like that.

236
00:28:05,330 --> 00:28:15,920
So, so, so here I if I have this convolution protocol and karma and karma and the beta in this version,

237
00:28:16,490 --> 00:28:22,280
that should be evidence to show that this, this whole this term,

238
00:28:22,340 --> 00:28:35,110
this term will be a from a star role longer so so the of the convolution terminal does this

239
00:28:35,150 --> 00:28:43,840
a scale of one lambda this is the beta this and the convolution should be top of row 1000.

240
00:28:45,440 --> 00:28:50,709
That's something I have to check. Then you have a conclusion.

241
00:28:50,710 --> 00:28:58,180
Then when you come home looking at them together that you should have followed for one month longer.

242
00:28:58,240 --> 00:29:07,040
Right. Yeah.

243
00:29:20,040 --> 00:29:35,300
More questions. How about projects from other groups?

244
00:29:35,480 --> 00:29:38,810
Have you started or your project data capture?

245
00:29:38,820 --> 00:29:45,350
And so yeah, so let me try to answer that.

246
00:29:45,350 --> 00:29:49,700
So full vaccination.

247
00:30:00,100 --> 00:30:13,720
So for many in this room format mortality data we have from a Johns Hopkins database, we can't get county level,

248
00:30:17,680 --> 00:30:28,660
county level mortality and county level infection, county level infection or confirm the cases.

249
00:30:29,440 --> 00:30:38,200
This is Dave Oak. I mean, it is good that that's occurring and the confirmed infections.

250
00:30:38,800 --> 00:30:46,450
Yes. Okay. So we have this day lethal and controllable.

251
00:30:46,510 --> 00:30:59,680
Yes, it's coming. So from Johns Hopkins. Yes. And from CDC, we can get the vaccine to have also vaccination data in John Hopkins database.

252
00:30:59,680 --> 00:31:03,490
Find any perfect vaccination data. Okay. There's no vaccination.

253
00:31:04,060 --> 00:31:09,090
So for for the CDC data, we can find weekly.

254
00:31:10,390 --> 00:31:20,270
Yes. Vaccination data by state based state.

255
00:31:24,210 --> 00:31:31,050
And. And by age group, right?

256
00:31:33,060 --> 00:31:44,290
By age group, yes. So how will you do this?

257
00:31:44,470 --> 00:31:52,640
Vaccination hesitancy. What? The only solution level you get to your data state level or county level level.

258
00:31:54,400 --> 00:31:58,510
Oh, how could you find a county level? It's from a survey.

259
00:31:59,230 --> 00:32:02,560
Always a survey. It's not from CDC. Right. Okay.

260
00:32:03,920 --> 00:32:07,540
It's I think we access it through the CDC, but it's.

261
00:32:09,520 --> 00:32:12,540
A separate survey. Okay.

262
00:32:12,970 --> 00:32:28,210
Just have the vaccine. Okay. So I think I mean, if I remember correctly, I think you can get the vaccination data on the county level.

263
00:32:30,820 --> 00:32:35,770
You can request that result of age group because if you want that age group.

264
00:32:36,880 --> 00:32:41,350
So if you want a resolution at the county level, you don't get the data from by age.

265
00:32:41,920 --> 00:32:48,100
If you get, you know there something that age group we only get a state local data so they don't give

266
00:32:48,100 --> 00:32:56,170
this sort of flu a deeper resolution or better resolution because of the data size.

267
00:32:56,470 --> 00:33:00,670
Yes. So that's something I can give a try and see.

268
00:33:03,700 --> 00:33:14,620
If we can't get the stay local vaccination data without, you know, requesting the age group level data.

269
00:33:15,580 --> 00:33:21,700
So you mean for quality about the vaccination?

270
00:33:23,530 --> 00:33:26,860
So we do have state vaccination. But I want county level.

271
00:33:26,980 --> 00:33:33,600
Yes, you can not. But I think you can make a request request for the county level for the county level vaccination data.

272
00:33:33,610 --> 00:33:40,479
But CDC will not give your age. So either your Austin resolution level or a county level worry.

273
00:33:40,480 --> 00:33:43,840
I ask the vaccination resolution in that age group.

274
00:33:43,960 --> 00:33:47,640
They don't give you both. Yes, that's my point. Yeah, the pilot.

275
00:33:48,720 --> 00:33:58,220
Yeah. And I can send links to you for the Johns Hopkins stuff where I got the data from.

276
00:33:59,510 --> 00:34:08,540
So obviously they actually the data sort of the data capture file that we started with this course is different.

277
00:34:08,540 --> 00:34:16,420
One, we used the CDC for mortality. Leave that. Oh, but that's only for the U.S. to provided without question.

278
00:34:17,060 --> 00:34:23,180
Right. Okay. The Johns Hopkins, CSC, COVID 19 data dataset has TIME series for the number.

279
00:34:23,180 --> 00:34:27,680
That's the number of infections at the county level. Okay. Sure.

280
00:34:29,300 --> 00:34:34,370
Do you need to do a lot of data preprocessing? No, it's very it's very minimal.

281
00:34:34,640 --> 00:34:44,930
So the so our group for the unemployment data, Daniela was able to get the VIP's code for each county state here.

282
00:34:45,040 --> 00:34:53,270
Right. And this Jan Hopkins is very nice and gives you the API, which means it's very easy to join the two datasets together.

283
00:34:53,510 --> 00:34:56,560
Yeah. I think every this is unique.

284
00:34:56,570 --> 00:35:07,160
Like, you know, this county coding is unique no matter if you get it where no wet weather data and or some data and so on.

285
00:35:08,300 --> 00:35:18,920
This morning I was at a meeting, the new president of University Santa Ono is pushing this climate change and the global health.

286
00:35:19,460 --> 00:35:28,490
So where actually one group has captured this ritual to put variables at air pollution variables.

287
00:35:28,910 --> 00:35:36,480
This is some kind of getting the big picture of this global, you know, climate change and public health.

288
00:35:37,190 --> 00:35:45,380
And I time was meeting I was thinking, oh, is this very summer related to the oh the weather change you guys.

289
00:35:45,410 --> 00:35:56,180
I have no idea and so that something you know that oh but we have data we can look at whether or not this actually association of that.

290
00:35:58,730 --> 00:36:07,910
Yeah. Okay, so it sounds like we need to work on the vaccine data to, um, to improve the sort of the resolution.

291
00:36:08,210 --> 00:36:11,720
So that's something we can focus on to.

292
00:36:11,930 --> 00:36:15,760
To do it. Okay.

293
00:36:15,770 --> 00:36:19,520
So anything else?

294
00:36:22,050 --> 00:36:28,630
And for the time being, of course we could. I think that's the.

295
00:36:34,710 --> 00:36:42,480
Yeah, I think there's quite a bit of heterogeneity across different counties in everything each state is, you know, state level.

296
00:36:42,480 --> 00:36:48,440
The vaccination rate is very limited for people to understand the county level agency

297
00:36:48,440 --> 00:36:54,840
analogy because some counties are relatively some states are in relatively homogeneous race.

298
00:36:55,290 --> 00:37:03,060
Some states are very hesitant. And, you know, so it's interesting to see.

299
00:37:06,240 --> 00:37:15,330
What's going on? Yeah. Anything else? Okay.

300
00:37:15,330 --> 00:37:23,430
So. Okay, so we're in the topic of the OC.

301
00:37:23,430 --> 00:37:29,640
I should mention that when I say they still don't were announcement,

302
00:37:30,000 --> 00:37:37,500
I didn't realized in my class this is or star class either people received the words last

303
00:37:37,500 --> 00:37:43,050
year or people just receive this year or people only we receive the word next year.

304
00:37:43,290 --> 00:37:48,979
I can see like quite a number of people receive awards this year and receive last year.

305
00:37:48,980 --> 00:37:52,650
Right. So congratulations. Yeah.

306
00:37:52,710 --> 00:37:57,900
It's amazing to. To see so many star students in my class this semester.

307
00:37:58,500 --> 00:38:06,180
Yeah. Anyway, so let me just continue the special Dale Carnegie for the error data analysis error data.

308
00:38:06,210 --> 00:38:13,770
You know, that is the the sort of spatial data where the the error boundaries have been fixed.

309
00:38:14,580 --> 00:38:18,240
So so this is a very spaceship like the country boundaries.

310
00:38:18,250 --> 00:38:26,430
The fixed this is something that you're you're not able to change and data collection or recording were

311
00:38:26,910 --> 00:38:35,940
surveillance whatever the data you have is defined according to this given area with fixed boundaries.

312
00:38:36,060 --> 00:38:40,150
Okay. So so that's basically exactly the data we're looking at, right?

313
00:38:40,230 --> 00:38:43,230
So at a country level, so we look at error data.

314
00:38:43,800 --> 00:38:51,720
So but we believe that some kind of spatial originality and spatial dependance that we need to really take care of.

315
00:38:52,020 --> 00:38:59,720
So this is different from the longitudinal data. 653 where you're trying to account for temporal dependance, right?

316
00:38:59,730 --> 00:39:04,020
So you have repeated measurements, you have short time stories, right?

317
00:39:04,020 --> 00:39:12,620
So you like to model your data. You, you do statistical analysis by count, by counting for tempo, word, you know,

318
00:39:12,640 --> 00:39:19,610
zero dependance here because data are collected from spatial locations, particularly from different areas, right?

319
00:39:19,620 --> 00:39:24,690
So this error data, we need to deal with spatial autocorrelation.

320
00:39:24,870 --> 00:39:36,690
Okay, we need to do this spatial dependance. So, so people believe that neighboring counties are more likely to be similar than the counties?

321
00:39:36,900 --> 00:39:44,370
Probably. Okay. So that's why you need to, you know, work on the dependance across spatial locations.

322
00:39:45,660 --> 00:39:49,860
So talking about models, basically we're talking about regression models.

323
00:39:49,860 --> 00:40:01,860
That's what I would like to do. So so we have basically two classes of spatial regression models that are really

324
00:40:02,100 --> 00:40:08,520
popular in literature and the spatial data analysis Y is called SA Summit.

325
00:40:08,520 --> 00:40:13,379
So continuous ultra regressive models and a regression model.

326
00:40:13,380 --> 00:40:20,010
This ultra regressive error is governed by a certain alt accretion, an otherwise called car.

327
00:40:21,270 --> 00:40:27,960
So all the conditional alpha regression models, the model for conditional distribution are neighbors.

328
00:40:28,590 --> 00:40:35,219
So I work a little bit on car model and we'll give you example of moving.

329
00:40:35,220 --> 00:40:40,980
Um, that may or may not be today, but next lecture I will star is the SA model first.

330
00:40:41,280 --> 00:40:46,379
Okay. So this is essentially a simple extension from typical regression model.

331
00:40:46,380 --> 00:41:01,680
Okay. So we have our package. Well, this is type of should be t0rm so space shoe, uh, spatial, all total o error.

332
00:41:01,760 --> 00:41:07,560
And so this is title. So as should be sp aut0lm.

333
00:41:07,890 --> 00:41:22,860
Okay, so that is incorrect. Okay. So you have the arrow sa error m and you have ESP auto error and this one and number one is the typo again.

334
00:41:22,860 --> 00:41:25,979
So ESP auto oh error.

335
00:41:25,980 --> 00:41:34,620
And so I switch l and all here in our function, those are our functions that we use.

336
00:41:35,910 --> 00:41:40,620
So let me just begin with SA model. SA model.

337
00:41:40,920 --> 00:41:47,729
Okay. So you can know that now you have y y is a vector, right?

338
00:41:47,730 --> 00:41:55,040
So y's vector, so the length of y is the number of countries like.

339
00:41:55,230 --> 00:41:58,230
So we suppose in the nation. Right.

340
00:41:59,100 --> 00:42:05,760
So we have 84 countries, 84 it doesn't matter.

341
00:42:05,970 --> 00:42:12,930
So his so you might note I use I so because I had said I from.

342
00:42:13,010 --> 00:42:16,879
One, two, five, but you can use whatever.

343
00:42:16,880 --> 00:42:23,930
So you have your life vector, which is a111215.

344
00:42:25,310 --> 00:42:33,380
So you can use the geocoding or some kind of the the county code codes to list them.

345
00:42:33,820 --> 00:42:46,460
And, you know, so, so whatever this is, my y y actually is a vector with the data collected from or areas under investigation.

346
00:42:46,910 --> 00:42:51,530
In Michigan, you have 84 counties that the meaning of y is 84.

347
00:42:52,400 --> 00:43:02,990
Now you have your X, which is some risk factors or covers like you want to study association between 1xx like y could be number of,

348
00:43:04,460 --> 00:43:12,410
you know, death or, you know, this proportion of deaths and each county mortality rate or X could be,

349
00:43:12,920 --> 00:43:19,850
you know, a time variable or X could be a vaccination variable or X could be the other

350
00:43:20,070 --> 00:43:26,450
unemployment rate and or a x could be either city like Detroit or is a suburb.

351
00:43:26,720 --> 00:43:30,050
Well, you can call whatever two X you like to.

352
00:43:30,710 --> 00:43:34,040
And that of course is the parameter of interest you want.

353
00:43:34,040 --> 00:43:37,490
Studied relationship between X and Y. Okay.

354
00:43:37,700 --> 00:43:42,320
In a special data analysis, of course you don't have independent replicates.

355
00:43:42,710 --> 00:43:44,300
All data are correlated.

356
00:43:44,840 --> 00:43:55,790
So so you need in this case, all the y want to y y I put it together, follow a multivariate distribution with certain covariance structure.

357
00:43:56,090 --> 00:44:01,310
You don't have the replicates of this vector. You only have one vector in the data.

358
00:44:02,000 --> 00:44:08,930
Okay? So you don't have this replication of the do y independently many times.

359
00:44:08,990 --> 00:44:20,030
Okay. So, so that's why we call this process or random field or something like that rather than having independent copies of same vector,

360
00:44:20,030 --> 00:44:23,630
many times you don't have that. Okay. So,

361
00:44:23,900 --> 00:44:29,180
so that if you want to do the sort of statistical inference for life sample theory

362
00:44:29,870 --> 00:44:35,450
of the number of countries will go to infinity because you don't have replicates.

363
00:44:35,570 --> 00:44:47,330
You only have this vector that sort of collects all the data from all of the sort of regions or areas or counties like you want to study.

364
00:44:47,510 --> 00:44:57,350
Okay. Now, so how do you model the dependance among those components of life so that this second part is the model,

365
00:44:57,560 --> 00:45:05,630
this this the way to model this dependance structure, spatial dependance structure is by this Simonton is ultra regressive.

366
00:45:06,230 --> 00:45:14,920
Okay, so this is the one to all. Okay, so first of all, you want to introduce you.

367
00:45:15,270 --> 00:45:22,460
You is something you want to specify as a something like random effects.

368
00:45:22,640 --> 00:45:30,170
But you know, it's little bit more specific and random effects basically is a factor.

369
00:45:30,470 --> 00:45:34,459
Okay. The factor could be specified in different ways.

370
00:45:34,460 --> 00:45:37,940
But you believe that there's a driving factor.

371
00:45:38,090 --> 00:45:54,520
Okay, it's more like this, you know? So it is something called lower dimension projection, right?

372
00:45:55,120 --> 00:45:58,570
So you have the covariance matrix, right?

373
00:45:58,660 --> 00:46:08,080
So you want to project this by a matrix and a factor you.

374
00:46:09,430 --> 00:46:13,510
So you can do this projection, right? So you have covariance matrix.

375
00:46:14,390 --> 00:46:17,980
You want to project the covariance matrix to do this.

376
00:46:19,510 --> 00:46:23,500
Some decompose it. Lower rank decomposition. Right. So you.

377
00:46:25,900 --> 00:46:36,940
So in this case, of course, that you will have one, one sort of loading loading coefficients.

378
00:46:37,210 --> 00:46:41,650
Okay. So this is more like factor analysis small of this one factor.

379
00:46:41,910 --> 00:46:50,910
Right. So, so anyway, so you do this lower sort of business model of covariance structure, right?

380
00:46:50,920 --> 00:46:56,890
So you basically do a lower dimensional projection project to one factor.

381
00:46:57,310 --> 00:47:05,890
Okay. So this to be specified. But anyway, this will carry some of this dependance you like to create.

382
00:47:06,430 --> 00:47:13,090
W is part of sort of neighboring structure configuration of the neighbors.

383
00:47:13,360 --> 00:47:26,650
Okay. So because this y one too, I could be randomly arranged, but you can shuffle this order of y1 to y you can arbitrarily y you want to put,

384
00:47:27,820 --> 00:47:37,570
you know, Jackson County at and next to western a county y you want to do that, your data recording maybe go a different way, you know.

385
00:47:37,780 --> 00:47:50,020
So there is no control that one Y has to be next to another Y one because different people may shuffle the order of y one to Y in many different ways.

386
00:47:50,320 --> 00:47:58,350
You have to put certain like spatial configuration of spatial relationships about those counties.

387
00:47:58,360 --> 00:48:04,840
Which one is the neighbor of which one? Doesn't matter how the wires are ordered.

388
00:48:05,230 --> 00:48:16,120
I saw you put this w matrix to control the topology of the positions of the of the counties.

389
00:48:16,390 --> 00:48:25,360
You're safe. Okay, so this w is needed. So that and the one minimal level you can define which way is the neighbors of number one.

390
00:48:25,480 --> 00:48:34,270
Okay. So that's something you've needed. And Lambda is actually to define the strengths of this spatial dependance.

391
00:48:34,510 --> 00:48:43,370
Okay. So. So then the ipsum, once you model your dependance structure in this way, more like a ran effects model, but more complicated.

392
00:48:43,480 --> 00:48:51,430
More complicated to rent brand effects model. Right? So that you have sort of structure to model the, you know, the dependent structure.

393
00:48:51,430 --> 00:48:55,340
Then the error term is mostly for measurement errors.

394
00:48:55,360 --> 00:49:00,520
They are independent. Okay. So the epsilon term is a factor.

395
00:49:00,640 --> 00:49:11,290
Okay. Same dimension on the y doesn't carry any of dependent structure, but serving as a just a vector of measurement errors.

396
00:49:11,620 --> 00:49:15,519
So those terms are i d in error.

397
00:49:15,520 --> 00:49:19,080
Error terms you get.

398
00:49:19,150 --> 00:49:26,560
Okay. So okay, so that's the the SA model.

399
00:49:26,650 --> 00:49:31,210
Okay. So SA model has a mean structure. It's more like fixed.

400
00:49:31,220 --> 00:49:39,400
In fact, if you use that language of random fixed model, they have this spatial model, dependance modeling structure and plus the error term.

401
00:49:39,640 --> 00:49:45,040
Okay. Very much like the way in the same spirit to run the fixed model.

402
00:49:45,430 --> 00:49:53,710
But here you put w to control the to pass spatial topology related to the neighboring relationships.

403
00:49:53,860 --> 00:50:01,840
Okay. So how do you specify that that's come to the different types of model you use?

404
00:50:02,350 --> 00:50:05,470
So you can use Y itself.

405
00:50:05,860 --> 00:50:09,010
Okay. So you y it's your you.

406
00:50:09,520 --> 00:50:17,650
So you can do that. Okay. If you do that, it comes to the the ACR lac model.

407
00:50:18,130 --> 00:50:21,400
Okay. So so y is called lat model. Okay.

408
00:50:21,910 --> 00:50:25,000
So, you know, we we can study. Oh.

409
00:50:25,270 --> 00:50:34,870
As an example later. Okay. So. So you replace you by this very spatial choice of you as your Y, right?

410
00:50:35,290 --> 00:50:39,850
So that W is the matrix. The pressure waves reflect in every relationship.

411
00:50:41,040 --> 00:50:47,950
That that's okay. That's what we know. And if so, the elements of the epsilon vector, it will be I the measurement error.

412
00:50:48,530 --> 00:50:52,280
So that we can rewrite this model in the following way.

413
00:51:00,490 --> 00:51:07,930
So. So because the why and the why are the same you can move the this term to the left so that

414
00:51:07,930 --> 00:51:17,910
you have I identity matrix minus blue and white equal to and so those are all the vectors.

415
00:51:17,920 --> 00:51:23,650
Right. Okay, so that's what you get. Well sorry you have.

416
00:51:26,550 --> 00:51:29,660
X plus person.

417
00:51:33,210 --> 00:51:49,130
Right. So if you if you do this, then you can say that if this is you in verbal, then why can't you read to us?

418
00:51:49,960 --> 00:51:54,200
I'm from LA W E versus x.

419
00:51:54,280 --> 00:51:58,810
But you know, plus I minus W.

420
00:52:00,300 --> 00:52:03,530
Oh, it's okay.

421
00:52:04,240 --> 00:52:16,860
Right. Hmm. So if you calculate the covariance of of of of the white heat, because you know that those wise are correlated, right?

422
00:52:17,380 --> 00:52:23,590
So if you calculate the covariance of, of this under this model, this sa black model, right?

423
00:52:23,590 --> 00:52:30,910
SA black model, if you calculate coherence of the white heat, the vector y according to SA black model,

424
00:52:31,600 --> 00:52:35,920
then this part doesn't contribute to very covers because this is fixed effect.

425
00:52:36,340 --> 00:52:40,660
It is this part that contributes to covers. So what is the covariance now?

426
00:52:48,370 --> 00:53:02,050
So that you've got your eyes w universe and the covariance of epsilon is I think like sigma squared is a is indeed.

427
00:53:02,470 --> 00:53:08,740
Oh right. Right. So now you have I myself do transpose.

428
00:53:10,210 --> 00:53:17,730
Okay. So this actually is your covariance matrix from this sort of model.

429
00:53:18,130 --> 00:53:21,850
Like this describes the covariance of your y one to y.

430
00:53:23,260 --> 00:53:32,709
So y loves to slide your spatial dependance because you have this spatial matrix w that tells

431
00:53:32,710 --> 00:53:41,140
you how the relationships in the spatial locations of those areas are under investigation.

432
00:53:41,230 --> 00:53:45,520
So this is this is right. Modeling, maybe.

433
00:53:45,520 --> 00:53:53,589
Maybe not. But this is one model. You just come to the famous saying that all models around something so useful, this is just one model.

434
00:53:53,590 --> 00:53:58,510
You choose the model and this is better than no modeling this.

435
00:53:58,510 --> 00:54:02,320
Right. So you know that there are spatial correlated, but this is one of that.

436
00:54:03,250 --> 00:54:14,319
So to argue the. Sort of suitability of this mall.

437
00:54:14,320 --> 00:54:21,780
All right. So back to this model. Okay. So so this is your explainer is your fix.

438
00:54:21,780 --> 00:54:27,750
The fact it's more like the the pattern of your Y X plan by X,

439
00:54:28,620 --> 00:54:37,470
but you have a self sort of dependance or self explanation or self sort of structure to drive this.

440
00:54:37,890 --> 00:54:44,700
Okay. So if you choose your W to be a lower triangle matrix.

441
00:54:45,900 --> 00:54:50,720
Okay. So you can choose the W in some way that you like to do pro.

442
00:54:50,730 --> 00:54:56,190
So you can choose the neighboring where if you choose a very special type of model.

443
00:54:56,220 --> 00:55:00,480
Right. So lower triangle matrix.

444
00:55:01,290 --> 00:55:05,400
So everything is zero except here. Nonzero.

445
00:55:08,240 --> 00:55:20,120
Four W matrix, you choose a very spatial W matrix to be a lower and lower triangle matrix, but diagonal element is zero and the other part of zero.

446
00:55:20,450 --> 00:55:23,960
Only you have non-zero values builds a diagonal line.

447
00:55:24,350 --> 00:55:29,030
So what your going to have. So y one to the first.

448
00:55:31,470 --> 00:55:34,700
Oh well now related to the count itself.

449
00:55:35,240 --> 00:55:39,770
Maybe this is kind of the reference committee.

450
00:55:41,450 --> 00:55:46,200
Okay. So you only have XP that possibly part of X one.

451
00:55:46,700 --> 00:55:52,730
Okay. Then you have second country, then you have x to fail to pass.

452
00:55:53,330 --> 00:56:00,800
Now if you have lower triangle matrix and you have lambda, then you have one that will coming here.

453
00:56:01,970 --> 00:56:15,980
Okay. So that you have w, you have lambda times w21 times yy1.

454
00:56:17,120 --> 00:56:19,760
Right? Because do you have lower triangle matrix?

455
00:56:20,930 --> 00:56:30,510
Then this first element will multiply the first element of your y that gives you y one and pass its name to it.

456
00:56:32,430 --> 00:56:39,420
You can imagine that, right? Y three will be x really better plus longer.

457
00:56:39,440 --> 00:56:46,490
w31. Y one plus lambda w three to y two plus some three.

458
00:56:48,320 --> 00:56:50,810
If you choose this lower rectangle measure,

459
00:56:51,020 --> 00:57:01,700
what I'm trying to see here is that now we'll introduce this dependance y one will be a covered explained way too.

460
00:57:02,150 --> 00:57:06,530
So one way. One way to are correlated in such a way.

461
00:57:06,800 --> 00:57:15,020
Right. And either y three that you have y1y to drive y three.

462
00:57:15,380 --> 00:57:22,970
Okay. So this becomes this sort of a dag, right?

463
00:57:23,410 --> 00:57:33,910
I said ascending. Direct this this this smallest swimmer's quality in right.

464
00:57:34,630 --> 00:57:37,370
So this director I saw like graph of the model.

465
00:57:38,410 --> 00:57:49,900
Okay so if you have only three, the white one drives to white two and y to drive to like three, white one, drive like three.

466
00:57:51,130 --> 00:57:55,660
This is a topology that you, you will impose.

467
00:57:55,660 --> 00:58:00,250
We use W to be a lower triangle matrix.

468
00:58:01,540 --> 00:58:12,880
You've used W to be a lower triangle matrix. This is a dependent structure that you could have and this the famous mediation structure, right.

469
00:58:13,960 --> 00:58:22,270
So this model allows you to even create that larger the DG directed actually graphic model,

470
00:58:22,900 --> 00:58:31,540
basically this tablet gives you some kind of option or flexibility how you're going to build up this relationship among the YS.

471
00:58:31,690 --> 00:58:33,730
Okay. So that's something you control.

472
00:58:34,120 --> 00:58:42,640
Unlike in the ran effects model where you just use some covariance structure, covariance matrix with no special structure there.

473
00:58:42,850 --> 00:58:47,860
Here you can impose certain spatial topology or structure, right?

474
00:58:48,190 --> 00:58:52,540
In which we want, for example, you want to model this like direct.

475
00:58:52,570 --> 00:58:57,790
I started off in this sort of relationship any way you like.

476
00:59:00,190 --> 00:59:09,100
So as long as you're willing to specify some structure, you can work out this as a dependent structure.

477
00:59:09,370 --> 00:59:15,100
Yeah, not the way people do is called spatial like SA spatial error model.

478
00:59:15,640 --> 00:59:23,469
So where the this is more like a moving average model that we learn in times four is basically the think about

479
00:59:23,470 --> 00:59:33,130
that the residuals are related to each other instead of doing this modeling through this factor model idea.

480
00:59:33,580 --> 00:59:43,360
So this is more like factor mode idea. So they just specify the error term directly using this sort of process.

481
00:59:43,960 --> 00:59:50,930
And this is quite popular where people think that the error terms is correct.

482
00:59:51,400 --> 00:59:56,950
Okay. So you model the spatial dependance by linear function of all neighbors.

483
00:59:57,580 --> 01:00:06,790
They say m m is number of neighbors that you're going to, you know, to, to, to, to model the dependance error terms.

484
01:00:08,800 --> 01:00:14,380
And, but here you use a linear system rather than a general covariance matrix.

485
01:00:14,590 --> 01:00:23,170
Okay, so concrete I will be dependent on all the countries, the end countries around this country.

486
01:00:23,440 --> 01:00:34,120
Okay. So this is very similar to a sort of auto regressive order m process than the number of neighbors.

487
01:00:34,120 --> 01:00:42,220
M may vary from one polygon to another and the coefficient describes the spatial dependance of between.

488
01:00:42,980 --> 01:00:47,410
Y is the error term of the county i and e j.

489
01:00:48,700 --> 01:00:57,819
And you want to say that the diagonal elements like b i to be zero because you don't want to model the self dependance,

490
01:00:57,820 --> 01:01:02,830
you only want to model the dependance of given county with neighboring counties.

491
01:01:03,520 --> 01:01:09,460
Okay, so this will give rise to a B matrix is something like this.

492
01:01:09,820 --> 01:01:20,770
Okay, so now you replace your you you'll replace your u by a by the residual.

493
01:01:21,100 --> 01:01:30,940
Okay, so, so and that W will be your B and this is the structure that you use.

494
01:01:34,970 --> 01:01:42,410
And so B is the coefficients in this relationship that you would like to estimate.

495
01:01:42,860 --> 01:01:49,100
And the U is replaced by the researcher because you want to really work out the dependance on the error terms.

496
01:01:49,520 --> 01:01:59,330
So this is just one specification people choose why you choose this because they want to model the dependance on the error terms.

497
01:01:59,720 --> 01:02:05,210
Okay. So y people want to do this. This is basically the idea of moving average like in time series.

498
01:02:05,510 --> 01:02:16,010
Okay. So the so if you do that, then you will simplify this as the I minus fuel y times this.

499
01:02:16,320 --> 01:02:29,490
Okay so. So basically this model is model dependance really and the researchers have.

500
01:02:31,700 --> 01:02:41,600
So it's more like a the traditional idea of in a time stories where you first do this theme between, okay, analysis.

501
01:02:42,380 --> 01:02:47,450
So computer analysis is that a first I remove the, the new structure.

502
01:02:48,570 --> 01:02:56,360
Okay. So I remove the new structure. Well, the first moment of the why is my global explainer.

503
01:02:56,810 --> 01:03:09,080
Okay. So if I do this, the twins were, you know, sort of removing the of the first moment structure, this basically of the e residual.

504
01:03:09,230 --> 01:03:15,360
Okay. This gives me a residual and I won't put the second in second stage.

505
01:03:15,380 --> 01:03:22,880
This is first stage in small. A small the first movie of the or the mean structure of my process.

506
01:03:23,450 --> 01:03:29,930
Then the second part is that I want to model the spatial dependance on the residual.

507
01:03:30,560 --> 01:03:36,980
That's how this is small. Okay. This residual is models for this structure.

508
01:03:36,990 --> 01:03:52,460
Basically, the covariance of E is essentially one minus the of the one minus the transpose.

509
01:03:53,210 --> 01:03:57,800
Right. So this is essentially the equivalent structure I would have.

510
01:03:58,400 --> 01:04:01,490
Okay. So now.

511
01:04:05,220 --> 01:04:08,940
Okay. So he is my ex.

512
01:04:08,940 --> 01:04:14,550
My speed up would have been. And so if I do the inverse that would be my users.

513
01:04:14,820 --> 01:04:18,059
And okay, this is my coverage structure.

514
01:04:18,060 --> 01:04:23,820
I'll be okay if you have this one might be moved to the right hand side that

515
01:04:23,820 --> 01:04:29,250
this is kind of second quarter stuff the the variance of this epsilon term.

516
01:04:33,590 --> 01:04:37,580
Then this is my whole structure of this.

517
01:04:38,350 --> 01:04:39,440
So you have two models.

518
01:04:39,470 --> 01:04:49,550
One model is all about the first moment of the process, and then you have the model that models the current structure of the residual,

519
01:04:49,970 --> 01:04:57,950
which evolves the parameters in B and also the course of the parents parameter from air time.

520
01:04:58,220 --> 01:05:02,690
Okay. So this is called oh s are a spatial error model.

521
01:05:02,720 --> 01:05:06,860
This is very popular model actually in practice to model.

522
01:05:08,840 --> 01:05:17,750
Okay. So that's exactly what. Okay. So one popular choice of B here.

523
01:05:19,040 --> 01:05:25,130
You want to do commission reduction because this this could be a very complex ball of structure.

524
01:05:25,760 --> 01:05:33,020
So what are people trying to do here in practice is really to simplify this, be as long on the fabric.

525
01:05:34,310 --> 01:05:39,680
Okay. So this this is back to the original sort of this SA model specification,

526
01:05:40,460 --> 01:05:51,860
the while to specify the speed matrix in a way that is low dimensional structure or attribute or

527
01:05:52,250 --> 01:06:00,440
a quantity where W is specified according to the special relationship of neighboring countries.

528
01:06:00,830 --> 01:06:06,170
And now Lambda is the strength of dependance. That's the only parameter that you need to estimate.

529
01:06:07,220 --> 01:06:17,720
Okay. So so that's basically the one step simplification to specify this sample,

530
01:06:17,750 --> 01:06:26,870
which is very popular in practice and actually is, you know, hour to hour package available for you to run this.

531
01:06:27,080 --> 01:06:33,800
Okay. So back to this of the Saraki's data,

532
01:06:33,890 --> 01:06:45,140
you still remember to see this exposure and this all the census tracts in the O of Syracuse you know area at that.

533
01:06:45,470 --> 01:06:56,150
So there they do want to understand about the leukemia of the incidence rate rates so across all the census tracts in the Syracuse area.

534
01:06:56,270 --> 01:07:01,700
Okay. So you use ESP Alto air in OC function.

535
01:07:01,700 --> 01:07:08,240
The R value z is the normalized incidence rate.

536
01:07:08,480 --> 01:07:11,490
Okay. Then you have the exposure. Exposure.

537
01:07:11,540 --> 01:07:17,030
This is basically measure as one over the distance from the county to the lake.

538
01:07:17,660 --> 01:07:17,960
Right.

539
01:07:18,620 --> 01:07:32,000
The name is a canal or a gondola lake where there's a, you know, chemical engineer manufacturer there that they dump the all the t c into the water.

540
01:07:32,300 --> 01:07:39,020
So they measure the exposure as one over distance of the, you know, the base of country to that lake.

541
01:07:39,950 --> 01:07:45,710
Okay. That's the sort of what we call the potential exposure variable for at a county level.

542
01:07:45,740 --> 01:07:52,460
Right. And then we have this percentage of age population, 65 or older.

543
01:07:53,180 --> 01:07:56,390
Younger than 65 is a what?

544
01:07:57,530 --> 01:08:02,240
Oh, this is person based. I'm sorry. Okay. So percentage of in the county.

545
01:08:03,290 --> 01:08:10,160
And then this is the percentage of homeowners to sort of measure the socioeconomic status.

546
01:08:10,490 --> 01:08:13,530
Okay. Well, you can either cover it so that you can run it.

547
01:08:13,550 --> 01:08:18,320
This is your x variables have three X variables.

548
01:08:18,890 --> 01:08:23,330
Now you want to look out to estimate the paid up right association.

549
01:08:23,810 --> 01:08:36,290
But of course, you understand that the this census tract in from Syracuse City or the area of the city are correlated spatial recorded.

550
01:08:37,310 --> 01:08:46,400
So what you're going to do here is you use this sample where you have the choice of this long value.

551
01:08:46,940 --> 01:08:57,590
Now, how do you choose? W Well, the, the, the already created is w using this sort of spatial function.

552
01:08:58,520 --> 01:09:01,190
So you have to create W first. Okay.

553
01:09:02,000 --> 01:09:10,850
There is a function called basically create the the neighboring relationship of this cup like census tracts in that region.

554
01:09:11,330 --> 01:09:18,690
So this is given that ws given. You can change this if you have different ways to define that.

555
01:09:19,010 --> 01:09:29,570
If you want to find them according to Quinn, or where you use some buffer or some distance and whatever,

556
01:09:29,720 --> 01:09:39,020
the way that you want to define it, you'll never. But that's the way you define adopting the Matrix could be a addition to the Matrix zero

557
01:09:39,020 --> 01:09:44,930
one indicating neighboring counties where you can define according to some distance.

558
01:09:45,230 --> 01:09:56,720
So that w matrix is a continuous. So have this a continuous sort of elements in W matrix defined a distance between two counties.

559
01:09:56,970 --> 01:10:07,430
And you know, by this sort of the geo distance between two counties in using the symptom of counties of anyway you want to

560
01:10:07,430 --> 01:10:15,050
define that in advance that that's about through matrix that you just run this analysis using this article.

561
01:10:16,160 --> 01:10:19,430
Okay. Now you'll give you a lambda.

562
01:10:19,730 --> 01:10:25,220
That's exactly the parameter that you like to estimate and you do like a ratio test.

563
01:10:25,910 --> 01:10:29,030
So P values this and standard arrows.

564
01:10:29,270 --> 01:10:34,090
So you can see that the lambda is significantly different from zero.

565
01:10:34,100 --> 01:10:38,330
So this p value is testing lambda equal to zero.

566
01:10:39,140 --> 01:10:42,720
When lambda equal to zero that means there's no spatial dependance.

567
01:10:42,740 --> 01:10:46,040
Right. Because your p matrix is completely zero.

568
01:10:46,850 --> 01:10:54,800
Right. So. So if I zero, then lambda times w is completely s zero so that there's no spatial dependance.

569
01:10:55,460 --> 01:10:59,210
But now it says that p value is more important.

570
01:10:59,210 --> 01:11:03,710
Zero five that we reject this now that there exists a spatial dependance,

571
01:11:03,920 --> 01:11:11,740
whether or not lambda w is the best spatial structure, the sort of the modeling of the spatial correlation,

572
01:11:11,750 --> 01:11:15,740
I don't know like you may have better at least them all the spatial but but this

573
01:11:16,700 --> 01:11:20,569
our function allows you to have one way to model it that's the way to model.

574
01:11:20,570 --> 01:11:26,360
And you find that there exist a spatial dependance under this model.

575
01:11:26,690 --> 01:11:34,940
Okay. So, so that you can say that first of all, age is very important predictor, right?

576
01:11:35,420 --> 01:11:44,840
So the H is very strong risk factor for this of leukemia incidence.

577
01:11:45,590 --> 01:11:56,900
And also the socioeconomic socio economic status is also very important risk factor for developing the leukemia in the different county level.

578
01:11:57,680 --> 01:12:03,050
However, exposure is not that too significant. So the p value is 0.09.

579
01:12:03,470 --> 01:12:11,030
Okay. So this the exposure to see if you see it is not.

580
01:12:11,540 --> 01:12:21,080
Okay. So now here is the comparison between running the ordinary score, ignoring the spatial dependance, namely lambda equal to zero.

581
01:12:21,410 --> 01:12:27,830
So this all as ordinary squared is equivalent that you run the case under noun or that equal to zero.

582
01:12:28,010 --> 01:12:31,730
No spatial dependance. This is the result again.

583
01:12:33,440 --> 01:12:36,979
Okay. So the the results are the point.

584
01:12:36,980 --> 01:12:42,350
Estimates are no, they have the same direction of estimate.

585
01:12:43,460 --> 01:12:51,830
And the standard error also somewhat but not the at the p values some somewhat different.

586
01:12:53,330 --> 01:12:59,720
So this difference is really something related to how you model the spatial dependance.

587
01:13:00,260 --> 01:13:08,490
Okay. Mm hmm. So. So you can look at that the the cold.

588
01:13:08,570 --> 01:13:17,330
Very fact. So you have three covered, right? So you can't have the X transposed through the hat.

589
01:13:17,480 --> 01:13:24,050
So you have this value from different countries in a different census tract here.

590
01:13:24,530 --> 01:13:27,440
So you can see that you have different colors.

591
01:13:28,310 --> 01:13:39,230
Each one is color coding, but for this you need a predictor x beta so that you can see that this is something you get.

592
01:13:39,530 --> 01:13:42,740
Okay. And now you also have the E.

593
01:13:42,980 --> 01:13:51,490
Mm hmm. So you have the E y minus X of of the x, Peter.

594
01:13:52,010 --> 01:13:58,340
So that's the so-called most of the stochastic city spatial dependance.

595
01:13:58,790 --> 01:14:04,310
You can see that there are more dependance in in the in the residuals.

596
01:14:04,670 --> 01:14:16,790
Okay. So what you can see here is that the are fixed in fact, explain apart cannot explain the much of this spatial heterogeneity.

597
01:14:16,790 --> 01:14:19,970
You look at this map, they're looking pretty homogeneous.

598
01:14:20,360 --> 01:14:26,930
But if you look at actually with the residual part, there are a lot of more spatial structure inside.

599
01:14:27,440 --> 01:14:31,310
Right. So that basically three X variables are not.

600
01:14:32,120 --> 01:14:38,179
Very powerful or efficient for you to explain the spatial dependance and you need to really

601
01:14:38,180 --> 01:14:45,990
Malta's spatial dependance to really capture this whole thing that cannot be explained by X data,

602
01:14:46,160 --> 01:14:54,649
right? So if you show not the arrow situation where lambda equal to zero, ignoring the spatial dependance,

603
01:14:54,650 --> 01:15:02,219
you basically think that this is hoping it would be i d did random noise but when you get a residual plot

604
01:15:02,220 --> 01:15:09,560
it you see this is actually very structured there are now that in fact a random noise you know so so

605
01:15:09,560 --> 01:15:16,010
that clear indicate that the modernist spatial dependance is very essential to require in this kind of

606
01:15:16,010 --> 01:15:25,219
analysis although numerically you don't see the change of conclusions but at least after you see this,

607
01:15:25,220 --> 01:15:33,860
then you will. You'll understand that there are a lot of additional things maybe you need to model in this spatial dependance structure.

608
01:15:34,700 --> 01:15:41,659
So that's very easy thing to run. You know, if you have mission data, right, so you have Michigan data,

609
01:15:41,660 --> 01:15:49,810
then you can use this model to run this and to look at the, you know, the association view.

610
01:15:50,030 --> 01:15:58,900
We have studied before. We, you know, did not really spatial that the studied of the spatial, you know, sort of dependance.

611
01:15:58,910 --> 01:16:01,790
Now after you learn this SA model,

612
01:16:01,820 --> 01:16:14,450
you can define w matrix where is just as the matrix which is neighbor into which and then you can really put that into this R function and you know,

613
01:16:14,540 --> 01:16:22,520
estimate the spatial correlation and the relationship between your Y and X variable.

614
01:16:24,410 --> 01:16:29,590
What a very interesting model. Okay. Where's my next?

615
01:16:36,990 --> 01:16:40,260
Moving on to I.

616
01:16:40,440 --> 01:16:57,750
Not without here. That's what these. File and.

617
01:17:07,070 --> 01:17:10,320
Right here. Okay.

618
01:17:11,880 --> 01:17:17,070
So in the last 5 minutes or so, I want to talk about this car model.

619
01:17:17,130 --> 01:17:26,390
I work on car model. So I, I think it's a little bit better to model the actually error data.

620
01:17:26,430 --> 01:17:37,290
So the same model is very much like the idea of extending traditional sort of armor model in time series but can't,

621
01:17:37,290 --> 01:17:42,780
although it's a little bit slightly different. Okay. So let me just come talk about this.

622
01:17:44,400 --> 01:17:50,490
So car model, strictly speaking, is really a special extension of ran effects model.

623
01:17:50,880 --> 01:17:57,150
Okay. So that's what I like. So suppose you have I polygons or I countries.

624
01:17:57,690 --> 01:18:01,320
So you have the data. Okay. The error data.

625
01:18:01,590 --> 01:18:06,540
Why is outcome interest excise a vector of culverts that you like to study?

626
01:18:07,080 --> 01:18:12,150
Now, here you, you know, looking at this typical ran effects situation.

627
01:18:12,310 --> 01:18:18,840
Okay. So you conditional that on Y and your mean parameter and variance parameter.

628
01:18:19,320 --> 01:18:30,180
So y I would you know I these support condition on this where your model is specified by the fixed effect and ran effects.

629
01:18:30,630 --> 01:18:37,890
Okay. VII is the red phase. So the run effects is a subject like context specific effect.

630
01:18:38,310 --> 01:18:42,840
That's something really important for us to to estimate.

631
01:18:43,710 --> 01:18:51,210
Just like in 653, you learn how to estimate the the subject specific effect use rent effects.

632
01:18:51,420 --> 01:18:57,860
Here the VII is counter level specific fact that's also very important for us to understand.

633
01:18:58,210 --> 01:19:05,730
You do this county level specificity so that you can identify some hotspots in the actually the analysis.

634
01:19:06,000 --> 01:19:14,010
So we I it's not just a term that you use to introduce core correlation among different counties.

635
01:19:14,460 --> 01:19:26,310
It's a county specific effect that can give you some additional usefulness to capture some special features of individual counties.

636
01:19:26,580 --> 01:19:31,980
For example, you know, something like, you want to identify some hotspot, okay, something like that.

637
01:19:32,640 --> 01:19:37,110
So this random effects formulation is, in my view,

638
01:19:37,110 --> 01:19:46,200
is more appealing than the sample would look at the where you just look at the fixed effect model and trying to model the error terribly,

639
01:19:46,210 --> 01:19:56,970
some sort of an covariance structure using the like fixed factor model structure or rank projection approach, which is hard to interpret that.

640
01:19:57,480 --> 01:20:02,639
But this ran effects has much better interpretation so that many people want to use this.

641
01:20:02,640 --> 01:20:10,950
I think this is more popular example. Okay, so there are three different types of this specification of VI.

642
01:20:11,850 --> 01:20:22,440
Okay. So the first one is that you use V, I only introduce the sort of cortical core very structure.

643
01:20:23,010 --> 01:20:33,089
You know, that the data from the error data are spatially correlated, but you do not want to specify any structure on the dependance.

644
01:20:33,090 --> 01:20:37,739
You just see, well, I have this sort of exchangeable version.

645
01:20:37,740 --> 01:20:43,740
I think the core rating among those polygons are exchangeable, right?

646
01:20:43,800 --> 01:20:49,379
They're equally correlated. So you just use a simple ran effects to do that.

647
01:20:49,380 --> 01:20:57,810
And then this is one model that you can test the variance equal to zero so that you can see whether or not there's a spatial dependance.

648
01:20:57,810 --> 01:21:02,970
That's something you learned and there's no structure of spatial dependance.

649
01:21:03,360 --> 01:21:06,870
So this is model two is actually the most popular one.

650
01:21:07,320 --> 01:21:13,290
Where do you want to, you know, link the neighboring countries in the part of Gran Effects?

651
01:21:13,890 --> 01:21:20,480
So here is, you know, they bring up some of the neighboring countries big.

652
01:21:20,570 --> 01:21:26,340
What is the other variations? The case of the beyond is fixed.

653
01:21:26,340 --> 01:21:31,680
In fact, can if can influence each other in the neighboring counties.

654
01:21:32,040 --> 01:21:41,850
So that's basically the the the the meaning of your rand effects using the random facts from neighboring countries.

655
01:21:42,270 --> 01:21:46,740
So this is very important specification in the car model.

656
01:21:47,880 --> 01:21:54,810
And next one is you basically have this sum of the two random facts.

657
01:21:54,810 --> 01:22:05,070
One is the sort of background dependance of as kind of homogeneous equal correlation among the of countries,

658
01:22:05,130 --> 01:22:08,550
this sort of base correlation based spatial causation.

659
01:22:09,080 --> 01:22:17,410
But you have additional term that follows the two car model that you believe that the state has.

660
01:22:17,420 --> 01:22:22,010
They will be correlated among the neighboring countries.

661
01:22:22,430 --> 01:22:28,370
So that's a mixture of the two previous to a random back structure.

662
01:22:28,730 --> 01:22:38,490
So those three models are most popular and they can be, you know, used to actually be modeling of the car model.

663
01:22:38,510 --> 01:22:48,020
That's quite a useful and I don't have time to continue, but I can give you more detail on sort of thing.

664
01:22:50,220 --> 01:22:56,030
And I don't have time for officer today because.

