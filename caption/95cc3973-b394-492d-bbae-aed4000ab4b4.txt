1
00:02:27,990 --> 00:02:34,950
Marisa Eisenberg: And awesome Thank you Morgan and so yeah so then today we're going to do parameter estimation so.

2
00:02:35,550 --> 00:02:45,150
Marisa Eisenberg: Yesterday, I think, Michael if i'm remembering right did a whole thing on stochastic models and you all talked about all of that good good good Okay, and so.

3
00:02:45,930 --> 00:02:54,270
Marisa Eisenberg: Because I don't know usually the stochastic model pieces is a lot, but hopefully it made some sense and everyone kind of got at least a.

4
00:02:54,570 --> 00:03:02,190
Marisa Eisenberg: sense of like what the commonalities are between deterministic models like differential equations and stochastic models, you know sort of got some sense of kind of.

5
00:03:02,400 --> 00:03:11,280
Marisa Eisenberg: What the commonalities and the differences are between those different kinds of flavors of model out there and today we're going to shift gears a little bit and instead of talking about.

6
00:03:13,590 --> 00:03:25,350
Marisa Eisenberg: plugging in my keyboard here, instead of talking about like different approaches to building models we're going to we're going to start this morning, talking about how do you estimate the parameters for models so.

7
00:03:26,250 --> 00:03:32,250
Marisa Eisenberg: You know I guess probably most people i'm assuming have done, maybe like.

8
00:03:33,000 --> 00:03:42,750
Marisa Eisenberg: I don't know linear regression where you've estimated betas and like with like least squares or something maybe things like that or or maximum likelihood or something okay some Yes, some nods I see.

9
00:03:43,560 --> 00:03:57,390
Marisa Eisenberg: Probably most of you have done some degree of sort of statistical model fitting and at some point in your life and we're going to talk here about how you set up that same kind of framework for.

10
00:03:57,780 --> 00:04:04,470
Marisa Eisenberg: complex systems types of models so So how do we set up a likelihood, how do we do parameter estimation for.

11
00:04:04,560 --> 00:04:09,270
Marisa Eisenberg: That flavor of model, and now the the big thing that.

12
00:04:09,330 --> 00:04:20,250
Marisa Eisenberg: I think, will be a little bit different is that these models are they're structured in a very different way and so, how you set it up is often kind of tricky and it's also if you've done like.

13
00:04:20,790 --> 00:04:33,240
Marisa Eisenberg: linear regression before there's there are like for many statistical models, there are sort of formulas that have been established for the optimum the sort of the the the optimal parameter estimate that's not the case for most.

14
00:04:33,690 --> 00:04:48,870
Marisa Eisenberg: Systems models, and so we have to use numerical optimization and so basically meaning our computer is going to try and find the best fit parameter so we're going to talk a little bit about that, let me share my screen i'm okay I don't need this.

15
00:04:49,980 --> 00:04:59,880
Marisa Eisenberg: got a little bit smart okay does this look okay to everybody, you can see, fine awesome okay um alright so so what we're going to do so.

16
00:05:00,390 --> 00:05:08,100
Marisa Eisenberg: So connecting models with data so depending on the parameters, you know models can give very different results if you change parameter be from.

17
00:05:08,310 --> 00:05:14,490
Marisa Eisenberg: five to one, it might completely change what your simulation is going to look like and so when you're trying to.

18
00:05:14,730 --> 00:05:22,380
Marisa Eisenberg: sort of connect your model with the real world it's really important to know what parameters or ranges of parameters, you want to be using.

19
00:05:22,620 --> 00:05:30,120
Marisa Eisenberg: And it's often the case that directly measuring the parameters for your model is quite difficult, you know it's you know it's very hard to sort of.

20
00:05:30,870 --> 00:05:39,240
Marisa Eisenberg: Even in just like an si our model, you can probably read measure, the rate at which people recover but it's often really hard to actually get a one contact rate.

21
00:05:39,540 --> 00:05:48,930
Marisa Eisenberg: That captures the average for the entire population in any kind of realistic way so figuring out what the beta in an si our model is going to be is often like.

22
00:05:49,200 --> 00:05:52,980
Marisa Eisenberg: Pretty tricky so the transmission parameter for front so in our model so.

23
00:05:53,280 --> 00:06:04,410
Marisa Eisenberg: If we have data on what we're observing about some of the variables in our model from the real world, like the number of infected individuals or how many people are getting cancer each year, or whatever it might be.

24
00:06:04,710 --> 00:06:12,690
Marisa Eisenberg: That might be able to tell us something about at least which parts of the parameter space or more realistic and so, then the question becomes, how do you connect your model with data.

25
00:06:12,990 --> 00:06:20,430
Marisa Eisenberg: And so, if you and I forgot, I guess, I usually animate this so pretend this little bottom area with rapid restoration isn't there for a moment.

26
00:06:20,640 --> 00:06:26,880
Marisa Eisenberg: So if you just think about I, like my instinct is to put my hand over it, but obviously none of you can see that but um so.

27
00:06:27,450 --> 00:06:30,480
Marisa Eisenberg: You know if you just had a model, you can simulate.

28
00:06:31,020 --> 00:06:40,020
Marisa Eisenberg: What happens to say, the number of cases, over time, in your model and get you know with these different lines various different simulations of what what might happen.

29
00:06:40,320 --> 00:06:48,510
Marisa Eisenberg: But for parameter estimation, what we need to do is basically take data on some of one of our one or some of our variables role model like cases.

30
00:06:49,800 --> 00:06:56,730
Marisa Eisenberg: and go backwards and try and estimate which parameter values in this model get us sort of closest to this data so.

31
00:06:57,030 --> 00:07:06,450
Marisa Eisenberg: The basic idea or assumption for all of parameter estimation, not just for systems models, but for any kind of model is really that parameters that give you model behavior.

32
00:07:06,690 --> 00:07:20,070
Marisa Eisenberg: That more closely matches the data we consider those parameter values to be better or more likely right so essentially if I have two sets of parameters and one of them gives me this simulation and one of them gives me this one.

33
00:07:20,640 --> 00:07:30,630
Marisa Eisenberg: i'm probably going to think if the dots are the data, then i'm probably going to think this set of parameters is more realistic than this set of parameters because it's it more it reflects the data better now.

34
00:07:31,170 --> 00:07:43,050
Marisa Eisenberg: Remember what we saw on Monday with like how different remember with the polio certain thing how different initial you know randomness could lead to quite different trajectories if you see something like an epidemic and you see something like this.

35
00:07:43,710 --> 00:07:50,280
Marisa Eisenberg: it's not necessarily true that this one data set is all that representative, so this assumption is a little bit.

36
00:07:50,670 --> 00:08:03,450
Marisa Eisenberg: is not always quite right, but that's you know that's sort of essentially we only have the data we have about the world and so that's the evidence we have so parameters that are closest to that evidence we're going to treat as better or more likely and.

37
00:08:03,990 --> 00:08:07,410
Marisa Eisenberg: And, and so, then, how do you find the sort of closest or best match.

38
00:08:07,950 --> 00:08:12,720
Marisa Eisenberg: or in a in a more distributional setting we're going to talk about Beijing estimation, in a minute.

39
00:08:13,020 --> 00:08:17,910
Marisa Eisenberg: How do you how do you determine how good the match is between your model and your data.

40
00:08:18,150 --> 00:08:26,010
Marisa Eisenberg: Well, so for that we were going to need to sort of more mathematical tools, so a cost function, or sometimes called an objective function.

41
00:08:26,220 --> 00:08:37,710
Marisa Eisenberg: Which is basically a way of saying how far away, is my model simulation from my data, so the cost function for this line should be low, meaning that it's good it's a it's a.

42
00:08:37,950 --> 00:08:42,330
Marisa Eisenberg: You know people, you can define high or low is better different people do it differently, but supposing.

43
00:08:42,570 --> 00:08:58,080
Marisa Eisenberg: lower cost is better this cost function is lower, and this cost function should be high, meaning that it does not fit the data very well and then optimization is just a way to adjust our model parameters to get the best match, ie.

44
00:08:59,580 --> 00:09:00,900
Oh yes, fine.

45
00:09:04,530 --> 00:09:06,870
Marisa Eisenberg: So I need to minimize the cost function.

46
00:09:07,290 --> 00:09:15,450
Marisa Eisenberg: And so we want to take these kind of mathematical ideas and then frame them from a statistical perspective, where rather than saying simply.

47
00:09:15,630 --> 00:09:25,980
Marisa Eisenberg: Okay, I minimize the cost function, we can talk about things like confidence intervals, or what the distribution of the parameter values might be, or you know those kinds of questions and so we're going to talk about that as well.

48
00:09:26,340 --> 00:09:28,560
Marisa Eisenberg: And okay so we're going to sort of start with.

49
00:09:29,040 --> 00:09:38,010
Marisa Eisenberg: optimization as an approach will do that with like least squares and then we'll talk about maximum likelihood, which is going to be sort of the statistical framework we're going to use.

50
00:09:38,250 --> 00:09:46,290
Marisa Eisenberg: And then, at the end we'll get into bayesian approaches which are I feel like put a lot of maximum likelihood.

51
00:09:46,560 --> 00:09:52,440
Marisa Eisenberg: On at least for me i'm more comfortable footing, but if you're a frequent just that is also a okay I don't have a real strong horse in this race.

52
00:09:52,740 --> 00:09:58,770
Marisa Eisenberg: I don't know if any of you have encountered this if you ever hang out with statistics or by stats folks.

53
00:09:59,310 --> 00:10:04,920
Marisa Eisenberg: There is, there is a an intense philosophical battle between frequent test invasion and.

54
00:10:05,400 --> 00:10:10,230
Marisa Eisenberg: And and i'm just saying way out of it but anyway yeah so just just a just a little thing okay.

55
00:10:10,560 --> 00:10:22,560
Marisa Eisenberg: So um alright so back to this so parameter estimation, so, in general, the goal is to find maybe an optimal fit to the data or to Kara characterize the distribution of parameters that's going to match the data in some way.

56
00:10:23,430 --> 00:10:38,520
Marisa Eisenberg: And so, a lot of optimization methods out there are built assuming a cost function surface that looks something like this, so imagine, this is one parameter on the X axis and a second parameter on the y axis and then the color of this.

57
00:10:39,270 --> 00:10:46,380
Marisa Eisenberg: heat map is saying how good the fit is to the data, so the lowest cost is here, and then it gets worse as you move out.

58
00:10:46,590 --> 00:10:57,480
Marisa Eisenberg: And so, a lot of optimization methods are sort of built assuming a nice bowl shape surface was one minimum that you can just you start somewhere and you just traverse down the ball and yeah you find the best fit.

59
00:10:57,750 --> 00:11:04,410
Marisa Eisenberg: But in reality, of course, a lot of our cost function surfaces look quite different than this, it might be that they have multiple minima.

60
00:11:04,650 --> 00:11:16,410
Marisa Eisenberg: That could be equally good are some are you know you have like kind of a surface that goes like blue yeah so you could get trapped in this like sort of shallow or bowl and not realize that there's a better solution over here and.

61
00:11:17,130 --> 00:11:21,420
Marisa Eisenberg: We also have a situation so for systems models, you find this a lot.

62
00:11:22,110 --> 00:11:31,500
Marisa Eisenberg: called and identify ability and so this is where it might be that it turns out that there's a trade off between some of your parameters and so you can actually get the.

63
00:11:31,950 --> 00:11:39,150
Marisa Eisenberg: This is somehow my heat map got cut off this is supposed to be dark purple down the middle of this Canyon but somehow you have like a Canyon basically.

64
00:11:39,420 --> 00:11:49,380
Marisa Eisenberg: Of parameter values that can all fit the data equally well so anything along this Canyon here fits the data just as well as any other so there's no.

65
00:11:49,650 --> 00:12:00,510
Marisa Eisenberg: unique single optimum fit to the data there's there's you can just like trade off between these parameters and get the exact same fit so there's a lot of things that can go wrong.

66
00:12:00,840 --> 00:12:11,760
Marisa Eisenberg: And, and we have to kind of bear that in mind i'll talk a little bit later about some how to address some of these issues um but yeah so there's data issues, you can have there's model issues, you can have.

67
00:12:12,600 --> 00:12:19,590
Marisa Eisenberg: All of this can make your cost function surface very difficult to work with and that's not even mentioning issues of model miss specification.

68
00:12:19,890 --> 00:12:29,370
Marisa Eisenberg: Most parameter estimation methods are assuming that you have the right model for the data which may or may not be true and we'll talk about that too, as well in a little towards the end.

69
00:12:30,240 --> 00:12:32,370
Marisa Eisenberg: Okay, so um.

70
00:12:33,210 --> 00:12:43,920
Marisa Eisenberg: For we're going to do this for differential equations and sort of our example that we work with at first, but all this stuff i'm going to talk about works for something anything else to you could just replace the equations with whatever.

71
00:12:44,130 --> 00:12:54,120
Marisa Eisenberg: You know other kind of system you're working with but just to kind of have give us a sort of concrete example to work with we'll talk about differential equations for the beginning here.

72
00:12:54,900 --> 00:13:02,610
Marisa Eisenberg: So imagine that you have some O D system, maybe a linear compartment a model or an SDR model or something like that.

73
00:13:03,360 --> 00:13:16,860
Marisa Eisenberg: And you have some unknown parameters that you want to estimate from your data and and you, you have time course data let's say so, if that's not always the case, but it'll be nice for plotting for our purposes so let's say you have time course data.

74
00:13:18,180 --> 00:13:23,310
Marisa Eisenberg: you're gonna we're going to use oh to ease because they're deterministic and it just means we don't have to deal with stochastic city.

75
00:13:23,970 --> 00:13:30,540
Marisa Eisenberg: And we can just think about measurement noise that that won't you know if you're using a stochastic model you'd have to tweak some of this a little bit, but that's OK.

76
00:13:31,680 --> 00:13:42,630
Marisa Eisenberg: OK, so the general setup if you're going to do a differential equation model is going to be something like this, so we've been talking about on Monday and.

77
00:13:43,110 --> 00:13:56,940
Marisa Eisenberg: The equations for the model system, so I just put one equation here, but you can imagine that you have many various multiple equations, for you know X one X two or s II II or whatever they might be, and.

78
00:13:57,300 --> 00:14:03,390
Marisa Eisenberg: That are given by some functions, whatever they are so this little X dot here is just.

79
00:14:03,870 --> 00:14:09,360
Marisa Eisenberg: meant to sort of represent all of your model variable equation so that's all the stuff we were talking about on Monday.

80
00:14:09,780 --> 00:14:20,640
Marisa Eisenberg: The stuff that Michael was probably mostly talking about yesterday, the the equations that describe your system and to that we're going to add some more thing so we're going to add what's called a measurement equation.

81
00:14:21,180 --> 00:14:31,200
Marisa Eisenberg: We saw this a little bit for just like a half second on Monday, but basically some sort of equation that describes what variables, you can actually measure so.

82
00:14:32,220 --> 00:14:33,390
Marisa Eisenberg: If you hear me.

83
00:14:35,070 --> 00:14:46,200
Marisa Eisenberg: Just do this other than making a whiteboard i'll just make it blank slide in the middle, to change club sharing So if you have a you know, like a maybe an si our.

84
00:14:48,660 --> 00:15:02,430
Marisa Eisenberg: model, and you know, then you get your s dot equation or DST T, which is maybe like minus beta s I n I dot equation is you know beta si.

85
00:15:03,390 --> 00:15:10,020
Marisa Eisenberg: minus gamma are and an R dot equation, which is gamma are.

86
00:15:10,350 --> 00:15:23,100
Marisa Eisenberg: And we're going to that's your standard sort of model of patients we're going to add to this something that describes what you can measure from your data, and so this is called a measurement equation so maybe we say why.

87
00:15:23,730 --> 00:15:43,110
Marisa Eisenberg: Maybe what I can measure is the total number of people who are currently in fact it so then maybe y is equal to it so that's you know the prevalence of the disease, over time, or maybe I measure something more complicated like I said, my entities, maybe I measure something like.

88
00:15:44,400 --> 00:15:53,340
Marisa Eisenberg: You know the the integral or the sum over a week of the incidents, so the the new cases that are happening i'm going to not.

89
00:15:54,510 --> 00:16:14,040
Marisa Eisenberg: worry too much about this, but you would you'd really do this as an integral but we're not really being super calculus II so i'm going to write something a little wrong, which is a some from you know today oh typing is going to be hard, it does not like my annotate what are today to.

90
00:16:15,180 --> 00:16:24,090
Marisa Eisenberg: Seven days ago so maybe a weekly some of how many new cases you've had or something, and you, but you might you might also do.

91
00:16:24,780 --> 00:16:42,060
Marisa Eisenberg: You might make, maybe your your measurement equation is let's make a different scenario where you can what you can measure is you can measure let's say a proportion, so people often do this come on oh annotate so sad, you can do it.

92
00:16:43,080 --> 00:16:53,700
Marisa Eisenberg: Oh that's not doesn't look at it and let her at all okay all right i'm going to give up on annotate in a minute, it always does this it like works for a little while and then it kind of Barcelona so.

93
00:16:54,210 --> 00:17:03,510
Marisa Eisenberg: Maybe we put a constant in front of this and we say we can measure, a proportion of the total infected individuals, because maybe i'm running some kind of longitudinal.

94
00:17:04,410 --> 00:17:13,740
Marisa Eisenberg: cohort or something and i'm measuring the prevalence of this disease in that cohort, and so I don't get every single infected individual in my whole population but I get.

95
00:17:13,980 --> 00:17:24,480
Marisa Eisenberg: a fraction of them, based on how i'm sampling from the population or something like that, so you decide kind of what it is that you're measuring with your data, you can have multiple measurements, maybe you have a second.

96
00:17:25,320 --> 00:17:30,870
Marisa Eisenberg: You know measurement like why two is going to equal Maybe you can also measure.

97
00:17:31,110 --> 00:17:42,630
Marisa Eisenberg: A proportion of the recovered population so maybe I I also know after someone has been infected I know when they recover, and so I know how many total people have recovered from this disease so.

98
00:17:42,870 --> 00:17:51,660
Marisa Eisenberg: You know, whatever it is that you can measure you write equations essentially describing that and and so me clear all this junk and.

99
00:17:54,060 --> 00:17:56,580
Marisa Eisenberg: Eventually we'll get there, we annotate.

100
00:17:57,630 --> 00:18:01,200
Marisa Eisenberg: So you write out your measurement equations.

101
00:18:01,860 --> 00:18:11,490
Marisa Eisenberg: So these are equations that describe whatever it is that you can measure with your data, so this is some, this is an additional piece of your model it's a model of what you're observing with your data.

102
00:18:11,760 --> 00:18:26,880
Marisa Eisenberg: So you write out those and then let's suppose that you get data time points to 130 and then what we're going to assume is that are observed data that we actually see is given by our model of the measurement why at the appropriate time.

103
00:18:27,480 --> 00:18:36,030
Marisa Eisenberg: plus some observation error some measurement error so there's some noise on whatever it is that we're doing so that's kind of the structure we're going to assume.

104
00:18:36,360 --> 00:18:45,690
Marisa Eisenberg: This is going to then be our complete sort of final overall model of what are observed data is going to look like and you see, you can see that we took.

105
00:18:45,960 --> 00:18:54,480
Marisa Eisenberg: A deterministic system here and we turned it into a just a probability distribution, you know wait by adding this measurement error, because this is going to be.

106
00:18:54,690 --> 00:19:06,870
Marisa Eisenberg: may be normally distributed, or we would assume perhaps that this is a plus on where the rate is this or something you know you're going to put basically whatever you choose, however, you choose your measurement error is going to turn this into now a random variable and so.

107
00:19:07,500 --> 00:19:13,230
Marisa Eisenberg: So that's going to be how we get to something kind of probabilistic for our observed data.

108
00:19:14,130 --> 00:19:16,830
Marisa Eisenberg: Okay, so that's going to be the structure that will use.

109
00:19:17,070 --> 00:19:27,900
Marisa Eisenberg: And this is usually something kind of like this is usually true even if you're doing other kinds of models so if you're doing a stochastic bottle it might be that this is now a stochastic set of equations.

110
00:19:28,170 --> 00:19:35,280
Marisa Eisenberg: And so the whole thing is a little more complicated, but the basic structure of this is is pretty common for how you kind of set up any.

111
00:19:36,180 --> 00:19:45,930
Marisa Eisenberg: Any systems model for premier destination okay does that make sense to folks this this kind of general idea okay awesome excellent okay good.

112
00:19:46,500 --> 00:20:00,270
Marisa Eisenberg: um okay so then um so so let's talk a little bit about the square so actually let me do a little show of hands, how many people have done parameter estimation before it's like done the squares before.

113
00:20:02,160 --> 00:20:06,810
Marisa Eisenberg: Some couple Okay, a few of you so so there's a reasonable chunk.

114
00:20:07,440 --> 00:20:14,580
Marisa Eisenberg: Okay, but not everybody alright so we're going to talk about these squares and then we'll dive into maximum likelihood after that and i'm going to in the.

115
00:20:15,030 --> 00:20:21,090
Marisa Eisenberg: i'm going to use the squares just sort of as an example of a cost function and then we're going to actually talk about numerical optimization.

116
00:20:21,300 --> 00:20:28,620
Marisa Eisenberg: we'll do a couple examples of numerical optimization and then we'll dive into into a more sort of proper statistical framework for this okay so.

117
00:20:29,280 --> 00:20:38,280
Marisa Eisenberg: These squares, so the goal here is, you know we said before that we want to kind of adjust our parameters to make the model simulation match the data as closely as we can.

118
00:20:38,610 --> 00:20:50,790
Marisa Eisenberg: So the residual is is what is basically if we take each data point and we subtract the model and the data to get this little distance between them that's what's called the residual so.

119
00:20:52,200 --> 00:21:01,950
Marisa Eisenberg: So you know Z minus y here is going to be our residual so that's essentially the distance vertical distance between the model and the data.

120
00:21:03,030 --> 00:21:17,700
Marisa Eisenberg: And so you want to do something that sort of minimizes the residuals in some way you might think, to yourself well why don't I just try and take the minimum of all of the some of the residuals but the wrong, then, is that you can have sign issues right so.

121
00:21:18,420 --> 00:21:27,180
Marisa Eisenberg: If one of issues you're subtracting Z minus y then so your data your your actual data LSA minus your model prediction for what.

122
00:21:27,930 --> 00:21:29,940
Marisa Eisenberg: The data is going to look like i'm.

123
00:21:30,390 --> 00:21:44,280
Marisa Eisenberg: Excuse me, I did everything and then, if you if you if you subtract those things you can get different signs right, so if the if the they're both like take this example the model is as below the data here and above the data here.

124
00:21:44,580 --> 00:21:57,900
Marisa Eisenberg: Then, your residual is going to be positive here and negative here, and so the overall some of the residuals zero making it look like a perfect fit, but of course the model doesn't really fit the data that well at all, and so i'm.

125
00:21:58,470 --> 00:22:02,010
Marisa Eisenberg: Just minimizing, this is not such a great idea, there are flavors of.

126
00:22:02,370 --> 00:22:11,490
Marisa Eisenberg: optimization where you would minimize the absolute value of of the of the some of the some of the absolute value of the residuals which would totally that's also totally worthwhile, but for.

127
00:22:11,910 --> 00:22:18,900
Marisa Eisenberg: A number of reasons it ends up being pretty common that what we do instead is to deal with the sign issues and also make.

128
00:22:19,140 --> 00:22:30,690
Marisa Eisenberg: The system kind of nicely behaves we take the square of the residual so we subtract Z and y and then we square that somehow I am missing a parentheses, and that is going to bother me for the rest of time.

129
00:22:31,080 --> 00:22:39,420
Marisa Eisenberg: or I just acquired an extra one anyway it's fine i'll fix that later um but so we take these subtract them square it and then you sum it up.

130
00:22:40,050 --> 00:22:52,500
Marisa Eisenberg: And this is nice, so it means that all your your residuals are going to have the same sign in your summation so that's great and also the square is also Nice because one it's smooth when you get close to zero, but to.

131
00:22:53,940 --> 00:23:11,250
Marisa Eisenberg: It has this this tendency that it penalizes larger error, so if you take a square like this, then when Z minus y is less than one the square is going to actually shrink the thing you can see it here so it's kind of tiny but.

132
00:23:12,300 --> 00:23:21,180
Marisa Eisenberg: If the square like X squared basically the residual so Z minus y squared is plotted as this like blue thing here.

133
00:23:22,170 --> 00:23:31,500
Marisa Eisenberg: And the line of you know y equals X essentially so residual equals whatever cost function thing we're using is is this is this line.

134
00:23:31,800 --> 00:23:41,340
Marisa Eisenberg: When you're when the residuals are small, the square tends to shrink them when the residuals are big the square tends to inflate them and so.

135
00:23:42,090 --> 00:23:51,660
Marisa Eisenberg: So essentially it'll mean that taking least squares will penalize you for having big errors it's not going to care that much if your error is really small.

136
00:23:51,900 --> 00:24:02,010
Marisa Eisenberg: But if you have major major like large residuals it's gonna say Oh, I really don't like that, and then it'll make your cost very hot so least squares is a nice choice in this way.

137
00:24:02,850 --> 00:24:13,170
Marisa Eisenberg: And so, how would one sort of optimize your least squares well Basically, this is a really goofy example it's kind of silly but whatever like.

138
00:24:13,680 --> 00:24:20,010
Marisa Eisenberg: Essentially you're what we're gonna do is we're going to code up a cost function and hand it off to a.

139
00:24:20,490 --> 00:24:26,250
Marisa Eisenberg: Like pre built function in our that is essentially just going to try a bunch of values and try and figure out what.

140
00:24:27,000 --> 00:24:36,750
Marisa Eisenberg: Which values best fit the data we're going to talk about how it decides which values to try in a minute, but the basic idea if you imagine that you had a one compartment model your data was.

141
00:24:37,170 --> 00:24:49,860
Marisa Eisenberg: y is equal to X one, so you have a one differential equation that just describes exponential decay and you start at five and maybe you're trying to estimate this parameter K, one which is the decay rate and.

142
00:24:50,310 --> 00:24:59,130
Marisa Eisenberg: So what you would do basically is you say okay i'm going to give my optimizer a guess for what I think this to one value is.

143
00:24:59,430 --> 00:25:06,660
Marisa Eisenberg: It will simulate the model with that gets it gets some trajectory so some simulation of what it thinks the concentration.

144
00:25:07,020 --> 00:25:14,640
Marisa Eisenberg: looks like over time it decays too fast, so it calculates the some of the squared residuals so basically it's attracts the model from the data.

145
00:25:15,180 --> 00:25:28,170
Marisa Eisenberg: For each of these timelines and squares that sums it up and gets this number so, then the the computer the program would say okay i'm going to try a new value of this co and it tries three.

146
00:25:28,500 --> 00:25:36,660
Marisa Eisenberg: And it turns out the summer squared residuals gets worse it gets higher so our cost function goes up right and so it's Okay, that was a bad direction.

147
00:25:37,080 --> 00:25:41,820
Marisa Eisenberg: let's go the other way, so instead of you know it went from two to three instead of going.

148
00:25:42,300 --> 00:25:52,350
Marisa Eisenberg: towards higher numbers rather sorry instead of going from two to three it's going to say okay two to three was bad let's go to let's go down, so it would say take one, for instance, and then.

149
00:25:52,920 --> 00:25:59,190
Marisa Eisenberg: It would notice basically the the the program would notice if the summer squared residuals is better.

150
00:25:59,460 --> 00:26:08,910
Marisa Eisenberg: than it then my best, you know that i've had so far, so you know I had 3.4 and now it's 1.6 so that's going down, so it would keep moving that direction so it's okay.

151
00:26:09,270 --> 00:26:15,180
Marisa Eisenberg: You know, a smaller co one seems to be working so make it even smaller so we'll try, maybe Point five.

152
00:26:15,480 --> 00:26:23,730
Marisa Eisenberg: And then it gets an even smaller some squared residual so it would actually then say okay keep going try something even smaller, so it would try Point two five.

153
00:26:24,000 --> 00:26:32,910
Marisa Eisenberg: And then it would notice ah whoops I went too far now some squared residuals is worse so obviously it wouldn't only just try those like very.

154
00:26:32,970 --> 00:26:34,140
Marisa Eisenberg: Nice friendly numbers.

155
00:26:34,350 --> 00:26:47,280
Marisa Eisenberg: But it would try a bunch of numbers, and then it would eventually sort of settle on Okay, it seems like Point five is the best and so really what it's going to do is to reverse this cost function surface, so we started.

156
00:26:48,060 --> 00:27:03,840
Marisa Eisenberg: You know here with a K K value of two we tried this three right discovered that was no good and then it would basically sort of meander its way down this Hill and.

157
00:27:04,620 --> 00:27:13,380
Marisa Eisenberg: So it's sort of we start here, it tries a little bit this way it tries a little bit this way, it would discover that it's better in the downward the Left direction.

158
00:27:13,590 --> 00:27:21,570
Marisa Eisenberg: And so it would go left keep going keep going keep going until it finds a spot that seems to be the lowest place it would try something else.

159
00:27:21,780 --> 00:27:30,120
Marisa Eisenberg: and discover like oh no that is worse so eventually it will sort of settle in here and say Okay, I think the best fit value is this one, and so that's what.

160
00:27:30,720 --> 00:27:40,500
Marisa Eisenberg: sort of numerical optimization is going to do now it's going to do that in a much more sophisticated way than what I just described, but that's the basic idea um so.

161
00:27:41,490 --> 00:27:51,930
Marisa Eisenberg: let's talk a little bit so maybe, let me pause there and then we'll talk about how optimization methods work does this whole setup feel okay to people as an idea.

162
00:27:52,590 --> 00:28:03,660
Marisa Eisenberg: Okay, good good good okay so cool okay good lots of thumbs up okay so let's talk a little bit about how optimization optimization works real quick so.

163
00:28:04,470 --> 00:28:07,140
Marisa Eisenberg: we're going to have an objective function or a cost function.

164
00:28:07,590 --> 00:28:17,460
Marisa Eisenberg: F of X, so this is going to be i've used F for a lot of things Sorry, I feel I think F is just my sort of general default function so sorry if it gets called many.

165
00:28:17,790 --> 00:28:24,690
Marisa Eisenberg: lots of things get called ass but anyway, some cost function, which is the function that we're going to try and minimize or maximize or goodness of fit.

166
00:28:25,590 --> 00:28:32,280
Marisa Eisenberg: And it can be a function of lots of different variables that can be set up lots of different ways, and this function is generally going to be.

167
00:28:32,700 --> 00:28:37,290
Marisa Eisenberg: Some sort of function of both our data and the parameters and variables of our model.

168
00:28:37,620 --> 00:28:47,610
Marisa Eisenberg: and historically for like just I don't know whatever historical reasons, the Convention is just to minimize is most optimized that you'll find built into our other packages.

169
00:28:47,880 --> 00:28:53,460
Marisa Eisenberg: are built to minimize a cost assumption so almost all of them want to go down and.

170
00:28:53,700 --> 00:29:00,960
Marisa Eisenberg: If you build your cost function, so that you want to maximize it instead so sometimes people will build a likelihood and you want to find the maximum likelihood.

171
00:29:01,260 --> 00:29:09,090
Marisa Eisenberg: Well it's easy just take negative of your cost function, and that will flip it upside down so now, the best thing, instead of being the top.

172
00:29:09,390 --> 00:29:14,970
Marisa Eisenberg: Is the bottom right, so you can just so just as a heads up that's nothing but you'll see okay so.

173
00:29:15,540 --> 00:29:21,960
Marisa Eisenberg: we're going to have this objective function or this cost function and then we're going to have some variables So these are going to be.

174
00:29:22,290 --> 00:29:27,540
Marisa Eisenberg: inputs and parameters, all these kinds of things, the inputs to the cost function that we can adjust or change.

175
00:29:27,900 --> 00:29:37,320
Marisa Eisenberg: And then we might have some constraints So these are any restrictions to our optimization so, for instance, you know, maybe there are limits on how far you can move in parameter space.

176
00:29:38,040 --> 00:29:53,220
Marisa Eisenberg: or whatever it is so here I just made something up find the biggest number, but our constraints are that it has to be even less than 11 okay if it has to be less than 11, it has to be even our number is 10 right, so you know, whatever so you have some constraints okay so.

177
00:29:54,270 --> 00:30:05,760
Marisa Eisenberg: optimizations method, so a lot of methods out there, do something along the lines of this they take some like we kind of described a minute ago they take in some starting parameter values which are called initial values.

178
00:30:06,870 --> 00:30:16,110
Marisa Eisenberg: And then they evaluate the nearby cost function landscape and then they should pick a new set of parameter values that is going to be better than the last set.

179
00:30:16,440 --> 00:30:25,380
Marisa Eisenberg: And they basically just repeat that, until no further improvement can be made or until they built up a large enough parameter sample that they're sort of complete.

180
00:30:26,130 --> 00:30:35,640
Marisa Eisenberg: And so that that's sort of the underlying idea behind it sort of most optimization methods, but let's talk a little bit more in detail about a few common months so.

181
00:30:36,780 --> 00:30:47,010
Marisa Eisenberg: One of the most common optimization methods out there is a thing called gradient descent, and so gradient descent, is a very fast and pretty easy to use and pretty easy to implement.

182
00:30:47,580 --> 00:30:55,410
Marisa Eisenberg: optimization method that is basically just imagine that your optimization program is going to act like.

183
00:30:55,770 --> 00:31:05,850
Marisa Eisenberg: A skier who's a thrill seeker and wants to always go on the steepest slope possible so they essentially if you have a cost function surface there's a little like.

184
00:31:06,450 --> 00:31:20,880
Marisa Eisenberg: skiing guy who will you start at the initial values and this little skier will determine which direction is the steepest slope and unerringly will ski down that steepest direction and so that's the idea behind.

185
00:31:21,270 --> 00:31:28,890
Marisa Eisenberg: A gradient descent, and they just keep doing that they you know steer death down as steep as they can until they find something where.

186
00:31:29,250 --> 00:31:37,740
Marisa Eisenberg: Everywhere else points up away from them so they're Basically, they just go down until they can't go down anymore, and then stop and so.

187
00:31:38,400 --> 00:31:42,450
Marisa Eisenberg: You know this is fast and and very quick to do.

188
00:31:43,260 --> 00:31:58,380
Marisa Eisenberg: it's a nice approach that it can get stuck in local minimum because it just once it hits the bottom of a minimum it stops that doesn't mean that it ended up in the best you know minimum out there and it can also have a lot of issues with certain kinds of weird surfaces so.

189
00:31:59,850 --> 00:32:03,750
Marisa Eisenberg: You can have this like zigzag issue that comes up a lot and.

190
00:32:04,080 --> 00:32:13,380
Marisa Eisenberg: there's another good example of one is I like to have you if you've ever had soft serve ice cream like this in the picture here, you know, like dairy queen style can ice cream, you know and then.

191
00:32:13,710 --> 00:32:18,450
Marisa Eisenberg: If you so imagine that you put your little skiing guy on the top of the ice cream.

192
00:32:18,840 --> 00:32:26,070
Marisa Eisenberg: And it's evaluating which direction to ski down well because of the ridges in the way the saucer swells up.

193
00:32:26,370 --> 00:32:38,250
Marisa Eisenberg: it's going to end up wanting to make a swirl he'll go this little skier will go down these little ridges like this, because the edges of the ridges suggest that the best direction.

194
00:32:38,850 --> 00:32:50,880
Marisa Eisenberg: Like if you're just a little tiny guy sitting on the ice cream is a weird analogy, but anyway um you know you'll always want to sort of go down the steepest way that you can he doesn't scare wouldn't realize that you could just.

195
00:32:51,540 --> 00:32:57,660
Marisa Eisenberg: ski down the side of the whole ice cream and go down faster that way does that make sense, everybody has a thing.

196
00:32:57,960 --> 00:33:05,820
Marisa Eisenberg: And so, always raining here our AC is out, so my windows are open so i'm sorry in advance if, like there's a bunch of background noise and but anyway.

197
00:33:06,450 --> 00:33:14,190
Marisa Eisenberg: So so surfaces like that, where there are like funny ridges or little kinds of.

198
00:33:15,000 --> 00:33:25,980
Marisa Eisenberg: stations in your surface can cause issues for gradient descent method, because they often don't realize that that's that that's there and that they could hop over it, and.

199
00:33:26,730 --> 00:33:39,600
Marisa Eisenberg: Okay, so gradient descent is is one option, there are much fancier gradient descent out there now, though, that you know can get around a lot of these issues, and one of the things that one of the other approaches out those that is.

200
00:33:39,600 --> 00:33:42,660
Marisa Eisenberg: kind of similar to gradient descent, but often a bit better.

201
00:33:43,200 --> 00:33:54,510
Marisa Eisenberg: wow it's debatable, if you would want to think of it as the other is what are called simplex algorithm so the most famous one is probably never need, and this is the default in a lot of optimization program so.

202
00:33:54,960 --> 00:34:11,640
Marisa Eisenberg: These are similar in in in sort of efficiency, often to gradient descent, but they get around some of the issues, the gradient descent hat, so the idea behind a simplex algorithm I like to the analogy, I like to use for this one is imagine that.

203
00:34:12,810 --> 00:34:25,230
Marisa Eisenberg: You have like a a handkerchief or a napkin or a like a is a paper towel, but that is willing to sort of like flop its way down a hill so it's like a little.

204
00:34:25,980 --> 00:34:33,180
Marisa Eisenberg: shaped like a square or triangle, or something like that and basically imagine it's like sitting on a hill and it's going it's tumbling downward.

205
00:34:33,450 --> 00:34:40,320
Marisa Eisenberg: You know, but it doesn't crumble, but so that it might like flop like this and flip like that and sort of flip flop its way down the hill.

206
00:34:40,560 --> 00:34:45,600
Marisa Eisenberg: And so i'm simplex algorithms are doing something akin to that, basically, you have.

207
00:34:45,930 --> 00:34:55,230
Marisa Eisenberg: A few points that are each evaluating where they are so you can see it here with this little triangle three points, maybe that each evaluate where they are.

208
00:34:55,440 --> 00:35:09,570
Marisa Eisenberg: And then, depending on who's the lowest point the triangle, or the you know, whatever it is sort of flip flops its way downward and chooses new points to be part of the triangle, based on sort of the direction of steepest descent and.

209
00:35:10,680 --> 00:35:17,580
Marisa Eisenberg: The triangle can shrink and shrink and expand and change size, but it can also sort of flip flop its way down and.

210
00:35:18,270 --> 00:35:31,500
Marisa Eisenberg: or slide or whatever and and and the nice thing about this is that, because you have multiple sort of points on this simplex you often get around some of the issues like that soft serve ice cream issue where it will, it will have points.

211
00:35:31,740 --> 00:35:42,600
Marisa Eisenberg: Over the edge of a ridge of the ice cream, and so it will manage to flip flop down the side of the ice cream so here's an example of a simplest algorithm sort of working, you can see it sort of.

212
00:35:42,990 --> 00:35:50,010
Marisa Eisenberg: split flaps its way until it finds them no and now it's still can get stuck in local minima right, so we started it here.

213
00:35:50,280 --> 00:35:59,340
Marisa Eisenberg: And it was clearly on the edge of this minimum so it never noticed the existence of these other minima and if one of these was better it wouldn't it wouldn't find it but.

214
00:35:59,850 --> 00:36:09,270
Marisa Eisenberg: But it does get around some of the issues that come up with with pure gradient descent and okay so so that's to kind of local methods so.

215
00:36:10,260 --> 00:36:15,570
Marisa Eisenberg: optimization methods often get broken up into to kind of broad categories, what are called local methods.

216
00:36:16,170 --> 00:36:28,770
Marisa Eisenberg: which use you start them in a position, and they act like a skier or a napkin or whatever you know they use local information right around where where that initial position is to kind of decide where to go next.

217
00:36:29,400 --> 00:36:39,540
Marisa Eisenberg: Global optimization methods, there are tons of them, but these tend to be more distribution based they tend to use large sort of samples of parameters and.

218
00:36:40,080 --> 00:36:46,410
Marisa Eisenberg: and evaluate the goodness of fit of all the elements of these large samples to explore the space more fully so.

219
00:36:47,280 --> 00:36:57,570
Marisa Eisenberg: They usually have sort of two characteristics, they often take like a large parameter sample and they explore the space more fully in that way, and then the other thing that they do is they often have some.

220
00:36:58,230 --> 00:37:09,270
Marisa Eisenberg: ability to accept slightly worse cost function values temporarily so in order to explore the space more in hopes of finding better cost function values later.

221
00:37:09,540 --> 00:37:16,320
Marisa Eisenberg: And they give a better view of the cost function surface and we're going to talk about one specific one MC MC later today.

222
00:37:17,160 --> 00:37:32,610
Marisa Eisenberg: But they are often a lot slower, so the downside of global methods is that you know, an optimization that I could run in a minute or two well, maybe not this is maybe you know, a minute or two with something like you know, like a like a.

223
00:37:34,080 --> 00:37:48,690
Marisa Eisenberg: One of these kinds of more local methods and might take like an hour or you know potentially even like longer depending on how much detail, you want to explore in of the whole cost function surface if you use a global method so and.

224
00:37:49,830 --> 00:38:04,140
Marisa Eisenberg: it's it's it's not well, maybe not an hour it's only a minute on the island, I don't know it's longer you can definitely have situations where you need to run a global optimization method for something on the order of days or things like that, if you have a complex model so.

225
00:38:05,250 --> 00:38:07,200
Marisa Eisenberg: So just as sort of a thing to.

226
00:38:07,530 --> 00:38:17,790
Marisa Eisenberg: keep in mind it's often what I often find for myself, is that I often end up using a bit of both so it's often the case that i'll do some initial exploration with something fast like a gradient descent or an elder need.

227
00:38:18,000 --> 00:38:22,650
Marisa Eisenberg: And then i'll use a global optimization method once I kind of get a sense of kind of how things are looking.

228
00:38:22,890 --> 00:38:33,600
Marisa Eisenberg: And do I feel like i'm kind of roughly on the right track okay great then now i'm going to let this thing run for a long while, and really fully explore the parameter space and use a global method and okay so.

229
00:38:34,650 --> 00:38:47,100
Marisa Eisenberg: you're you know Those are just some like vignettes to kind of give you a feel for some of these things if you're interested, you know you can I can post some materials, and you can look up there's lots you can read about all these different optimization methods and.

230
00:38:48,270 --> 00:38:58,260
Marisa Eisenberg: yeah we'll talk about I did and identify ability, a minute we can hit on this already i'm going to pause there, and maybe do a couple of examples other questions about any of the stuff we've talked about so far.

231
00:39:01,800 --> 00:39:05,580
Marisa Eisenberg: hey so let's do a couple little examples here.

232
00:39:11,280 --> 00:39:18,450
Marisa Eisenberg: So Okay, so I i've got a couple, these are these are posted if you go to the.

233
00:39:21,810 --> 00:39:34,170
Marisa Eisenberg: To the the page for today on Wednesday there's a parameter estimation page and in there, if you click optimization examples you'll find this the same the same page and.

234
00:39:34,680 --> 00:39:38,040
Marisa Eisenberg: And so okay so so we're going to do a couple different examples.

235
00:39:38,910 --> 00:39:46,590
Marisa Eisenberg: So i'm going to apologize, in advance, I last night was converting I usually teach a different class for optimizations that I teach in Python.

236
00:39:46,860 --> 00:39:55,950
Marisa Eisenberg: And I was converting it to our and I didn't quite have time to convert all of them, and so we're going to do one example at all in Python but i'll be fine well we'll just.

237
00:39:56,310 --> 00:40:04,020
Marisa Eisenberg: i'll just it's structured very much the same as the rest of these so just ignore the syntax and just we'll just talk through it and I think it'll be okay.

238
00:40:04,320 --> 00:40:10,350
Marisa Eisenberg: Okay, so let's do one quick optimization example here so let's do a one dimensional function so.

239
00:40:10,530 --> 00:40:20,970
Marisa Eisenberg: Imagine that our cost function so i'm not even going to make a real model i'm just going to do, like a goofy silly example, so in this case, imagine that our cost function.

240
00:40:21,240 --> 00:40:34,470
Marisa Eisenberg: for whatever reason, is equal to parameter one squared plus two, so this is a totally made up thing but suppose it's parameter one squared plus two and we're trying to estimate the value of parameter one so parameter one is our parameter we're estimating.

241
00:40:35,460 --> 00:40:38,160
Marisa Eisenberg: This is too small, can you read this if I do this, is that better.

242
00:40:39,600 --> 00:40:47,400
Marisa Eisenberg: To help with the visibility okay so parameter one is the parameter we're adjusting and F is going to be our cost function values, so we want to minimize as.

243
00:40:47,700 --> 00:40:56,130
Marisa Eisenberg: We want to basically adjust P one to minimize that so we're going to use the optimum function in our to do this to try and do this optimization so.

244
00:40:56,610 --> 00:41:03,570
Marisa Eisenberg: You can you can take this example to and try other ones, you can totally just change the cost function to whatever you want and see how it works so.

245
00:41:03,750 --> 00:41:17,640
Marisa Eisenberg: we're going to first define our cost function as a function so we're going to say cost is going to be a function of one parameter and it's going to return the parameter squared plus two so it's just literally a function that returns this this cost function value.

246
00:41:18,120 --> 00:41:30,690
Marisa Eisenberg: And then um let's just plot first What does this mean in this case it's such a simple cost function that we can just plot what it looks like so here I just said here's some parameter values, I made a sequence from minus five to five.

247
00:41:30,990 --> 00:41:37,200
Marisa Eisenberg: And I wanted 100 points between minus one minus five to five, and then I took the cost.

248
00:41:37,500 --> 00:41:48,990
Marisa Eisenberg: function value of each of these points and and call that F vous here and then let's plot P one vowels are soft vowels and you get something like this, so it's a nice little Square and.

249
00:41:49,590 --> 00:41:58,530
Marisa Eisenberg: That you know that's fine so it's a nice little it's, this is the nice bull shape surface that I wrote yay on when we were looking at those heat maps right, this is perfectly fine.

250
00:41:58,980 --> 00:42:02,490
Marisa Eisenberg: it's not complicated at all, will do a more tricky one in a minute.

251
00:42:02,970 --> 00:42:13,260
Marisa Eisenberg: But so okay so so let's do some optimization so let's suppose that we start at four just for kicks I pick something at random so we're going to start our search over here at four.

252
00:42:13,680 --> 00:42:18,030
Marisa Eisenberg: And then here's where we actually do the optimization so we're going to run.

253
00:42:18,870 --> 00:42:21,660
Marisa Eisenberg: The opt in function which will do our optimization.

254
00:42:21,960 --> 00:42:35,340
Marisa Eisenberg: What opt in takes us arguments, it wants the initial guests for our parameter so we're going to give it our initial guests for P, one which is going to be for it once the cost function so that's this one so that's we're going to pass it.

255
00:42:35,850 --> 00:42:42,600
Marisa Eisenberg: input to the optimization function this cost function that we made and and then over here you don't have to put this.

256
00:42:43,170 --> 00:42:51,240
Marisa Eisenberg: The default method for opt in is a builder Mead and but no other need remember know the meat is the one that was the flopping napkin.

257
00:42:51,570 --> 00:43:05,280
Marisa Eisenberg: And if you only have a one dimensional surface, you have a very narrow napkin and it's a weird imagine having a triangle and then you're trying to put it in one day right, so it gets collapsed into like a weird line right and so um.

258
00:43:06,360 --> 00:43:13,770
Marisa Eisenberg: It never me can be unreliable for one day optimization so I picked a gradient kind of method instead so we're just going to use that but.

259
00:43:13,920 --> 00:43:25,410
Marisa Eisenberg: It will it will give you a warning message, but work perfectly fine if you take this out and just use an elder meat, so you don't need this, but just for cleanliness I decided to do it so we're going to run this optimization function.

260
00:43:25,800 --> 00:43:35,340
Marisa Eisenberg: And and and and so you should get something like this, so we get if you, you get I called this rez results, whatever you want to call it, and and so.

261
00:43:35,850 --> 00:43:49,290
Marisa Eisenberg: We get this this estimates, and so, if you print what's in here, you can see that rest contains a few different things, so this little I, at the end of this little chunk of code I printed what was in rez and so.

262
00:43:49,590 --> 00:43:55,710
Marisa Eisenberg: What it spits out is this, so this is what the console would would return speaking of which actually.

263
00:43:55,890 --> 00:44:02,220
Marisa Eisenberg: Yesterday, somebody asked what the best way to record what's in your console would be if you wanted to save it for future stuff.

264
00:44:02,400 --> 00:44:09,420
Marisa Eisenberg: And I didn't I didn't think about it, but you should actually if you want to do that in a nice way you should look into our markdown, which is what i'm using to do this.

265
00:44:09,630 --> 00:44:15,150
Marisa Eisenberg: And it's a little kind of notebook style thing instead of running things in the console and by scripts.

266
00:44:15,330 --> 00:44:25,410
Marisa Eisenberg: You run you build a little notebook where you can kind of keep your notes of what you're doing and then everything that spits out of your console gets recorded into the notebook automatically so that's something you might want to kind of.

267
00:44:25,590 --> 00:44:29,160
Marisa Eisenberg: play with it it's just nice for a lot of different things, you can use it to build.

268
00:44:29,670 --> 00:44:35,730
Marisa Eisenberg: Like you could actually write your whole paper in our markdown if you want to they have some nice like templates and stuff and then all your code.

269
00:44:36,060 --> 00:44:47,280
Marisa Eisenberg: Like it will live run everything and compile your favorite it's kind of a I know people who do this and it feels very intense to me to have your code and you're writing all in one thing I don't particularly like it myself but.

270
00:44:47,490 --> 00:44:55,260
Marisa Eisenberg: To each their own so um yeah but anyway, you might want to check it out, because if that would be a that's a clean way to yeah so anyway okay so.

271
00:44:56,310 --> 00:45:06,900
Marisa Eisenberg: In this case I recorded the console output here, and so what we get is the the parameters, so this is the parameter value that it found so it finds this.

272
00:45:07,560 --> 00:45:15,750
Marisa Eisenberg: teeny tiny number, which is very close to zero, you can see here that the parameter that it up to find basically the optimum value is right here around zero.

273
00:45:16,020 --> 00:45:20,190
Marisa Eisenberg: And so, this is very, very close 10 to the negative 12 it's basically found zero.

274
00:45:20,460 --> 00:45:30,900
Marisa Eisenberg: The cost function value at that point, so this is saying the best fit parameter I could find was 2.8 negative 2.89 times 10 to the negative 12 whatever that is.

275
00:45:31,260 --> 00:45:47,190
Marisa Eisenberg: The cost function value at that point of parameter space is to and then here, it shows us a bunch of information about and essentially did how many function, how many times they need to call the cost function to figure this out how many times they need to evaluate the gradients.

276
00:45:48,210 --> 00:45:58,650
Marisa Eisenberg: And then did did the system converge and a zero is good, so that means yes there's no warning message, or anything like that so that's sort of what you get so that that looks all fine and Nice.

277
00:45:58,980 --> 00:46:09,690
Marisa Eisenberg: And so let's plot, what did we find for our best fit parameter value it found this teeny tiny you know right there just right around zero so that that looks good that's what we would want to see.

278
00:46:10,050 --> 00:46:22,500
Marisa Eisenberg: And so that's a 1d optimization example and we'll do least squares in just a second, I want to do a 2d example, but I didn't have time to convert the plots over because they're nice heat maps so we're going to do this 2d example in Python.

279
00:46:22,920 --> 00:46:29,520
Marisa Eisenberg: And just bear with me, it will be I think it'll be okay, are there questions about the 1d example before we go do the 2d.

280
00:46:31,140 --> 00:46:40,590
Marisa Eisenberg: Does that make sense to everybody feel Okay, the basic thing is just the syntax for how you like the formatting for how you run this opt in function is really the idea.

281
00:46:41,940 --> 00:46:57,000
Marisa Eisenberg: Okay, so let's do that we're going to look at some Python code here real quick, this is linkedin the optimization examples thing it's the same the Python you if you're just curious Actually, this is the same example that we just did but it's coated in Python instead.

282
00:46:58,140 --> 00:47:14,520
Marisa Eisenberg: And OK, so now let's look here so oh no that's different methods that's why you don't need that let's do an optimization with two parameters Okay, so what we're gonna do is we're going to do a two parameter version instead and.

283
00:47:15,270 --> 00:47:22,830
Marisa Eisenberg: Where we're going to do, hang on, I think, actually, I did not hilariously well First things first let me fix a couple things here.

284
00:47:24,420 --> 00:47:33,330
Marisa Eisenberg: I load all this junk up in in yeah well, I think we need any of this but, just in case I put something in here, though.

285
00:47:33,870 --> 00:47:37,620
Morgan Caitlin Byrd: geisenberger you have a comment in the chat if you could possibly do man.

286
00:47:37,620 --> 00:47:39,120
Marisa Eisenberg: hello, yes, yes, yes.

287
00:47:39,990 --> 00:47:41,550
Marisa Eisenberg: Here we go how's that better.

288
00:47:43,980 --> 00:47:44,310
Please.

289
00:47:49,230 --> 00:47:52,530
Marisa Eisenberg: is choosing basically little zoom out a little bit how's that.

290
00:47:55,620 --> 00:48:03,390
Marisa Eisenberg: Okay okay so let's try a two parameter example um so what we're gonna do.

291
00:48:03,690 --> 00:48:11,670
Marisa Eisenberg: Is we're going to make a cost function that has two parameters now so we're going to optimize not just one parameter, but we now need to adjust.

292
00:48:11,880 --> 00:48:24,630
Marisa Eisenberg: parameter one and parameter to to find to minimize this cost function, and so I made up some function again, this is just it doesn't have any real meaning I just picked something that I was like Okay, whatever it's going to be something nice a nice shape.

293
00:48:25,350 --> 00:48:41,070
Marisa Eisenberg: So so so let's suppose that our cost function looks like this, and so now we want to pick the values of P one P two that minimize this cost function, so I ran this clearly with one of my weird examples last time, so let me not do that so i'm.

294
00:48:45,600 --> 00:48:46,320
Doing.

295
00:48:48,810 --> 00:48:52,530
Marisa Eisenberg: Okay, all right, so what we're gonna do is we're gonna we're gonna.

296
00:48:53,130 --> 00:49:06,360
Marisa Eisenberg: we're gonna run a 2d optimization you can do the exact same thing with optum i'll post the code using optum in our to do this example I just didn't have time to do the plot league nice surface plot thing that.

297
00:49:06,840 --> 00:49:19,170
Marisa Eisenberg: That I was going to do in Python and so i'll turn that into a nice our plot and just a little bit later today, I was late so didn't quite quite get to it, but the ideas same idea so it's actually.

298
00:49:19,620 --> 00:49:30,450
Marisa Eisenberg: kind of a useful thing it's a different language, but the structure of the code is exactly the same like not the not how you write it not the syntax but the logical structure of the code is exactly the same so.

299
00:49:31,200 --> 00:49:40,110
Marisa Eisenberg: So what we're going to do is we're going to define a function, so the formatting for this looks different, but the idea is the same as it would be an art we're going to define a function.

300
00:49:41,010 --> 00:49:44,970
Marisa Eisenberg: The cost is going to be a function of two parameters P one P two.

301
00:49:45,240 --> 00:49:55,980
Marisa Eisenberg: And it's going to return this little cost function that we decided up here, so this is going to be our cost function we're going to try some other different cost functions in a little bit, but those are commented out for now.

302
00:49:56,220 --> 00:50:01,620
Marisa Eisenberg: And then, what we're going to do let's make a plot of what this cost function looks like so.

303
00:50:03,210 --> 00:50:09,810
Marisa Eisenberg: I really run this whole thing all is one big blob I do okay so we're gonna we're going to make a cost function.

304
00:50:10,020 --> 00:50:18,630
Marisa Eisenberg: And then we'll plot i'm not going to don't worry too much about the syntax here but we're going to plot what this cost function looks like and then we're going to.

305
00:50:19,620 --> 00:50:30,120
Marisa Eisenberg: Basically use instead of opt in in our we're going to use the minimize function, which is a, which is in the optimized package in.

306
00:50:30,750 --> 00:50:41,640
Marisa Eisenberg: In Python and we're going to we're going to do this, but it's the same idea you give it the cost function you give it the initial parameter values, you tell it what method you want to use and it runs the optimization so the code.

307
00:50:42,210 --> 00:50:48,600
Marisa Eisenberg: It looks different, but it is the exact same structure as what we would do in the other so let's just run this really quick.

308
00:50:50,340 --> 00:50:59,160
Marisa Eisenberg: Okay, and so what we get so if we plan our cost function, it looks I probably should have said, my axes ranges differently, but whatever it looks like a nice ball right so.

309
00:50:59,370 --> 00:51:04,800
Marisa Eisenberg: It gets better, as you go towards I was playing with color schemes to so it gets better, as you go towards green.

310
00:51:05,730 --> 00:51:15,870
Marisa Eisenberg: And you can see there's a nice kind of ball shape to it, so if you run the optimization if it runs successfully, and if you can see what.

311
00:51:16,230 --> 00:51:27,030
Marisa Eisenberg: The cost function value was etc, and then here this little will will do this in our later, and you can see what it looks like, but these are the two values that are found for P one.

312
00:51:27,690 --> 00:51:41,490
Marisa Eisenberg: Here and P to here, so these are the optimal parameter values that it found so if we plot that on top, you can see that it finds nice good reasonable optimum parameter values right it gets down to the bottom of the ball and that looks perfectly fine but.

313
00:51:42,270 --> 00:51:50,040
Marisa Eisenberg: let's fiddle with this a little bit, I want to do a slightly different one and that will let us.

314
00:51:51,540 --> 00:51:58,050
Marisa Eisenberg: sort of look at some weird costs functions that can cause some issues so i'm going to change this.

315
00:51:59,220 --> 00:52:04,890
Marisa Eisenberg: To be a different cost function and what this cost function is going to do.

316
00:52:05,220 --> 00:52:12,900
Marisa Eisenberg: Is this is a different it's got a P one to the fourth minus P one squared whatever it's some formula, but.

317
00:52:13,170 --> 00:52:23,820
Marisa Eisenberg: The reason I picked This is because it's a it's a function that has to minimum, so this is like it'll have to to basins that you could land in either one.

318
00:52:24,180 --> 00:52:32,520
Marisa Eisenberg: And they're very close to each other and we'll see that this actually causes some issues for how the optimizer behaves so what happens so first let's start here so.

319
00:52:34,530 --> 00:52:55,170
Marisa Eisenberg: Wait interesting Okay, so we have two different minimum here, you can see that that it, you know we've we've got to two, so this there's one here and one here, so these are two different little balls to minimize that it could find and and actually wait, I meant to I forgot to do.

320
00:52:56,190 --> 00:53:00,630
Marisa Eisenberg: let's do it first without using elder meat, so the default in.

321
00:53:01,770 --> 00:53:11,460
Marisa Eisenberg: Python so the default an artist, you know the meat, the default in Python I forget it, which method, it is to use but it's a gradient based method is is the default in Python so.

322
00:53:11,940 --> 00:53:19,770
Marisa Eisenberg: So the the default here is going to be a gradient descent type of algorithm and what we're going to see yeah okay so we've got these two minima.

323
00:53:20,550 --> 00:53:27,300
Marisa Eisenberg: It converges successfully, so it says, I got to I found a minimum I was successful and.

324
00:53:27,570 --> 00:53:36,690
Marisa Eisenberg: It shows the the details about how it did it and how what the jacoby and and all the different things look like, so this is a bunch of stuff about how the gradient descent algorithm behave.

325
00:53:37,050 --> 00:53:44,820
Marisa Eisenberg: But it terminates successfully, so it says, I found a minimum I did it and then it gives you the values of q1 and q2 that it found.

326
00:53:45,360 --> 00:53:52,170
Marisa Eisenberg: If you look at where it actually landed, it did not find either minimum, which was kind of interesting so not only did it.

327
00:53:52,890 --> 00:54:01,530
Marisa Eisenberg: gets you know, usually you think of when you have multiple minima, but the problem is that your optimization algorithm could get stuck in one of the two minutes or one of.

328
00:54:01,860 --> 00:54:17,040
Marisa Eisenberg: However, many minima you have, but in this case it actually got stuck on the Ridge between the two minutes so it's like you have two bowls and then a little lip in between, and he got perched right on that lip in between, and so.

329
00:54:17,910 --> 00:54:22,830
Marisa Eisenberg: What happened probably is that we started I don't know where we started but let's say we started somewhere up here.

330
00:54:23,070 --> 00:54:36,150
Marisa Eisenberg: It came down in gradient descent and ended up landing in between, and for whatever reason, decided that that was good enough and didn't end up finding either one of these two minimum which is kind of interesting so um.

331
00:54:36,660 --> 00:54:47,460
Marisa Eisenberg: I wanted to point that out as an example that you can have situations where your optimization it's a it's an algorithm and it's not perfect right like it will there will be situations where it says.

332
00:54:47,760 --> 00:54:51,990
Marisa Eisenberg: I did it, I found a minimum success it worked and.

333
00:54:52,470 --> 00:55:03,120
Marisa Eisenberg: You but it's actually not and, in this case we're lucky because there's only two parameters, so we can plot it and visualize, but if you had a high dimensional system was 15 parameters that you were fitting.

334
00:55:03,900 --> 00:55:09,540
Marisa Eisenberg: I don't know how you, you know you would you would be very hard visually to plot this and be able to detect it so.

335
00:55:09,810 --> 00:55:21,060
Marisa Eisenberg: it's really important to do some additional exploration of the parameter space try different methods do some parameter sampling those kinds of things, because if you just kind of let it.

336
00:55:21,450 --> 00:55:30,420
Marisa Eisenberg: run it can spit something out that seems like your work, but didn't actually work, and so, if you instead use builder need so let's change this code.

337
00:55:33,510 --> 00:55:34,950
Marisa Eisenberg: us know to meet instead.

338
00:55:36,000 --> 00:55:42,510
Marisa Eisenberg: Now it finds this and that's because the simplex is is is nice a single gradient descent.

339
00:55:42,780 --> 00:55:56,400
Marisa Eisenberg: Only checks information for a single point at a time, whereas a simplex algorithm you know it's like that napkin so it has multiple points that it's aware of at all times and so that means that when it gets to this Ridge, it is, it has kind of like.

340
00:55:57,180 --> 00:56:08,850
Marisa Eisenberg: more space that it's occupying, and so it can tell oh actually like you know it clearly landed slightly to the right on the Ridge, and so it ended up falling into the right pin minimum.

341
00:56:09,090 --> 00:56:17,010
Marisa Eisenberg: So it still doesn't deal with the fact that you have two different minimum on it only found one, but it does at least find a minimum correctly so.

342
00:56:17,850 --> 00:56:23,760
Marisa Eisenberg: Okay, so that's the 2d example there's a bunch of other 2d examples in here, if you want to.

343
00:56:24,510 --> 00:56:29,730
Marisa Eisenberg: You have access to this file, but you can't edit it, but you can make your own copy and then, if you want to try running it.

344
00:56:29,970 --> 00:56:40,650
Marisa Eisenberg: yourself with different ones, you can go for it i'll also convert this example into our later today and then put it up there, I just didn't have time to make the fancy heat map plots and so.

345
00:56:41,310 --> 00:56:48,600
Marisa Eisenberg: i'll do that in a little bit and then you'll be able to play with this, but I put a few other class functions in here that you can try, so you can.

346
00:56:48,810 --> 00:56:56,220
Marisa Eisenberg: play around with like weird functions that have many different minima and stuff like that, just to kind of goof off with them just see what what it, how it works.

347
00:56:57,450 --> 00:56:59,430
Marisa Eisenberg: OK questions about any of that.

348
00:57:01,980 --> 00:57:15,390
Marisa Eisenberg: So hopefully everybody kind of gets a feel for sort of like the issues that can crop up with some of these methods okay good good good good Okay, so let me switch back and have one more example that I wanted to do.

349
00:57:16,890 --> 00:57:33,420
Marisa Eisenberg: yeah okay so let's do one more example and then we'll get back to the slides and I just want to show you something those were totally made up, and you know cost functions, and so I want to show, something that has a little bit of a more.

350
00:57:34,680 --> 00:57:44,760
Marisa Eisenberg: Realistic model that is underlying the cost function so let's do a like a least squares kind of linear regression, just as a isn't going to be really silly was just.

351
00:57:45,360 --> 00:57:55,260
Marisa Eisenberg: Like one one variable one input variable and we're gonna we're gonna literally fit the line y equals mx to some data is what we're gonna do here right so super simple.

352
00:57:55,770 --> 00:58:02,190
Marisa Eisenberg: But just as an example, just to kind of give us a sense of like How would you use this opt in function.

353
00:58:02,430 --> 00:58:12,150
Marisa Eisenberg: When you have something a little more realistic than just this made up kind of cost function so let's do something that's a little more more real so suppose that we have our model, a model.

354
00:58:13,980 --> 00:58:25,320
Marisa Eisenberg: y equals mx and m here is going to be a parameter that we're estimating and and X is a is a is a variable that we have some data so.

355
00:58:25,860 --> 00:58:35,190
Marisa Eisenberg: So basically X is going to be like a cove area it's something like an input variable of some kind, and why is going to be an output variable so maybe this is a risk factor for something.

356
00:58:35,460 --> 00:58:45,630
Marisa Eisenberg: And this is the you know incidents, or whatever of that disease or something I guess and okay as a function of whatever this input variable is and so.

357
00:58:46,380 --> 00:58:54,870
Marisa Eisenberg: As an example that I just totally made it up, I tried to find data on this actually but it turns out that it's it's more like everything in life.

358
00:58:55,440 --> 00:59:04,380
Marisa Eisenberg: It is more complicated than you might think, so I thought I thought to myself oh what am I get data and humidity level, and then the number of inches of rain that you get and I thought for sure.

359
00:59:04,710 --> 00:59:10,500
Marisa Eisenberg: i'll have like a nice simple association that we can fit it does not so.

360
00:59:11,340 --> 00:59:22,710
Marisa Eisenberg: So one day, think of a nice easy one, but anyway say let's say daily humidity level so X is the humidity level for the day and why is the number of inches of rain that you get and so.

361
00:59:23,250 --> 00:59:31,350
Marisa Eisenberg: We would say, and then would be the slope or the sort of association or effect of humidity on rainfall levels and so.

362
00:59:33,090 --> 00:59:50,310
Marisa Eisenberg: In our like made up example, but so let's make instead of using real data we're going to simulate some fake data on humidity and rainfall just to kind of make up something so let's suppose we're going to generate some simulated data and we're going to make the true.

363
00:59:51,450 --> 01:00:03,120
Marisa Eisenberg: value is going to be point two, five and so, then I just picked some numbers, so I made a little sequence here from 20 to 100% humidity so between 20% 100% humidity.

364
01:00:03,480 --> 01:00:13,260
Marisa Eisenberg: And, and I picked 50 points across 20 to 100% humidity and then I just said let's suppose the rainfall data is normally distributed.

365
01:00:14,250 --> 01:00:21,270
Marisa Eisenberg: With we're going to take 50 normally distributed points where each one is going to have the mean is going to be.

366
01:00:21,810 --> 01:00:27,690
Marisa Eisenberg: The the what our model would predict right, so the mean is going to be m times X and.

367
01:00:28,290 --> 01:00:41,580
Marisa Eisenberg: The standard D let's say is going to be five I just made that up so if you get if you do this so you draw a normally distributed sample or for each point each humidity level we basically sample a normal distribution with the.

368
01:00:41,790 --> 01:00:49,890
Marisa Eisenberg: mean being m times X for that humidity level and the standard Eve being five you get a nice, you know little.

369
01:00:50,610 --> 01:00:54,090
Marisa Eisenberg: Fake data set that looks like that, so our fake data set is going to be.

370
01:00:54,270 --> 01:01:05,790
Marisa Eisenberg: humidity here inches of rainfall here, and you have a nice happy in your relationship okay so pretend that in the real world, this was how humidity and rainfall are associated, and this is the data that we actually got.

371
01:01:06,570 --> 01:01:15,540
Marisa Eisenberg: And then, now we would say pretend I got this data, and I want to fit it with a linear model so i'm going to try fitting this thing with a linear model.

372
01:01:15,810 --> 01:01:23,580
Marisa Eisenberg: So what we're gonna do is we're actually going to now our cost function gets a little more complicated we're going to actually have our cost function.

373
01:01:24,390 --> 01:01:30,540
Marisa Eisenberg: rely on we're going to layer our functions basically so we're going to build a function that runs our model.

374
01:01:30,870 --> 01:01:38,670
Marisa Eisenberg: And then the cost function is going to use that model function to calculate the cost so we'll make a function called model.

375
01:01:39,000 --> 01:01:45,480
Marisa Eisenberg: And this function is just going to be our y equals mx model here, so when you when you.

376
01:01:45,810 --> 01:01:57,060
Marisa Eisenberg: When you feed in an M and N X to our model function it returns, why it just gives you back and times X right and so that's what this is so, this is basically just.

377
01:01:57,570 --> 01:02:06,030
Marisa Eisenberg: A code way of capturing the equation y equals mx right that's what this is and then we're going to write a least squares cost function.

378
01:02:06,630 --> 01:02:15,300
Marisa Eisenberg: So here what we're going to do is we're going to say the cost is going to be a function of your m your your current value of your parameter and.

379
01:02:15,930 --> 01:02:22,110
Marisa Eisenberg: The humidity data and the rainfall data, and so what we're going to do so, we have.

380
01:02:22,500 --> 01:02:33,990
Marisa Eisenberg: Actual data on rainfall we're going to use the model function here to calculate what the model would predict the rainfall is based on the humidity and the slope parameter.

381
01:02:34,260 --> 01:02:42,450
Marisa Eisenberg: So the model will say Okay, if the humidity is this and the slope is that then here's what I would predict the rainfall should be.

382
01:02:42,840 --> 01:02:51,600
Marisa Eisenberg: Will calculate the difference, the residuals between the model and the data, so our model prediction here and the actual data that came from the input to the function.

383
01:02:52,140 --> 01:02:59,730
Marisa Eisenberg: Will calculate the residuals will square them to get the sum of square and some them to get the sum of squared residuals that's our cost value.

384
01:03:00,030 --> 01:03:05,760
Marisa Eisenberg: And then we're going to just return that cost, and so this little cost function so notice that it takes in.

385
01:03:06,180 --> 01:03:24,990
Marisa Eisenberg: The current value of the parameter and the data that we have and it uses this model function that we built over here to calculate the cost, and so we could run this model function and make sure that it works, we can run the cost function and make sure that it works seems like it's fine.

386
01:03:26,310 --> 01:03:33,690
Marisa Eisenberg: So notice that here, if you give a slope of one and the humidity data that we have here's what it predicts for the number of inches of rainfall.

387
01:03:33,930 --> 01:03:45,660
Marisa Eisenberg: Not probably super realistic because we never got up to 100 in the in the actual one, but you can see what it does and then you can calculate the cost function value for that okay so that's going to be our setup.

388
01:03:46,170 --> 01:03:57,090
Marisa Eisenberg: Now, what we have out of this is a function that when you feed it the parameters and a little bit extra stuff the data and whatever it returns, the cost function value right.

389
01:03:57,450 --> 01:04:07,860
Marisa Eisenberg: And so, this is what optum needs to run right often just needs it doesn't care what the function is it just needs a function that it can optimize the parameters for and so.

390
01:04:08,610 --> 01:04:17,430
Marisa Eisenberg: we're going to say suppose our initial guests for em is going to be one then here this opt in call is where we're actually going to do the.

391
01:04:18,030 --> 01:04:27,270
Marisa Eisenberg: The optimization and so we're going to say i'm going to take my initial guests i'm going to use the cost function as my cost function and then.

392
01:04:27,660 --> 01:04:42,930
Marisa Eisenberg: So all the other costs functions that we built before they only took one argument, they took a single parameter or a pair of parameters, whatever it is, as their arguments, so that the only argument, they took was the thing that often needs to optimize here.

393
01:04:43,980 --> 01:04:53,730
Marisa Eisenberg: This cost function has some extra arguments that it needs to calculate the cost, but that aren't the parameters, you want to optimize right, this is the data.

394
01:04:53,970 --> 01:05:01,260
Marisa Eisenberg: So we're not going to optimize this this is stuff that the cost function just needs in order to be able to calculate the cost and so.

395
01:05:01,800 --> 01:05:12,060
Marisa Eisenberg: What we're going to do is we give those to the optimum function here, and it will know that if I say X data is this why data is that.

396
01:05:12,420 --> 01:05:20,790
Marisa Eisenberg: It will know that those are arguments that it needs to fit our inputs, that it needs to hand to the cost function for the cost function to run, so it will say.

397
01:05:21,030 --> 01:05:32,190
Marisa Eisenberg: Oh okay I don't have any arguments called X data or why data, but I guess, this must be something that the cost function needs, and so it will just give that to the cost function, and then the cost function will run so.

398
01:05:32,700 --> 01:05:43,410
Marisa Eisenberg: it's almost the same setup but now we've got a little bit more fancy this and, and so, if we run this so run this optimization and print out what we get.

399
01:05:43,680 --> 01:05:53,940
Marisa Eisenberg: You can see that it actually gets pretty close to the right answer right it's not exactly right, so you know when we simulated the data we used a slope value of point two five.

400
01:05:54,150 --> 01:06:06,000
Marisa Eisenberg: And it gets point 2598 right so it's like point two, six, basically, but that's pretty darn close it's not bad, it shows you what the cost function value was there, how many times it evaluated the gradient etc and.

401
01:06:06,690 --> 01:06:14,730
Marisa Eisenberg: And we can plot the fit to the data, so if we plot the line that we we got you can see it's a pretty decent fit to the data right so.

402
01:06:14,970 --> 01:06:23,160
Marisa Eisenberg: yay success we we were able to basically fit a line to the data so that's the that's the idea with numerical optimization.

403
01:06:23,400 --> 01:06:31,350
Marisa Eisenberg: Here we did it with least squares in a minute we'll try do it changing that into a maximum likelihood setup but yeah but does that make sense to folks.

404
01:06:31,920 --> 01:06:40,140
Marisa Eisenberg: Okay, do you feel like you could take this and and like for a different linear model or something kind of tweak this example okay i'm seeing some nods do you feel like.

405
01:06:40,470 --> 01:06:57,030
Marisa Eisenberg: This is yeah okay Okay, so you can play with this, if you want to do you want to kind of try out other things, and then the lab today we'll do a more complicated you'll do this with like an si our model and and and we'll see how to do that there too okay so let's go back to the slides.

406
01:06:59,700 --> 01:07:02,310
Marisa Eisenberg: Okay, so those are some examples.

407
01:07:03,900 --> 01:07:14,040
Marisa Eisenberg: Alright, so that kind of ends the like initial sort of nitty gritty of how does the optimization work and how do you actually kind of set it up in the code and all that kind of thing.

408
01:07:14,400 --> 01:07:19,830
Marisa Eisenberg: So now we're going to so maybe, let me pause one more time, are there any questions about any of the stuff we've covered so far.

409
01:07:23,820 --> 01:07:24,270
Okay.

410
01:07:25,950 --> 01:07:34,050
Marisa Eisenberg: Okay, if there are you can ask him later to it's every deal and okay so so then next we're going to move to.

411
01:07:34,320 --> 01:07:40,230
Marisa Eisenberg: we've been talking about this cost function in a very generic way we introduced least squares is one example, but.

412
01:07:40,440 --> 01:07:44,580
Marisa Eisenberg: let's talk about how you would actually sort of frame this in this more statistical framework.

413
01:07:44,790 --> 01:07:52,200
Marisa Eisenberg: Because eventually we want to be able to talk about distributions and confidence intervals and those kinds of things rather than just finding the best fit parameter value.

414
01:07:52,380 --> 01:07:56,130
Marisa Eisenberg: We want to be able to say something about how good is that parameter value and what does it mean.

415
01:07:56,550 --> 01:08:13,080
Marisa Eisenberg: And so, for that we need to bring in a little bit of statistics and so we're going to talk about maximum maximum likelihood, and so the idea behind maximum likelihood is that you want to view your model so remember when we set up the ot model originally included here.

416
01:08:14,220 --> 01:08:19,320
Marisa Eisenberg: We took a deterministic model here and we added this measurement.

417
01:08:19,860 --> 01:08:38,940
Marisa Eisenberg: error observation error, which turned it into a probability distribution or a random variable really so we've got some some distribution on our model, which makes it into kind of a random variable, and so the idea behind maximum likelihood is really that we want to view our model.

418
01:08:40,110 --> 01:08:49,290
Marisa Eisenberg: As a probability distribution where what we're going to do is we're going to suppose that we know sort of the general form of the density function, but not the parameter values.

419
01:08:49,920 --> 01:08:52,740
Marisa Eisenberg: And then the idea is that if we knew.

420
01:08:53,550 --> 01:09:02,100
Marisa Eisenberg: This is maybe easier to see with an example we'll do an example in just a second if we knew the parameters, we could calculate the probability of a particular observation.

421
01:09:02,400 --> 01:09:16,830
Marisa Eisenberg: And so we're going to then take the parameters and tweak them until we find which parameters make the model, most likely to generate the data that we see, so I think this is a little easier to understand, if we do an example.

422
01:09:18,180 --> 01:09:33,270
Marisa Eisenberg: And so the the real basic idea here is that for most systems like when you think about how you would frame parameter estimation, in a statistical way usually what you what you would naturally ask yourself is.

423
01:09:33,930 --> 01:09:43,410
Marisa Eisenberg: I have some data given my data, what is the probability that this set of parameters is the correct one right that's kind of what you would.

424
01:09:43,680 --> 01:09:51,900
Marisa Eisenberg: Usually sort of think to yourself you'd say Okay, given the information I have, namely this data what's the probability that these parameters are the right friend.

425
01:09:52,530 --> 01:10:02,010
Marisa Eisenberg: and vision estimation, will help us actually do that, but it turns out that saying, given my data what's the probability of my parameters is.

426
01:10:02,430 --> 01:10:13,770
Marisa Eisenberg: Around is yeah given my data what's the probability of my parameters is is actually really hard and it turns out that it is usually easier.

427
01:10:14,550 --> 01:10:20,520
Marisa Eisenberg: To to work out the probability of observing a particular data set so so saying i'm.

428
01:10:21,240 --> 01:10:30,840
Marisa Eisenberg: Given the easier thing to calculate it turns out, almost always is given my parameters what's the probability that I observed this data right and so.

429
01:10:31,680 --> 01:10:36,090
Marisa Eisenberg: That, unfortunately, is less helpful to us, we have the data we don't have the parameters.

430
01:10:36,930 --> 01:10:49,440
Marisa Eisenberg: But it turns out that the easier thing to mathematically work out is given the parameters what's the probability of this data and so maximum likelihood makes essentially a little cheat it says well.

431
01:10:49,920 --> 01:10:58,140
Marisa Eisenberg: The only thing I can calculate is given my parameters what's the probability of this data so pick the parameters that make the probability of the data as high as I can make it.

432
01:10:58,470 --> 01:11:08,910
Marisa Eisenberg: And so that's basically what maximum likelihood is really doing it's it's it's not exactly answering the question we actually want to answer which is given my data what's the probability of my parameters.

433
01:11:09,180 --> 01:11:23,880
Marisa Eisenberg: But it is answering the question that we can answer mathematically which is given these these parameters what's the probability of my data okay pick the parameter values that make the data, I have the most likely thing to see and so.

434
01:11:24,570 --> 01:11:26,580
Marisa Eisenberg: that's the idea behind maximum likelihood.

435
01:11:27,030 --> 01:11:33,540
Marisa Eisenberg: let's do a couple of examples to kind of illustrate what I mean by this and then, when we get to bayesian estimation you'll see this is part of why.

436
01:11:33,810 --> 01:11:38,940
Marisa Eisenberg: I really liked the bayesian frameworks, because it allows you to actually answer the question you intended to answer in the first place.

437
01:11:39,450 --> 01:11:49,950
Marisa Eisenberg: So, but so let's talk about this for a minute so um so, for example, suppose that we have a coin and it might be a bias coin, so we want to know.

438
01:11:50,250 --> 01:12:01,800
Marisa Eisenberg: what's the probability that if you flip this coin, you get a heads and so we're going to call this parameter that if you flip the the probability that a coin flip results and heads we're going to call that p.

439
01:12:02,220 --> 01:12:09,030
Marisa Eisenberg: And so, this is the parameter, we want to estimate, we want to estimate the probability that our coin is going to give a heads and so.

440
01:12:09,690 --> 01:12:18,900
Marisa Eisenberg: let's let's suppose that our data will call the Z is we measure it we take, we flip the coin, I should have gotten a coin that I could use, but I forgot to so.

441
01:12:19,560 --> 01:12:25,320
Marisa Eisenberg: We flipped the coin, we can use this poster we flipped this coaster three times and and you get.

442
01:12:25,710 --> 01:12:38,310
Marisa Eisenberg: You get three data points you get heads tails heads, so our data is we got a heads or tails and a heads and and let's say the order doesn't matter I don't know what order they happen to it, but I just I know I got a heads or tails.

443
01:12:39,000 --> 01:12:52,410
Marisa Eisenberg: Okay um so because it's all independent so um so so what's the probability that the the p is equal to point five, given this data so.

444
01:12:52,770 --> 01:13:03,150
Marisa Eisenberg: That is tough to say like I don't know if I if I give them the data heads tails heads what's the probability that the parameter value is point five, I don't know.

445
01:13:04,080 --> 01:13:18,600
Marisa Eisenberg: But what I can say is that, if I knew P, I can definitely calculate the probability of getting heads tails heads given a particular P value right parameter, so if I know that this is a fair coin, then.

446
01:13:19,080 --> 01:13:25,950
Marisa Eisenberg: Like or whatever value of point if I know that P, is the probability of getting heads, then the probability of getting heads tails heads.

447
01:13:26,160 --> 01:13:35,190
Marisa Eisenberg: Is P times one minus P times P right that's the problem like essentially the probability of getting heads times the probability of getting tails one minus p.

448
01:13:35,520 --> 01:13:41,640
Marisa Eisenberg: times the probability of getting heads again right, so the probability of getting heads tails heads is this, so if I knew.

449
01:13:42,090 --> 01:13:46,350
Marisa Eisenberg: P, I could definitely calculate the probability of seeing this data.

450
01:13:46,890 --> 01:14:02,790
Marisa Eisenberg: Given this data I can't actually calculate the probability of what this parameter value is not just from the data, I have here, I need more information, so what you can do to what we're going to do instead, this is what we're going to call the likelihood so we're going to think of this.

451
01:14:04,320 --> 01:14:13,110
Marisa Eisenberg: As a function that we're going to basically optimize P, to make the probability of observing heads tails heads very high.

452
01:14:13,290 --> 01:14:20,910
Marisa Eisenberg: So we're gonna we're going to basically maximize we're going to choose T such that it maximizes the probability of observing heads tails heads.

453
01:14:21,180 --> 01:14:29,490
Marisa Eisenberg: And that will be our estimate for what the value of peace, so this is basically the assumption that we're making so let's actually just do it, and then we'll talk about what it means so.

454
01:14:30,120 --> 01:14:43,710
Marisa Eisenberg: The data Z is three coin flips heads tails heads or likelihood function, then, is going to be the probability of observing this data for a given value of P so P times one minus P times B.

455
01:14:44,100 --> 01:14:51,750
Marisa Eisenberg: And then, what we're going to do is we're going to try and find a value P that maximizes this so that's that makes it very likely that you would observe heads tails heads.

456
01:14:52,080 --> 01:14:59,730
Marisa Eisenberg: And so the probability of P that So if you if you simplify this equation, you get P squared minus P cube and.

457
01:15:00,570 --> 01:15:06,270
Marisa Eisenberg: And it's actually a constrained optimization problem, because your P, is a probability, so it can only live between zero and one.

458
01:15:06,540 --> 01:15:17,820
Marisa Eisenberg: So we could code that up with opt in and everything, but you can also just plot it, and then we can look at it and what you can see is that here's what this likelihood function looks like So this is the probability of.

459
01:15:18,540 --> 01:15:27,780
Marisa Eisenberg: Observing heads tails heads, given the parameter roll up and you can see that it goes it has this kind of Nice shape and the maximum.

460
01:15:28,110 --> 01:15:39,210
Marisa Eisenberg: Of this, the value of P, that makes it most likely that you would observe heads tails heads, is where P equals, two thirds, so if P is two thirds, which makes sense there's two thirds heads here right.

461
01:15:39,420 --> 01:15:52,170
Marisa Eisenberg: Then, if he has two thirds that makes it most likely that you should observe heads tails heads, as your data and so um so that is what we would estimate for our parameter roll up does that make sense to everybody has an idea.

462
01:15:52,710 --> 01:16:03,060
Marisa Eisenberg: thing okay okay so i'm going to do one more example that I stole from I haven't like well, we talked before class started, I have a two year old daughter and.

463
01:16:03,540 --> 01:16:08,130
Marisa Eisenberg: And she we have this book bayesian probability for babies and it is an.

464
01:16:08,430 --> 01:16:13,500
Marisa Eisenberg: amazing book I love this book so much it's actually a really good introduction to patient probability.

465
01:16:13,740 --> 01:16:22,410
Marisa Eisenberg: And we'll use it for the vision section and a little bit later, but um but it's a great introduction formation probability, and also for maximum likelihood and so we're going to use.

466
01:16:22,710 --> 01:16:32,220
Marisa Eisenberg: This book right now to teach ourselves about maximum likely, because I think it's fantastic so and also, if you ever want a fun book on bayesian probability I highly recommend this one.

467
01:16:32,610 --> 01:16:41,160
Marisa Eisenberg: But anyway, okay so Beijing probability for babies, so the idea here is that some cookies candy and some don't and.

468
01:16:41,820 --> 01:16:45,180
Marisa Eisenberg: And so you've got some cookies with candy some don't have candy.

469
01:16:45,930 --> 01:16:48,750
Marisa Eisenberg: Our data is that we take so i've.

470
01:16:49,050 --> 01:17:00,570
Marisa Eisenberg: You know, add annotated this book a little bit because they don't talk about data or likelihoods or any of that they just talk about cookies but which is obviously more fun for a little kid but for us we're going to talk a little bit more so.

471
01:17:00,960 --> 01:17:09,750
Marisa Eisenberg: You our data is that we take a bite, and it has no candy in this fight Okay, so this this cookie bite that we took that's our data that's our only information that we have it has no candy.

472
01:17:10,260 --> 01:17:19,290
Marisa Eisenberg: So our model and our question are going to be, we want to know is this from a candy cookie or a no candy cookie right so we're trying to estimate.

473
01:17:20,010 --> 01:17:25,170
Marisa Eisenberg: Our model is candy cookie versus a no candy cookie so did this, we took the site.

474
01:17:25,620 --> 01:17:34,560
Marisa Eisenberg: Does the rest of the cookie have candy or not our model parameters we're just going to have one parameter, which is a parameter C or C equals zero.

475
01:17:34,740 --> 01:17:51,360
Marisa Eisenberg: If it was a no candy cookie and C equals one if it was a candy so we want to estimate the value of C is equal to zero or one and so um so our parameter estimation problem is to estimate C to C equals 01 so we want to know did this fight, come from a candy cookie or no can.

476
01:17:52,500 --> 01:18:05,550
Marisa Eisenberg: And okay so what's the likelihood well the the likelihood that C equals zero it, given that it was a no candy bite is just the probability of getting a no candy bite.

477
01:18:06,090 --> 01:18:12,750
Marisa Eisenberg: condition, given that C equals zero so if see was equal to zero, meaning, it was a no candy cookie.

478
01:18:13,020 --> 01:18:25,830
Marisa Eisenberg: So if the cookie had no candy then everybody would have no candy so the probability of getting a no candy bite given a no candy cookie is equal to one so that's the likelihood value for see equal to zero and Okay, and then.

479
01:18:26,610 --> 01:18:32,340
Marisa Eisenberg: The if if the if it was a candy cookie so the likelihood value for sequel to one.

480
01:18:32,580 --> 01:18:48,210
Marisa Eisenberg: Is the probability of getting a no candy bite, given that it was a candy cookie so if it was a candy cookie then only one third of the bites have no candy so the probability of getting a no candy by, given that it was a candy cookie.

481
01:18:48,540 --> 01:18:58,500
Marisa Eisenberg: Is one third right, so the likelihood value for a candy cookie is one third, the likelihood value for a no candy cookie is one.

482
01:18:59,760 --> 01:19:08,670
Marisa Eisenberg: Given that one is bigger than one third, the maximum likelihood estimates means that it probably came from a no candy cookie.

483
01:19:09,030 --> 01:19:19,560
Marisa Eisenberg: So our next likely, I would say Aha the probability of getting a no candy Bytes is greater for a no candy cookie than it is for a candy cookie.

484
01:19:19,770 --> 01:19:30,330
Marisa Eisenberg: So, given no other information if I just know that I have a bike that has no candy in it, it probably came from a no candy cookie and so that's that's that's a nutshell.

485
01:19:31,260 --> 01:19:47,430
Marisa Eisenberg: And in a minute, which is just like delightful when we when we well it'll probably after the break, but when we get to Beijing estimation, they do prior isn't everything with this it's very nice anyway, I highly recommend this book, if you want yeah yeah yeah good go for it.

486
01:19:48,510 --> 01:19:54,120
Benjamin Cristol: So um the fact that you mentioned kind of this conditional probability setup where.

487
01:19:55,380 --> 01:20:07,470
Benjamin Cristol: In this optimization sort of context we approach it from the the mindset of given the it's easier, I guess, to you know, statistically deal with, given the.

488
01:20:08,970 --> 01:20:13,950
Benjamin Cristol: Given the data we have what would What would the parameter beer, the model, or what have you.

489
01:20:15,360 --> 01:20:23,670
Benjamin Cristol: Is that kind of why, even in other areas of like statistic or bio stats with like P value even it's kind of like given the data we see.

490
01:20:24,000 --> 01:20:34,110
Benjamin Cristol: Assuming that know is true, no hypothesis is true that we would see a value that's extreme or even yesterday with the Markov models, it was kind of the same sort of idea with.

491
01:20:34,590 --> 01:20:50,100
Benjamin Cristol: One of the things Dr house she had said around like we kind of it's easier, he said something about it being easier to approach it from that framework of like given the data we have, how does this, you know stochastic sort of framework you work with the Markov model so.

492
01:20:50,310 --> 01:20:51,810
Marisa Eisenberg: yeah so there's different.

493
01:20:51,960 --> 01:20:53,190
Marisa Eisenberg: Like like.

494
01:20:53,220 --> 01:21:10,470
Marisa Eisenberg: details and a lot of those, but it is often the case that, yes, that we are like constrained, essentially in terms of what we can like what which which probable probable listed questions, we can actually answer or calculate and so, then it ends up that.

495
01:21:11,520 --> 01:21:22,410
Marisa Eisenberg: For a lot of okay i'm showing my biases here if anybody has a strong frequent is i'm going to apologize for what i'm about to say which is for a lot of frequencies situations, I find that.

496
01:21:22,650 --> 01:21:29,520
Marisa Eisenberg: The way that you have to frame the statistical question feels a little backwards for what you're actually trying to answer and.

497
01:21:30,060 --> 01:21:44,730
Marisa Eisenberg: For a lot of like the p value kinds of things, a lot of these statistical questions get much cleaner, I think, if you go to a patient framework and because, then you can actually ask the question you're trying to ask um but yeah but yeah that's a that's a good intuition there, yes.

498
01:21:44,760 --> 01:21:46,650
Marisa Eisenberg: that's often similar kinds of issues that are.

499
01:21:46,650 --> 01:21:47,400
Marisa Eisenberg: cropping up yeah.

500
01:21:47,430 --> 01:21:59,460
Benjamin Cristol: yeah perhaps that's why, when you kind of start off with learning starts with the p value stuff it's so like counterintuitive maybe that frequent frequent test approach kind of is less intuitive to most do students I don't.

501
01:22:00,030 --> 01:22:10,620
Marisa Eisenberg: think so, although frequent test methods were developed first and I like I I I mean there's plenty of times that I use frequently statistics all the time it's not like I am like like a.

502
01:22:11,670 --> 01:22:19,260
Marisa Eisenberg: You know, a writer dive Asian vision kind of person, but um but it but it's yeah, but it is like I do think that.

503
01:22:19,530 --> 01:22:28,830
Marisa Eisenberg: oftentimes the Beige and setting it lets you kind of more intuitively ask the question but yeah so yeah, so I would, I think that's right but yeah but others may feel differently.

504
01:22:29,250 --> 01:22:40,680
Marisa Eisenberg: You know I you know people have there are there are real philosophical reasons why you might want to go with a frequent this framework, but I am partial to vision but yeah yeah okay.

505
01:22:41,700 --> 01:22:47,700
Marisa Eisenberg: All right, does it so this feels okay to folks this feels comfortable cookie questions.

506
01:22:49,710 --> 01:23:06,180
Marisa Eisenberg: Okay, good um alright so so just to kind of recap, so our likelihood function is the probability of seeing the data values that we have assuming we knew the parameter values and we're and basically the idea here is really that um.

507
01:23:06,810 --> 01:23:12,630
Marisa Eisenberg: So to maybe say this in a little bit more of a mathematical way than we've been doing with the coin flip and the cookies.

508
01:23:13,680 --> 01:23:28,230
Marisa Eisenberg: When when we're thinking about our likelihood it turns out that it's easier to say to calculate the probability of seeing the data given our parameters being fixed but that function of our.

509
01:23:28,740 --> 01:23:36,150
Marisa Eisenberg: That this probability of seeing this data given our parameters being fixed is actually just a function of both our data and our parameters.

510
01:23:36,360 --> 01:23:47,040
Marisa Eisenberg: And so what the what the kind of clever trick with likelihood is is that we think instead of thinking of our parameters as being fixed and our data is the thing we're asking about.

511
01:23:47,400 --> 01:23:53,040
Marisa Eisenberg: It flips this and says, think of your data is being fixed and move your parameters around.

512
01:23:53,340 --> 01:24:01,200
Marisa Eisenberg: By basically so, for instance, if you this is a normal equation for a normal distribution, not that you would have to know this off the top your head, but just if you.

513
01:24:01,470 --> 01:24:10,830
Marisa Eisenberg: look on Wikipedia or whatever, this is the equation for a normal distribution and it's the probability of seeing this data given.

514
01:24:11,700 --> 01:24:18,120
Marisa Eisenberg: You know your you know the probability density, I should say, of seeing the data given your mean and your variants.

515
01:24:19,080 --> 01:24:29,940
Marisa Eisenberg: But if you look at the actual equation it's just a function of your data your mean in your back right so it's just some function of the Z them you and the Sigma squared so you could.

516
01:24:30,660 --> 01:24:36,540
Marisa Eisenberg: Take this function and fix Z at the the data that you observed and then.

517
01:24:37,200 --> 01:24:45,780
Marisa Eisenberg: move optimized view and Sigma squared even though that's not kind of what it was set up for, and that would be what our likelihood is going to be essentially.

518
01:24:46,080 --> 01:24:58,680
Marisa Eisenberg: And so we find the value of the parameters in this case, new and Sigma squared that makes you most likely to observe the data Z that we have and so that's basically the idea.

519
01:24:59,160 --> 01:25:03,750
Marisa Eisenberg: So this this probability becomes then our cost function and now.

520
01:25:04,350 --> 01:25:08,580
Marisa Eisenberg: This likelihood or or you know that we've got here this function and.

521
01:25:08,820 --> 01:25:16,110
Marisa Eisenberg: You want to maximize the likelihood so actually what you usually do is you take the negative log of this and that becomes your cost function.

522
01:25:16,320 --> 01:25:24,570
Marisa Eisenberg: Negative so that it flips it upside down so that you can minimize because most of the numerical methods that are out there, they want to minimize maximize and then.

523
01:25:25,170 --> 01:25:31,710
Marisa Eisenberg: The log is actually for a bunch of different reasons, it makes it a lot more nicely behaved and a bunch of things.

524
01:25:32,520 --> 01:25:41,670
Marisa Eisenberg: You usually take the log of your likelihood function and just to kind of use any worries if people are thinking about does that mess up what estimates, you get.

525
01:25:42,510 --> 01:25:48,240
Marisa Eisenberg: The log imagine that you have like a well i'm going to now I have annotate as whatever so.

526
01:25:48,780 --> 01:25:57,060
Marisa Eisenberg: If you have a bowl shape, so you had a likelihood function that was shaped like this, you took the negative of it, so now, you can find the minimum instead of the maximum.

527
01:25:57,360 --> 01:26:08,280
Marisa Eisenberg: Taking the log just kind of flattens it it doesn't actually change, where the best fit value is so it will change the the sort of shape of the thing but it won't change, where the minimum is located.

528
01:26:08,850 --> 01:26:13,470
Marisa Eisenberg: And it makes it a lot more nicely behave most of the time so often we take locks.

529
01:26:14,220 --> 01:26:32,370
Marisa Eisenberg: Okay, so so that's a sort of mathematical way to view what a likelihood function is I don't want to do a visual way of viewing what the likelihood function is just to kind of get everybody on board with that too, and I think I have animation on this, let me stop my share and do this.

530
01:26:33,420 --> 01:26:34,290
Oh right oh my.

531
01:26:35,310 --> 01:26:36,600
gosh your desktop.

532
01:26:40,380 --> 01:26:59,010
Marisa Eisenberg: Okay, so um so so we've got our our our likelihood function, so I want to talk a little bit about a visual way of doing this just to kind of give another way of thinking about it so oh I forgot with this, for some reason I can't see my own mouse.

533
01:27:00,630 --> 01:27:01,680
Marisa Eisenberg: that's fine it's okay.

534
01:27:02,700 --> 01:27:03,540
Marisa Eisenberg: Okay, so.

535
01:27:04,860 --> 01:27:12,420
Marisa Eisenberg: So, usually, when we plot it probability distribution, we think of it as something like this the the data value is on the X axis.

536
01:27:12,630 --> 01:27:24,030
Marisa Eisenberg: And what we look at is basically so, for instance with like a heads tails kind of coin flip thing or a normal distribution or whatever we think of the probability density of saying what's the probability of observing this.

537
01:27:24,450 --> 01:27:31,980
Marisa Eisenberg: Data value along the X axis, and so the y axis is something telling us about how probable that area of.

538
01:27:32,670 --> 01:27:40,680
Marisa Eisenberg: data is likely to be so, you know, in this case, if it was like a something kind of normal ish distributed then i'm very likely to observe values.

539
01:27:40,950 --> 01:27:47,310
Marisa Eisenberg: right around the D and data there and then less likely to observe values kind of off towards the tail right um.

540
01:27:47,910 --> 01:27:58,080
Marisa Eisenberg: But you know, there are parameters that control the shape of this distribution right, so the mean or the standard deviation and things like that so maybe we actually got to sort of think of it more like this, so.

541
01:27:59,010 --> 01:28:07,950
Marisa Eisenberg: there's a third axis, which is the value of let's say one parameter in reality it has many other axes for all your different parameters so let's say just one so.

542
01:28:08,790 --> 01:28:16,530
Marisa Eisenberg: there's a third axis here that's the value of your parameter and so really it's like you have for a given parameter value you.

543
01:28:16,770 --> 01:28:27,960
Marisa Eisenberg: As you move the parameter value around the distribution is going to shift and so it's something more like this, and so we can think about this as like a heat map i'm going to make this into a heat map so.

544
01:28:29,010 --> 01:28:40,710
Marisa Eisenberg: kind of like go in here the shading is hand drawn so it looks really stupid, but the shading is meant to be sort of the heat map of what this looks like so a slice in this direction.

545
01:28:41,070 --> 01:28:51,720
Marisa Eisenberg: The horizontal direction where you fix a parameter value at a given number is the probability density function for like a normal distribution with a mean of five or something right.

546
01:28:52,860 --> 01:29:09,630
Marisa Eisenberg: What we're doing when we take the likelihood value the likelihood function is this we're switching the access that we're fixing so that we're that we're shifting that we're looking at so we're going to now fix the data to this particular value down here down at the bottom by the a.

547
01:29:11,130 --> 01:29:17,580
Marisa Eisenberg: So we fixed the data to a specific value and we're going to shift the parameter value along the y axis there.

548
01:29:17,910 --> 01:29:25,920
Marisa Eisenberg: and adjust the parameter value until we find the parameter value that makes the likelihood as high as we can, for that data value.

549
01:29:26,370 --> 01:29:38,010
Marisa Eisenberg: Does that make sense that's the thing so really we're taking a slice in the in a so when we think of this as a probability density function, then we're fixing the mean let's say at five.

550
01:29:38,280 --> 01:29:49,020
Marisa Eisenberg: When we think of this as a likelihood function, we fix the data at whatever we observe and we adjust the mean until we find the value that makes that data, most likely and so.

551
01:29:49,590 --> 01:29:59,190
Marisa Eisenberg: So that's what the likelihood function is really doing now, the thing that's tricky here, and the reason that this is awkward is that when you slice it this way you're actually slicing.

552
01:29:59,400 --> 01:30:04,530
Marisa Eisenberg: you're taking little tiny pieces from many different probability density functions right like.

553
01:30:04,800 --> 01:30:14,160
Marisa Eisenberg: Each one of those horizontal slices is actually a probability distribution and we're shifting across different probability distributions as we move our parameter right and so um.

554
01:30:14,730 --> 01:30:31,320
Marisa Eisenberg: So this thing that we get by slicing this way isn't a probability distribution really in we if we're going if we're moving in this direction it's actually kind of letting a shift across many different probability distributions and and so so that's that's the that's the idea.

555
01:30:32,370 --> 01:30:37,920
Marisa Eisenberg: With with maximum likelihood in a sort of more visual way i'm so.

556
01:30:38,760 --> 01:30:50,220
Marisa Eisenberg: So yeah so you you go this direction and it turns out i'm making it sound very dire that this is not a probability distribution and that this is an answering the question we really want to ask it turns out that.

557
01:30:50,490 --> 01:31:03,090
Marisa Eisenberg: In almost all circumstances, this is actually very close to being a probability distribution and it's just not normalized correctly, so it won't it won't integrate to one in the way that probability distribution should and and so.

558
01:31:03,900 --> 01:31:09,870
Marisa Eisenberg: When we get to the bayesian estimation piece, it will turn out that this is actually a perfectly fine thing to do, almost always it just.

559
01:31:10,050 --> 01:31:21,690
Marisa Eisenberg: It what we will really end up be doing by doing this is we're assuming that we don't have any prior information on on our parameters and so, but we'll get to why that is, the thing later.

560
01:31:22,590 --> 01:31:34,770
Marisa Eisenberg: Okay it's 1001, let me just really quick wrap up here and then we'll take our break so maximum likelihood is a bunch of very nice statistical properties it's it's consistent so.

561
01:31:35,340 --> 01:31:41,130
Marisa Eisenberg: If you if you have enough observations you can find the value of the parameter with arbitrary precision precision.

562
01:31:41,370 --> 01:31:49,620
Marisa Eisenberg: It has normality so that means that your the distribution of your maximum likelihood estimate tends towards a normal distribution, and it has this nice property that it.

563
01:31:49,950 --> 01:31:57,450
Marisa Eisenberg: It achieves the premier outbound as you get to high sample sizes, so if you if you know much statistics, this means basically that.

564
01:31:57,870 --> 01:32:05,520
Marisa Eisenberg: No other consistent estimator has a lower mean squared error than the less likely it is to basically the camaro bound is this this.

565
01:32:06,120 --> 01:32:19,890
Marisa Eisenberg: bound on the uncertainty for your for your for your it's a lower bound on the uncertainty for your estimation, and this will achieve that, as you get to large enough sample sizes and so anyway it's a very nice.

566
01:32:20,460 --> 01:32:24,750
Marisa Eisenberg: it's a it's a nice statistical framework, it has lots of good properties to it.

567
01:32:25,530 --> 01:32:35,970
Marisa Eisenberg: Just to kind of recap, and then we'll take our break So the idea, just to remind ourselves is that you are likelihood is just the probability distribution of your data, assuming that you knew your parameter values.

568
01:32:36,270 --> 01:32:48,240
Marisa Eisenberg: And then we just think of that probability distribution as a function of the parameters, with the data fixed and that's the likelihood, so we find the value of the parameters that make the probability of observing your data as high as we can.

569
01:32:48,930 --> 01:32:51,840
Marisa Eisenberg: And so we're going to make that likelihood your cost function.

570
01:32:52,050 --> 01:32:54,090
Marisa Eisenberg: And then find parameter values that maximize.

571
01:32:54,090 --> 01:32:58,140
Marisa Eisenberg: It and so that's the basic idea, and with that, I think.

572
01:32:59,220 --> 01:33:11,670
Marisa Eisenberg: i'm I will do this with code when they come back so let's pause here let's take a break it's 1003 so we'll come back at like 1018 let's say and and we'll start back up then.

573
01:33:18,300 --> 01:33:18,600
Marisa Eisenberg: Okay.

574
01:33:20,610 --> 01:33:21,840
Marisa Eisenberg: Welcome back everybody.

575
01:33:25,050 --> 01:33:37,920
Marisa Eisenberg: There we go okay so um welcome back um so yeah so I guess to to recap the idea of maximum likelihood um let's.

576
01:33:39,060 --> 01:33:48,120
Marisa Eisenberg: To kind of see what that looks like a little bit in more detail let's revisit our least squares example that we did in the code so.

577
01:33:48,900 --> 01:33:59,970
Marisa Eisenberg: You know, basically How does that how so like I kind of described this idea of you know you've you you're you calculate the probability of observing your data given your parameters, but.

578
01:34:00,330 --> 01:34:08,700
Marisa Eisenberg: How does it work when you have more complicated models, how do you actually calculate that probability, what does it look like so usually what you do, basically, is.

579
01:34:09,870 --> 01:34:20,670
Marisa Eisenberg: Well, I dive right in I should actually have open any questions, did you like, as you were thinking, while we were on the break anything that came up by day I forgot to ask, please comfy OK.

580
01:34:23,250 --> 01:34:23,640
OK.

581
01:34:24,660 --> 01:34:37,350
Marisa Eisenberg: OK alright so So the idea, then, is when you're trying to do something like this with more complicated oh wait, we have people still connecting and wait till we get started.

582
01:34:37,380 --> 01:34:39,360
Maria Victoria Salgado: There we go okay good.

583
01:34:40,170 --> 01:34:42,810
Marisa Eisenberg: Okay, so so so when you're trying to do.

584
01:34:44,250 --> 01:34:50,910
Marisa Eisenberg: This kind of mass likelihood with something that's a more complicated model, the idea is that usually what we do is we view.

585
01:34:52,020 --> 01:35:08,910
Marisa Eisenberg: The model as describing one of the sort of parameters or features of the distribution so, and so we we view our model as like, for instance, describing the mean or something like that of our distribution so.

586
01:35:09,420 --> 01:35:20,700
Marisa Eisenberg: So, for instance, maybe the data sorry there's a trash truck coming through our neighborhood it's going to be loud for a little second, hopefully, you can still hear hear me okay so we've used the data.

587
01:35:21,540 --> 01:35:31,830
Marisa Eisenberg: As coming from a normal distribution, but where maybe the mean is given by our linear model prediction and we suppose that maybe we know are estimating the standard deviation so.

588
01:35:32,400 --> 01:35:39,450
Marisa Eisenberg: So, for instance in our in our linear regression model what we would basically say is that our X our humidity here.

589
01:35:40,260 --> 01:35:56,640
Marisa Eisenberg: Is is known and our Why is is is or in this case or Z whatever you want to call it is data and, and so our data here is is is this Z and our model is y equals mx and so we're going to say that the data is given by a gaussian.

590
01:35:57,060 --> 01:36:09,030
Marisa Eisenberg: Where the mean is the model prediction, so a normal distribution with a mean in a standard deviation and, in this case we're going to assume that the mean is given by our model prediction.

591
01:36:09,450 --> 01:36:18,780
Marisa Eisenberg: Why, I and, in this case it's such a simple model that we can actually plug that in and say that we're going to assume really what we're what our model is is that the data.

592
01:36:19,410 --> 01:36:25,440
Marisa Eisenberg: is coming from a normal distribution, where the mean is n times X I and.

593
01:36:26,310 --> 01:36:37,230
Marisa Eisenberg: The you know standard deviation is this Sigma here and so so that's sort of how we would set this up let's just revisit our code real quick and and go back to that least squares linear.

594
01:36:37,590 --> 01:36:45,330
Marisa Eisenberg: regression example and just kind of talk through what that would actually mean if you were going to write the cost function out, so let me do that.

595
01:36:46,350 --> 01:36:48,240
Marisa Eisenberg: But the but the bottom line.

596
01:36:49,620 --> 01:36:50,130
Is.

597
01:36:51,330 --> 01:36:55,710
Marisa Eisenberg: Okay, so so remember We made this little.

598
01:36:55,770 --> 01:37:02,040
Marisa Eisenberg: linear regression example where we simulated some data, and then we made a model function here.

599
01:37:02,400 --> 01:37:04,620
Marisa Eisenberg: That was, you know y equals mx.

600
01:37:05,010 --> 01:37:06,750
Marisa Eisenberg: And then we made a cost function.

601
01:37:07,110 --> 01:37:07,590
Adriana Perez: and

602
01:37:07,620 --> 01:37:08,820
Marisa Eisenberg: You know, over here.

603
01:37:09,150 --> 01:37:11,430
Marisa Eisenberg: But oh sorry yeah.

604
01:37:14,280 --> 01:37:14,580
Adriana Perez: Here we go.

605
01:37:15,210 --> 01:37:22,050
Marisa Eisenberg: Okay, and so so so we made a model function here, and then we made a cost function.

606
01:37:22,860 --> 01:37:33,600
Marisa Eisenberg: That that is that we remember We calculated the summer squared residuals so now let's change our cost function, or we fit fit it to the data and we got this nice little linear fit so let's go back.

607
01:37:33,840 --> 01:37:39,450
Marisa Eisenberg: And now let's talk about how would we actually change your cost function if we wanted to do this from a maximum likelihood perspective so.

608
01:37:39,930 --> 01:37:47,670
Marisa Eisenberg: Now the model stays the same y equals mx is still our model, but our Constitution should now be a likelihood function, instead of just least squares and so.

609
01:37:48,330 --> 01:37:55,680
Marisa Eisenberg: we're going to suppose will actually show in just a minute that least squares and and a normally distributed maximum likelihood end up being the same thing.

610
01:37:55,950 --> 01:38:12,570
Marisa Eisenberg: But um but let's suppose that our data is a sample from a normally distributed random variable for the mean is given by our model and the standard deviation is let's say known to be five so, then the likelihood function for one data point for our our situation here would be this so.

611
01:38:13,770 --> 01:38:27,900
Marisa Eisenberg: The probability of observing di or Z I pardon me the probability of observing Z I given our slope parameter N, is a normal distribution, where the mean is view times MSI and the standard divas five.

612
01:38:28,710 --> 01:38:37,230
Marisa Eisenberg: And Z is one point and and so, then what we would do is we assume all of our data is independence, so this is assuming.

613
01:38:37,770 --> 01:38:47,580
Marisa Eisenberg: I mean you have to ask yourself if that's realistic, so what we're really assuming there is that all of our humidity and rainfall data, each time point is independent.

614
01:38:48,000 --> 01:39:02,250
Marisa Eisenberg: he's probably not quite true right like if if it's very high humidity today it's probably more likely to be high humidity and ready tomorrow or yesterday, right like so data points that are near one another, probably not fully independent but.

615
01:39:02,850 --> 01:39:15,900
Marisa Eisenberg: I assume that it is it's a made up example anyhow, then what we can do is, we can take the likelihood for each of our data points here and multiply them all together to get our overall likelihood of observing this whole data set.

616
01:39:16,830 --> 01:39:26,550
Marisa Eisenberg: You know, given our parameters based on our parameter, and so the negative log of this big old product is going to be our new cost function because we're going to take.

617
01:39:26,790 --> 01:39:33,720
Marisa Eisenberg: A likelihood you want to maximize life ahead so negative, so that we minimize a log to make it behave nicely and so luckily.

618
01:39:34,470 --> 01:39:39,360
Marisa Eisenberg: You don't have to write out remember earlier, I wrote out the equation for the normal distribution and everything.

619
01:39:39,600 --> 01:39:45,300
Marisa Eisenberg: You don't really actually have to do that because it's all built into our so we don't even have to write out the normal distribution equation.

620
01:39:45,630 --> 01:39:52,830
Marisa Eisenberg: Where we can just do is say okay our new cost function, so it still takes in the slope parameter and it still takes him a data as its inputs.

621
01:39:53,190 --> 01:40:04,290
Marisa Eisenberg: we're still going to calculate the model prediction with our same model that we had, but now, instead of using taking this this model prediction and doing this summer squares we're going to say our likelihood.

622
01:40:04,680 --> 01:40:12,630
Marisa Eisenberg: Is the normal distribution, so if you use the D norm command in our that will give you the density function for a normal distribution.

623
01:40:12,960 --> 01:40:18,900
Marisa Eisenberg: i'm using our data our model prediction and the standard diva five.

624
01:40:19,290 --> 01:40:28,770
Marisa Eisenberg: And so you can just it's built into our already if I wanted instead of a normal distribution if I wanted to say that it was puts on and the model was predicting the rate.

625
01:40:28,980 --> 01:40:43,470
Marisa Eisenberg: You could just use deep was you know any whatever thing you, you want to use, you can just kind of go with that and then, so this is our likelihood and then our cost the cost that we're going to use that we're going to optimize with is the negative log likelihood.

626
01:40:44,670 --> 01:40:47,550
Marisa Eisenberg: And once you take the log This is another reason that it's convenient.

627
01:40:47,790 --> 01:40:57,510
Marisa Eisenberg: Instead of a product, you can just take some because, once you take a log of a product, you know you get a some and it's probably for many of you, been a long time since he thought about log rules but.

628
01:40:57,810 --> 01:41:06,630
Marisa Eisenberg: You know the log of a product is a solid and all that jazz so that's why this is a some instead of a product, I took the negative log of each of my likelihood values for each of my.

629
01:41:07,740 --> 01:41:12,780
Marisa Eisenberg: Z eyes and then some of them up here so okay so that's going to be our cost function.

630
01:41:13,950 --> 01:41:25,320
Marisa Eisenberg: here's what you get if you try that out with something so let's try rewriting our optimization same code for the optimization as before we're going to start it at one run run it this way and let's see what we get.

631
01:41:26,430 --> 01:41:36,330
Marisa Eisenberg: If we get actually like the exact same number that we got before right so to 2.25986 whatever.

632
01:41:37,560 --> 01:41:44,670
Marisa Eisenberg: You get the same line that we got before, so it works just the same it's gonna we're going to look in just a minute will show actually that.

633
01:41:44,970 --> 01:41:49,380
Marisa Eisenberg: least squares turns out to be you can frame least squares as being maximum likelihood.

634
01:41:49,590 --> 01:41:56,010
Marisa Eisenberg: With a normal distribution and so that's why it ends up being the same that won't always be the case least squares of maximum likelihood won't always match up.

635
01:41:56,280 --> 01:42:03,210
Marisa Eisenberg: But in this case, they do, and so you can kind of see that it all works out very nicely, and you get that Nice line and all that good stuff so.

636
01:42:03,990 --> 01:42:13,680
Marisa Eisenberg: This example is posted up on the website, you can play around with it and kind of see what what's in there um but that's how you would basically, the idea is this is how you would.

637
01:42:14,370 --> 01:42:23,250
Marisa Eisenberg: put together a likelihood function you you assume that the model is describing one of the features of your distribution in this case the model is describing the mean.

638
01:42:23,820 --> 01:42:34,740
Marisa Eisenberg: And then you basically just plug in your model output your model prediction as the mean or whatever you know shape parameter, it is that you're deciding that.

639
01:42:35,070 --> 01:42:46,260
Marisa Eisenberg: The model represents for your distribution and and then that becomes your life so that's that's the idea um yeah questions about that, but any of this stuff.

640
01:42:49,770 --> 01:42:56,730
Marisa Eisenberg: we'll do another example, with an si our model in the lab to so you'll see a different another one kind of have the same flavor.

641
01:42:58,020 --> 01:43:05,370
Marisa Eisenberg: Okay, so I wanted to do one kind of writing out the equations for this with a differential equation model.

642
01:43:05,790 --> 01:43:07,830
Marisa Eisenberg: Just so that we can kind of see how it would look.

643
01:43:08,160 --> 01:43:18,600
Marisa Eisenberg: i'm going to do a normally distributed gaussian you know distributed and then also a plus on just to kind of see you can kind of see how it, how it plays out so remember that.

644
01:43:18,840 --> 01:43:34,140
Marisa Eisenberg: In all of these cities situations where we're going to have to sort of have a set of differential equations and our measurement equation, the we're going to assume that the data is given by some distribution, where the model prediction becomes like one.

645
01:43:35,190 --> 01:43:39,570
Marisa Eisenberg: Part of the distribution actually out of probably have written this a little bit differently, but.

646
01:43:40,350 --> 01:43:49,980
Marisa Eisenberg: figure fiddle with that leader and you could you could think of it as adding measurement error on to the model prediction or you could think of it as saying the model that we're we're.

647
01:43:50,430 --> 01:43:55,770
Marisa Eisenberg: modeling the data as being normally distributed, with a mean of why essentially it's really what we're doing.

648
01:43:57,510 --> 01:44:06,150
Marisa Eisenberg: Okay, so the measure data can be viewed as a sample from a gaussian distribution with a mean of of y and a variant Sigma squared.

649
01:44:06,360 --> 01:44:16,770
Marisa Eisenberg: we're going to suppose all of our measurements are independent and then basically the idea, so I didn't write out the equation when we did the code, but if we weren't going to write out the equation, this is what you would get so.

650
01:44:17,220 --> 01:44:24,570
Marisa Eisenberg: A single data point one data points likelihood would be this, so this is a gaussian distribution.

651
01:44:25,380 --> 01:44:36,420
Marisa Eisenberg: With you know the the data, the mean and Sigma except that what we're going to do is we're going to replace the mean with our model prediction of the knee basically the idea right and so.

652
01:44:36,960 --> 01:44:44,730
Marisa Eisenberg: So now we're going to say Okay, so this now connects up our model parameters, because this model prediction of the mean is dependent on the parameters with.

653
01:44:45,000 --> 01:44:51,480
Marisa Eisenberg: Our gaussian are normally just normal normal density function, and so, then we're going to take this little guy.

654
01:44:52,110 --> 01:45:02,130
Marisa Eisenberg: will say assume that all of our ci are independent so then all of our observations you know at every time point is independent of one another and so.

655
01:45:02,640 --> 01:45:08,670
Marisa Eisenberg: Now we just take the product of all of these different contributions to the overall likelihood function and.

656
01:45:09,360 --> 01:45:14,760
Marisa Eisenberg: And that becomes our our overall likelihood function is this product of all these things.

657
01:45:15,750 --> 01:45:26,550
Marisa Eisenberg: Okay, a lot of times this chunk of it is a little bit much, but hopefully the idea of sort of taking that your likelihood function when you're dealing with a with an odd model.

658
01:45:27,030 --> 01:45:34,320
Marisa Eisenberg: Really it's just the, the idea is really just that the we're going to use our model to become.

659
01:45:34,800 --> 01:45:41,730
Marisa Eisenberg: One of the parameters in our in our gaussian or our normal distribution or whatever distribution you want, and so that's sort of the idea.

660
01:45:42,060 --> 01:45:48,630
Marisa Eisenberg: And if you take this likelihood function that we've been putting together here this big ol product.

661
01:45:48,990 --> 01:45:54,450
Marisa Eisenberg: i'm not going to go through in the slides there's a bunch of the algebra for how to do this, but if you simplify this out.

662
01:45:54,630 --> 01:46:05,970
Marisa Eisenberg: So if you if you take this product of all of these things, and you actually plug in the equation for each of these Apps you get something that's kind of interesting, so you get this equation don't bother trying to.

663
01:46:06,270 --> 01:46:11,160
Marisa Eisenberg: Do the algebra or any of that stuff i'm not worried about that, but what I want you to notice is that.

664
01:46:11,670 --> 01:46:24,660
Marisa Eisenberg: This is actually the sum of squared residuals, this is the data minus the model squared and then some so there's something in here that looks like least squares right, and so it turns out that um.

665
01:46:25,200 --> 01:46:30,240
Marisa Eisenberg: If you take this and you do a bunch of simplifying algebra so remember this, is the likelihood.

666
01:46:30,450 --> 01:46:36,990
Marisa Eisenberg: So we actually want to take the negative log of this so we take the negative log likelihoods know even log and an exponential.

667
01:46:37,200 --> 01:46:42,390
Marisa Eisenberg: You have to do a bunch of algebra to kind of like tease out how all that plays out and what happens a blah blah blah.

668
01:46:42,660 --> 01:46:50,220
Marisa Eisenberg: The algebra is in here, if you want to keep if you want to kind of step through the steps, but i'm going to skip past it and you can make some assumptions and blah blah blah.

669
01:46:50,670 --> 01:46:57,720
Marisa Eisenberg: At the end of the day, what you end up finding is that if you simplify this likelihood function, all the way down.

670
01:46:58,050 --> 01:47:04,980
Marisa Eisenberg: Then, taking the negative log likelihood turns out to give you least squares it's the same thing so actually it turns out that.

671
01:47:05,280 --> 01:47:14,850
Marisa Eisenberg: anytime that you're assuming your model you're assuming your data is coming from a normal distribution, where the model is predicting the mean, and you know the standard deviation.

672
01:47:15,090 --> 01:47:22,320
Marisa Eisenberg: Or the standard deviation is a constant, even if it's unknown you're going to get least squares and so so that's just kind of a nice feature.

673
01:47:22,770 --> 01:47:32,340
Marisa Eisenberg: That it turns out that a lot of times for for a common situation that you would use the maximum likelihood you actually get the squares, which is kind of cool and.

674
01:47:33,000 --> 01:47:39,390
Marisa Eisenberg: it's not always the squares so just as a little example, if you do the same thing for plus on so let's say.

675
01:47:39,600 --> 01:47:48,330
Marisa Eisenberg: You have a differential equation model, you have some equation, for whatever it is that you're going to measure so like maybe y is equal to the number of new infections on a given day or something.

676
01:47:48,750 --> 01:47:57,270
Marisa Eisenberg: And, and your data you're going to assume this is coming from a plus on distribution, where the mean or the rate is is this why.

677
01:47:57,570 --> 01:48:10,350
Marisa Eisenberg: And you assume all your data points are independent, the plus on probability mass function is this, if you take this and do that same negative log likelihood and then product it all together and do all the algebra with all this junk.

678
01:48:11,010 --> 01:48:23,760
Marisa Eisenberg: You get something that just is something I don't this doesn't this one isn't least squares it's not anything really nice it's just like a thing that you know, like whatever, this is an equation that you could have I.

679
01:48:23,760 --> 01:48:24,720
Marisa Eisenberg: guess right like so.

680
01:48:24,900 --> 01:48:26,580
Marisa Eisenberg: It doesn't to me, have any real.

681
01:48:26,580 --> 01:48:27,330
Adriana Perez: intuition.

682
01:48:27,690 --> 01:48:32,070
Marisa Eisenberg: it's got apart from the fact that it's it's a function of the data and the model.

683
01:48:33,480 --> 01:48:33,750
Adriana Perez: But.

684
01:48:33,780 --> 01:48:34,470
Marisa Eisenberg: Other than that.

685
01:48:34,530 --> 01:48:35,520
Marisa Eisenberg: there's nothing in here that.

686
01:48:36,810 --> 01:48:39,030
Marisa Eisenberg: yells out to me like Oh, this is a good cost.

687
01:48:39,030 --> 01:48:44,010
Marisa Eisenberg: function I wouldn't have come up with this on my own right like so sometimes maximum likelihood.

688
01:48:44,550 --> 01:48:46,080
Adriana Perez: Cost functions will be just kind of.

689
01:48:46,110 --> 01:48:46,710
Marisa Eisenberg: Whatever they are.

690
01:48:46,920 --> 01:48:49,260
Marisa Eisenberg: You don't actually really ever need to.

691
01:48:49,590 --> 01:48:56,820
Marisa Eisenberg: For the most part, for most standard distributions when you're coding it up you don't need the equations for what the distribution is they're all built into our.

692
01:48:56,850 --> 01:49:02,400
Marisa Eisenberg: So you can just and Python two if you're a Python person, they also have packages that have all.

693
01:49:02,400 --> 01:49:02,610
Adriana Perez: This.

694
01:49:03,150 --> 01:49:09,540
Marisa Eisenberg: Distribution equations built in you don't really need to have all, and you know to be able to sort of calculate all the details of this.

695
01:49:09,750 --> 01:49:11,820
Marisa Eisenberg: There are times, though, when it does turn out to be.

696
01:49:11,820 --> 01:49:12,810
Adriana Perez: convenient because.

697
01:49:13,020 --> 01:49:13,650
Marisa Eisenberg: There are.

698
01:49:13,920 --> 01:49:15,330
Adriana Perez: So, like in this plus on.

699
01:49:16,110 --> 01:49:18,870
Marisa Eisenberg: Maximum likelihood I dropped a couple of terms that.

700
01:49:19,980 --> 01:49:21,450
Marisa Eisenberg: That will tend to be.

701
01:49:22,230 --> 01:49:27,030
Marisa Eisenberg: Like large constants that can be problematic and can make your optimizer behave badly so.

702
01:49:27,420 --> 01:49:27,780
Adriana Perez: If you're.

703
01:49:27,960 --> 01:49:28,590
Marisa Eisenberg: If you're.

704
01:49:28,980 --> 01:49:43,560
Marisa Eisenberg: A little more of a like comfortable with equations it's not a bad thing to kind of be able to do this by hand, but it is definitely not required and plenty you don't really need to be able to dive into all this stuff and okay.

705
01:49:44,400 --> 01:49:48,750
Marisa Eisenberg: So that is just kind of the the guts of sort of how you would.

706
01:49:49,110 --> 01:49:59,220
Marisa Eisenberg: calculate a likelihood for an O D, but just to kind of summarize here, so the basic idea is that you're usually for oldies you're assuming only measurement error for stochastic models you'll have.

707
01:50:00,000 --> 01:50:02,550
Marisa Eisenberg: randomness in the actual model process to.

708
01:50:02,910 --> 01:50:13,170
Marisa Eisenberg: And and you're going to assume that the data is given by a distribution where our model output usually is is assumed to be the meat, whatever the shape parameter that describes the mean.

709
01:50:13,410 --> 01:50:17,520
Marisa Eisenberg: that's what you would assume, and then you assume each time point of your data is independent.

710
01:50:17,790 --> 01:50:22,860
Marisa Eisenberg: And then basically you just use the probability density function or probability mass function to calculate the likelihood.

711
01:50:23,130 --> 01:50:34,200
Marisa Eisenberg: Take the negative log and then hand that over to opt in or whatever optimization function you're using and then like use that to optimize your parameter values and that's sort of the basic structure of kind of how.

712
01:50:34,470 --> 01:50:39,030
Marisa Eisenberg: classic maximum likelihood with differential equations would work and okay.

713
01:50:41,910 --> 01:50:45,570
Marisa Eisenberg: Well before I kind of continue on questions about any of that stuff.

714
01:50:48,390 --> 01:50:52,650
Rishi Chanderraj: um if if our their methods for like if.

715
01:50:53,970 --> 01:50:58,890
Rishi Chanderraj: Time points are independent when like conditional stuff is there.

716
01:50:58,920 --> 01:50:59,250
yeah.

717
01:51:01,320 --> 01:51:05,010
Rishi Chanderraj: Modification modifications, you can use for independence is violated.

718
01:51:05,340 --> 01:51:08,880
Marisa Eisenberg: Yes, exactly yeah so independence is violated which.

719
01:51:09,240 --> 01:51:15,600
Marisa Eisenberg: i'll be honest, it almost always is even though we frequently assume that time points are independent, they basically never are but.

720
01:51:15,810 --> 01:51:23,220
Marisa Eisenberg: it's it's it's often not the worst assumption that your measurement error on the time points is independent that's probably kind of reasonable, but.

721
01:51:23,520 --> 01:51:35,460
Marisa Eisenberg: But it's also pretty often the case that they are not independent and then yeah you you, it is a lot more of a pain in the butt and it's more mathematical you have to do the actual calculation on how the conditionality plays out and so.

722
01:51:35,970 --> 01:51:42,120
Marisa Eisenberg: There are there are methods out there for doing that and people have modeled that before sometimes it's actually.

723
01:51:42,570 --> 01:51:50,010
Marisa Eisenberg: If it's really so sometimes you can do it and it's and you can actually calculate the conditional probabilities and you just use your maximum likelihood kind of framework.

724
01:51:50,280 --> 01:51:57,000
Marisa Eisenberg: But now it's just that you don't take a product, you have to have this kind of like complicated chain of conditional depending on what's going on and.

725
01:51:57,600 --> 01:52:06,900
Marisa Eisenberg: Sometimes you end up moving to something more like approximate bayesian computation where you don't actually calculate the true likelihood or the true posterior.

726
01:52:07,680 --> 01:52:16,080
Marisa Eisenberg: If your innovation setting you calculate something close that is reasonably statistically guaranteed to give you the right distribution and so.

727
01:52:16,500 --> 01:52:30,870
Marisa Eisenberg: Sometimes you have to do something like that, if it's just it can be it can really make a mess of things when you can't make that independence assumption because figuring out how to get to those conditions can be quite hard but yeah so that's that's a very good question yeah.

728
01:52:31,320 --> 01:52:34,320
Rishi Chanderraj: And I mentioned those take forever to run.

729
01:52:36,120 --> 01:52:36,540
Rishi Chanderraj: Probably.

730
01:52:39,030 --> 01:52:39,390
Rishi Chanderraj: Unless.

731
01:52:39,420 --> 01:52:40,590
Rishi Chanderraj: Unless your data is like.

732
01:52:41,640 --> 01:52:43,710
Rishi Chanderraj: Really small it's probably really hard.

733
01:52:44,370 --> 01:52:57,870
Marisa Eisenberg: yeah exactly yeah they usually they usually come up for like like it often ends up for like like if you're it often is the case that one, it takes forever to run and to the model like.

734
01:52:58,560 --> 01:53:02,100
Marisa Eisenberg: you end up using something more like a network model or something like this, where you're.

735
01:53:02,340 --> 01:53:10,680
Marisa Eisenberg: You have to think about the relationships between people, for your model to accurately, be able to simulate the conditional is in the first place, and so, then like.

736
01:53:11,010 --> 01:53:25,620
Marisa Eisenberg: it ends up being yeah, it is a bear it's definitely you know, like a big process to do but, but you can do it and people do but yeah if you if you can cheat and assume independence, it is definitely makes your life easier, so you know yeah that's yeah.

737
01:53:26,970 --> 01:53:41,580
Marisa Eisenberg: that's good okay all right so then um okay so then uncertainty i'm gonna not dive into too much right now we'll get into it a bit when we talk about Beijing approaches to parameter estimation.

738
01:53:42,780 --> 01:53:54,810
Marisa Eisenberg: And mostly going to give you kind of to Google terms that you can then go search for if you want more, and I can definitely put some.

739
01:53:55,230 --> 01:54:03,870
Marisa Eisenberg: materials up on the website, if this is something that you want to know about, but if you're using a maximum likelihood framework and you want to get confidence intervals or uncertainty.

740
01:54:04,740 --> 01:54:11,730
Marisa Eisenberg: ranges for your your estimates is money so we'll do that one, but if you want to get some uncertainty.

741
01:54:12,240 --> 01:54:19,200
Marisa Eisenberg: You know, basically, you want some like standard deviation or confidence range, for your for your parameter values.

742
01:54:19,470 --> 01:54:31,620
Marisa Eisenberg: These can be calculated in a couple of different ways i'm probably one of the most common ways to do it is with the fishery information matrix, which is based on that premier outbound idea that we talked about very briefly earlier.

743
01:54:31,890 --> 01:54:42,180
Marisa Eisenberg: But this is like a sort of acid tonic method that assumes large large sample size for your data, and you can use that to calculate.

744
01:54:42,960 --> 01:54:49,500
Marisa Eisenberg: Confidence ranges and you can also use Monte Carlo methods, there are plenty of times, where people will use.

745
01:54:50,220 --> 01:55:05,700
Marisa Eisenberg: MC MC which we're going to talk about later today and they'll use gradient descent, or you know, whatever optimization to find their optimal parameter values and then run them cmc to find their uncertainty on those parameter values, one of the most common methods for.

746
01:55:06,840 --> 01:55:24,120
Marisa Eisenberg: Systems models is what are called profile likelihoods, and so it turns out that these are these are kind of strike a nice balance between fairly cheap to calculate they don't take that long to run but they the acid tonic methods that come from the fisher information matrix.

747
01:55:26,190 --> 01:55:33,840
Marisa Eisenberg: Because they assume a large sample size for your data and often with these kinds of models this the.

748
01:55:34,440 --> 01:55:46,050
Marisa Eisenberg: The end, for your data that you would need to be able to make these confidence intervals accurate is very, very large because they're just very complicated models and we often don't have as much data as we would need so.

749
01:55:46,350 --> 01:55:51,720
Marisa Eisenberg: These these Fisher information kind of asked them Tata competence ranges are often overly optimistic.

750
01:55:52,020 --> 01:55:58,560
Marisa Eisenberg: And so profile likelihood method to kind of become a pretty common way of doing this, so if you're looking for one method.

751
01:55:58,890 --> 01:56:05,160
Marisa Eisenberg: If you want to do this on a model of your own and you're looking for you're thinking about what method you want to do to figure out your competence intervals.

752
01:56:05,370 --> 01:56:15,240
Marisa Eisenberg: If you're using a maximum likelihood framework i'd recommend checking out profile likelihoods and I can post some examples of kind of how to do this i'm in the on the on the website later on.

753
01:56:16,800 --> 01:56:30,180
Marisa Eisenberg: Okay, so I was gonna oh yeah oh yeah I know exactly there was a fishery you there needs to view of that would be good, I do have I i've got posted up on the website actually already.

754
01:56:31,230 --> 01:56:36,720
Marisa Eisenberg: Some resources from some tutorial workshops and other summer schools that i've taught in the past.

755
01:56:36,990 --> 01:56:45,630
Marisa Eisenberg: And we do the fisher information in more detail in those and so you could you can look there there's some good like just slides and different things that kind of talk about it.

756
01:56:46,410 --> 01:56:52,200
Marisa Eisenberg: And, and one of them has videos also if you want to like see the lectures and stuff from that so there's stuff in there.

757
01:56:52,860 --> 01:57:02,100
Marisa Eisenberg: i'll also I can post a couple papers to on Fisher information approaches and kind of like a little overview kind of stuff there yeah so good awesome okay.

758
01:57:02,760 --> 01:57:14,370
Marisa Eisenberg: um okay so then i'm always have to hang on I gotta make a note to myself to actually do it, or else I will forget, by the time i'm done teaching and then I won't actually ever push.

759
01:57:21,240 --> 01:57:33,900
Marisa Eisenberg: cool Okay, and so so that's sort of the basic idea for uncertainty when you're doing parameter estimation, like this i'm gonna do i'm very teeny i'm actually you know what.

760
01:57:34,830 --> 01:57:43,020
Marisa Eisenberg: it's a little bit funny we're going to do base and a lot more detail in a minute and so i'm actually going to skip past these.

761
01:57:44,760 --> 01:57:49,260
Marisa Eisenberg: Yes, okay we're gonna will we're gonna do.

762
01:57:50,460 --> 01:57:56,670
Marisa Eisenberg: yeah okay well you're going to do this in a little bit in the interest of time i'm going to skip this now because we're going to do, will do.

763
01:57:56,880 --> 01:58:05,850
Marisa Eisenberg: bayesian estimation, and a lot more detail in a minute so we're going to skip past this in a moment, for the moment, and then we're let's do this and then we'll kind of come back to it okay so.

764
01:58:06,780 --> 01:58:16,140
Marisa Eisenberg: The arc of what we're going to do next we're going to talk about some of the issues that come up for parameter estimation, for these kinds of complex, you know systems models we're going to talk about.

765
01:58:18,030 --> 01:58:28,800
Marisa Eisenberg: How you do model comparison sensitivity analysis, a bit of those kinds of things and then we'll come back to Beijing estimation, as the wrap up for the for the for the morning session so that's what we're going to do and.

766
01:58:29,370 --> 01:58:38,970
Marisa Eisenberg: i'm looking at the time and it's already getting close to 11 I don't think we are going to actually have time to work on the lab today during the class section.

767
01:58:39,300 --> 01:58:46,980
Marisa Eisenberg: And the lab that's posted for today i'll i'll sort of step you through it just you know kind of walk you through the sections, but.

768
01:58:47,550 --> 01:58:54,660
Marisa Eisenberg: It is it's one of the tougher labs I think for the class well I don't know Michael writes hard labs too so it's not like whatever but.

769
01:58:55,320 --> 01:59:05,160
Marisa Eisenberg: But I think it's a little it requires a little more time to kind of play with and work with so we won't really have time to work on it, but I recommend it, this is a topic area that you're interested in.

770
01:59:05,520 --> 01:59:13,950
Marisa Eisenberg: You can to like work on it on your own time, and after the classes done, you know tonight or after the week is over and there's there's.

771
01:59:14,760 --> 01:59:28,260
Marisa Eisenberg: A github repository that goes along with the lab that has code that you can use and it's got you know kind of you can use it's got like sort of the full set of code that does everything that would happen in the left so.

772
01:59:28,950 --> 01:59:39,270
Marisa Eisenberg: So anyway so that's also posted up there, and for now I want to talk about a particular issue that comes up a lot in these kinds of.

773
01:59:39,750 --> 01:59:54,960
Marisa Eisenberg: Systems models types of approaches and it's the issue of identify ability so and identify ability is asking a question of Is it actually possible to uniquely determine your parameters from the data that you have so.

774
01:59:55,890 --> 02:00:02,940
Marisa Eisenberg: It turns out that often for the kinds of models that we will we build for systems approaches.

775
02:00:03,360 --> 02:00:11,760
Marisa Eisenberg: And so, for statistical models people have checked the identify abilities these models extensively a lot of most regression models have been around for a long time, and so.

776
02:00:12,570 --> 02:00:21,390
Marisa Eisenberg: You know it's already sort of well established which flavors of models have identify ability issues and which ones don't and there's kind of a like you don't most sort of.

777
02:00:22,740 --> 02:00:32,520
Marisa Eisenberg: Like if you're an epidemiologist who's like just working with statistical models you don't ever really have to think about identify ability statisticians have kind of already handled all that for you, a long time ago.

778
02:00:34,260 --> 02:00:44,280
Marisa Eisenberg: For the kinds of models that we're building here, because each one is custom built and and and they don't sort of necessarily follow in as consistent of a framework.

779
02:00:44,640 --> 02:00:54,840
Marisa Eisenberg: identify ability issues come up and so um So the idea here there's two flavors of identify ability problems, what are called practical identify ability and structural identify ability.

780
02:00:55,140 --> 02:01:04,800
Marisa Eisenberg: And these are two kind of overlapping broad categories, but the basic idea is practical identify ability is getting it when is it when are you going to have issues with estimating your parameters because of.

781
02:01:05,760 --> 02:01:15,510
Marisa Eisenberg: noisy data or you didn't measure at the right times, and so you missed a certain feature of the data or things like this sort of issues that are kind of more practical about how your.

782
02:01:15,930 --> 02:01:23,550
Marisa Eisenberg: measurement happened, and you know how you structured how you how you took your data versus structural identify ability which are.

783
02:01:24,480 --> 02:01:33,750
Marisa Eisenberg: really getting at it's possible for you to write down a model structure that fundamentally you can't estimate all the parameters for even if you had perfect data so.

784
02:01:34,410 --> 02:01:42,270
Marisa Eisenberg: For instance, suppose that you wrote down this little model here y equals m one plus two times X plus B.

785
02:01:42,990 --> 02:01:49,320
Marisa Eisenberg: it's because of the way this is set up, if the things you're measuring our y and X, then.

786
02:01:49,680 --> 02:01:55,500
Marisa Eisenberg: No matter how good your data is even if these red dots are exactly on the line, with no noise at all.

787
02:01:55,800 --> 02:02:06,480
Marisa Eisenberg: You can never estimate m one m to individually, because any combination of em one and empty that gives the correct overall slope is going to give you the same fit to the data.

788
02:02:06,690 --> 02:02:24,900
Marisa Eisenberg: And so you wouldn't be able to tell the difference so say the slope is five then two and three or one and four or pie and five minus pi whatever it is, you know any combination that adds up to five here and go course me i'm going to just a second.

789
02:02:49,860 --> 02:02:58,140
Marisa Eisenberg: Sorry, the the the there's a like a the gardener's are here there there's some sort of one thing happening so.

790
02:02:58,800 --> 02:03:11,370
Marisa Eisenberg: Okay, so any any combination of em one and them to that adds up to the same slope, the same total is going to give you the same fit to the data and and so you wouldn't be able to tell the difference, so that means that.

791
02:03:11,610 --> 02:03:24,420
Marisa Eisenberg: Basically Edwin and up to can trade off with one another, so that you, you always get the same fits the data, so we can never estimate and wanting them to separately, so we would say that i'm wanting them to hear our unidentifiable.

792
02:03:25,500 --> 02:03:36,150
Marisa Eisenberg: And, and so that that can cause, you know serious issues if you're trying to estimate, if you need the value of em one to be able to determine what intervention, you should do, then.

793
02:03:36,630 --> 02:03:42,750
Marisa Eisenberg: you've got a problem right like, and so you know that that's these kinds of issues can come up a lot in.

794
02:03:43,380 --> 02:03:49,410
Marisa Eisenberg: In the kinds of models that in systems models and the thing that's unfortunately said here, you know there's sort of.

795
02:03:49,770 --> 02:03:56,550
Marisa Eisenberg: Assuming you don't actually need the values of them wanting them to separately it's sort of an obvious fix right like you look at this and you would say well okay.

796
02:03:56,760 --> 02:04:08,130
Marisa Eisenberg: yeah I can estimate and one an empty separately, but why don't I just changed my model just only ever estimate their total and call that so and then you know we can kind of move on with our day, this is a perfectly fine well and.

797
02:04:08,640 --> 02:04:15,480
Marisa Eisenberg: And that idea is what's called the idea of identifiable combination, so you can find combinations of parameters that are.

798
02:04:15,750 --> 02:04:26,250
Marisa Eisenberg: sort of linked to one another and combine them and then sort of rewrite your model in terms of those combinations The thing that makes it tricky is that for most systems models it doesn't turn out so.

799
02:04:26,640 --> 02:04:31,260
Marisa Eisenberg: Clean like this, where they're in one term together the parameters that might be sort of.

800
02:04:31,740 --> 02:04:37,350
Marisa Eisenberg: Linked or entangled with one another, that can trade off from one another, might be in different equations.

801
02:04:37,560 --> 02:04:45,870
Marisa Eisenberg: They may not sort of be in any way kind of obvious when you do things like non dimensional eyes in your system or any other kinds of usual tricks that people often.

802
02:04:46,170 --> 02:04:57,240
Marisa Eisenberg: use to kind of simplify down their system and, and so it turns out that figuring out what the unidentified abilities are and your model can be.

803
02:04:57,900 --> 02:05:07,860
Marisa Eisenberg: kind of a whole process, and if you need parameter information from your model not just sort of predictions of what you know simulations of what are going to happen.

804
02:05:09,240 --> 02:05:18,240
Marisa Eisenberg: You know, or even what you know, essentially, if you want to be able to work with your model and sort of understand what's going on with your parameters, you need to be able to think about identify ability issues and so.

805
02:05:19,680 --> 02:05:26,910
Marisa Eisenberg: When we get to the sort of sampling based methods i'll talk about how you can use some of the output from those to diagnose some of these issues and figure out what's going on.

806
02:05:27,480 --> 02:05:33,840
Marisa Eisenberg: But I wanted to just kind of make sure that everybody was aware that this is something that comes up a lot for these types of models and so.

807
02:05:34,140 --> 02:05:44,790
Marisa Eisenberg: it's it's surprisingly still common even now for people to do something like propose a new systems model, you know i'm modeling whatever the incidence of.

808
02:05:46,440 --> 02:05:54,390
Marisa Eisenberg: Cancer in such and such population whatever estimate the parameters with something like an elder meet or an optimization method you know.

809
02:05:54,990 --> 02:06:10,440
Marisa Eisenberg: And then just sort of report the results that you get without sort of thinking about whether there were other parameter values that would also fit equally well or what the distribution of those might be, or you know any of those kinds of questions, and so there can be a lot more.

810
02:06:11,880 --> 02:06:20,250
Marisa Eisenberg: You saw what happened when we just had to minimum right right next to one another and that can cause all kinds of issues, for your parameter estimation here you have.

811
02:06:20,490 --> 02:06:28,350
Marisa Eisenberg: Much more complicated scenarios, where you have canyons and like runnels and many minima that all kind of confit equally well and so.

812
02:06:28,650 --> 02:06:38,550
Marisa Eisenberg: That kind of more complicated cost function surface can be very challenging and you really need to kind of think through it and make sure that you do some sampling and some exploration of your parameter space.

813
02:06:38,850 --> 02:06:42,420
Marisa Eisenberg: And not just sort of take the results of your optimization is given and then move on.

814
02:06:43,590 --> 02:06:51,360
Marisa Eisenberg: Okay, so usually the way that these three pictures that we looked at before kind of get frame from an identify ability perspective.

815
02:06:51,630 --> 02:07:03,120
Marisa Eisenberg: Is that this would be called globally or uniquely identifiable this would be called non uniquely or locally identifiable, meaning that there are local minima or kind of very a few different.

816
02:07:03,450 --> 02:07:12,600
Marisa Eisenberg: Possible minimum that might fit equally well, and this would be called unidentifiable so there's some sort of like trade off this is like the M one and two example there's some kind of.

817
02:07:12,990 --> 02:07:19,890
Marisa Eisenberg: You know, you can trade off and one of them to you and you get an infinite number of possible you know values that would fit the data equally well.

818
02:07:20,280 --> 02:07:31,230
Marisa Eisenberg: And, and so structural identify ability is is a similar kind of idea, so you know we were talking about here is is is is our premier.

819
02:07:31,590 --> 02:07:43,050
Marisa Eisenberg: This is structural identify ability practical identify ability as a similar idea so, then I say and but it's getting at the same kinds of issues, the same kinds of you know.

820
02:07:43,800 --> 02:07:48,630
Marisa Eisenberg: You know, do you have one minimum do you have multiple minimize do you have a sort of Canyon of minima.

821
02:07:48,990 --> 02:07:56,730
Marisa Eisenberg: And, but it can come up, not just from the structure of your model, but also from how your data was collected if you're if you only.

822
02:07:56,940 --> 02:08:08,400
Marisa Eisenberg: I mean, whatever as like a really silly example let's say you're going to estimate two parameters, but you only take one data point you can't actually estimate two parameters from a single data point or you know.

823
02:08:09,030 --> 02:08:20,610
Marisa Eisenberg: Probably many of you have kind of heard this this like rule that if you have more than you, you need at least as many data points, as you have parameters to estimate, you know those kinds of things, there are situations where.

824
02:08:21,510 --> 02:08:25,290
Marisa Eisenberg: You won't because of the way your data was collected or the noisiness of your data.

825
02:08:25,530 --> 02:08:37,170
Marisa Eisenberg: It may be that you what would under perfect circumstances be identifiable becomes unidentifiable or has multiple minima or whatever, because of the way the data is collected and.

826
02:08:37,680 --> 02:08:47,370
Marisa Eisenberg: Like a silly example of this, that actually happened to me was I was working when I was a graduate student I was working on a project, where we were modeling thyroid hormone and.

827
02:08:48,240 --> 02:09:02,670
Marisa Eisenberg: thyroid hormone so there's a regulatory system, one of the hormones involved in the system is is called tsh thyroid stimulating and tsh has a very strong circadian rhythm and we were modeling this hormone overtime and.

828
02:09:03,810 --> 02:09:15,540
Marisa Eisenberg: So we had a set of experimental collaborators, we were working with, and they were going to give us data on how tsh changes over time that we could then use to fit the model, so that we could estimate them the model parameters.

829
02:09:16,020 --> 02:09:31,140
Marisa Eisenberg: They they measured the data for tsh in their patients at the same time, every day, and unfortunately because tsh has a circadian rhythm if you measure it at the same time, every day, you get a constant value right and so.

830
02:09:32,070 --> 02:09:39,300
Marisa Eisenberg: We couldn't estimate anything about the circadian variation of tsh because we only ever got the value at noon and so.

831
02:09:39,540 --> 02:09:46,950
Marisa Eisenberg: You know, so that meant that you know if you so if you if you in a perfect world if they had measured that more time points.

832
02:09:47,160 --> 02:09:53,250
Marisa Eisenberg: We would have that that piece of the model would have been identifiable but because of the way that the data was measured.

833
02:09:53,580 --> 02:10:10,770
Marisa Eisenberg: It was only measured at the same time, the circadian portion of our model was on identified so that's an example of how practical identify ability, you know the practicalities of how your data is collected can make what might seem like an identifiable model and identify okay um.

834
02:10:12,150 --> 02:10:19,290
Marisa Eisenberg: And these are some points about local minimum, and all this kind of thing I think we can kind of skip past these, I guess, I will say one thing.

835
02:10:19,680 --> 02:10:27,330
Marisa Eisenberg: always be sure to try and visualize the fit of your models, the number of times that i've had graduate students come to me and say I estimated the parameters of my model and they just give me.

836
02:10:27,690 --> 02:10:39,450
Marisa Eisenberg: A likelihood value or something like that, without having visualize what it looks like you know you want to you want to visualize the fit of your model in whatever way you can so that you can evaluate how well it's fitting the data.

837
02:10:40,350 --> 02:10:56,820
Marisa Eisenberg: Okay, the rest of this, I think we can skip it's almost 11, let me just step us through what is in the lab and then we're not going to do it right now but we'll just i'll just sort of take a look at it, and then, and then you all can work on it.

838
02:10:58,080 --> 02:11:04,590
Marisa Eisenberg: separately, so in the lab the way the lab is set up, basically, is so.

839
02:11:05,700 --> 02:11:09,690
Marisa Eisenberg: The we're gonna we're going to take data from a.

840
02:11:11,250 --> 02:11:17,640
Marisa Eisenberg: I just made this up, I guess, apparently, I was playing started to valley at the time that I wrote this lab which is funny to me but um but so.

841
02:11:18,240 --> 02:11:34,080
Marisa Eisenberg: it's a flu like illness that's been spreading through star do valley county county and you've been asked to develop a model for this disease and so we're going to do an si our model and so you've got an si our model here and a measurement equation, so what you can actually measure.

842
02:11:35,160 --> 02:11:44,460
Marisa Eisenberg: And then you're going to code up this model and run it and thought it and then what I what I give you here is basically template code.

843
02:11:44,790 --> 02:11:51,630
Marisa Eisenberg: That is almost all filled in except for this part, so you just have to fill in what the likelihood would be.

844
02:11:51,900 --> 02:12:00,630
Marisa Eisenberg: And it sort of steps you through it in the comments sort of with like what each piece, so you know sort of what each piece of the this code should be and so.

845
02:12:00,810 --> 02:12:07,590
Marisa Eisenberg: that's sort of the the main crux of the problem is to kind of figure out Okay, what is the likelihood function that I should fill in here.

846
02:12:07,800 --> 02:12:18,120
Marisa Eisenberg: And there's some strict instructions at the top you're going to use a plus on distribution all this kind of thing and then you'll run your likelihood function for a couple of different parameter values, just to try it out and then basically.

847
02:12:19,380 --> 02:12:30,930
Marisa Eisenberg: you'll you'll plug that in here the code is already written for this so essentially once you write your likelihood function up here you'll just need to run this code and it'll run the optimization.

848
02:12:31,260 --> 02:12:43,170
Marisa Eisenberg: And then you'll be able to sort of see how well your your model fits the data and then I asked you to try doing some forecasting so you'll estimate what if you only had the first.

849
02:12:44,280 --> 02:12:54,300
Marisa Eisenberg: What is it the first five data points and then you want it to forecast and the reason I put this in the lab is to get at these practical identify ability issues, it turns out that.

850
02:12:54,600 --> 02:13:02,700
Marisa Eisenberg: The model is fine, when you have the full trajectory and you're just trying to estimate the parameters, but if you only have the first five data points of the of the epidemic.

851
02:13:02,880 --> 02:13:10,290
Marisa Eisenberg: You don't have enough information to be able to actually estimate all your parameters and so you'll see some identify ability issues crop up the estimator will.

852
02:13:10,470 --> 02:13:16,410
Marisa Eisenberg: it'll take a lot longer for it to run and it'll seem to have some issues and you'll kind of start to see like oh.

853
02:13:16,710 --> 02:13:21,870
Marisa Eisenberg: This seems you know I guess actually no I was, I was cute so it's fine.

854
02:13:22,170 --> 02:13:30,780
Marisa Eisenberg: For the first five data points, but then it will barf when you get to the first Ford when you cut it down to just the first four data points so at that point.

855
02:13:31,050 --> 02:13:37,440
Marisa Eisenberg: You you won't be able to run the estimation very well anymore, and so you'll get bad predictions and it'll all kind of journey.

856
02:13:38,190 --> 02:13:50,400
Marisa Eisenberg: to being a mess, so if you want to explore some of these ideas you can play around with this most of the code is basically written and so you just have to fill in this likelihood, and then the github.

857
02:13:51,090 --> 02:13:56,130
Marisa Eisenberg: repository there's some examples of likelihoods and things like that, where this is already filled in So if you.

858
02:13:56,460 --> 02:14:04,590
Marisa Eisenberg: have a hard time here, you can also just go to that repository and steal the code from there and then it's basically the same code so so that's fine.

859
02:14:05,520 --> 02:14:17,610
Marisa Eisenberg: Okay, and so, then you do this forecasting and then this last piece and we'll talk about in a minute, because we haven't covered the ice, but this is for doing model comparison and I put some extra problems just for fun.

860
02:14:18,570 --> 02:14:26,340
Marisa Eisenberg: That you can you can play with if you want to do okay so that's the lab i'm not going to have us do it because I just don't really have time but.

861
02:14:26,790 --> 02:14:36,180
Marisa Eisenberg: If if if any of these topics are of interest to you i'd recommend it's a good like just kind of exercise where you can sort of step through a lot of these ideas and and do them.

862
02:14:36,690 --> 02:14:44,310
Marisa Eisenberg: In a kind of realistic setting, even though obviously it's a fake disease, but you know you could replace that data instead with like.

863
02:14:44,580 --> 02:14:57,780
Marisa Eisenberg: For instance, coded data from one of the more recent waves and try just test it out, you know you could or you know pick your favorite flu season, or whatever it is, and just see you know how it works, you could start to just kind of play around okay.

864
02:14:59,220 --> 02:14:59,910
Marisa Eisenberg: So.

865
02:15:01,140 --> 02:15:11,400
Marisa Eisenberg: So that's identify ability there's a couple of other little topics that I wanted to just kind of brush through really quick and then we'll talk about bayesian approaches to parameter so.

866
02:15:13,260 --> 02:15:21,810
Marisa Eisenberg: I wanted to talk a little bit about sort of how we think about model fitting and how do we compare different models and all of those kinds of questions so.

867
02:15:23,490 --> 02:15:34,590
Marisa Eisenberg: I said before that, when you're doing parameter estimation it's really important to plot your model and your model fit to the data, and so, then, what do you look for when you're looking at a plot like that right so.

868
02:15:35,190 --> 02:15:41,070
Marisa Eisenberg: When you're looking at the model fit to the data and the first thing is just I call this the eyeball test which is like.

869
02:15:42,000 --> 02:15:46,830
Marisa Eisenberg: Humans are good pattern matters, and so does your model just.

870
02:15:47,250 --> 02:16:01,560
Marisa Eisenberg: add a gut check kind of first look does it look like the data right like at all, and is there anything that you kind of noticed that seems weird or anything like that so just kind of look at it and see what you think you know how does it seem like it's matching.

871
02:16:02,880 --> 02:16:08,340
Marisa Eisenberg: is the first thing you can also, of course, look at the values of your negative log likelihood your your residual some of squares.

872
02:16:08,910 --> 02:16:22,260
Marisa Eisenberg: And you look at your parameter uncertainties and maybe the correlations between your parameters, you should also look at the distribution of your residuals So if you assume so, for instance, you know if you assume that your.

873
02:16:23,640 --> 02:16:31,230
Marisa Eisenberg: Your your data was coming from a normal distribution, where we know the mean, and we have a constant standard deviation.

874
02:16:31,830 --> 02:16:42,900
Marisa Eisenberg: Then the residuals ought to be normally distributed after you do your fit and if they're not then probably your assumption about this distribution for the data was not correct right and so.

875
02:16:43,590 --> 02:16:50,250
Marisa Eisenberg: So it's important to go back and look at your residuals and see if they actually make sense for based on what you assume, for your likelihood.

876
02:16:50,580 --> 02:17:07,290
Marisa Eisenberg: And there's other kinds of things that you can look at there's so a lot of times people look at the correlation of their residuals So if you have so, for instance, here, this is maybe not a bad fit in terms of how closely it's you know decently close to the data and everything but.

877
02:17:08,370 --> 02:17:15,870
Marisa Eisenberg: The residuals or maybe these are better the residuals are all on one side right so they're very there'll be very correlated like many.

878
02:17:16,080 --> 02:17:26,220
Marisa Eisenberg: If you have if you're above the data, then you're going to be above the data for a while, so there will be a kind of serial correlation you can also have things like this, where you get correlation of the residuals.

879
02:17:26,820 --> 02:17:33,930
Marisa Eisenberg: In an Anti correlated way this looks like a great fit right like it's going right through them, it looks nice but.

880
02:17:34,560 --> 02:17:40,020
Marisa Eisenberg: It is weird and perhaps problematic that the data is alternating every single time.

881
02:17:40,350 --> 02:17:48,870
Marisa Eisenberg: Right like it's above it's below it's above it's below it's above its flow buffalo if it was actually random sometimes you should have to above, or you know.

882
02:17:49,260 --> 02:17:51,930
Marisa Eisenberg: Like a couple above and then you know the other direction.

883
02:17:52,320 --> 02:18:01,800
Marisa Eisenberg: This is actually kind of suggested that maybe there's an oscillation or something that we're missing or something about how the data is getting measured that is weird you know something that something is going on.

884
02:18:02,190 --> 02:18:12,120
Marisa Eisenberg: it's it's odd that it's so exactly up down up down, and so you would catch this in an Anti correlation in your residual in your correlation of residuals and so.

885
02:18:12,420 --> 02:18:20,580
Marisa Eisenberg: Things like that you, you want to kind of keep an eye on you know you don't even necessarily need to run an actual correlation coefficient or anything but just.

886
02:18:21,660 --> 02:18:23,370
Marisa Eisenberg: Looking at it visually and seeing.

887
02:18:24,420 --> 02:18:35,760
Marisa Eisenberg: there's another thing that gets I love this because it has a really fancy sounding name and it's a really simple it's really it's a really stupid like kind of simple thing, so the world Wolfowitz runs test it just means.

888
02:18:36,150 --> 02:18:41,610
Marisa Eisenberg: Literally count how many times was did you get residuals that we're all on one side, so.

889
02:18:43,140 --> 02:18:52,200
Marisa Eisenberg: If you if you get a long run, so, like many data points in a row that are all on one side, that means you're probably missing something right so like this fits pretty decently.

890
02:18:52,440 --> 02:18:54,660
Marisa Eisenberg: But all of these data points are above.

891
02:18:54,960 --> 02:19:03,810
Marisa Eisenberg: And so, probably something is wrong with our model because we got this very correlated sequence of residuals that are all on the on the wrong side of the data so.

892
02:19:03,990 --> 02:19:08,850
Marisa Eisenberg: Here, for instance, you know you're we're consistently, above all, of the All of the data.

893
02:19:09,150 --> 02:19:19,230
Marisa Eisenberg: And you know through except for the first one, and so it's suggested that there's something systematically wrong in our in our model that's causing it to over under predict right and so.

894
02:19:19,470 --> 02:19:24,930
Marisa Eisenberg: keeping an eye on sort of when you notice things like that, when you're looking at your model fits the data is a good thing.

895
02:19:25,980 --> 02:19:39,330
Marisa Eisenberg: Okay, so i'm going to talk really briefly about sensitivity analysis and the icy man, the summer course is a crash course these we could have a day on each one of these topics, so it is.

896
02:19:40,110 --> 02:19:54,870
Marisa Eisenberg: it's a lot and I know you all are so hang in there, I guess that's really what i'm trying to say um okay so sensitivity analysis and is another is another kind of related to parameters and parameter estimation type of topic so.

897
02:19:56,220 --> 02:20:07,890
Marisa Eisenberg: This is this and and and when we talk about Ai see they're related to parameter estimation, but they're not exactly in not but not exactly parameter estimation so.

898
02:20:08,070 --> 02:20:14,670
Marisa Eisenberg: sensitivity analysis is getting at this idea that we expect our parameters to be somewhat wrong, even when you estimate parameters data.

899
02:20:15,000 --> 02:20:20,040
Marisa Eisenberg: You know you don't have perfect data, so your parameter values are probably a little bit wrong in certain ways.

900
02:20:20,280 --> 02:20:31,560
Marisa Eisenberg: And so you want, you may want to know how robust are the model predictions to changes or errors in your parameter estimates and so sensitivity analysis helps us address this question so and.

901
02:20:32,310 --> 02:20:39,690
Marisa Eisenberg: sensitivity analysis is basically looking at how much does your output your whatever.

902
02:20:40,380 --> 02:20:45,750
Marisa Eisenberg: variable of interest that you're tracking so maybe it's your measured variable or it could be something else.

903
02:20:46,200 --> 02:20:54,870
Marisa Eisenberg: change how much does the the outcome of interest from your model change as you change your parameter values and so there's a bunch of different measures of this.

904
02:20:55,500 --> 02:21:09,330
Marisa Eisenberg: Relative sensitivity is a common one, but, basically, the idea is that you want to sort of capture the change in your output of interest as a function of you're changing your parameters and so.

905
02:21:10,920 --> 02:21:22,140
Marisa Eisenberg: People do this in a number of different ways, you can actually just calculate these derivatives, people do that a lot that's a common thing called the local sensitivity and you can you can you can calculate.

906
02:21:22,920 --> 02:21:29,370
Marisa Eisenberg: variations on this so maybe you normalize it you say what's the fractional change in my output of interest.

907
02:21:29,880 --> 02:21:34,680
Marisa Eisenberg: as a function of the fractional change of the parameters, which is what we're doing here and.

908
02:21:35,160 --> 02:21:42,300
Marisa Eisenberg: sensitivity is like a is a tricky thing so when you're trying to ask the question how much does my outcome of interest changes, a function of my parameter.

909
02:21:42,660 --> 02:21:54,450
Marisa Eisenberg: And it depends on where you are in parameter space right, so in this part of parameter space your outcome of interest doesn't change very much in this part of parameters is it changes, a lot and so.

910
02:21:54,840 --> 02:22:05,100
Marisa Eisenberg: sensitivity analysis is kind of complicated, if you have a set of parameter estimates, then you can kind of check near your estimates, but it's if you're trying to get a sort of global picture of how much.

911
02:22:05,580 --> 02:22:13,050
Marisa Eisenberg: Your your outcome of interest changes as a function of your parameters, it can be kind of complicated so it's a very local kind of an attribute.

912
02:22:13,650 --> 02:22:33,630
Marisa Eisenberg: But people have have often kind of wanted to know sort of well what what's the sort of overall global sensitivity of my outcome of interest in my parameter, and this comes up a lot when you know, maybe you're doing parameter estimation and you're having issues with the optimizer converging.

913
02:22:35,160 --> 02:22:42,660
Marisa Eisenberg: And you're starting to sort of realize that well, maybe I just have sort of to any parameters and there's some unidentified ability issues and it's kind of.

914
02:22:43,770 --> 02:22:52,710
Marisa Eisenberg: it's like none of it is really behaving very well one way to kind of clean out or reduce the dimension of your parameters face is to figure out which parameters are.

915
02:22:53,040 --> 02:22:56,820
Marisa Eisenberg: insensitive so they don't change the outcome of interest very much.

916
02:22:57,240 --> 02:23:10,590
Marisa Eisenberg: Then you can just fix those to some reasonable values and know that, even if that value is wrong it's not going to matter because it doesn't change your outcome of interest much right, and so, and so that way you can kind of.

917
02:23:11,010 --> 02:23:23,100
Marisa Eisenberg: Think through how to kind of simplify your parameter space and so people have thought about this a lot i'm going to skip through some of this this is sort of the details of how to evaluate sensitivity.

918
02:23:23,520 --> 02:23:33,420
Marisa Eisenberg: But when you're trying to do this sort of global sensitivity analysis idea what people often do is is they use sort of a number of different approaches so.

919
02:23:34,020 --> 02:23:43,230
Marisa Eisenberg: um one simple thing to do is is literally just take a big sample of parameters and then look at the like histogram of your outcome of interest.

920
02:23:43,440 --> 02:23:56,820
Marisa Eisenberg: And figure out which parameters don't have very much spread in the outcome of interest and then those are the ones that are probably not very sensitive right so so that's a kind of simple you know you can do things like scatter plot or histograms so that.

921
02:23:58,110 --> 02:24:08,610
Marisa Eisenberg: But there are two approaches that get used a lot in the literature to kind of general classes of methods so regression based methods and variance based methods and so.

922
02:24:09,600 --> 02:24:15,630
Marisa Eisenberg: The idea here is you take a big sample of your so it's a little bit funny.

923
02:24:16,200 --> 02:24:21,840
Marisa Eisenberg: What we're going to do is we're going to do correlation coefficients and and like linear models.

924
02:24:22,110 --> 02:24:30,720
Marisa Eisenberg: of our parameters and our simulated outcome of interest so we're we're actually fitting a statistical model to simulated data from our our.

925
02:24:30,990 --> 02:24:38,910
Marisa Eisenberg: Our math model, our systems model so basically take a big sample of parameters calculate your outcome of interest for all of those parameters.

926
02:24:39,180 --> 02:24:46,410
Marisa Eisenberg: And then figure out which parameters correlate most closely with your outcome of interest, basically, is the idea right so.

927
02:24:47,220 --> 02:24:51,990
Marisa Eisenberg: regression based methods use sort of correlation coefficients to kind of figure out.

928
02:24:52,650 --> 02:25:05,280
Marisa Eisenberg: You know which parameters correlate most closely with the outcome of interest there's a bunch of restrictions here I I don't think we'll dive into all the details there's a fantastic paper by.

929
02:25:05,640 --> 02:25:16,140
Marisa Eisenberg: Marino at all that i've posted up on the website that does a nice overview of both regression based methods and variance based method, so if you want to dive into some of this i'd recommend just reading that.

930
02:25:17,760 --> 02:25:23,340
Marisa Eisenberg: there's a so variants based methods are a similar kind of idea to.

931
02:25:24,000 --> 02:25:27,390
Marisa Eisenberg: like an A Nova or something that does the decomposition of variance.

932
02:25:27,690 --> 02:25:39,840
Marisa Eisenberg: Here, what you do is you take a big sample of your parameters and you want to know which parameters explain the most variation in your outcome of interest, and so you can do a decomposition of variance to basically figure out.

933
02:25:40,080 --> 02:25:45,270
Marisa Eisenberg: How much of the variance that you see in your outcome of interest is due to each parameter.

934
02:25:45,450 --> 02:25:59,280
Marisa Eisenberg: And then the parameters that aren't contributing much to the explaining the variance in your outcome of interest you those are your insensitive parameters and you could just sort of fix those and and kind of drop them in some sense so okay.

935
02:26:00,840 --> 02:26:05,310
Marisa Eisenberg: That was, like the world's most whirlwind tour of sensitivity analysis and.

936
02:26:06,360 --> 02:26:23,190
Marisa Eisenberg: Hopefully, the general idea of using sensitivity analysis to to try to understand sort of how, if your parameters are wrong, how much does changing your parameters affect the outcome you're trying to simulate.

937
02:26:23,760 --> 02:26:29,280
Marisa Eisenberg: that's sort of the basic idea and then there's a bunch of different methods for doing that, essentially, is so the take home.

938
02:26:29,940 --> 02:26:44,790
Marisa Eisenberg: And, and I would recommend sort of reading this Marino paper, if you want to kind of find out more so that's that's sort of the short version of all this and the other sort of mini topic before we get to evasion estimation is modeling this specification so.

939
02:26:46,170 --> 02:26:55,500
Marisa Eisenberg: All the stuff we've been talking about with parameter estimation, as soon as that we're using the right model for the data right, so that we have a model for our system.

940
02:26:55,740 --> 02:27:00,960
Marisa Eisenberg: That is is is at least close enough that we'll get something sensible out of it right.

941
02:27:01,470 --> 02:27:10,410
Marisa Eisenberg: But all models are wrong to some degree right there you know whether that's a statistical model or a you know, a.

942
02:27:10,890 --> 02:27:18,270
Marisa Eisenberg: mechanistic model like an si our model and none of them are exactly correct for the real world and so.

943
02:27:19,170 --> 02:27:31,500
Marisa Eisenberg: What happens if that incorrectness in the model structure, you know the real world doesn't have a system of differential equations underlying it right, it has whatever process is actually going on.

944
02:27:31,800 --> 02:27:36,900
Marisa Eisenberg: that's driving all of the interactions people talking to each other and doing whatever they're doing.

945
02:27:37,890 --> 02:27:49,740
Marisa Eisenberg: That, and so we build these models as simplified representations of the real world, but they're all wrong in some ways, or another, we have to make assumptions that simplify our models so that we can use them.

946
02:27:50,130 --> 02:28:04,170
Marisa Eisenberg: And so, when do those assumptions cause trouble and how do we deal with that so one way to deal with model miss specification is to do model comparison.

947
02:28:04,530 --> 02:28:08,520
Marisa Eisenberg: And and basically build a lot of different models that.

948
02:28:09,120 --> 02:28:20,520
Marisa Eisenberg: reflect different potential assumptions, for how the system works, and if you do if you build a good range of different possible models The hope is that, then you'll capture.

949
02:28:20,790 --> 02:28:27,240
Marisa Eisenberg: The different variations in in what your assumptions might predict about what's going to happen.

950
02:28:27,630 --> 02:28:36,690
Marisa Eisenberg: And so, then you need then some way of being able to compare those different models and see which one seems to be fitting the data best or you know do those kinds of things and so.

951
02:28:36,900 --> 02:28:43,500
Marisa Eisenberg: For that there's a bunch of different approaches to doing that one of the most common that's out there for sort of comparing.

952
02:28:44,610 --> 02:28:56,460
Marisa Eisenberg: different models that are fit to the same data is what's called the aka Ek information criteria or the ic, and so the fdic is basically an approach to let us.

953
02:28:58,320 --> 02:29:16,740
Marisa Eisenberg: sort of compare the idea behind the ic is that the more complicated your model is the more flexible, it probably is, and so the more it can probably fit the data better just by virtue of its complexity right So if I fit.

954
02:29:18,270 --> 02:29:23,700
Marisa Eisenberg: Like let's say this so if I let me draw a little fake example say we have some data.

955
02:29:25,050 --> 02:29:30,630
Marisa Eisenberg: make it a little noise, maybe another one over here, so we have some data like this.

956
02:29:31,650 --> 02:29:37,320
Marisa Eisenberg: I could fit a simple linear model right maybe something like this, plus B, maybe.

957
02:29:38,370 --> 02:29:48,570
Marisa Eisenberg: And I fit a linear model to it so very bad whatever you get the idea for the linear model to it, I can also write a much fancier model, maybe why label.

958
02:29:50,040 --> 02:30:04,650
Marisa Eisenberg: And maybe y is equal to like I don't know P X, to the fifth plus P P five or something you know P for X, to the fourth you know some giant.

959
02:30:05,460 --> 02:30:15,420
Marisa Eisenberg: polynomial thing and then I don't know, maybe i'll add a sign term into X and I don't know what else Oh, you know you can make some enormous complicated model.

960
02:30:15,690 --> 02:30:23,400
Marisa Eisenberg: That maybe fits the data incredibly well, maybe, to the point of overfitting right if you've heard that term before and.

961
02:30:23,700 --> 02:30:33,690
Marisa Eisenberg: But, essentially, the idea is that because I put all this complication into this model, it has a lot more flexibility, it has a lot more different shapes that it can make it can twirl it can.

962
02:30:33,930 --> 02:30:45,570
Marisa Eisenberg: It can go up and down and squiggle it can't whereas this model is way simpler all it can do is be a lot right, and so the more complexity that you put into the model, the more it can wiggle around but.

963
02:30:46,590 --> 02:30:59,700
Marisa Eisenberg: complexity for complexity sake, is just at some point becomes overfit right so at some point, this is not probably real variation that I just did here where when I fit to this model.

964
02:31:00,030 --> 02:31:09,540
Marisa Eisenberg: This is probably actually just noise and the better model would probably really have been to keep the line right and just fit aligned to this data, because.

965
02:31:09,810 --> 02:31:18,960
Marisa Eisenberg: All of this other wobble is actually probably just stochastic it it's just it's just the normal distribution or whatever you know era, distribution, I was using.

966
02:31:19,620 --> 02:31:31,470
Marisa Eisenberg: Adding noise on to my to my until what should probably really just be alive, and so the Ai see basically is trying to balance flexibility of fancier models with.

967
02:31:31,980 --> 02:31:50,820
Marisa Eisenberg: overfitting so it essentially is trying to say what's the simplest model it's it's it's a way of giving ourselves a penalty function that says, I want the best fit, but it has to be the simplest model that gives me the best fit so it's essentially.

968
02:31:52,200 --> 02:32:00,750
Marisa Eisenberg: A way of accounting for both goodness of fit with a penalty for being too complicated, and so what the ice is.

969
02:32:03,390 --> 02:32:12,090
Marisa Eisenberg: So what the ice is it's it's this so it's basically you have a likelihood Tom in your Ai see.

970
02:32:13,110 --> 02:32:21,810
Marisa Eisenberg: And then, a term for the number of parameters so basically this this term gets smaller, the better your fit is so.

971
02:32:22,260 --> 02:32:36,240
Marisa Eisenberg: If you have a better fit you get a lower a ic and and lower I see is better and and this term gets smaller the simpler your program that simpler your model is so the fewer parameters that has the better this.

972
02:32:36,900 --> 02:32:44,550
Marisa Eisenberg: This term will be so a small this essentially is a balancing of two things that can make your Ai see better smaller.

973
02:32:45,420 --> 02:32:52,560
Marisa Eisenberg: Your you can have a better fit or you can have a simpler model and either of those gives you a better I see, and so the AC is essentially.

974
02:32:52,860 --> 02:33:05,100
Marisa Eisenberg: A goodness of fit with a penalty term for over parameter ization, and so what you can do is you can you can calculate the ic for different models and then compare across different models, using the APP so.

975
02:33:06,510 --> 02:33:15,720
Marisa Eisenberg: You can derive the ice from information theory, it can be thought of as the information last that you get when you're using one model versus another.

976
02:33:16,020 --> 02:33:26,520
Marisa Eisenberg: And the actual value that you get for the ice so it's a large number or a small number or a negative number doesn't really matter all that matters is comparing.

977
02:33:27,180 --> 02:33:33,240
Marisa Eisenberg: The ice of one model versus another and whichever has the smaller a I see is the better it's a better choice.

978
02:33:33,750 --> 02:33:49,560
Marisa Eisenberg: Well, not the better choice, but is at least preferred it from a personality perspective and so yeah so there's a lot of different versions of the ic that do different kinds of things there's a bayesian information criteria there's a lot of different things and.

979
02:33:50,700 --> 02:33:58,050
Marisa Eisenberg: Ai sees get a lot of use in the literature and people often use them when they're doing model comparison and.

980
02:33:58,800 --> 02:34:06,270
Marisa Eisenberg: But they're not magical I think sometimes people kind of fall in love with the SEC, and then they use it for everything all the time and.

981
02:34:06,720 --> 02:34:19,170
Marisa Eisenberg: It just is telling you that this among the models you tried this is the simplest model that it can find that fits the data well that doesn't actually mean it's the right model so here's a simulation where.

982
02:34:20,610 --> 02:34:32,910
Marisa Eisenberg: We actually simulated so these little dots here are the data we actually simulated this data from a model of an epidemic that was spreading across two towns that were interacting with one another.

983
02:34:33,930 --> 02:34:45,600
Marisa Eisenberg: But if you fit the data with so here's town one and here's town to, and if you calculate if you count count the number of cases of disease that you see some across both towns, you get this and.

984
02:34:45,900 --> 02:34:52,560
Marisa Eisenberg: From that came this sort of simulation of this data, but if you fit it with a one town model so.

985
02:34:53,040 --> 02:35:01,530
Marisa Eisenberg: model that only includes one location you actually do a better job from an agency perspective because you get a similarly good.

986
02:35:01,980 --> 02:35:13,860
Marisa Eisenberg: fit to the data but it's a simpler model, so the afc will always pick the simplest model that gives you a decent fit but doesn't mean that's actually the true model so it's a useful tool, but it's not a guarantee that you get the right.

987
02:35:15,600 --> 02:35:15,960
Marisa Eisenberg: Okay.

988
02:35:17,340 --> 02:35:27,720
Marisa Eisenberg: So that was all the stuff that I wanted to do before we got to Beijing stuff other questions about any of that you know, so the last.

989
02:35:32,160 --> 02:35:32,310
thing.

990
02:35:35,250 --> 02:35:43,410
Marisa Eisenberg: it's kind of a trip to the zoo is what I call these kinds of lectures right you're like Oh, the Tigers and the snakes like you know you can kind of be like.

991
02:35:43,980 --> 02:35:54,990
Marisa Eisenberg: A deer over there, you know, like so you get to kind of like wave hello to a lot of different methods, but you don't like get to really kind of dive in So hopefully at least you got to kind of sort of see what everything was was.

992
02:35:55,500 --> 02:36:10,560
Marisa Eisenberg: see if any of it was of interest and then you can kind of use that as a way to jump off and Google and look more deeply Okay, and so the last chunk of the classroom last 40 minutes we're going to talk about bayesian approaches to parameter estimation and and so.

993
02:36:12,060 --> 02:36:21,360
Marisa Eisenberg: Right okay so so this gets at this remember when we talked about maximum likelihood and we talked about how we're not really answering the question that we actually want to answer.

994
02:36:21,690 --> 02:36:30,480
Marisa Eisenberg: And we're kind of cheating and using something we can calculate to answer the question, even though it's not exactly what we really want.

995
02:36:31,350 --> 02:36:41,520
Marisa Eisenberg: So some sort of magic of bayesian estimation, is that we can use bayes theorem to to actually answer the question we want to answer so bayes theorem.

996
02:36:42,210 --> 02:36:51,720
Marisa Eisenberg: You many of you have probably seen bayes theorem before in a more sort of general probability setting, but here i've written it down for parameter estimation problems so.

997
02:36:52,800 --> 02:37:00,840
Marisa Eisenberg: Here, what we have is the probability of our parameters, given our data which, so this is the question we actually wanted right, we wanted to know.

998
02:37:01,020 --> 02:37:11,520
Marisa Eisenberg: what's the probability that these are the correct parameters, given that this is the data that I have right so that thing that we want can be calculated from this.

999
02:37:12,030 --> 02:37:26,100
Marisa Eisenberg: From the probability of my data given my parameters times the overall probability of seeing these parameters divided by the overall probabilities of this data now these two are hard right, this is actually it turns out.

1000
02:37:27,240 --> 02:37:33,360
Marisa Eisenberg: The likelihood function so we'll talk, we can talk about that in a minute these two are a little bit difficult so.

1001
02:37:33,900 --> 02:37:40,080
Marisa Eisenberg: If you just taking this little equation and kind of writing it out, this is the thing we've actually kind of wanted all along.

1002
02:37:40,650 --> 02:37:45,720
Marisa Eisenberg: This is actually our likelihood function, the probability of our data given our parameters.

1003
02:37:45,960 --> 02:37:57,870
Marisa Eisenberg: You know, we were kind of cheating before and we were taking this and saying okay i'm just going to fiddle with my parameters until I find the value of the parameters that maximizes the probability of the data that we actually you know that we saw.

1004
02:37:58,230 --> 02:38:07,680
Marisa Eisenberg: But here, what we can do is we say okay that likelihood function if you times it by some information that you have about what's the overall probability of seeing.

1005
02:38:08,040 --> 02:38:13,650
Marisa Eisenberg: This set of parameter values and divided by this what's the overall probability of seeing this data.

1006
02:38:14,010 --> 02:38:20,010
Marisa Eisenberg: You can actually get the thing you actually want now how do we actually get any of this stuff Okay, so this is our likelihood function.

1007
02:38:20,520 --> 02:38:28,500
Marisa Eisenberg: This is going to be what's called the prior, so this is um you often have some prior information about what.

1008
02:38:29,100 --> 02:38:36,900
Marisa Eisenberg: The distribution of your parameter values might be like it might not be very much information, but you often know, for instance, that your parameters can't be negative.

1009
02:38:37,170 --> 02:38:45,240
Marisa Eisenberg: And there's probably some upper limit that's realistic for the system you're looking at right so like it might be that your parameter your prior distribution is like a big.

1010
02:38:45,750 --> 02:38:48,330
Marisa Eisenberg: You know, uniform distribution or something like that, but.

1011
02:38:48,780 --> 02:38:59,160
Marisa Eisenberg: But you usually have some information and if there have been previous studies that have been running a similar population where you've used this model or done anything like this, then there may actually be.

1012
02:38:59,940 --> 02:39:11,070
Marisa Eisenberg: Distribution information for some of these parameters that's out there right So if you have information like this, you can fill that in as the prior if you don't have any information, you can use.

1013
02:39:11,610 --> 02:39:15,540
Marisa Eisenberg: there's a lot of distributions out there for or you know approaches out there for.

1014
02:39:16,500 --> 02:39:24,720
Marisa Eisenberg: For for uninformed priors people call them so there's a thing, called the jeffries prior that sometimes people use, but a most common thing for this is just to use.

1015
02:39:25,110 --> 02:39:38,160
Marisa Eisenberg: A big infinite uniform, so this is a weird distribution, you have to kind of it takes some mathematical fiddling to think about this, but basically assume you know nothing about where the parameters live and you just you know.

1016
02:39:38,670 --> 02:39:46,950
Marisa Eisenberg: Every parameter value is equally probable essentially and so you could do that so whatever information you have about your parameter distribution goes here.

1017
02:39:47,550 --> 02:40:02,130
Marisa Eisenberg: This thing is a normalizing constant, so this is actually not a function of your parameters at all, and you, you integrate overall a parameter space and say what's the overall probability across all the parameter space of observing this data.

1018
02:40:03,630 --> 02:40:08,310
Marisa Eisenberg: So this is a constant it doesn't change as you're trying to fit your parameters, but.

1019
02:40:08,850 --> 02:40:16,260
Marisa Eisenberg: it's a really annoying constant because you have to take a very high dimensional integral to calculate this thing, and so the reason that.

1020
02:40:16,740 --> 02:40:28,650
Marisa Eisenberg: Like maximum likelihood kind of got started in some sense it's not that people didn't realize that bayes theorem was the thing and they didn't realize that they could calculate the thing they could answer the question they actually wanted to answer.

1021
02:40:29,040 --> 02:40:41,340
Marisa Eisenberg: With base there and it's that for a long time until we came up with Markov chain Monte Carlo this was to mathematically difficult to calculate so you could never really do this, and so.

1022
02:40:42,030 --> 02:40:50,100
Marisa Eisenberg: there's a thing called national law posterior story that was used for a long time and there's different ways that people kind of dealt with conjugate priors and different things.

1023
02:40:50,340 --> 02:40:59,670
Marisa Eisenberg: But essentially this constant on the in the denominator and is is the troublemaker that makes this whole approach very difficult in certain situations and.

1024
02:41:00,330 --> 02:41:08,520
Marisa Eisenberg: it's kind of an interesting constant this actually represents what's the probability of your of this model generating the data.

1025
02:41:09,090 --> 02:41:17,310
Marisa Eisenberg: Overall, across all of parameters face right and so that's kind of an interesting questions when we're thinking about like model comparison and Ai sees and things like that, but.

1026
02:41:17,580 --> 02:41:24,150
Marisa Eisenberg: Anyhow suffice to say for our purposes, this is just a very annoying constant that's really difficult because it has a very high dimensional integral.

1027
02:41:24,870 --> 02:41:32,310
Marisa Eisenberg: Okay, so we're going to talk about how we deal with that in a minute, but the idea is basically for Beijing estimation, is to use bayes theorem.

1028
02:41:32,610 --> 02:41:43,800
Marisa Eisenberg: to switch from our cheat where we were using the likelihood and trying to maximize the likelihood to actually maximizing the probability of our parameters, given our data.

1029
02:41:44,730 --> 02:41:53,910
Marisa Eisenberg: And if you kind of ignore this annoying constant on the bottom since it's just a constant and one thing you can notice is that, if your prior.

1030
02:41:54,240 --> 02:42:00,150
Marisa Eisenberg: Is uninformed, meaning that you you're you're assuming every parameter is equally likely.

1031
02:42:00,420 --> 02:42:10,290
Marisa Eisenberg: Then, this is also just a constant right, you know, for every parameter value, this is also just a constant, which means actually that your likelihood is actually just.

1032
02:42:10,770 --> 02:42:19,980
Marisa Eisenberg: off by a constant from the thing you actually want to estimate so using maximum likelihood is not actually that bad it's that's a very reasonable thing to do.

1033
02:42:20,250 --> 02:42:27,480
Marisa Eisenberg: it's just not normalized correctly, essentially for to be the probability distribution that we actually want to estimate.

1034
02:42:27,780 --> 02:42:36,120
Marisa Eisenberg: And if you have any prior information about your parameters, then you should incorporate it here, but if you're using a totally uninformed flat prior then.

1035
02:42:36,870 --> 02:42:52,620
Marisa Eisenberg: Maximum likelihood is also a totally reasonable thing to do so okay so with that I want to take us back to the cookies for a second because I really like how they do prior is in this book so remember, we were talking about some cookies and candy and some don't.

1036
02:42:53,910 --> 02:43:05,310
Marisa Eisenberg: And you take your data is that you take a bite, and it has no candy and you want to estimate did it come from a candy cookie or a no candy cookie so we're estimating that that see.

1037
02:43:06,300 --> 02:43:17,730
Marisa Eisenberg: The candy cookie versus no candy and remember we said the likelihood was the probability of a no candy bite, given that it was either no candy cookie.

1038
02:43:18,090 --> 02:43:26,370
Marisa Eisenberg: or a no candy bike, given that it was a candy cookies so we calculated the likelihood for a no candy cookie as being one.

1039
02:43:26,730 --> 02:43:30,240
Marisa Eisenberg: And then we calculated the likelihood for a candy cookie is being one third.

1040
02:43:30,510 --> 02:43:46,770
Marisa Eisenberg: And since one is greater than one third, we said it's probably coming the no candy bite or data probably came from a no candy cookies so that would be the maximum likelihood estimate for whether we had a candy cookie or no candy cookie it would say it's probably a no candy.

1041
02:43:47,790 --> 02:44:03,060
Marisa Eisenberg: But what about the prior distribution of cookies So what if we knew that we pulled this cookie from the cookie jar that had 10 cookies in it, and all of the cookies in the cookie jar had candy except for one.

1042
02:44:03,600 --> 02:44:17,130
Marisa Eisenberg: So that changes our calculus now right, because now there's a lot more candy cookies than there are no candy cookies so we get a no candy bite, is it really still true that we probably got that fight, from a no candy cookie.

1043
02:44:19,350 --> 02:44:26,430
Marisa Eisenberg: cookie, so this is where base there i'm helps us so if we knew that there were 10 cookies and they all had candy except for one.

1044
02:44:26,730 --> 02:44:35,850
Marisa Eisenberg: This becomes our prior our prayer is that there are nine candy cookies and one no candy cookie and that's the distribution of cookies in the universe, and so.

1045
02:44:37,170 --> 02:44:53,580
Marisa Eisenberg: Our likelihood tell us tells us that we had a no candy so our data, you know Sorry, I know I wrote this way our data tells us, we have a no candy Bytes so how many of the bites in our jar would be a no candy.

1046
02:44:54,240 --> 02:45:02,400
Marisa Eisenberg: fight so if we take a bite out of each cookie in our whole jar it turns out that you get for no candy bites so.

1047
02:45:02,730 --> 02:45:13,980
Marisa Eisenberg: Three of the no candy bites come from candy cookies and one of the no candy bites comes from a no candy cookie so once you account for the prior basically so.

1048
02:45:14,790 --> 02:45:27,810
Marisa Eisenberg: You turns out that you actually had a three fourths chance of taking a bite from a candy cookie rather than a no candy cookie just because there's so many candy cookies in the candy in the cookie jar right so.

1049
02:45:28,830 --> 02:45:33,720
Marisa Eisenberg: Does that make sense, everybody has a thing so once we account for the prior distribution.

1050
02:45:34,920 --> 02:45:49,470
Marisa Eisenberg: We we it ends up turning out that even though, even though the likelihood would tell us that it would come from a no candy cookie the likelihood times the prior.

1051
02:45:50,400 --> 02:45:59,850
Marisa Eisenberg: meeting there are so if you if you sort of if you actually do the likelihood of times the prior got the calculation down here you end up with a probability of.

1052
02:46:00,720 --> 02:46:08,190
Marisa Eisenberg: Three quarters that that no candy bite came from a candy cookie rather than a no candy cookie just because.

1053
02:46:08,520 --> 02:46:18,930
Marisa Eisenberg: Of the nine candy cookies you would get one third of the time you would get a no candy bite and of the so that's that's these are those are all the candy cookie option so.

1054
02:46:19,710 --> 02:46:22,560
Marisa Eisenberg: want three candy cookies will give you a no candy bites.

1055
02:46:22,800 --> 02:46:30,720
Marisa Eisenberg: And you know so there's three three bites that would come from candy cookies and only one bite that would come from I know candy cookie, and so it ends up turning out.

1056
02:46:30,930 --> 02:46:39,330
Marisa Eisenberg: That the probability of a candy cookie with a no candy bite is three fourths and so it's actually most likely that.

1057
02:46:40,050 --> 02:46:47,910
Marisa Eisenberg: We had a candy cookie here, not a no candy cookie that makes sense, so the prior distribution was this this 10 cookies.

1058
02:46:48,180 --> 02:46:55,920
Marisa Eisenberg: The posterior distribution is these bites that are consistent with the data, so you took bites out of all of the cookies.

1059
02:46:56,280 --> 02:47:09,930
Marisa Eisenberg: And then you said I these four were consistent with the data that we have the no candy bite that's our posterior and it's going to be three quarters probability So you can see now that we're actually calculating rather than calculating.

1060
02:47:11,310 --> 02:47:18,120
Marisa Eisenberg: A likelihood we're actually calculating a probability that we had a candy cookie versus the probability that we had a no candy cookie.

1061
02:47:18,510 --> 02:47:34,170
Marisa Eisenberg: The probability that we have a candy cookies three quarters the probability that we had a no candy cookie is one for us one quarter and so it's a higher probability that we had a candy cookie, and so our vision estimate would be We actually had a cake so that's the idea, the time estimation.

1062
02:47:35,460 --> 02:47:39,030
Marisa Eisenberg: So yes, a lot of fun um okay so.

1063
02:47:40,590 --> 02:47:48,780
Marisa Eisenberg: Does that kind of makes sense as an idea by accounting for the prior information about what we know about the probability distribution of our parameters, namely.

1064
02:47:49,050 --> 02:48:00,810
Marisa Eisenberg: Of candy cookies and no candy cookies in the universe, we get a better estimate of you know what our probability distribution of our parameters, given our data was right does that make sense, everybody is the thing.

1065
02:48:01,230 --> 02:48:07,230
Marisa Eisenberg: That we've generally okay good okay so then let's talk a little bit about this denominator, so this thing.

1066
02:48:07,890 --> 02:48:14,850
Marisa Eisenberg: Is the probability of seeing the data from our model across all the parameter space we integrate over all the parameters based remember that.

1067
02:48:15,180 --> 02:48:22,380
Marisa Eisenberg: Most of the time, your parameters are we've been dealing with situations where you sort of only have one parameter two parameters, but you usually have like.

1068
02:48:22,680 --> 02:48:29,160
Marisa Eisenberg: 10 parameters, you know, so this is a very high dimensional integral it doesn't have a mathematical closed form solution.

1069
02:48:29,550 --> 02:48:37,440
Marisa Eisenberg: And even evaluating it computationally is usually very difficult, so if you have three parameters, so the parameter space is three dimensional.

1070
02:48:37,710 --> 02:48:44,370
Marisa Eisenberg: Then, if you took 1000 grid points in each direction, then in order to integrate over this whole grid.

1071
02:48:44,940 --> 02:48:54,750
Marisa Eisenberg: You need tend to the ninth points that you have to run the model for and sum up over it ends up being really pretty much impossible to sort of numerically calculate this way and.

1072
02:48:55,260 --> 02:49:03,330
Marisa Eisenberg: So people have done a few different things to deal with this one thing that people have done is to just since it's a constant.

1073
02:49:04,350 --> 02:49:13,260
Marisa Eisenberg: If you don't need the distribution of your parameters, so if you if you just want to know what the best fit parameter value is.

1074
02:49:13,590 --> 02:49:17,970
Marisa Eisenberg: You can actually just optimize the numerator of this, because this is just a constant and that's fine.

1075
02:49:18,780 --> 02:49:30,630
Marisa Eisenberg: But if you want the actual distribution of parameters in your in your parameters based on Unfortunately, you know you still sort of need this thing, so if you only need the best fit thing then maximum operas theory is great.

1076
02:49:31,860 --> 02:49:32,220
Marisa Eisenberg: But.

1077
02:49:33,420 --> 02:49:40,140
Marisa Eisenberg: The swell i'm going to actually skip another thing you can do is conjugate priors there's certain kinds of situations where, if you have.

1078
02:49:40,470 --> 02:49:48,510
Marisa Eisenberg: A nice distribution on your prior and a nice distribution on your likelihood that works perfectly with your prior you actually can do the math.

1079
02:49:48,810 --> 02:49:58,620
Marisa Eisenberg: On the denominator term and it all turns out really nice and clean and that's fine, but it has never in practice for me happen so i'm going to just skip it and.

1080
02:49:58,950 --> 02:50:10,320
Marisa Eisenberg: If you don't have that very clean nice mathematical setup or you don't have right and you actually do want the distribution of your parameters across parameter space, what do you do.

1081
02:50:10,950 --> 02:50:21,900
Marisa Eisenberg: So this is where we get to kind of the MC MC piece of this, and this is the MC MC is a tool that isn't just used for Beijing estimation, but.

1082
02:50:22,230 --> 02:50:35,370
Marisa Eisenberg: Once MC MC was invented it made bayesian estimation really workable and that's the engine that really runs most of vision estimation, so if we want to actually get the distribution and we can't calculate this denominator term.

1083
02:50:35,850 --> 02:50:42,420
Marisa Eisenberg: Then, what we have to use this something like MC MC so Markov chain Monte Carlo methods So these are sampling based methods.

1084
02:50:43,350 --> 02:50:53,130
Marisa Eisenberg: that are used to do parameter estimation they're also used for a lot of other things you use them in cryptography you use them in physics for calculating neutron diffusion.

1085
02:50:53,340 --> 02:51:05,430
Marisa Eisenberg: Is MC MC for all kinds of stuff but for our purposes we're going to use it for parameter estimation and and so it's a method really MC MC is just a method for sampling from a distribution.

1086
02:51:05,970 --> 02:51:13,200
Marisa Eisenberg: um let's break down the jargon, a little bit so i'm Markov chain Monte Carlo Markov chain.

1087
02:51:14,130 --> 02:51:18,900
Marisa Eisenberg: chain yesterday didn't you I don't have to describe a Markov chain yeah okay good hey I forgot that this is a.

1088
02:51:19,170 --> 02:51:30,690
Marisa Eisenberg: Good time okay so whatever Markov chain is a kind of discreet mark off process, I was gonna say what Markov means it's memory list of stuff but you saw this yesterday, so we can skip that part so we're going to make a mark off chain.

1089
02:51:31,560 --> 02:51:42,330
Marisa Eisenberg: And and Monte Carlo is a word that means, so this is a class of methods that use randomness to solve deterministic problems so.

1090
02:51:43,740 --> 02:51:49,590
Marisa Eisenberg: In this case, the deterministic problem is that we want to approximate this.

1091
02:51:49,980 --> 02:52:02,190
Marisa Eisenberg: integral this you know or really we want to approximate this distribution and we can't solve this integral and so we're going to use random exploration random sampling of our parameter space as a way to do this.

1092
02:52:03,240 --> 02:52:16,830
Marisa Eisenberg: Okay, so let's talk a little bit about the main idea, so the main idea for Markov chain Monte Carlo is to make a Markov chain that will converge, to the distribution we're trying to sample for so.

1093
02:52:17,850 --> 02:52:27,900
Marisa Eisenberg: The Markov chain is going to have some transient dynamics, which are called burnin and then it'll reach an equilibrium distribution, which is the one we're trying to approximate so imagine it this way, if we have.

1094
02:52:28,980 --> 02:52:30,780
Marisa Eisenberg: let's say some sort of.

1095
02:52:31,950 --> 02:52:39,390
Marisa Eisenberg: Whatever I have annotate let's see so suppose we have parameter one and parameter to and suppose that.

1096
02:52:40,200 --> 02:52:50,550
Marisa Eisenberg: If we could do the calculation of that giant integral and do all the stuff the actual distribution of parameter one parameter to, that is, you know the distribution of parameters.

1097
02:52:50,910 --> 02:52:57,630
Marisa Eisenberg: that matches the data, so if you fit the parameters to the data and you wanted to know what's the distribution of parameters that I get from doing that.

1098
02:52:58,590 --> 02:53:07,920
Marisa Eisenberg: looks something like whatever like this let's say so i'm going to make a little heat map here so pretend it looks something like that Okay, so you get some distribution of parameters and then.

1099
02:53:08,130 --> 02:53:15,120
Marisa Eisenberg: You could, if you wanted to take up you know if you had this distribution of parameters, you could say okay here this you know the the.

1100
02:53:15,840 --> 02:53:25,020
Marisa Eisenberg: most probable parameters are in this region, and then you know the next most probable parameters are over in this region.

1101
02:53:25,710 --> 02:53:37,980
Marisa Eisenberg: You know this is like these parameters for the data Okay, and these parameters for the data kind of badly, but you know they sort of fit decently you know, whatever something you have a distribution of how well your parameters, the data.

1102
02:53:40,200 --> 02:53:49,020
Marisa Eisenberg: We we can't calculate this because the integral is too hard, but but suppose it looks like this if you could calculate it Okay, so what MC MC is going to do.

1103
02:53:49,500 --> 02:53:59,940
Marisa Eisenberg: Is it's going to make a little random walk so it's going to like make a little point that's going to randomly wander around the parameter space.

1104
02:54:00,330 --> 02:54:14,550
Marisa Eisenberg: And it's going to make a little so like essentially it's going to sample randomly from the parameter space, but it's going to sample in such a way so it's going to the simpler we're going to use this some kind of Markov chain, so the Markov chain.

1105
02:54:15,030 --> 02:54:31,020
Marisa Eisenberg: Is not the model, the underlying model, the markup change here is a part of our sampling method, so the the there's going to be assembling methods it's going to sample the space where it samples more from here sort of medium from here.

1106
02:54:32,400 --> 02:54:39,420
Marisa Eisenberg: and rarely from out here, so that at the end of this random sampling process finishing.

1107
02:54:39,870 --> 02:54:48,390
Marisa Eisenberg: It spits out a histogram or a or a set of samples that come from this distribution does that make sense, so that's the idea so we're going to make.

1108
02:54:48,780 --> 02:55:04,680
Marisa Eisenberg: A little random sampler I often call it's usually like a little random walk that essentially walks around the parameter space, but it does it in such a way that it will sample from the highest person portions of the distribution most and then the next highest and the next to the next.

1109
02:55:05,700 --> 02:55:09,990
Marisa Eisenberg: Okay, so that's the idea so let's talk through how that works so.

1110
02:55:10,380 --> 02:55:19,650
Marisa Eisenberg: There are a lot of MC MC methods like I said, are based on random walks there's there's you set up this random walk so that it spends more time in higher probability regions.

1111
02:55:20,040 --> 02:55:36,510
Marisa Eisenberg: And then, it turns out that, in order to spend more time in higher probability regions you don't need the actual distribution you just need something proportional to the distribution, so that you can tell yourself what fraction of the time to spend in different regions and so.

1112
02:55:37,920 --> 02:55:49,260
Marisa Eisenberg: We don't actually need to calculate that denominator, we can just use the numerator and use that to drive how it works so let's take a look so here's an example, I spent a lot of time drawing that thing and I already made one I forgot sorry.

1113
02:55:49,770 --> 02:56:03,330
Marisa Eisenberg: Okay, so so let's take an example, so here's um here's two parameters was a likely let's say this is the likelihood times the prior is plotted here, so the the posterior has a shape that sort of like this basically so.

1114
02:56:04,380 --> 02:56:17,400
Marisa Eisenberg: The the you've got a higher kind of peek over here, and then a lower peak here but there's kind of two bucks right so parameter one parameter to this is the region that fits the data, the best and then the fifth day to kind of gets worse as you move out okay.

1115
02:56:18,060 --> 02:56:26,310
Marisa Eisenberg: So I don't know why I started it way over here, out of where the plot was that was also silly but it's fine, so what happens with MC MC.

1116
02:56:26,730 --> 02:56:35,160
Marisa Eisenberg: is basically you give it an initial much like when we did the gradient optimization or any of these other optimizer you give it an initial set of parameters values.

1117
02:56:35,520 --> 02:56:43,410
Marisa Eisenberg: And then it's going to wander randomly around parameter space until it sort of finds where the distribution is.

1118
02:56:43,890 --> 02:56:54,840
Marisa Eisenberg: And then it'll start wandering but it'll be biased and it's random wanderings so that it's it tends towards the places that are higher probability.

1119
02:56:55,440 --> 02:57:03,120
Marisa Eisenberg: And that bias is calibrated just right, so that it will actually end up wandering more from the highest regions of probability.

1120
02:57:03,420 --> 02:57:14,340
Marisa Eisenberg: and less from the regions that are less probable and so as you let it kind of wander around it explores the distribution and so you can see, this kind of random walk is kind of walked all over this distribution.

1121
02:57:15,000 --> 02:57:26,310
Marisa Eisenberg: If you plot all the points that it wandered across here's what you get so this is actually a sample from this this distribution that we started with.

1122
02:57:26,760 --> 02:57:36,570
Marisa Eisenberg: And you can use it to get marginals of your different parameter values, you can use this to calculate confidence regions and things like that, so now you just have a distribution, for your parameters, you want to calculate.

1123
02:57:36,810 --> 02:57:43,050
Marisa Eisenberg: The 95% competence region well it's the 90 if you just take quintiles right so that's it's it's as simple as that so.

1124
02:57:43,800 --> 02:57:57,060
Marisa Eisenberg: So you can do a lot of nice things this way, and the idea is basically to walk through parameters based spending more time in places that are higher probability so that you capture the shape of your overall distribution okay.

1125
02:57:57,720 --> 02:58:05,910
Marisa Eisenberg: I have a whole thing, where I talk about exactly how we do that with the metropolis algorithm and all these things i'm going to skip it.

1126
02:58:06,660 --> 02:58:17,310
Marisa Eisenberg: And I recommend just flip through the slides if you want to kind of see how it works and I posted on the website also some links to some of my other classes, where we have some examples, and you can kind of.

1127
02:58:17,730 --> 02:58:25,020
Marisa Eisenberg: Actually code one of these up, but the short version is basically the the the this random wall.

1128
02:58:26,220 --> 02:58:32,730
Marisa Eisenberg: compares the value of the likelihood times the prior at its current point versus its next point.

1129
02:58:33,090 --> 02:58:38,820
Marisa Eisenberg: And it tends to go to better if the new likelihood and prior is better it will tend to go there.

1130
02:58:39,090 --> 02:58:53,640
Marisa Eisenberg: If the new likelihood times prior is worse, it will have a certain probability of accepting it so it will wander across the space with a tendency towards the better fitting regions, but it is allowed to wander across all of the space as well, so essentially.

1131
02:58:54,960 --> 02:58:57,600
Marisa Eisenberg: The the devils in the details here but.

1132
02:58:58,740 --> 02:59:12,390
Marisa Eisenberg: We can talk a little bit more about that, after if people want to i'm going to skip ahead, because I know we only have 15 minutes left if we have time at the end we'll come back and we can do metropolis and talk about exactly why this works and.

1133
02:59:13,530 --> 02:59:23,340
Marisa Eisenberg: Okay, all right, one of the things that you have to do when you set up this random Walker is, you have to set up what's called a proposal distribution, which is.

1134
02:59:23,910 --> 02:59:38,190
Marisa Eisenberg: How does the random Walker if it's here let's say how does it decide where to step next where to what next point to sample and so, for this, you have to choose something called a proposal distribution.

1135
02:59:38,730 --> 02:59:52,320
Marisa Eisenberg: And, and the proposal distribution is usually something simple so like a just a normal so if there's too many distributions in this lecture but anyway, but it's just a normal distribution on where you are so if i'm.

1136
02:59:52,860 --> 03:00:01,770
Marisa Eisenberg: In parameter space me annotate again so say i've got parameter one parameter to and let's say the distribution looks like this.

1137
03:00:02,880 --> 03:00:10,170
Marisa Eisenberg: Whatever that's a terrible distribution, but you get the idea and let's say my random Walker is here.

1138
03:00:11,370 --> 03:00:21,180
Marisa Eisenberg: And so I want to choose what district what point to choose next the most common thing to do is just a normal distribution centered on your current point.

1139
03:00:21,510 --> 03:00:27,450
Marisa Eisenberg: So it would just be you you so viewing that in two dimensions, it would be something like this, so like.

1140
03:00:27,780 --> 03:00:43,830
Marisa Eisenberg: You are most likely to pick something quite close to where you are and then decaying likelihood of picking something further away, and so i'm your proposal distribution tells you sort of how you jump from the current sample point to the next sample point.

1141
03:00:45,000 --> 03:00:48,300
Marisa Eisenberg: And the choice of your proposal distribution.

1142
03:00:48,660 --> 03:01:03,630
Marisa Eisenberg: Is tricky sometimes so a normal distribution is very common, but how do you decide the meaning of that normal distribution is your current point, but how do you decide the standard deviation for that normal distribution and it turns out that that can really affect how well your.

1143
03:01:04,770 --> 03:01:14,040
Marisa Eisenberg: Your random walk your MC MC behaves so here's a little example not even gonna worry about the details of how this was set up, but.

1144
03:01:15,150 --> 03:01:34,500
Marisa Eisenberg: OK, so these these are called fuzzy caterpillar plots people do this, all the time, the axes are a little weird to me the the y axis here is the parameter value the X axis is which iteration in the random walk you're on, so the here we start with.

1145
03:01:36,330 --> 03:01:44,010
Marisa Eisenberg: A parameter value of five for this new parameter, and then the random walk selects this next, then this and this and then.

1146
03:01:44,550 --> 03:02:00,330
Marisa Eisenberg: The the X axis is just showing what it what the order was of the samples that you took essentially So these are all the sample values that you took for this parameter mew and then this is the order in which they happened how, when they got sampled bite by the random walk so.

1147
03:02:01,620 --> 03:02:09,630
Marisa Eisenberg: If you choose so here, just so people use this a lot for diagnosing problems with convergence of bayesian estimation so.

1148
03:02:10,470 --> 03:02:22,830
Marisa Eisenberg: Here, what we did we started with a value of five, this is what's called the burden, so this is a transient initial wander as it's trying to figure out where the distribution actually lives, so this is the equivalent of.

1149
03:02:24,000 --> 03:02:29,340
Marisa Eisenberg: This where it was like wandering around for a while and then the distribution is here Okay, I get it out.

1150
03:02:30,150 --> 03:02:41,070
Marisa Eisenberg: And so that's what happened here, we started over here, and then we wandered to where we figured out where the distribution, actually, so the bulk of the distribution actually is, and then it's samples.

1151
03:02:42,120 --> 03:02:54,510
Marisa Eisenberg: a bunch of different values as it kind of wanders around it's bias towards where the distribution is highest is higher and so it tends to sort of stay mostly where the distribution list, and this is a good.

1152
03:02:56,280 --> 03:03:06,090
Marisa Eisenberg: caterpillar plot, the reason I call him fuzzy caterpillar plots is that they're supposed to kind of look like a fuzzy caterpillar so it's supposed to look sort of like uncorrelated but basically sort of.

1153
03:03:06,660 --> 03:03:17,610
Marisa Eisenberg: Like a like if you squash this down at all to look sort of roughly like a normal distribution so it's got kind of a it's basically sort of living on the distribution it's got a pretty nice kind of fuzzy caterpillar shape.

1154
03:03:18,090 --> 03:03:29,550
Marisa Eisenberg: here's what happens if you change the proposal with and you pick something that's that's not a good fit for the problem you're working so if you make your proposal with too small, so this means that.

1155
03:03:30,420 --> 03:03:45,120
Marisa Eisenberg: When your random walk walks it can only take really teeny tiny steps so it can only The next point is very constrained to be very close to the current point right, then it can't explore the distribution very well, because it can only ever go.

1156
03:03:46,320 --> 03:03:51,420
Marisa Eisenberg: tiny little steps so it takes it a long time for it to actually find where the distribution book is.

1157
03:03:51,720 --> 03:04:02,760
Marisa Eisenberg: And all of the samples that you get are very correlated so here they look a lot more noisy and kind of like white noise here, and you can see they're very correlated so this would be too small, of a proposal would.

1158
03:04:03,060 --> 03:04:14,010
Marisa Eisenberg: I call this the goldilocks problem, because you know it's like a you know goldilocks doesn't like the first porridge and then she doesn't she got to find the one that's just in the middle, so this is too small, of a proposal with and.

1159
03:04:14,760 --> 03:04:28,050
Marisa Eisenberg: If you make the proposal with too big so here, I make the standard deviation for our proposal distribution to large what happens is it finds that the location, the right region of space for the distribution is just fine.

1160
03:04:29,070 --> 03:04:43,110
Marisa Eisenberg: But then, unfortunately, because the proposal distribution is too large what's happening, I can go back to this picture is that it gets here and then it's drawing so let's say i'm a random Walker whoops.

1161
03:04:45,900 --> 03:04:55,800
Marisa Eisenberg: let's say my my random walk is here, instead of drawing things that are kind of in the neighborhood of this, the next sample that I draws probably over here.

1162
03:04:56,340 --> 03:05:11,130
Marisa Eisenberg: So it's a very bad sample and the probability of accepting a sample that is worse is proportional to how good or bad, it is, and so, if it's a very bad simple then most of the time, what happens is that.

1163
03:05:12,990 --> 03:05:13,650
Go back.

1164
03:05:14,910 --> 03:05:26,670
Marisa Eisenberg: Our our random Walker will say that new sample that you do is actually so bad that i'm not going to accept it and i'm not going to go over because the bias towards higher.

1165
03:05:27,120 --> 03:05:37,950
Marisa Eisenberg: probability locations will mean that it mostly just doesn't accept the new samples, and so what you see here is that it reaches the main region of the distribution and then it just kind of sits.

1166
03:05:38,400 --> 03:05:46,590
Marisa Eisenberg: And stays on one value for a really long time and then it'll jump to a new value and be stuck for a long time, and it jumps and then it's stuff for a long time so.

1167
03:05:46,890 --> 03:05:53,550
Marisa Eisenberg: It doesn't explore the distribution very well because most of the new samples that it's drawing are just really bad samples because.

1168
03:05:53,850 --> 03:05:59,190
Marisa Eisenberg: The proposal distribution is too wide and it's wider than the actual distribution that you're trying to sample from.

1169
03:05:59,490 --> 03:06:04,530
Marisa Eisenberg: And so it's mostly sampling things that are just out in the middle of nowhere, and so it just doesn't really work so.

1170
03:06:04,800 --> 03:06:07,650
Marisa Eisenberg: You have to think about your proposal woods when you're doing these kinds of things.

1171
03:06:07,890 --> 03:06:21,330
Marisa Eisenberg: A lot of methods out there, if you use a package for doing bayesian estimation, many of them are adaptive and they will figure out the right proposal with for you, but I just wanted to kind of like raise that as a thing that you should be keeping an eye on so okay.

1172
03:06:22,680 --> 03:06:36,480
Marisa Eisenberg: What tends to happen is you get something sort of like this, so you have some information on your prior your likelihood is this red guy and then you get a sample posterior that's capturing the information from your prior and your likelihood.

1173
03:06:37,530 --> 03:06:47,040
Marisa Eisenberg: And so that gives you a distribution of your parameter space there's a bunch of different methods out there for how you assess burden and so there's a lot of these.

1174
03:06:47,580 --> 03:06:55,200
Marisa Eisenberg: gelman rubin criteria and different kinds of things like this, and I do want to kind of give one.

1175
03:06:56,190 --> 03:07:04,530
Marisa Eisenberg: sort of heads up a lot of times when people do convergence they'll look at these fuzzy caterpillar plots and if they see something like this they'll say okay.

1176
03:07:04,980 --> 03:07:14,070
Marisa Eisenberg: it's converged we finished the burnin everything looks fine this looks good we're good to go great i'm done, I want to point this is from a friend of mine Ralph Smith.

1177
03:07:14,640 --> 03:07:21,330
Marisa Eisenberg: who wrote a great book on uncertainty quantification actually, and that is that I totally recommend everyone to read, he works.

1178
03:07:22,260 --> 03:07:33,660
Marisa Eisenberg: Just to like make us all nervous about our lives, he works on nuclear engineering and and so, and so this is a real world example from something that he encountered with a graduate student so.

1179
03:07:34,410 --> 03:07:41,580
Marisa Eisenberg: I usually do this more dramatically and I kind of go like oh this hadn't happened yet, so he ran.

1180
03:07:42,330 --> 03:07:56,460
Marisa Eisenberg: He in his graduate students worked on a problem they They ran the MC MC they got to hear and they're like okay great this looks like a fuzzy caterpillar this looks fine I think we're good with our parameter estimates this looks all right and.

1181
03:07:57,450 --> 03:08:05,880
Marisa Eisenberg: Then the Grad student was running this on a cluster forgot to stop the code, so it kept running over the weekend and left when he.

1182
03:08:06,000 --> 03:08:09,570
Marisa Eisenberg: did his own thing whatever came back after the weekend and this had happened.

1183
03:08:09,990 --> 03:08:18,270
Marisa Eisenberg: It turned out that they've been stuck in a local minimum they've been sampling from a piece one piece of the distribution, but the main bulk of the.

1184
03:08:19,770 --> 03:08:22,530
Marisa Eisenberg: show the random sample or had actually not found yet.

1185
03:08:23,010 --> 03:08:32,790
Marisa Eisenberg: And so, then over the weekend, it found this and it totally shifted you know what the estimate of the parameter value would have been so it's important to be careful when you're assessing convergence this way.

1186
03:08:33,030 --> 03:08:46,590
Marisa Eisenberg: You don't really want to just run a single run of mcc you should start many different Markov chains running from different locations and parameter space, so that you make sure that you kind of explore the parameter space as much as you can.

1187
03:08:47,850 --> 03:08:58,140
Marisa Eisenberg: Okay there's a bunch of different methods out there sort of leave that leader i'm a preservation computation i'm going to actually also.

1188
03:08:59,490 --> 03:09:01,080
Marisa Eisenberg: I wonder, well, I don't know.

1189
03:09:02,310 --> 03:09:07,440
Marisa Eisenberg: We can talk about this, but are there questions about any of the stuff we've talked about so far you feel like it's been a lot.

1190
03:09:07,470 --> 03:09:08,460
Marisa Eisenberg: yeah yeah.

1191
03:09:08,490 --> 03:09:16,320
Rishi Chanderraj: yeah so for for me so metropolis metropolis i've written, you know the.

1192
03:09:18,270 --> 03:09:23,310
Rishi Chanderraj: You don't know the I guess i'm just like you know what that heat map looks like.

1193
03:09:25,020 --> 03:09:34,500
Rishi Chanderraj: But you don't know the function that generated it so you can't determine though some that you can't determine that denominator, so you.

1194
03:09:34,530 --> 03:09:35,040
know.

1195
03:09:36,240 --> 03:09:40,980
Rishi Chanderraj: So you don't know what like the all of this parameters face like sums to.

1196
03:09:41,340 --> 03:09:42,720
Marisa Eisenberg: Exactly oh yeah.

1197
03:09:42,750 --> 03:09:48,870
Rishi Chanderraj: But you know, but you do know what you know that looks like this, but you don't know this, some.

1198
03:09:49,110 --> 03:09:54,420
Marisa Eisenberg: Exactly it's like essentially you know the shape but, but the height could change right.

1199
03:09:54,450 --> 03:10:08,580
Rishi Chanderraj: got it okay so so so it's not like, so you okay um, I guess, I just have a hard time of like conceptualizing why I don't know why you know that shape and not.

1200
03:10:09,360 --> 03:10:15,720
Marisa Eisenberg: yeah like, how is it possible that you know the shape, but you don't know the height and And why is that a problem.

1201
03:10:15,750 --> 03:10:17,220
Marisa Eisenberg: So, like i'm.

1202
03:10:17,490 --> 03:10:23,280
Marisa Eisenberg: The reason so like the the shape, but not the height thing I mean mathematically it just comes.

1203
03:10:23,280 --> 03:10:25,110
Marisa Eisenberg: From not knowing the denominator right but.

1204
03:10:26,550 --> 03:10:34,770
Marisa Eisenberg: I think more fundamentally, it feels like if you can make a heat map like this that should be all the information you need so like, why do we need to do this whole thing right.

1205
03:10:35,040 --> 03:10:35,280
Rishi Chanderraj: and

1206
03:10:35,700 --> 03:10:38,190
Marisa Eisenberg: The issue is that if I told you, I want.

1207
03:10:39,330 --> 03:10:51,720
Marisa Eisenberg: The I want 95% of the mass of this you don't know how far out to go because, so I don't know what fraction of the overall mass this yellow part makes up.

1208
03:10:51,960 --> 03:10:53,130
Marisa Eisenberg: Because I don't know.

1209
03:10:53,730 --> 03:11:03,390
Marisa Eisenberg: Like how much I know it's it's higher here, but I don't really know actually how much of the mass is there versus is you know what I mean like.

1210
03:11:03,960 --> 03:11:08,400
Rishi Chanderraj: Okay, so this is like a tool to quantify the precision.

1211
03:11:10,020 --> 03:11:17,610
Rishi Chanderraj: you'd like so like okay I got it like maximum like, if you want to maximize ways, you want to get the best estimate of the parameter.

1212
03:11:18,330 --> 03:11:34,410
Rishi Chanderraj: Like you don't necessarily even need to go through all this trouble, but if you want to quantify how like how precise or like you know your uncertainty around that parameter, and then you would sample in this way.

1213
03:11:34,710 --> 03:11:45,840
Marisa Eisenberg: Exactly exactly exactly, and this is why oftentimes people end up doing like they'll they'll do something like you know, a simpler optimization method to find the best.

1214
03:11:45,840 --> 03:11:46,920
Marisa Eisenberg: fit first.

1215
03:11:47,370 --> 03:11:57,000
Marisa Eisenberg: And then it it just so you might like run some simple gradient descent optimization kinds of things and find okay there's a maximum here there's a maximum here.

1216
03:11:57,150 --> 03:12:02,790
Marisa Eisenberg: And then you run in cmc just from here, so that you don't have some of these convergence issues or any of these kinds of things.

1217
03:12:03,000 --> 03:12:13,170
Marisa Eisenberg: You just sort of say okay now i'm just going to run that cmc I know roughly where the right parts of the distribution are, but I want to get my uncertainty, so you run them cmc from there, and you can.

1218
03:12:13,230 --> 03:12:14,640
Rishi Chanderraj: configure all that yeah okay.

1219
03:12:15,900 --> 03:12:16,590
Rishi Chanderraj: yeah so.

1220
03:12:16,650 --> 03:12:16,950
Rishi Chanderraj: yeah.

1221
03:12:17,010 --> 03:12:21,150
Marisa Eisenberg: But that's that's exactly it is counterintuitive you see this heat map and you're like why.

1222
03:12:21,630 --> 03:12:34,500
Marisa Eisenberg: I have it obviously like it's literally right there so like Why did I need to do this, but you don't know how much of the distribution is in this chunk you know you know that it's more but not how much more you know so yeah.

1223
03:12:36,240 --> 03:12:37,140
yeah okay cool.

1224
03:12:39,150 --> 03:12:49,080
Marisa Eisenberg: yeah well it's 1138 this was a very dense lecture I think i'm going to skip a preservation computation rather than trying to cram that into two minutes but um.

1225
03:12:49,410 --> 03:12:57,510
Marisa Eisenberg: So let's pause there and Michael is going to pick it up in the afternoon and he's going to do, I think some game theory and economic models.

1226
03:12:58,350 --> 03:13:01,290
Marisa Eisenberg: And then tomorrow we'll be back with.

1227
03:13:01,830 --> 03:13:08,760
Marisa Eisenberg: agent based models and we'll do some network models will also talk a little bit about more general sampling methods so.

1228
03:13:08,970 --> 03:13:19,530
Marisa Eisenberg: If you want to, not just for parameter estimation, but just for exploring the behaviors of your model, what are some other sampling approaches that you can use to explore the parameters face this came up.

1229
03:13:19,830 --> 03:13:32,880
Marisa Eisenberg: In on Monday, when we were sort of talking about how do you explore, you know how do you choose your parameters so we'll talk about that, tomorrow, so yeah have a good lunch everybody and yeah see you all tomorrow, Michael will be back to the afternoon.

1230
03:13:35,160 --> 03:13:35,430
Thank.

