1
00:00:05,150 --> 00:00:08,660
Morning, guys. We should get started.

2
00:00:09,500 --> 00:00:16,990
So just a very brief refresh, remembering what we have been doing as debates over.

3
00:00:17,000 --> 00:00:20,690
You got to remember where we are. So know we are.

4
00:00:20,710 --> 00:00:24,570
We are looking at a simple linear, linear regression.

5
00:00:24,650 --> 00:00:28,860
There are two parameters based on zero in the beta 100 intercept.

6
00:00:29,120 --> 00:00:37,160
No one is the slope. And what we did was what we were doing was to work.

7
00:00:37,640 --> 00:00:43,970
We already showed how to estimate both parameters using a square estimate.

8
00:00:45,080 --> 00:00:52,920
Then a little bit of luck. And we got those. The impressions of the other information in there.

9
00:00:52,940 --> 00:01:03,290
Agree with you. This is what we do, right? I think a majority of the of some of the least squares but are able to find out what needs to be done.

10
00:01:04,100 --> 00:01:08,230
And then you started to show that, you know,

11
00:01:08,240 --> 00:01:14,360
both the estimate versus the are unbiased in a sense that the bearings foundations are responding to that.

12
00:01:14,960 --> 00:01:22,610
The reason that one this is important is because and as illustrated by this one over here,

13
00:01:22,610 --> 00:01:29,180
it's really easy to take samples, even repeat it to calculate the speed based on the deeper datasets.

14
00:01:29,960 --> 00:01:34,340
And then we were going to have a reporter for different datasets where we look have different values.

15
00:01:34,730 --> 00:01:40,370
But if you look at look, there's usually his gram obviously provided you'll see that we estimate the his around

16
00:01:40,370 --> 00:01:45,919
centered around it is true that she on average he will get back the full value.

17
00:01:45,920 --> 00:01:53,810
So that's not what used to mean that the exaggerations are equal to that corresponding truth.

18
00:01:54,680 --> 00:02:02,220
And on the other hand, one another thing we want to we want to know is what are the variation is in the disclosure.

19
00:02:02,250 --> 00:02:08,550
And that's what we were doing at the very end of last night. We were trying to calculate the zero had done better.

20
00:02:08,610 --> 00:02:12,890
So beta one that and I think we finished this one but no one had.

21
00:02:13,190 --> 00:02:18,620
But now we will start this larger by calculating the variance of beta zero and.

22
00:02:24,090 --> 00:02:29,620
Yeah. This is where we stopped. So for a little calculated, the variance of being a zero.

23
00:02:29,640 --> 00:02:38,170
And now again the calculation is some algebra by using Harvard's function.

24
00:02:39,160 --> 00:02:46,030
So there is have been a zero, had been a zero. We already calculated it is equal to the surface of this.

25
00:02:46,030 --> 00:02:54,729
But without the virus, this is not the sum of the full range of variables or difference,

26
00:02:54,730 --> 00:03:00,040
but the difference can be considered as a softness of the virus or the sample preliminaries.

27
00:03:00,430 --> 00:03:06,009
And then by using the crocodile, the parent advantage, the virus of the first guy,

28
00:03:06,010 --> 00:03:14,370
plus the variance of the size of the guy, the minus five minus two times the the covariance between which in this.

29
00:03:16,630 --> 00:03:23,140
So that's how we got here, right? Well, the clearance notice that I hear about this as far as I'm concerned.

30
00:03:23,140 --> 00:03:29,230
So that's why we have the protocol, this clearance and similar forbearance.

31
00:03:29,770 --> 00:03:41,500
Now there is a material guide. So as far as we have, as far as clear with this, all these are just the properties of the items function.

32
00:03:43,510 --> 00:03:48,159
And then on that slide, we are going to show that of this virus here.

33
00:03:48,160 --> 00:03:56,649
This is going to be zero. This is in zero. So so then what is left is if we look at this guy here,

34
00:03:56,650 --> 00:04:05,680
the variance of y bar that is Sigma Square over in that Cablevision went by the symbol

35
00:04:05,680 --> 00:04:11,200
average of why we are in the symbol academy is just a seamless square over here.

36
00:04:11,890 --> 00:04:16,030
And this variance here are the same terms.

37
00:04:16,120 --> 00:04:24,790
I mean, we have already calculated the variance of one half that is simply looks like this and that is the result.

38
00:04:25,090 --> 00:04:32,410
But now we need to show that this covariance is indeed equal to what is left.

39
00:04:35,620 --> 00:04:39,579
Now the computer is equal to zero. This is also easy to show.

40
00:04:39,580 --> 00:04:47,530
Again, whites algebra plus the properties of the covariance functions like covariance for beta one.

41
00:04:48,310 --> 00:05:00,190
We have already category beta one is equal to this. If we just plug beta one patch into this expression and then this here this s as x.

42
00:05:00,190 --> 00:05:09,309
This does not depend on Y only the amount of x, right? So it's a we assume at this stage so we can move it out of the programing function.

43
00:05:09,310 --> 00:05:17,190
So this is a constant to us. So that's why here we have this one over S and X, right?

44
00:05:18,070 --> 00:05:26,200
And then what is left is actually the covariance between what I and the the numerator of the second term.

45
00:05:27,440 --> 00:05:33,140
Now notice that a heater for listening to her. We have a song we have a song from one over in.

46
00:05:33,860 --> 00:05:44,560
And then if you again apply the property of the covariance function, it's actually the sum of the same term.

47
00:05:44,570 --> 00:05:51,020
This is a constant. So that is the constant and we move out of the coherence and then the covariance between

48
00:05:51,290 --> 00:05:59,820
y and each sorry y bar and each one I hear that is the covariance between y bars.

49
00:05:59,870 --> 00:06:03,080
And in my mind we have the sum.

50
00:06:06,170 --> 00:06:11,300
So all these calculations, they they really, you know,

51
00:06:11,350 --> 00:06:19,000
one needs some familiarity with the properties of the various covariance function in order to carry out this obvious calculation.

52
00:06:20,710 --> 00:06:26,680
Okay. So once we got here, now notice the Y bar is simply the symbol average of.

53
00:06:26,980 --> 00:06:34,830
Now we use a different index. We use J just to avoid, you know, being the same as Y.

54
00:06:34,870 --> 00:06:41,480
So we use a different index, Jane. So this is not well here.

55
00:06:41,490 --> 00:06:53,170
We simply replace this wine bar by the symbol average of an event because we assume that the error terms they are uncorrelated.

56
00:06:53,190 --> 00:06:55,350
So for individuals they are uncorrelated.

57
00:06:55,830 --> 00:07:05,010
Not only is the comparison between y and a y j for I'm not able to j for de for eyes and js all these Colbert as they are equal to zero.

58
00:07:05,820 --> 00:07:17,100
So then if we look at this coherence between the sum of YJ and Y and then the comparison between each y,

59
00:07:17,100 --> 00:07:23,520
g and y i, as long as J is not able to, I will never see zero.

60
00:07:23,730 --> 00:07:30,780
So it disappears. So what it's left then is just the covariance between this one over and times.

61
00:07:30,780 --> 00:07:35,850
Y I know this y because all the other ways, if you calculate the covariance,

62
00:07:35,880 --> 00:07:40,260
all the other libraries, medical variances would that would have one high.

63
00:07:40,300 --> 00:07:48,570
They're all equal to zero. So only this covariance between y and this is why I left.

64
00:07:48,570 --> 00:07:58,340
By then there is still one internal. And this cobras here, this cobras, this is equal to sea as well.

65
00:07:58,350 --> 00:08:02,100
And this is easy to see because this one over and he's a constant.

66
00:08:02,470 --> 00:08:08,430
Now we can first move it out of the coherence and then it becomes one of our end times covariance.

67
00:08:09,240 --> 00:08:13,530
Why and why? And covariance between one and one i.

68
00:08:13,560 --> 00:08:16,620
And that's simply the variance of why I see my square.

69
00:08:17,170 --> 00:08:23,710
So because it's. You see my square. So that's obviously muscular.

70
00:08:26,590 --> 00:08:33,450
And then while this is a seamless score over end, we can actually know.

71
00:08:36,760 --> 00:08:40,990
Okay. So since this is seamless, we're over end. This is constant.

72
00:08:41,380 --> 00:08:44,940
So we can move it out of the songs. Right, this.

73
00:08:45,120 --> 00:08:51,610
And then here we have sequence of over end times divided about as at times the song.

74
00:08:52,000 --> 00:08:58,090
But this song is equal to zero just because it's the sum of exercise times.

75
00:08:58,090 --> 00:09:02,140
The column minus the sum of x bar is equal to zero.

76
00:09:02,650 --> 00:09:07,870
So in the end, now we show that this whole thing is equal to zero.

77
00:09:10,750 --> 00:09:21,410
And this this is shows that in the end the variance of why zero pattern is equal to this thing.

78
00:09:26,700 --> 00:09:47,079
Okay. Any questions about this company? So then you can see that what we have done is we have calculated the viruses of both.

79
00:09:47,080 --> 00:09:56,280
Why had a zero and why had a beta zero, had a beta one, had four goals.

80
00:09:56,350 --> 00:10:01,660
And so we have this unknown quality of a square. So assume a square is our number.

81
00:10:01,750 --> 00:10:08,739
We assume that the error absolutely follows some decision with mean zero and assume a square very soon.

82
00:10:08,740 --> 00:10:14,970
A square, assume a square is unknown. So we have to estimated using our data and this is how we are estimating.

83
00:10:15,490 --> 00:10:19,030
This seem like we you seem to have square two asking me to see my square.

84
00:10:19,390 --> 00:10:25,360
That is the sum square of the residuals or sum of square of the areas.

85
00:10:25,630 --> 00:10:35,110
So that's called SC the sum of square of errors or someone square other regions divided by this edge over two and minus two.

86
00:10:35,980 --> 00:10:41,770
And I will explain briefly, very briefly why we've divided by and over to all.

87
00:10:42,310 --> 00:10:53,230
But this is how we estimate see my square. And this s major, this Sigma Square hat we call age mse.

88
00:10:53,230 --> 00:11:04,790
The mean square error is just of. It means wherever the MCU is.

89
00:11:05,210 --> 00:11:14,600
So in somebody near Russia we use that means we're Edward to estimate for first assume I had a square estimate on Sigma Square

90
00:11:15,080 --> 00:11:25,440
and the absolute I here in New Zealand some absolute I had here this is just the observed Y minus the estimating of each.

91
00:11:27,580 --> 00:11:32,480
This is the estimate of the decision. Okay.

92
00:11:33,290 --> 00:11:42,319
And the CMA Square, it's it reflects the square distance between the White and White House.

93
00:11:42,320 --> 00:11:44,600
Right. Because that's how it's always calculated.

94
00:11:44,600 --> 00:11:55,549
And so it's every sort of average square distance between the white and predicted white or estimated Y, of course, larger boundaries of CMA.

95
00:11:55,550 --> 00:12:02,090
How to square it. It means that there is a large amount of variation of the white are around this line.

96
00:12:02,270 --> 00:12:14,660
So here we call that this this is this is the treated lot this is the beta zero had a speed of one half because as I say, this is this is the line.

97
00:12:14,990 --> 00:12:22,370
And and these dots are the true observed white eyes.

98
00:12:22,760 --> 00:12:30,380
And so the larger seem to have square is the more spread the data points around this line.

99
00:12:30,800 --> 00:12:38,770
So so CMA Square had a quantifies right estimates of variation of all these dots around this like.

100
00:12:43,390 --> 00:12:50,590
So that's how how we estimate this unknown quantity Sigma Square for the band for the two variances.

101
00:12:54,160 --> 00:12:55,870
And here. Well, then.

102
00:12:56,170 --> 00:13:04,390
Well, then there is a question why we have this A-minus, too, in the denominator, because recall that when we estimate assemble veterans,

103
00:13:04,900 --> 00:13:15,510
if we simply have a sample, random sample, and we estimate of the sample various reuse use, we define less than the sum of square.

104
00:13:15,520 --> 00:13:20,620
This is what I minus y bar squared divided by in minus one.

105
00:13:21,580 --> 00:13:27,590
But here for a simple linear regression. Now for simple linear regression, we have two parameter beta.

106
00:13:27,760 --> 00:13:34,150
Is there a better one? We actually take the difference sum of squares of this difference what I and what I had.

107
00:13:34,480 --> 00:13:42,490
But we divide it by in minus two. So this difference, this this seems to be very subtle, right?

108
00:13:42,520 --> 00:13:46,329
So one is divided by M on this one was divided minus two.

109
00:13:46,330 --> 00:13:50,140
So there is a difference. But differences are subtle. So why do we have such a difference?

110
00:13:51,270 --> 00:13:58,690
Now, one explanation is actually from the perspective of so-called degree of freedom.

111
00:13:59,270 --> 00:14:03,610
This is one explanation.

112
00:14:04,090 --> 00:14:13,419
So this degree of freedom thing, this is I mean, especially at this stage, it may seem a little bit abstract or a little bit strange.

113
00:14:13,420 --> 00:14:21,639
So it's okay that we do not have a full understanding of this because as we proceed and also,

114
00:14:21,640 --> 00:14:29,650
I think signal to say so one we talk more about the dispute is you've got a better idea of what we mean by the word freedom.

115
00:14:29,920 --> 00:14:34,240
But here, let me just explain this intuitively and in an intuitive way.

116
00:14:35,440 --> 00:14:43,960
So if we look at our sample our sample process of data from in individuals, we have subjects in Vietnam.

117
00:14:44,800 --> 00:14:50,460
So if we look at this in individuals, I mean this in numbers, you can very freely.

118
00:14:50,470 --> 00:14:54,970
So when we combine data from individual, there is no restriction on the data from this individual.

119
00:14:55,360 --> 00:14:58,570
So so we have a voice that I can very freely.

120
00:14:58,660 --> 00:15:03,520
So in the in this whole sample that you are freedom is actually equal to the

121
00:15:03,520 --> 00:15:08,190
number of observations in this dataset because we have in the points that again,

122
00:15:08,680 --> 00:15:12,030
there is no restriction and there are three platform.

123
00:15:12,190 --> 00:15:21,060
One, we estimate of the variance. We have to use all of these data points to estimate a wide bar to estimate.

124
00:15:22,210 --> 00:15:32,140
And that's that's what happened here. So when we estimate the sample bearers three while with unknown while with unknown mean.

125
00:15:32,140 --> 00:15:43,660
Right. So here we are concerned. I know me. So one meal is I know we have to use all the data points to count, assemble average to estimate the mean.

126
00:15:44,890 --> 00:15:49,810
And that means we already use our data points to estimate unknown quantity.

127
00:15:50,830 --> 00:15:59,200
So we lose what they were afraid of. So that's why when we estimate the virus here, we divide it by minus one.

128
00:16:02,050 --> 00:16:12,520
Not for me, for symbol linear regression. Now, for a simple linear regression, we have two quantities to estimate a beta zero and beta one.

129
00:16:12,520 --> 00:16:16,780
So we use all our and data points to estimate this to harvest.

130
00:16:17,380 --> 00:16:23,560
So that means we lose to do our freedom. So that's why in the denominator of.

131
00:16:25,540 --> 00:16:33,130
This varies well estimated for the cinema square for a simple linear regression

132
00:16:34,060 --> 00:16:38,590
of securities dividing the by amount as to because we use our in order to

133
00:16:38,590 --> 00:16:50,530
use our data to estimate both beta zero and beta left so we lose two d roughly and a similar to later when we talk about a multiple linear regression,

134
00:16:50,650 --> 00:16:58,690
when you have more balance, when you have more X so that you have more balance, and this number here will also change.

135
00:16:59,050 --> 00:17:05,590
This is out of Armando's too. Generally speaking, you have at minus PS number of betas in our model.

136
00:17:06,220 --> 00:17:12,550
So this is one explanation from the high degree freedom perspective.

137
00:17:12,880 --> 00:17:18,940
That's one way to look at it. Another way to look at this is more of a mathematical way.

138
00:17:19,900 --> 00:17:26,940
So we can be sure that. By dividing by in minus two.

139
00:17:27,660 --> 00:17:33,090
Then that incarnation of See my had square is equal to Sigma Square so that if we have a

140
00:17:33,090 --> 00:17:38,640
I'm biased as measure of seamless quick so this is more of a mathematical perspective.

141
00:17:39,150 --> 00:17:48,030
So if you want if you simply divide it by another factor like let's say by, in either by end or minus one,

142
00:17:48,330 --> 00:17:52,710
then you would have captured then the expansion of this would not be equal to this.

143
00:17:53,640 --> 00:17:59,040
However, if you divide this by minus two by taking the newer freedom into account,

144
00:17:59,100 --> 00:18:06,389
you will see that now we are able to show that the that seem to have squared exactly equal to a single C1 squared.

145
00:18:06,390 --> 00:18:13,370
So which means that well even for the various we have an unbiased estimate and this is unbiased matrix.

146
00:18:13,920 --> 00:18:24,000
So this is not like another explanation, more mathematical explanation why here we have at minus two and similar to here,

147
00:18:24,120 --> 00:18:35,790
by dividing this by minus one, we are able to show that this exaggeration of this estimate is equal to the variance of one.

148
00:18:36,000 --> 00:18:39,300
So. So we have unbiased as a measure.

149
00:18:42,560 --> 00:18:46,630
So this is sort of another interpretation, Ashleigh.

150
00:18:46,760 --> 00:18:50,809
I like the second one better. I mean, because it's quite a symbolic one.

151
00:18:50,810 --> 00:18:56,290
It doesn't involve so-called freedom, but the freedom is a very important concept.

152
00:18:56,360 --> 00:19:01,280
I mean, it's not only important for Europe, but in many, many areas.

153
00:19:01,280 --> 00:19:11,860
It's very important concept. But I think in this particular case, I think it's easier to understand there's a difference here by by knowing that.

154
00:19:11,870 --> 00:19:17,720
Well, by doing by having this difference now we have I'm biased as major we are.

155
00:19:19,720 --> 00:19:23,950
I take it with you roughly on account of.

156
00:19:25,700 --> 00:19:36,270
On any questions. Okay.

157
00:19:36,660 --> 00:19:43,800
And then we have an example. Now we are going to go over this example, not in details, but very quickly, because this is our example.

158
00:19:44,100 --> 00:19:53,819
I think you can I have already uploaded a bit of that under our or B folder so you can if you guys want and you can play with this.

159
00:19:53,820 --> 00:19:58,380
I do this out a little bit and if I, you know, by running this is our code.

160
00:19:58,780 --> 00:20:03,120
But look here, we just do focus on some important things for this example.

161
00:20:03,120 --> 00:20:11,130
So this is a study to examine the relationship between a house age and the amount of child sleeps per night.

162
00:20:11,970 --> 00:20:18,690
So age here is the x variable covariate and the amount of the amount of time a child sleeps.

163
00:20:19,590 --> 00:20:27,750
This is why this is. Yeah, we want to see whether this X and y, they have a strong linear association.

164
00:20:29,040 --> 00:20:33,660
So here's a sample. Size is very small, right? So we have data point from 13 children.

165
00:20:36,820 --> 00:20:38,320
And the why. Oh, I'm sorry.

166
00:20:38,680 --> 00:20:47,140
What is actually, though, in this regard is the why is the average number of minutes slipped overnight over one long, not over one week.

167
00:20:47,240 --> 00:20:52,450
And that's that's the way the data that you can see what this is.

168
00:20:52,660 --> 00:20:58,300
This is one of the data that looks like here. So we have a bunch of rules.

169
00:20:58,810 --> 00:21:00,820
You know, we have the idea for this children.

170
00:21:01,060 --> 00:21:12,400
So here it just shows the first six children that we have, the one or this is our one average 2 minutes of sleep per night.

171
00:21:12,790 --> 00:21:29,170
And also we have the age and that's X. And we want to see whether this X or Y actually has A's, has a Y and X has a strong linear association.

172
00:21:29,710 --> 00:21:39,130
Now, this fuel lies the R code, as in calculating the sum summary statistics for for for X and for Y.

173
00:21:40,230 --> 00:21:43,140
And so in total, we have, you know, 13 children.

174
00:21:43,590 --> 00:21:53,880
And this this article calculated the mean for for X and Y, the center of division for X and Y and the minimum battle for X and Y offers.

175
00:21:54,150 --> 00:21:59,800
You can get other statistics if you like, to fight this.

176
00:21:59,830 --> 00:22:02,890
This helps while these are descriptive statistics.

177
00:22:02,910 --> 00:22:09,480
Right. Just help us to get my idea of what the data looked like all September.

178
00:22:09,820 --> 00:22:16,710
All of this is based on conclusion are as you can see if you want to check, you can calculate these numbers manually.

179
00:22:16,860 --> 00:22:21,890
For example, you can you can apply to the formula calculating several values.

180
00:22:22,170 --> 00:22:26,340
You can you can see that while the central division agrees with that.

181
00:22:32,930 --> 00:22:42,370
For example, here I mean, here, this this number right here, this is the standard deviation right there is exactly what it's equal to.

182
00:22:42,380 --> 00:22:46,100
If you want to calculate this manually, it's as with equal to the.

183
00:22:50,800 --> 00:22:54,160
X minus X bar square.

184
00:22:54,760 --> 00:22:58,810
So this is a sample variance of of of X.

185
00:22:59,260 --> 00:23:02,560
And then you take that square root, you get to the center deviation.

186
00:23:03,490 --> 00:23:09,040
So if you want to do it through this matter where you can, you can check that it is indeed equal to this guy.

187
00:23:09,910 --> 00:23:19,000
So this is the standard deviation of X. And then there's just a more.

188
00:23:22,870 --> 00:23:27,460
Statistics. So this is a plot of what, an X?

189
00:23:28,570 --> 00:23:29,799
So we can see that indeed,

190
00:23:29,800 --> 00:23:38,200
if there is a very strong once we see such a plot in a European practice and we rarely see such a strong linear association,

191
00:23:38,200 --> 00:23:42,640
but in this case, we do see a very strong new trend, a very clear trend.

192
00:23:45,430 --> 00:23:48,300
So if we and then we can fit to the model.

193
00:23:48,310 --> 00:23:57,190
So this article has only fixed the linear regression model, and these are the output from linear regression model.

194
00:23:59,950 --> 00:24:04,929
If we look out of the output, we can see that we have here.

195
00:24:04,930 --> 00:24:09,340
This is the residuals calculated as on the minimal residual.

196
00:24:09,520 --> 00:24:12,610
The first part, how the median, the third part out and the maximum.

197
00:24:12,850 --> 00:24:17,410
So in total, we have 13 residuals because we have 13 children's and I have 13 residuals.

198
00:24:17,740 --> 00:24:23,350
Now this this is a table summarizing that the minimum maximum quarter the

199
00:24:23,350 --> 00:24:29,410
quarters mean and these are the results from the freedom and regression model.

200
00:24:29,710 --> 00:24:32,740
So this column here, these are the beta.

201
00:24:32,750 --> 00:24:44,319
So this one is beta zero and this one is beta one at the intercept end of the slope estimated values and we have the estimate of

202
00:24:44,320 --> 00:24:51,040
the center outers and relative output and the p value we have about one of the key values are what are the P values are later.

203
00:24:51,160 --> 00:24:58,750
We haven't talked about this yet but but for now, I mean here the these two values these are beta zero hadn't been one hatching.

204
00:25:03,100 --> 00:25:15,000
Okay. And then. And then we are able after we finish this line, we are able to calculate the Y hand.

205
00:25:15,130 --> 00:25:20,010
Right. This is what I had. This is the estimated Y weeks.

206
00:25:20,280 --> 00:25:24,120
So we observe Y for each subject.

207
00:25:24,570 --> 00:25:27,960
But now, after feeding this line, we are able to calculate.

208
00:25:32,420 --> 00:25:38,990
The estimated. What are they planning for Baylor? How to pick up the house into your rationale?

209
00:25:39,290 --> 00:25:46,610
We can see that they are fairly close. You can look at the numbers for each children and see that they are very close.

210
00:25:48,200 --> 00:26:00,620
Now, if we make a plot of the predictive value and so here each circle represents one the data point from one one child.

211
00:26:01,190 --> 00:26:06,200
And this red line, this is the fishing line. Red line is this red line here.

212
00:26:06,410 --> 00:26:09,920
This is a zero and possibly one that points X.

213
00:26:12,950 --> 00:26:19,070
And then each you know, each dodge on this line, these are the corresponding estimated value.

214
00:26:22,330 --> 00:26:33,730
So for example, let's say let's say let's say for this particular for this particular child, this is the why the actual observed response.

215
00:26:34,030 --> 00:26:41,890
And this start over here on this line, this is what I had. This is the estimated value for this particular child.

216
00:26:47,920 --> 00:27:03,550
And then. Okay, so these middle calculations here, I think we will skip this because we want to know what you want to see.

217
00:27:04,600 --> 00:27:11,230
But these here, these are just by doing some hand calculation.

218
00:27:11,710 --> 00:27:20,980
So recall that especially while recall, for example, let's take a look at this one small recall that we've calculated for you to one hat.

219
00:27:21,400 --> 00:27:27,790
So our formula tells us beta one hat is equal to this guy.

220
00:27:28,430 --> 00:27:31,840
That's that's what we do, right? This guy.

221
00:27:32,260 --> 00:27:36,460
Now, that means that if you're you're are able to calculate S as X, Y,

222
00:27:36,820 --> 00:27:44,410
and also if you calculate as S X just manually and you take the region these two numbers, you should get 3 to 1 hat.

223
00:27:45,070 --> 00:27:50,410
And these well, here this is just to check that was indeed the case.

224
00:27:50,680 --> 00:27:56,440
So if you calculate both this and this and then calculate the ratio,

225
00:27:56,740 --> 00:28:05,649
you will see that while it is equal to just number and then you will see that it will be the one hat from the linear regression model.

226
00:28:05,650 --> 00:28:10,420
After you use R to say to the model, it gives you exactly the same number.

227
00:28:10,870 --> 00:28:18,939
So this is just to double check that indeed. No, the results from our output there totally agree with what we what we do.

228
00:28:18,940 --> 00:28:27,400
Right. If you calculate this better by followers are for you because we're just trying to get better from our universe.

229
00:28:28,000 --> 00:28:35,110
You will get the same results. And these are just, you know, checking for some some of the parameters.

230
00:28:35,110 --> 00:28:38,919
So if you're interested, you can check check those.

231
00:28:38,920 --> 00:28:43,989
But really, the R is based on the formula.

232
00:28:43,990 --> 00:28:48,340
We calculated it. So they have to agree it can be different.

233
00:28:49,420 --> 00:28:58,240
But now let's actually take a look at the interpretation of of these true estimates.

234
00:28:58,250 --> 00:29:06,550
So the inverse and the slope. So when we interpret the results, there are certain things we would be able to pay attention to.

235
00:29:06,850 --> 00:29:10,060
So one is what are the human is?

236
00:29:10,480 --> 00:29:15,810
Because the interpretation involves for one other change. In X we have this chip, this much machine wire.

237
00:29:15,850 --> 00:29:20,350
So we have to pay attention to the during the max and also we have to head into the direction

238
00:29:20,680 --> 00:29:28,150
whether Y increases as x increases Y equals is as y increases and also by how much.

239
00:29:29,260 --> 00:29:35,830
And also, we need to pay attention that these are estimates because because our model fears

240
00:29:35,950 --> 00:29:40,839
are based on estimating the violence and the increase in white or decreasing Y.

241
00:29:40,840 --> 00:29:45,460
That's all average. So we need to pay attention to all these things.

242
00:29:45,700 --> 00:29:50,770
So for example, now for the intersection, the this is an estimated intersection.

243
00:29:51,250 --> 00:30:05,079
What this means is that the estimated average or the mean screen time for someone who is a zero and that's what age when X is equal to zero.

244
00:30:05,080 --> 00:30:16,600
So for someone who is age zero on average the estimating the sleep time that's this much and this this this 90 minutes per night.

245
00:30:18,400 --> 00:30:22,750
So that's the interpretation of beta zero and for beta one.

246
00:30:23,620 --> 00:30:32,349
So it's actually the estimated mean difference in sleep time for one year.

247
00:30:32,350 --> 00:30:38,110
So for one unit increasing next now the X is measured in years or units years.

248
00:30:38,110 --> 00:30:45,580
So for every one year increase in X in H, now here we see that this is a 19 number.

249
00:30:45,590 --> 00:30:49,180
So the estimated mean difference is negative number.

250
00:30:49,750 --> 00:30:56,680
So in other words, not for one year increase in age for one year increase in age on average.

251
00:30:57,920 --> 00:31:11,930
There will be mothers on average that the children will sleep these minutes 40 minutes less per night with one year increase in energy.

252
00:31:13,010 --> 00:31:16,160
So that's the interpretation.

253
00:31:16,430 --> 00:31:26,209
So of course, now this interpretation, now this we have to interpret, interpret this result, these numbers in terms of the sun and the sun.

254
00:31:26,210 --> 00:31:36,080
But question we are looking. So that in itself, you can communicate with the investigator from other areas what exactly what this number means.

255
00:31:41,660 --> 00:31:51,710
Okay. Any questions about this example? I'm here with we to focus on the interpretation of some of your results.

256
00:31:51,740 --> 00:31:57,640
We skipped some had a calculation of that on the prompter about.

257
00:31:58,310 --> 00:32:04,280
But if you're interested, you can check those yourself just to get those just to make sure that little to to

258
00:32:04,790 --> 00:32:10,580
to confirm that indeed the results from using are in the results you had recently.

259
00:32:16,190 --> 00:32:24,100
Any questions before we. Okay.

260
00:32:24,700 --> 00:32:34,390
So yeah, this slides they are some of the slides they were made by if I was a student here now she's a supervisor at the University of Iowa.

261
00:32:35,140 --> 00:32:40,000
Okay. So if there is no question that we are going to.

262
00:32:47,620 --> 00:32:58,510
Look at Model C and I have absolutely Model C on campus.

263
00:33:20,780 --> 00:33:30,530
Okay. So for more, see, we're going to take a look at the how to make inverse, how to how to construct scenarios.

264
00:33:30,530 --> 00:33:36,570
How would you have all those has to be those type of thing but here in the first few slides there.

265
00:33:36,980 --> 00:33:41,210
Just refresh your memory, summarizing what we have done so far.

266
00:33:41,960 --> 00:33:53,720
So what we have done so far is that now based on a real ensemble, we have individuals, we have individuals.

267
00:33:54,290 --> 00:33:58,040
And while this is our data and it based on this data,

268
00:33:58,040 --> 00:34:04,430
we actually assume the lower how we assume Y and acts they have some such such linear association linear relationship.

269
00:34:04,760 --> 00:34:12,079
Now we want to fit in this model and to estimate beta zero and beta one and indeed we derived one zero ahead and

270
00:34:12,080 --> 00:34:20,090
then at one time R and by the second the least squared estimation method minimizing the sum of square residual.

271
00:34:20,090 --> 00:34:33,230
We are sample squared errors. We actually estimated both balance on end and this B has as they are they are real new variables

272
00:34:33,470 --> 00:34:40,150
because I mean this this Y is random because both parties had to involve one Y agility.

273
00:34:40,160 --> 00:34:45,170
So run the network. So so both beta had a beta one zero had been a one.

274
00:34:45,440 --> 00:34:49,220
They are run variables. But after we had a beta,

275
00:34:49,640 --> 00:34:55,219
if I give you a beta that if you plug in those concrete values into this two

276
00:34:55,220 --> 00:35:00,110
followers now of course you will get numerical values for both beta zero and one.

277
00:35:02,350 --> 00:35:12,040
Okay. All. And this actually this feels like the first few slides that are exactly the same as the slides we used from last lecture.

278
00:35:12,370 --> 00:35:17,460
So here I'll be I'll be quick. And then we actually show that, you know,

279
00:35:17,470 --> 00:35:27,130
the installation of the tool parameter and two parameters that are equal to the corresponding truth and also we calculated or their variances.

280
00:35:27,700 --> 00:35:32,350
These are these results are important to make inference.

281
00:35:33,880 --> 00:35:43,960
They're going to see in this module of now the van, we talk about how to estimate this sigma squared hat.

282
00:35:44,150 --> 00:35:47,350
We use the mean square error to estimate see must.

283
00:35:48,190 --> 00:35:55,960
And once we have that we can replace because both also the variance involve this Sigma Square.

284
00:35:56,200 --> 00:36:00,910
So after we have an estimate for sigma squared, we can replace sigma squared by.

285
00:36:02,180 --> 00:36:08,970
In this way every time I see him. Hard square. That's how we estimate the two variances.

286
00:36:12,540 --> 00:36:18,290
Look, yeah, all these two slides for these are older slides just together.

287
00:36:18,290 --> 00:36:26,779
Refresh your memory. Now, one thing here we want to pay attention to is that so far we haven't found these results.

288
00:36:26,780 --> 00:36:30,710
So far, the results, they were derived without assuming independence.

289
00:36:31,220 --> 00:36:35,220
Now, if we simply assume the error terms, they are uncorrelated.

290
00:36:35,240 --> 00:36:44,060
If you look at all the calculations, we only use the fact that of y and y Jake, there are uncorrelated so that a covariance is equal to zero.

291
00:36:44,510 --> 00:36:50,750
And also we never use the normality assumption. So this is actually for estimation.

292
00:36:50,930 --> 00:36:56,630
So for because so far we only focus on estimation of beta zero and beta one.

293
00:36:56,810 --> 00:37:03,590
So for estimation of beta zero and beta one, well, it turns out that we do not need to choose long of our assumptions.

294
00:37:03,930 --> 00:37:07,339
We do not need this assumption work normally assumption.

295
00:37:07,340 --> 00:37:19,790
We seem to assume that the error terms are uncorrelated and that that's that's good enough to estimate all the parameters.

296
00:37:20,180 --> 00:37:21,660
But really, this I mean.

297
00:37:21,740 --> 00:37:31,280
Well, a, I don't think this this is a really a big deal because for linear regression, I mean, we will never stop just at estimation.

298
00:37:31,280 --> 00:37:36,620
We will we will need to move on to inference at a was we talk about inverse we

299
00:37:36,620 --> 00:37:40,070
will need to make a normality assumption and also independence assumption.

300
00:37:40,460 --> 00:37:45,830
So really this is really is not that we should know.

301
00:37:45,830 --> 00:37:57,650
But but but it is not it's not like. Very, very crucial thing.

302
00:37:57,950 --> 00:37:59,630
So. So again.

303
00:38:00,200 --> 00:38:11,419
Because if if people ask if people are asked on what assumptions are behind linear regression models and that assumptions are the line assumption,

304
00:38:11,420 --> 00:38:21,800
right? L i and we talking about like linearity and independence and normality and equal boundaries for assumptions.

305
00:38:23,960 --> 00:38:34,820
Now for these two s majors, the beta one hat and beta zero hat, it turns out that we can write it both as linear as linear combination of one.

306
00:38:35,360 --> 00:38:42,750
So for example, for y one hat, if you look at ay1 hat, y one hat is equal to this.

307
00:38:42,770 --> 00:38:50,740
That's what we derived. And the s s, x, y, and that's actually equal to this guy.

308
00:38:50,860 --> 00:38:57,410
The gist just this is how just how what a x x x s as x y is.

309
00:38:59,630 --> 00:39:15,840
Now, if we look at this sum here, if we con y this notion that if we give it a name, we call it a W, one Y, and that's that's the double one.

310
00:39:16,700 --> 00:39:20,570
And then we realize that the beta y and beta one hat.

311
00:39:20,650 --> 00:39:33,660
That's that's just a linear combination of the actual Y as we observe a combination and the the division these w one

312
00:39:34,960 --> 00:39:45,660
and a similarly similar if we're y if we're beta zero had been a zero and it's actually derived it's equal to that.

313
00:39:47,930 --> 00:39:56,329
And the Y bar of course is the sample average and beta one had we have we have just showed that it's

314
00:39:56,330 --> 00:40:04,580
actually a linear combination of the Y that's that's equal to this linear combination Y an event.

315
00:40:04,610 --> 00:40:09,920
Here we have a linear combination of Y, the first guy. This is a linear combination of Y and a second ago.

316
00:40:09,950 --> 00:40:12,050
This is also a linear combination of y.

317
00:40:12,260 --> 00:40:18,530
All time is constant because it doesn't matter because this whole thing can still be considered as a condition of y.

318
00:40:19,190 --> 00:40:22,400
And then we have the difference between those two.

319
00:40:22,580 --> 00:40:26,900
We have the we have the difference between this and this.

320
00:40:26,990 --> 00:40:30,370
We have a difference. So the first term is a linear validation.

321
00:40:30,380 --> 00:40:31,850
So in terms of the combination.

322
00:40:32,030 --> 00:40:43,790
So the difference on still gives us a numerical measure, Y and the combination that's what we've called this guy, number zero zero.

323
00:40:45,590 --> 00:40:50,780
And so it shows that the beta zero hat is also a linear combination of Y.

324
00:40:52,970 --> 00:40:57,950
Now, the importance of these of of these facts, both facts.

325
00:40:58,730 --> 00:41:03,490
The important thing is that we have assumed y following our decision.

326
00:41:03,870 --> 00:41:10,940
So I mean here on this slide, it does say but but if you recall the fundamental assumption behind linear regression,

327
00:41:11,330 --> 00:41:22,250
we assume y followed normal distribution because each of the normal distribution and the both betas are linear combination of this one i's.

328
00:41:23,030 --> 00:41:27,770
So both beta heads should follow linear function to follow normal distribution.

329
00:41:28,100 --> 00:41:33,800
And that's just a property of all of this. So if you have a bunch of variables followed, normal distribution,

330
00:41:34,250 --> 00:41:39,200
that leaner and meaner combination of the other variables will still follows normal solution.

331
00:41:39,860 --> 00:41:46,820
So based on this facts, actually, we know that the both the betas will have normal distribution.

332
00:41:47,330 --> 00:41:53,680
And this is a very important thing because later we're going to need to use this fact through maybe numbers.

333
00:42:00,330 --> 00:42:03,390
Yeah. This actually. Then summarize. What do we.

334
00:42:06,640 --> 00:42:10,600
What I just described. So because we assumed.

335
00:42:11,050 --> 00:42:25,330
Absolutely. Now consider this assumption epsilon followed normal the security then because y is equal to beta zero plus beta one X plus epsilon.

336
00:42:27,670 --> 00:42:35,500
The why, of course, followed normal distribution. Would this mean this mean for here and with the same numbers Sigma Square.

337
00:42:37,950 --> 00:42:46,910
So this is under the line assumption. The color line function, you know, stands for linearity and that's simply assuming magic.

338
00:42:46,950 --> 00:42:56,010
Note Here we have a leaner model. That's what else. And of course, the spy stands for independence.

339
00:42:56,870 --> 00:43:01,650
This ls l stands for linearity.

340
00:43:02,370 --> 00:43:11,300
And I stand for independence that is given by different g given individuals.

341
00:43:11,310 --> 00:43:15,260
They are independent to the absolute so absolute.

342
00:43:15,630 --> 00:43:20,350
That means this means absolute I and absolute j.

343
00:43:20,430 --> 00:43:25,120
We are independent when I is not in knowledge.

344
00:43:25,710 --> 00:43:32,580
So this is for independence and this in stands for normality assumption.

345
00:43:32,790 --> 00:43:44,340
We assume normal the absolute following normal distribution and this e stands for equal variance and that means this see my square here?

346
00:43:44,910 --> 00:43:48,200
It does not vary across for different individuals.

347
00:43:48,540 --> 00:43:56,310
It stands in the same constant sigma strength. So these are the four fundamental assumptions behind your rational.

348
00:44:00,770 --> 00:44:13,100
Okay. So under this assumption, the way we follow normal distribution, the Y is a normal distribution, not as we just showed on our slides,

349
00:44:13,640 --> 00:44:25,340
both beta had and beta one have and beta zero that are linear combinations of y i's not because each y follows a normal distribution,

350
00:44:26,120 --> 00:44:30,650
the linear combination of these y ice much of the following normal distribution.

351
00:44:30,800 --> 00:44:39,730
So that's odd. But we have already derived the expansion of valence and variance of beta.

352
00:44:39,890 --> 00:44:43,470
For example, for beta one that we have already calculated,

353
00:44:43,820 --> 00:44:50,540
the expansion is the expansion is equal to one and we have currently is variance is variance is equal to this line.

354
00:44:51,290 --> 00:45:00,600
And and also now we know that beta one have followed normal distribution and normal distribution is completely determined by its mean and it's values.

355
00:45:00,680 --> 00:45:06,020
Once we know the mean bitterness, normal distribution, we know where it is moving next.

356
00:45:06,500 --> 00:45:13,970
So in other words, now, because we know the mean and awareness of beta one hat and also we know in normal distribution.

357
00:45:14,540 --> 00:45:18,620
So the beta one hat has to follow this particular model.

358
00:45:18,630 --> 00:45:31,040
The solution with this mean and mysterious and a similarity beta zero hat if all normal distribution with this mean and this this.

359
00:45:33,970 --> 00:45:34,540
This matters.

360
00:45:35,200 --> 00:45:46,660
So in other words, what do we have done so far is that we have figured out what did this fusion better one had of photos and what are this fusion?

361
00:45:47,190 --> 00:45:55,900
Peter had zero followers, both for a normal distribution, and we know exactly what the normal distributions are.

362
00:45:55,960 --> 00:46:01,660
We know their names, we know their numbers.

363
00:46:03,190 --> 00:46:06,210
Okay, so now all of.

364
00:46:09,730 --> 00:46:17,940
So if we focus on, you know, one hack because you know what happens here, if an interest beta one had a fun device,

365
00:46:18,460 --> 00:46:27,730
it quantifies the beta one quantifies the effect of X or Y, and that's usually the main increased benefit linear regression on a single linear.

366
00:46:27,730 --> 00:46:35,560
Right. We are interested in the association or the effect of X or Y, so beta, what qualifies that?

367
00:46:36,880 --> 00:46:40,480
So beta one is usually about the parameter of interest.

368
00:46:40,780 --> 00:46:46,809
Now if you look at a beta one because beta one followed this normal distribution beta one back then,

369
00:46:46,810 --> 00:46:57,280
if we subtract beta one had by its mean by beta one divided by the square root of errors, that's the standard deviation.

370
00:46:57,730 --> 00:47:06,750
Then it will follow a standard normal distribution. This is just a normalized version of baseline.

371
00:47:07,180 --> 00:47:14,830
So we follow standard exclusions and our inference is that sort of based on this fact here.

372
00:47:15,640 --> 00:47:24,070
So once we know the spirit and follows, then we can proceed to calculate how many is enrolled to Cowgill,

373
00:47:24,070 --> 00:47:29,140
P-value and to talk, to have all the testing which we will talk about today.

374
00:47:31,040 --> 00:47:38,360
However, there is still one complication. The complication is that we do not know the true value of sigma squared.

375
00:47:39,770 --> 00:47:47,840
Remember our remote only assume we have a low cost of the same square, but we never really know exactly what it is.

376
00:47:48,110 --> 00:47:50,300
So we have to ask Marty to use our data.

377
00:47:50,930 --> 00:48:01,520
And as we mentioned, we ask me to see my square by Sigma Square hat, and that's the main square area we use me square error estimate of Sigma Square.

378
00:48:02,240 --> 00:48:12,740
So was we estimate a single square plaza that we replace this single square by estimating Sigma Square by the new square?

379
00:48:18,260 --> 00:48:28,340
And then you're able to look at this quantity or because of the replacement of C Square by MSI.

380
00:48:28,850 --> 00:48:32,060
Now, this guy does not follow normal diffusion anymore.

381
00:48:32,900 --> 00:48:39,020
This is the one one complication. So it is no longer fallen under the solution for standard movement exclusion.

382
00:48:39,770 --> 00:48:47,260
So in order to proceed to make inference, to construct computation, we had to figure out what exclusions this guy follows.

383
00:48:50,090 --> 00:48:55,190
And that's actually what this fusion we talk about immediately.

384
00:48:55,250 --> 00:48:59,920
That's the so-called XI distribution. But we will look at that.

385
00:48:59,940 --> 00:49:04,999
So so the more morosi the mystery is on this module, and especially for today,

386
00:49:05,000 --> 00:49:11,030
we are going to focus on the testing and testing the slope and intercept as meters.

387
00:49:11,030 --> 00:49:19,040
And also there is script table and some examples of we have all we have first.

388
00:49:26,030 --> 00:49:31,700
So this slide is your summarizes the facts we have so far.

389
00:49:34,700 --> 00:49:40,160
So so far we have calculated, you know, one head and it's equal to this.

390
00:49:40,900 --> 00:49:44,900
And we have calculated the beta zero ahead and it is equal to this.

391
00:49:45,140 --> 00:49:55,640
So we do. Right. Both beta zero have been one. And we have shown that, you know, both are unbiased because variation is equal to the truth.

392
00:49:56,180 --> 00:50:00,140
Both are our allies. And also we have calculated the variance of laws.

393
00:50:03,120 --> 00:50:04,649
And also we have argued the matter.

394
00:50:04,650 --> 00:50:11,760
We have shown that both as we follow normal disputes, because both are in a combination of wide support for the solution.

395
00:50:13,890 --> 00:50:17,880
And then we actually have an escalator for CMA Square.

396
00:50:18,810 --> 00:50:26,650
That's the MLC. Right. That's that's the sum of square areas divided by among us to get him on a stool.

397
00:50:26,670 --> 00:50:33,920
This is because of the freedom taking the through our freedom into account so that we seem to have

398
00:50:33,930 --> 00:50:40,050
square is also a to have some idea of see my square for the parents we also have a modest house and.

399
00:50:42,190 --> 00:50:46,060
On and on and on.

400
00:50:46,690 --> 00:50:52,120
So end of this is actually a fact that we will not show here.

401
00:50:52,210 --> 00:50:58,090
But later we are going to show this model F, but now we're simply using this as a fact.

402
00:50:58,480 --> 00:51:02,470
In order to proceed, we have to take this as a fact.

403
00:51:02,890 --> 00:51:06,760
So this is us in divided by Sigma Square.

404
00:51:06,760 --> 00:51:10,280
It follows a Chi Square distribution and minus two.

405
00:51:10,310 --> 00:51:19,240
You are free. This is we need this fact in order to figure out what is this fusion, this guy.

406
00:51:21,000 --> 00:51:25,120
We have to rely on that fact. But now, for now, let's just take it as a fact.

407
00:51:25,390 --> 00:51:32,840
And we are going to show that later in this course. Okay.

408
00:51:32,860 --> 00:51:36,310
So, yeah, let's take a minute break.

409
00:51:36,370 --> 00:51:46,937
You know, we will give you idea.

410
00:51:52,196 --> 00:52:01,286
So now for now, before we talk about making you work here, if there's one more thing on the slide,

411
00:52:02,426 --> 00:52:12,296
clarify that is, if you look at a virus, we calculated the variance of beta, what hat it's equal for this time.

412
00:52:12,746 --> 00:52:23,696
And when we estimate the various beta one, how do we replace a square meter square that if we look at the expression for libertarians,

413
00:52:23,876 --> 00:52:28,826
we see that as us acts, appears in the denominator.

414
00:52:29,516 --> 00:52:37,076
What this means is that the larger the s x is, the smaller the various.

415
00:52:37,076 --> 00:52:40,676
Is it because it's divided by six x?

416
00:52:41,316 --> 00:52:45,676
But what is as as x x is actually the.

417
00:52:46,336 --> 00:52:55,796
Some of square's. It's this guy, right?

418
00:52:56,126 --> 00:52:59,996
So what does he mean by an asset? By having a smaller asset?

419
00:53:00,416 --> 00:53:03,956
So here, let's consider two cases.

420
00:53:04,976 --> 00:53:09,216
One case is that here is this is actually this is one.

421
00:53:09,896 --> 00:53:16,346
One case is that we observe necessary observe these data points.

422
00:53:16,376 --> 00:53:28,486
These are our data points to observe. Have some more banks versus another case where we observed these data points.

423
00:53:34,586 --> 00:53:37,676
The variation in why they're roughly the same.

424
00:53:38,216 --> 00:53:42,226
But the variation, yes, they are dramatically different right here.

425
00:53:42,266 --> 00:53:50,996
You can see that we have a very wide range of X. So that means if you count calculated this guy, this value is much larger in the first plot.

426
00:53:51,276 --> 00:53:57,146
In the second plot because but because there is much variation in X in the first part.

427
00:53:57,536 --> 00:54:00,986
And this is actually it's sort of a variation, right?

428
00:54:00,986 --> 00:54:06,896
Because if you divide this by one over n minus one, you will get a variance of the central division of X.

429
00:54:07,166 --> 00:54:12,866
So this s as X is is a quantification of the variation in X distribution.

430
00:54:13,676 --> 00:54:19,106
Now, the first plot here, it has a much larger S as X and I interviewed him.

431
00:54:19,106 --> 00:54:24,776
But here we are not going to get it into mathematical details. But intuitively you can look at this is very easy to see that.

432
00:54:27,016 --> 00:54:33,946
These points in pretty much one did pretty much determine a lot.

433
00:54:33,976 --> 00:54:42,496
I think that's right. There's not much variation in the slope of of this of this line because, I mean, all these data point,

434
00:54:42,496 --> 00:54:50,706
they're so spread out once you try to use all these point to determine this y this line and the slope of this line is pretty much determined.

435
00:54:51,066 --> 00:54:55,936
There's not too much variation and not too much uncertainty associated with the slope.

436
00:54:56,656 --> 00:55:00,166
However, here, if you look at the these beautiful lines in the sand of the plot,

437
00:55:00,976 --> 00:55:07,296
whether the lines like this or the lines are like this, the small is less.

438
00:55:07,926 --> 00:55:17,236
It's less clear. Right. So in other words, the sand plot here, the there is much more variation in the slope of this line.

439
00:55:17,746 --> 00:55:26,736
You better one. So then, as indeed is, agrees with this formula here, this is a sort of graphic illustration of this formula.

440
00:55:26,746 --> 00:55:36,676
So the virus in the estimate of being a one in a slope actually depends on this ssx there is an x,

441
00:55:37,156 --> 00:55:43,396
so if the variation x is large like in the first plot, then the virus in beta one hat.

442
00:55:44,176 --> 00:55:49,476
It's got to be small but important. S x is smaller than the others.

443
00:55:49,486 --> 00:55:54,976
It can determine how it's going to be large. So this idea has practical implication.

444
00:55:55,996 --> 00:55:58,996
The implication is that we have data.

445
00:55:59,416 --> 00:56:03,146
You do not want to collect. Just for example, we'll study that.

446
00:56:03,176 --> 00:56:13,946
Every fact you probably want to collect data from like what range of age range from, let's say 25 to 55 or 60.

447
00:56:14,866 --> 00:56:20,266
So you do not want to simply look at people your age between 30 and 35.

448
00:56:20,626 --> 00:56:29,146
So that because that's going to give you a much more about variation in the estimate of what estimates long.

449
00:56:32,496 --> 00:56:42,996
So that's it. What was one small thing before we move on to look at the major topic of this lecture?

450
00:56:44,616 --> 00:56:53,406
So as we mentioned, we now we are trying to figure out the disputing of this guy because we are first of all,

451
00:56:53,706 --> 00:56:56,855
there is sort of a few things I have pointed out about.

452
00:56:56,856 --> 00:56:59,376
I think it's good to emphasize them again.

453
00:56:59,616 --> 00:57:08,026
So first of all, this beta one is our main interest video regression model because it quantifies the effect of X and Y.

454
00:57:08,046 --> 00:57:16,836
So this is typically our main interest. And in order to make inference about this because beta ones are not.

455
00:57:16,866 --> 00:57:19,146
So first we have to ask medium and a second.

456
00:57:19,746 --> 00:57:29,586
Well, we have to really see how how much variation there is in our estimation and to make an inference to do this.

457
00:57:30,186 --> 00:57:35,516
But in order to do all these, we have to know that these years we use beta one, have one.

458
00:57:35,796 --> 00:57:45,935
We have to know the dispersion of this one. So now what we are doing next is try to figure out what the future response for us from here.

459
00:57:45,936 --> 00:57:54,096
A voluntary disclosure policy wide volunteer exposure to recall that she does fusion is related to normal fusion.

460
00:57:54,126 --> 00:57:58,116
And so if we have a central normal and you have a chi square test,

461
00:57:58,126 --> 00:58:08,435
you run the variable and if both are invalid and then we divide instead of normal by the square root of because we're divided by this old,

462
00:58:08,436 --> 00:58:11,546
you got freedom, then we have a solution.

463
00:58:11,736 --> 00:58:17,496
We have to run the tedious fusion. And with tedious fusion it has a lock.

464
00:58:17,546 --> 00:58:24,695
It's pdaf. It's very similar to normal pdaf, but it has heavier tail and ability.

465
00:58:24,696 --> 00:58:30,786
Where freedom is very large, that becomes almost become the norm of this fusion.

466
00:58:32,526 --> 00:58:46,316
Okay, so now let's see why this guy followed Peters Fusion because we already know about no one had to follow normal and then if we allowed does it.

467
00:58:46,326 --> 00:58:55,536
So that means if we subtract beta one had a biased mean divided by its center deviation, then the Z will follow standard normal.

468
00:58:57,826 --> 00:59:05,776
So we already have a standard global distribution. So Z now follow the standard rule and that s square.

469
00:59:06,706 --> 00:59:11,566
And in our last line we've now we will take this as a fact, a later algorithm to prove this.

470
00:59:13,426 --> 00:59:20,686
So if we divide this by SC divided by single square in a follow Chi Square distribution will come invite us to your freedom.

471
00:59:22,196 --> 00:59:25,486
And it turns out that Z and ask we are they are independent.

472
00:59:26,926 --> 00:59:30,286
They're investment. And then.

473
00:59:34,306 --> 00:59:47,086
Okay. So now we are able to show this quality so that if we look at Z divided by square bias, we know that this should follow a transfusion.

474
00:59:47,896 --> 00:59:53,176
Go figure. One is beta. One had a minus beta one divided by.

475
00:59:56,346 --> 01:00:07,526
As soon as that's what I is. And then divided by here as square is SC divided by C Square.

476
01:00:07,786 --> 01:00:12,066
That's what a square is. And the degree of freedom is minus.

477
01:00:12,906 --> 01:00:24,996
So we have. Everybody is on your feet.

478
01:00:29,656 --> 01:00:36,156
Sorry. Divided by his own legal freedom.

479
01:00:36,426 --> 01:00:39,996
So this guy is sure to follow a tedious fusion with minus tweeting nothing.

480
01:00:40,326 --> 01:00:45,606
Based on, you know, based on this back here she is on the side here.

481
01:00:46,086 --> 01:00:49,116
But then this. She must wear a council with this thing, a square.

482
01:00:49,566 --> 01:00:52,656
And we know that SC divided by m minus two.

483
01:00:56,016 --> 01:00:59,586
That's that's amazing, right? That's what it means.

484
01:00:59,586 --> 01:01:04,186
Square areas. So if you take all this into account.

485
01:01:04,206 --> 01:01:07,716
So this is actually this guy now becomes the M.C.

486
01:01:08,526 --> 01:01:12,996
And so then that means we get to this guy over here.

487
01:01:15,036 --> 01:01:25,716
And this is just to kind of show you guys where this come from and why this follow to this building is simply based on this back over here.

488
01:01:26,136 --> 01:01:35,956
By actually making up this the other variable and making up this square, by making a central living there and chi square there.

489
01:01:36,576 --> 01:01:49,406
And both are really funny so that we have even in the end, this guy, she's usually right and in the middle here.

490
01:01:49,416 --> 01:01:57,366
What are we what I ruled out here, this is just, you know, some elder bird is trying to show that this guy indeed is equal to this guy.

491
01:01:57,976 --> 01:02:01,086
And all that is followed. She used fusion.

492
01:02:02,946 --> 01:02:08,816
Okay. So now the the the import of the thing is, is the fact of that now okay.

493
01:02:08,826 --> 01:02:15,606
Now we we we found that this guy followed this fusion with in -21 freedom.

494
01:02:18,066 --> 01:02:28,516
And this actually helps us. I mean, our whole following discussion or our following procedure and we have all this testing and and calculated,

495
01:02:28,946 --> 01:02:34,576
you know, how they are based this back here, this this guy followed Tito's fusion.

496
01:02:34,746 --> 01:02:42,486
And so now let's let's look at the oh, this has to be true here.

497
01:02:43,056 --> 01:02:47,576
This is actually the most important thing or one of the most important things.

498
01:02:47,576 --> 01:02:54,726
So you slow down a little bit to try to talk about I mean, the formal concept of this.

499
01:02:54,756 --> 01:02:57,486
You will see this in six or through next semester.

500
01:02:58,056 --> 01:03:07,716
But a here, let's look after the idea behind how this testing so to have others testing is a very important aspect of statistics.

501
01:03:08,166 --> 01:03:12,366
So it's a procedure where you're formulating or send him a question as a hypothesis.

502
01:03:13,356 --> 01:03:20,796
So let's take this as a generic example that's a pharmaceutical company has has developed a drug asserted right to treat a certain disease.

503
01:03:21,786 --> 01:03:27,006
Now they want to carry out a clinical trial to see whether indeed the drug is effective.

504
01:03:28,416 --> 01:03:31,686
Now, they want to formulate this as studying this problem.

505
01:03:32,026 --> 01:03:35,766
Now, of course, one way of studying this problem is to follow the hypothesis,

506
01:03:36,636 --> 01:03:41,105
the hypothesis that, no, this drug has no effect on treating this disease.

507
01:03:41,106 --> 01:03:45,606
You are to that's that's the how good of the so-called null hypothesis.

508
01:03:45,756 --> 01:03:55,775
It's not zero that formulated is now your immune smothers know it now in terms are a beta that means beta one equal to zero.

509
01:03:55,776 --> 01:04:02,886
For example there is no association between what next. There is no drug treatment is that there's no linear association.

510
01:04:03,186 --> 01:04:08,376
So that's why it's called novel. This is there is no association and an alternative.

511
01:04:08,976 --> 01:04:10,746
Each one do not have alternative.

512
01:04:10,806 --> 01:04:17,356
Alternative a leader working to see that there are different ways of satisfying this alternative or to give your take.

513
01:04:17,886 --> 01:04:22,175
It's the contrary to the novel. This is alternative. Your disease says that.

514
01:04:22,176 --> 01:04:29,126
Oh, there there is some. In fact, there there is either positive effect or not, in fact, for some effect.

515
01:04:31,236 --> 01:04:34,786
So that's that's the novel. This is an alternative novel is it's not.

516
01:04:34,806 --> 01:04:37,596
But this is, again, in a sense that there is no effect at all.

517
01:04:39,456 --> 01:04:47,945
So once you formulate this hypothesis, a hypothesis, then you will neutral data base our data to test this your your analysis,

518
01:04:47,946 --> 01:04:55,686
whether you while others this agrees with you on a data whether we're data supports what happened is it's or not.

519
01:04:56,586 --> 01:05:06,066
And by test in testing that hypothesis, we have to formulate a so-called constructive form of past the Gnostic and all these concepts.

520
01:05:06,576 --> 01:05:09,336
I know the formal definition. You will see them, you say so true.

521
01:05:10,026 --> 01:05:21,096
But here I mean for our simple linear regression, we are going to use this as a statistic and that's actually exactly what on right here.

522
01:05:30,476 --> 01:05:34,295
SSX. Okay. So this is the testosterone.

523
01:05:34,296 --> 01:05:40,926
So-called testosterone is where we use in order to test whether one is equal to zero or not.

524
01:05:41,976 --> 01:05:46,566
And if you look at this test statistic, it involves the parameter of interest,

525
01:05:47,046 --> 01:05:53,046
paid pay, paydays, all kinds of data, and it is computable using observed data.

526
01:05:53,046 --> 01:06:02,106
So once you can do it, then once you, you know, then I can counter this here under non blizzard speed of what is equal to zero.

527
01:06:02,286 --> 01:06:06,876
So under normal this is now beta one becomes zero. So we passed the standard become this.

528
01:06:07,266 --> 01:06:10,506
But generally speaking, not is equal to this.

529
01:06:11,646 --> 01:06:21,026
So I can calculate this after you a data. And also we need to have assembly distribution and we have to derive the segment description.

530
01:06:21,306 --> 01:06:25,846
She follows t on the back of the under.

531
01:06:25,956 --> 01:06:29,256
What if now this is true? Okay,

532
01:06:29,256 --> 01:06:35,795
so we have a a statistic and then we compute this as a mistake based on our

533
01:06:35,796 --> 01:06:40,656
data and then we need to preserve this by the so called significance level.

534
01:06:41,946 --> 01:06:54,276
The significance level is that when we test sort of hypothesis, when we pass all this is we are going to make certain errors.

535
01:06:55,056 --> 01:06:58,866
Well, specially it was physically error is what kind of error is.

536
01:06:59,756 --> 01:07:04,176
Well, if this hypothesis is true, if indeed there is no association.

537
01:07:05,466 --> 01:07:11,886
But as we reach out, it is not. What is this? We think it is fast, but as it is true so that we make it now.

538
01:07:12,876 --> 01:07:17,886
That's the so-called pipeline error. And this significance is actually, you know,

539
01:07:17,886 --> 01:07:25,466
actually the top one that we want to control that I have now want to have a really long time on error, so little control.

540
01:07:25,476 --> 01:07:30,036
So if my hypothesis is true, you do not want to reject it very often.

541
01:07:31,326 --> 01:07:35,615
So this alpha is, as we said, a very low number.

542
01:07:35,616 --> 01:07:42,296
You raise 1.5. So to have pulled all five levels as a control of how one never seemed to know,

543
01:07:42,336 --> 01:07:51,065
to have an error that are often so significance level this alpha able to all five maybe I would just

544
01:07:51,066 --> 01:07:58,925
write it down here I mean this is the so-called type one error but the formal definition of this,

545
01:07:58,926 --> 01:08:02,436
again, this is something you see see 26 on two.

546
01:08:03,996 --> 01:08:06,635
But again, this kind of an error means that if not,

547
01:08:06,636 --> 01:08:13,926
how about this is is true but we reject problems with made they can ever find rejecting true obviousness.

548
01:08:15,306 --> 01:08:21,546
We want to keep this under control. So that's why we take Alpha Eagle 2.5 and set it to be a very small lot.

549
01:08:23,736 --> 01:08:33,456
And then in order to make a conclusion, well, we can follow three approaches which we will talk about one by one.

550
01:08:33,816 --> 01:08:39,366
So why is the so-called rejection region matter? And one is a so-called a p value.

551
01:08:40,116 --> 01:08:44,016
Another one is the so-called confidence interval method.

552
01:08:47,286 --> 01:08:54,845
So I know that is worthwhile for for I think most of you probably haven't taken of course is similar to say so true so these

553
01:08:54,846 --> 01:09:02,166
concepts today they are probably new concepts but what we are going to talk about each one of these in the next few slides,

554
01:09:02,316 --> 01:09:08,345
so hopefully you will get to some we're not going to talk about or even the very formal mathematical definition of these,

555
01:09:08,346 --> 01:09:14,976
but I'm trying to do intuitive understanding of of these different concepts.

556
01:09:15,846 --> 01:09:23,196
So then that means we can actually after we formulate, look, I don't know what this is.

557
01:09:23,496 --> 01:09:29,676
After we calculate the test statistic and we sat on a significant level, able to pull that,

558
01:09:29,976 --> 01:09:39,336
then we can take any of these three approaches to proceed and to make a conclusion.

559
01:09:39,336 --> 01:09:46,836
And we are going to see how to take this three approaches for was we take one of these approaches that we had.

560
01:09:47,256 --> 01:09:53,366
Again, we'll have a conclusion either to reject a novel this is or not to reject the business.

561
01:09:53,766 --> 01:09:58,026
And then we can make a final conclusion in 2 hours every week.

562
01:09:59,106 --> 01:10:09,606
So that's the sort of, you know, a very quick look, patriarchal view of hypothesizing.

563
01:10:09,606 --> 01:10:13,326
So any questions before we go here?

564
01:10:13,326 --> 01:10:18,155
We are not trying to make the things super rigorous because that's beyond the scope of this course.

565
01:10:18,156 --> 01:10:23,796
But I tried to give a logical, intuitive explanation of these concepts.

566
01:10:25,446 --> 01:10:34,336
So any questions before? Okay.

567
01:10:37,646 --> 01:10:43,896
So as we mentioned, for the symbol in Iraq, the most important thing is actually a better one.

568
01:10:43,946 --> 01:10:49,866
So we want to test whether beta one is equal to zero or not, which means we want to pass by the y axis.

569
01:10:49,886 --> 01:10:55,976
There is a strong linear association and we are using this test statistic.

570
01:10:56,786 --> 01:11:01,916
This is beta one minus beta 0.0 hundred not, but this is equal to zero.

571
01:11:02,426 --> 01:11:07,796
And that's why here we have essentially this has been a one had a -0.

572
01:11:08,506 --> 01:11:13,796
But this is. So here I'll make it clear under the hypothesis.

573
01:11:13,916 --> 01:11:17,606
This follows Achilles fusion with minus one.

574
01:11:18,066 --> 01:11:20,426
And so if we are still to truth,

575
01:11:20,876 --> 01:11:31,796
this follow at2 solutions and then we are going to look out of this three different methods in how to test this novel is.

576
01:11:39,556 --> 01:11:44,626
The first one is the so called rejection region method which actually region method.

577
01:11:45,316 --> 01:11:57,846
So now we are assuming now because we have formulated without but so now we are assuming our task was not having those.

578
01:11:58,276 --> 01:12:03,826
We are assuming this is true. So the recruitment method work or any of this matter.

579
01:12:04,306 --> 01:12:15,296
So we first we assume this is true and then we summarize our data into the test statistic and see how well it agrees with them.

580
01:12:15,316 --> 01:12:21,406
This not loses. If our data agrees with anomalies, then we tend to think not all this is true.

581
01:12:21,946 --> 01:12:28,656
If our data does not agree with not was X, then we will reject this as we tend to think are sparse.

582
01:12:29,056 --> 01:12:39,586
And so so was we form labels. Then we will summarize our data use if it has the statistic and then somehow we

583
01:12:39,586 --> 01:12:45,516
are going to look at make an assessment how well that summary has a solidity,

584
01:12:45,786 --> 01:13:00,046
the risk with our business and this rejection region method, we are going to look at how likely it is to observe what are we have observed.

585
01:13:01,036 --> 01:13:11,406
And so based on our data, we are able to calculate this test statistic and then where we look at under this not of others,

586
01:13:11,406 --> 01:13:18,946
is how likely that we would observe this cheat if none of this is true.

587
01:13:20,116 --> 01:13:28,206
So so if in order to have a this is it, it's very unlikely to observe what are we have observed does not mean small.

588
01:13:28,366 --> 01:13:33,016
Our data does not agree with novelists very well. So we will reject this.

589
01:13:34,066 --> 01:13:45,826
So but how to tell whether like how likely to observe that the data we have observed this is based on, you know, the diffusion of this team.

590
01:13:46,066 --> 01:14:01,126
So we know that a t follows this. T follows a key distribution of TV for freedom of t t news vision has this one such a pdf and this is t distribute.

591
01:14:01,426 --> 01:14:08,986
So the business sheet is different with that if you are real effort and here we need this so called a concept of.

592
01:14:10,846 --> 01:14:20,896
Accused person have accused the president how we mean that the area to the left of this queue the area here.

593
01:14:21,046 --> 01:14:24,046
This is equal to the area is equal to Q.

594
01:14:24,496 --> 01:14:30,906
This is what we mean by queues. Some have. This is an accused person.

595
01:14:33,816 --> 01:14:36,986
This is actually. She?

596
01:14:40,196 --> 01:14:48,266
She was the personnel simply means the number, the personnel of the tedious fusion site for that of the area to the left of this.

597
01:14:50,006 --> 01:14:55,916
And this number, this guy is equal to. Q End of the rejection region.

598
01:14:58,166 --> 01:15:09,956
But first, while we need to to determine a cutoff for a given significance level, you are the significance level is equal to 4.5 L angle 2.05.

599
01:15:10,526 --> 01:15:13,556
So once we fix that, we need to determine the cutoff.

600
01:15:14,846 --> 01:15:25,826
The cutoff is used to determine whether the observed t observed the value of T, whether that's extreme or not.

601
01:15:26,426 --> 01:15:30,836
So we compare t to this term of the.

602
01:15:35,096 --> 01:15:40,616
And it is kind of usually we take a look up to the accused present power of a fusion.

603
01:15:41,246 --> 01:15:50,215
So for example, if we take Alpha Ego 2.05, then this Q is equal to 495, the four No.

604
01:15:50,216 --> 01:16:14,246
Five. And again, we compare we compare this observed t statistic to this principle two to either T or t this half.

605
01:16:19,506 --> 01:16:28,566
So this cut out for me is that, you know, the area through this guy to the left of this guy is more than 5.5.

606
01:16:29,226 --> 01:16:39,066
So what this means is that if we use this as the cutoff, I mean, here I'm trying to emphasize this this contact this person that here.

607
01:16:39,306 --> 01:16:46,956
So if we use this person now as our cut up, what it means is that we will compare the calculated P value to this cutoff.

608
01:16:47,736 --> 01:16:53,136
And if this T is larger than this cutoff. We think this T is extreme.

609
01:16:53,466 --> 01:17:00,035
Is extreme is very unlikely to be observed under anomalies because then t fall somewhere

610
01:17:00,036 --> 01:17:05,286
over here to the right means that it is not going to do something wrong with this.

611
01:17:05,286 --> 01:17:11,366
And that means what the is extreme is very likely to observe or not.

612
01:17:11,586 --> 01:17:15,606
So we will reject this because our data does not support this.

613
01:17:16,086 --> 01:17:22,026
However, if t falls to the left of this half to the shaded region,

614
01:17:23,196 --> 01:17:32,846
then while there is 95% 1.95 probability that a t fall below is below this cutoff result,

615
01:17:32,856 --> 01:17:37,686
it's likely that t fall below this cutoff so that t is not an extreme.

616
01:17:38,286 --> 01:17:44,586
So that means our data has agrees with the models as well, so that we will not reject analysis.

617
01:17:45,036 --> 01:17:49,446
And so this is the the idea behind the so-called rejection region.

618
01:17:50,046 --> 01:17:56,106
So we determine a cutoff, that a cutoff actually determines whether this T is extreme or not.

619
01:17:56,646 --> 01:18:01,416
So if T falls to the right of this cutoff, then we consider to be extreme.

620
01:18:02,136 --> 01:18:07,476
And if t fall left of the cutoff because not to be possible.

621
01:18:09,036 --> 01:18:20,166
And if t extreme, that means well, I want you to observe this t underline analyzes so we will reject the numbers of we are going to.

622
01:18:20,706 --> 01:18:24,796
Yeah. So we are going to see three examples hopefully. Yeah.

623
01:18:25,476 --> 01:18:33,936
Why don't we just jump into this example. I think this helps a lot to make the idea more specific.

624
01:18:35,136 --> 01:18:39,456
So now let's look at first let's look at this, not about this alternative.

625
01:18:39,546 --> 01:18:44,586
So but not how this is is always been a one equal to zero.

626
01:18:47,286 --> 01:18:53,586
And now we are casting this against this better one larger than zero.

627
01:18:54,136 --> 01:19:03,996
Suppose we have no prior knowledge saying that if is not equal to zero then we know what has to be positive that this specification of this.

628
01:19:05,076 --> 01:19:08,386
This alternatively depends on the prior knowledge.

629
01:19:08,406 --> 01:19:12,936
The process that is built one is not equal to zero and it has to be positive.

630
01:19:13,266 --> 01:19:17,736
So that's why we see this by this model. This is out of this alternative.

631
01:19:19,116 --> 01:19:25,056
Now, to test this alternative, then the rejection region is actually given by this because.

632
01:19:28,866 --> 01:19:33,156
This is the key decision. So in this case, this is the corner.

633
01:19:33,396 --> 01:19:43,536
This is the key. While we have, you know, minus to your freedom, because as we proved, the crisis that followed, the kids were in minus three.

634
01:19:45,306 --> 01:19:55,566
And we use this column, which means that the area to the left of this bar, you know, this probability is point 95, is called nine five.

635
01:19:56,586 --> 01:19:59,976
And in this case, we look at the test statistic.

636
01:20:01,836 --> 01:20:03,756
Now we compare it to this cutoff.

637
01:20:05,736 --> 01:20:16,025
So if t falls to the right of this corner, if TIF falls somewhere here, somewhere here, then we think t is very extreme,

638
01:20:16,026 --> 01:20:25,036
is unlikely to be observed because there's 40.05 probability of observing active outlook false to the right, at least this context.

639
01:20:25,486 --> 01:20:30,546
And so that means while Keys Extreme is unlikely to be observed.

640
01:20:31,146 --> 01:20:36,276
So again, it does not agree with our analysis so that we will reject the system.

641
01:20:36,756 --> 01:20:39,756
So the rejection region in this case is actually.

642
01:20:43,396 --> 01:20:49,276
From this guy to poetry to infinity. So this is the reactive region in this case.

643
01:20:51,866 --> 01:21:00,686
So in other words, if the team the past statistic falls in this region, we will reach out to a lot of business.

644
01:21:01,406 --> 01:21:06,876
Otherwise, we will not redact. Okay.

645
01:21:07,066 --> 01:21:12,196
So that's one case. Another case is.

646
01:21:14,816 --> 01:21:20,086
Well, suppose our panel decides that if better zero is better, why is not equal to zero?

647
01:21:20,096 --> 01:21:24,656
The beta one has to be 90. So the association came back to us.

648
01:21:24,836 --> 01:21:29,156
I mean, if there is association also saying it can only be 90. So in this case.

649
01:21:42,986 --> 01:21:45,986
We will use this cut out here on.

650
01:21:52,836 --> 01:21:55,836
So this is actually what is true.

651
01:21:58,566 --> 01:22:03,246
We will take off here. The air here to the left of this bar.

652
01:22:03,276 --> 01:22:13,906
That's the what this is all about. We use this corner because in this case, well, the better one is either 049.

653
01:22:15,666 --> 01:22:19,866
So only when the value is very, very negative.

654
01:22:20,046 --> 01:22:25,146
And to the to the left of this bar, we consider it to be extremely close to all the values.

655
01:22:25,146 --> 01:22:31,686
I mean, we consider them to be to be, to be not only street values.

656
01:22:32,316 --> 01:22:35,356
So in other words, only the values fall below this bar.

657
01:22:35,376 --> 01:22:42,306
We consider them to the extreme. And of course, based on the property of Tito's fusion.

658
01:22:51,116 --> 01:22:56,176
Because T's is symmetric. So this cutoff is actually equal to 19th.

659
01:22:56,726 --> 01:23:00,205
This kind of I mean, this is this bar here.

660
01:23:00,206 --> 01:23:07,096
This is t. This past year was.

661
01:23:13,596 --> 01:23:20,436
So because it's symmetric. So now when we test this alternative hypothesis versus this alternative hypothesis.

662
01:23:20,826 --> 01:23:32,366
So if the the calculated test statistic falls below this part, you know, this model for the region, then we consider it to be an extreme.

663
01:23:32,376 --> 01:23:41,166
And that means. Well, if it falls below this part, that it means is unlikely to observe this gene.

664
01:23:41,406 --> 01:23:45,816
So the rejection region in this case is actually negative infinity.

665
01:23:45,876 --> 01:23:52,866
True. And this health.

666
01:23:57,066 --> 01:24:00,846
And what if she falls to the right of this bar, to this cut off?

667
01:24:01,206 --> 01:24:06,996
We can see her tea is likely to occur because there is porter by five, probably to the right of this bar.

668
01:24:07,356 --> 01:24:13,236
So so it has a very high probability that for tea to fall above this bar.

669
01:24:14,556 --> 01:24:18,066
So it's beautiful above this bar that is not extreme.

670
01:24:19,106 --> 01:24:22,766
And it does not it is not against it. Marvelous.

671
01:24:23,496 --> 01:24:28,896
So that's the same in the case. Then the third case is well made up.

672
01:24:28,956 --> 01:24:39,216
One is not equal to zero. So this is the case where we do not have any promotion to say, well, whether beta was positive or negative.

673
01:24:39,366 --> 01:24:47,166
We just know that if it is, if one is even novel, this is is, is not true,

674
01:24:47,316 --> 01:24:51,365
then we can say, well, the only thing we can say that of there is some association.

675
01:24:51,366 --> 01:24:55,746
We do not know whether that's positive association or negative association. So in this case.

676
01:25:00,096 --> 01:25:03,426
The cut off is going to be here and here.

677
01:25:04,146 --> 01:25:26,926
So this is. So in this case only one lucky the test does not enter your files either to this and a here or this and here.

678
01:25:27,196 --> 01:25:28,846
We consider it to be extreme.

679
01:25:29,716 --> 01:25:39,226
If T falls between these two bars, if T falls anywhere between these toolbars, well, that's a likely value for chief distribution.

680
01:25:40,906 --> 01:25:44,926
So so again, the rejection region in this case is.

681
01:26:04,526 --> 01:26:09,716
Union of Mystery Novels. Is the union on the train.

682
01:26:22,236 --> 01:26:25,716
Okay. So this is how we determine the rejection region.

683
01:26:26,076 --> 01:26:33,306
So again, the rejection region is determined by looking at latitudes fusion.

684
01:26:34,176 --> 01:26:38,556
And look at what are the U.S. what are the cutoff is determining the cutoff.

685
01:26:38,946 --> 01:26:46,626
So and the cutoff is determined by your specific alternative hypothesis, or depending on whether you have prior knowledge or not.

686
01:26:47,856 --> 01:26:54,456
And then based on the cutoff, you can determine whether your P statistic is extreme or not.

687
01:26:54,606 --> 01:27:00,186
Even a false out of sight. The bar on the side of the cutoff that is considered extreme.

688
01:27:01,086 --> 01:27:12,546
If the false like in the third case, if the source falls between these two bars return, this took out those considered to be not extremes.

689
01:27:13,266 --> 01:27:17,046
Then if it's not extreme, then we do not have regional analysis.

690
01:27:17,436 --> 01:27:22,116
But it is extreme. That means it does not agree with our data.

691
01:27:22,386 --> 01:27:29,026
Well, because under our this is very unlikely to observe, very extreme.

692
01:27:30,006 --> 01:27:39,066
So we will reject witnesses. That's the novel idea behind the rejection region mask.

693
01:27:40,886 --> 01:27:44,306
So any questions before the next one?

694
01:27:52,666 --> 01:27:57,675
Okay. So that's what we tried to read.

695
01:27:57,676 --> 01:28:05,056
And you can see that we don't read. It is crucial to determine the cut off and to decide whether to use extreme or not.

696
01:28:06,106 --> 01:28:16,726
So the second method is so-called or the pivotal method, the key value method to summarize the data into a single quantities, the so called P value.

697
01:28:18,166 --> 01:28:23,086
So again, here we are now going to talk about a very rigorous definition, a p value,

698
01:28:24,136 --> 01:28:33,256
but a p value is the probability that you observe what you have observed or even more extreme matters.

699
01:28:34,366 --> 01:28:39,656
So again, this is the tedious fusion, right?

700
01:28:39,736 --> 01:28:52,006
With M minus two, the more freedom. I suppose that suppose that you observe this is your observed cheek statistic.

701
01:28:52,996 --> 01:29:02,115
So key to the follow key decision. But let's say for this particular data that you observe t to be somebody over here and

702
01:29:02,116 --> 01:29:11,656
then the p value p value is the probability of observing the data you have observed.

703
01:29:11,716 --> 01:29:15,946
That's that's the capital key and even more extreme models.

704
01:29:17,986 --> 01:29:25,246
In this case, the more extreme, extreme amount of always are always valueless to hotels because of the values on those hills.

705
01:29:25,456 --> 01:29:29,536
They are in the last night. We observe values. The values in the middle.

706
01:29:29,776 --> 01:29:40,756
That's always more likely to be observed. So in this case, we calculated the probabilities of the tail probability to the right of t.

707
01:29:42,826 --> 01:29:47,236
And then that actually is the value that actually tells us.

708
01:29:47,896 --> 01:29:53,115
Summarize the data like what is the probability of observing what we have observed,

709
01:29:53,116 --> 01:30:03,196
which is teen or even more extreme values, but even more extreme now as it depends on your specific alternative.

710
01:30:04,486 --> 01:30:10,845
So let's say that A you are testing H now versus this beta not equal to zero.

711
01:30:10,846 --> 01:30:22,096
With this two side you have others. In this case, the more extreme value means either the value to the left of the teams or the value.

712
01:30:24,706 --> 01:30:29,056
The Black Magic. So both against the Dark were instrumental.

713
01:30:29,056 --> 01:30:35,626
So that's why I hear the p value is two times the probability that p larger than the absolute value of ti.

714
01:30:36,076 --> 01:30:44,076
So. So in this case the p value is probably d either larger than T or the probability less than 70.

715
01:30:44,626 --> 01:30:48,946
So this is the p value because in this case is two sided.

716
01:30:49,406 --> 01:30:54,526
There's no one, one, one side. So the piano is not to the right.

717
01:30:56,296 --> 01:31:00,706
But if you look at it, this alternative theta one is positive.

718
01:31:03,526 --> 01:31:12,406
So in this case the p value. Well, let's say we observe t that's as over here as over here.

719
01:31:12,736 --> 01:31:15,076
So in this case, the more extreme value,

720
01:31:15,406 --> 01:31:23,446
that's the balance to the right of T because the alternative says that beta can only be positive if it's not a zero make only be positive.

721
01:31:24,106 --> 01:31:32,116
So the extreme values of t are the values to the right of what are you have observed here where the little values to the right of

722
01:31:32,686 --> 01:31:44,236
g so then the p value is actually that in the probability that t statistic exclusion is to the right of what you have observed.

723
01:31:44,896 --> 01:31:57,196
And similarly, when we test against this alternative hypothesis, okay, so the p value of now in this case, not in this particular case.

724
01:31:57,946 --> 01:32:04,936
In this particular case that corresponds to this now this second case that corresponds to.

725
01:32:08,316 --> 01:32:19,996
So you. Minister is here because it becomes a disaster that I can only be involved now.

726
01:32:20,166 --> 01:32:26,646
So. So the extreme values are the values to the left.

727
01:32:27,366 --> 01:32:32,346
To the left here. So the p value is actually that we match.

728
01:32:32,706 --> 01:32:36,066
The true fusion is less than what you have observed.

729
01:32:36,876 --> 01:32:44,616
These are the more extreme matters. And the third scenario, one that corresponds to.

730
01:32:49,756 --> 01:32:57,556
So I think this is more tedious and or negative and probably to the right of this ballot and probably to the left of this type.

731
01:33:07,026 --> 01:33:12,246
So the p value again is calculated depending on your alternative hypothesis.

732
01:33:13,236 --> 01:33:18,456
So whether it's one sided, whether it's two sided, or which direction it goes.

733
01:33:19,206 --> 01:33:23,796
And then based on that, you would determine what the extreme values are.

734
01:33:24,426 --> 01:33:32,926
And then the p value is. The probability, again, is the probability that the owner of anomalies is observed.

735
01:33:34,626 --> 01:33:42,906
What are the probability of observe getting results at least as extreme as the ones you observe?

736
01:33:43,576 --> 01:33:48,635
It's a little probably observing what you have observed, even what is, you know what not.

737
01:33:48,636 --> 01:33:51,936
Apparently it is. So if that problem is very small.

738
01:33:53,466 --> 01:33:59,076
So in other words, the probability of observing what you have observed, what you have actually observed,

739
01:33:59,286 --> 01:34:02,856
or even where history values the probability is very small, not a means.

740
01:34:03,156 --> 01:34:07,535
Is there a likely to observe what is unreliable?

741
01:34:07,536 --> 01:34:14,096
This is not to me, as Ribera does not agree with or not, but this is very well so number of the models.

742
01:34:15,066 --> 01:34:19,386
So in other words, if the p value if the pivot is very small.

743
01:34:19,896 --> 01:34:27,816
So here we search the cardinal threshold, which would be .05 p value is less than five and these are very,

744
01:34:28,056 --> 01:34:31,386
very small p value, very unlikely to observe.

745
01:34:31,976 --> 01:34:42,096
So we have observed that of Jackie. But if the p value is not a small, then we will not reject the analysis.

746
01:34:49,526 --> 01:34:57,426
We'll get any questions. Okay.

747
01:34:57,486 --> 01:35:01,046
With a p value. This comes out of your value. Yeah.

748
01:35:01,086 --> 01:35:08,106
I can't use one more because the p value is is a very fundamental cause of instability.

749
01:35:08,126 --> 01:35:15,275
But it's because here we are not, you know, looking at it like the very super rigorous definition of p value.

750
01:35:15,276 --> 01:35:22,536
So but I do want to emphasize the interpretation. So now make sure that we all have a good understanding of values.

751
01:35:23,406 --> 01:35:25,596
So again, P value is a single number.

752
01:35:26,136 --> 01:35:36,906
Now to summarize the evidence in the data and well that when you compare it 2.05 so that I took two to see whether it agrees with our data,

753
01:35:37,716 --> 01:35:41,346
sorry to see whether you would want to know how well this is or not.

754
01:35:46,476 --> 01:35:51,126
Okay. So no question. And we'll look at it next.

755
01:35:52,626 --> 01:35:57,396
This is not covered as a normal method. So for confidence interval.

756
01:35:59,856 --> 01:36:04,836
Recall that. Well, so far we have a point estimate of our beta one.

757
01:36:04,996 --> 01:36:10,716
That's beta one hat. Well, the company is interval. We construct a interval estimate.

758
01:36:11,196 --> 01:36:17,946
So interval estimated looks like this. So beta one had a minor something and beta one had a plus something.

759
01:36:19,086 --> 01:36:24,126
And this is something that involved this tedious building upon how to do certain times.

760
01:36:24,546 --> 01:36:31,876
This is actually the center of. Listener error, a bit of it.

761
01:36:33,026 --> 01:36:39,086
Okay. So this is how we consider our comedies.

762
01:36:39,086 --> 01:36:47,006
Comedies, you know, you can. And then, of course, after your data, you can very easily capital this confidence interval.

763
01:36:48,176 --> 01:36:58,196
So what this means is that all of that, what a confidence interval means is that now you have an interval.

764
01:36:58,196 --> 01:37:02,546
So you will know what we do not know exactly what the true value is,

765
01:37:03,416 --> 01:37:12,056
but we have like a high confidence that this true value, this unknown true value has it fall inside this inner.

766
01:37:13,016 --> 01:37:20,906
By high confidence, we mean that, you know, if we repeated a class our data, you really collect a thousand data sets.

767
01:37:21,266 --> 01:37:29,876
And for each dataset we construct in confidence, you know that almost 95% times out of this a thousand capsule,

768
01:37:29,876 --> 01:37:32,996
that means not hundreds of 50 combination of all.

769
01:37:32,996 --> 01:37:35,996
So we constructed out of these a thousand confidence intervals,

770
01:37:36,746 --> 01:37:45,865
we recover the truth about 95% on top recovered but true but I know that so that's done the confidence

771
01:37:45,866 --> 01:37:54,566
interval so now the well we pass such a number this is here you are the beta one is equal to 000.

772
01:37:56,756 --> 01:38:07,916
And we look at whether that value of false insight, this confidence, you know, if the confidence interval does cover this, that must be one.

773
01:38:09,056 --> 01:38:19,176
Then we will not reject them. Now this. But if the code is suitable, does not cover the blind people who fall outside of this common scheme.

774
01:38:19,926 --> 01:38:27,046
Then we will reject the nonsense. Again would it be what is out of sight in a conference room?

775
01:38:29,986 --> 01:38:36,006
So this is not the in approach, so we just need to calculate interval.

776
01:38:36,736 --> 01:38:41,916
So after we got our data, everything here is not so weak.

777
01:38:41,926 --> 01:38:45,606
We can capture events in our world and then we can look at what we can do.

778
01:38:45,616 --> 01:38:51,646
We can see whether this blunt force is on the scene or not and then decide whether to reject this.

779
01:38:56,476 --> 01:39:02,836
So this is sort of the interpretation, whether we reject a novel, this is not so easy for the novel.

780
01:39:02,836 --> 01:39:10,606
This is is rejected that it means, oh, well, we can't believe this is this is not true.

781
01:39:10,846 --> 01:39:14,656
So that means there is no significant association.

782
01:39:15,296 --> 01:39:21,675
Oh, sorry. Let it means. Well, there is a significant association between X and y, y of X.

783
01:39:21,676 --> 01:39:27,046
They are significantly injury associated. That's why we reject a novel disease.

784
01:39:28,486 --> 01:39:32,536
Or in other words, the slope of a regression line is significantly different from zero.

785
01:39:33,796 --> 01:39:43,486
That's the conclusion we make when we reject this. But if we fail to reject a novel, as it's usually, we do not say that.

786
01:39:44,716 --> 01:39:48,006
Well, okay, so there is there's no association at all.

787
01:39:48,026 --> 01:40:00,376
There's no evidence of such association is zero or I mean beta 100 of four is that we would have say that our data do

788
01:40:00,376 --> 01:40:08,276
not provide it is do not provide a sufficient evidence or there's insufficient evidence that a slope is one zero.

789
01:40:09,586 --> 01:40:15,946
This is just a more diplomatic way of saying saying this.

790
01:40:16,786 --> 01:40:22,246
So because, I mean, we fail to reject this doesn't mean this is this is true.

791
01:40:23,416 --> 01:40:32,925
Actually, in reality, the beta beta one is never going to be exactly equal to a particular number, even though it's not never happens.

792
01:40:32,926 --> 01:40:38,566
It's just that our data do not provide evidence in support of this nonsense.

793
01:40:38,896 --> 01:40:42,576
So that's what we mean by we fail to reject analysis.

794
01:40:42,886 --> 01:40:49,126
So there is no no, not enough evidence that a slope is not a zero.

795
01:40:51,376 --> 01:40:55,216
So the test of the test, you wrote it.

796
01:40:55,216 --> 01:41:00,226
Do not tell us whether now or alternative whether that's exactly true.

797
01:41:01,216 --> 01:41:09,526
But what it tells us is whether we have evidence, we have strong evidence to support the bar.

798
01:41:09,556 --> 01:41:20,336
But this is simply not. Okay.

799
01:41:20,916 --> 01:41:31,146
So in some occasions we do not I mean, actually, we do not want to generous meaning we do not want to over interpret the significant results.

800
01:41:31,416 --> 01:41:40,206
So here this is a graphical illustration. So in both cases, we can see that while we will reject this novelist,

801
01:41:40,236 --> 01:41:50,136
we may reject this novel as the first case is quite straightforward because apparently there is a strong linear association between Y next.

802
01:41:50,436 --> 01:41:56,136
So it's very natural to see that we reject the novel is not in a the case.

803
01:41:57,246 --> 01:42:03,216
Well of course based on the plot, based on the cause we see that there is a carpentry here is definitely not linear.

804
01:42:04,776 --> 01:42:10,746
However, there is a very strong increasing trend in all these data points.

805
01:42:11,316 --> 01:42:19,685
So if you simply pass to whether this based on what people who zero or not well likely we will reject this side whether it's because I mean,

806
01:42:19,686 --> 01:42:27,306
if you fit this model, you will see a positive slope. If you if you fitted Peter line to all these data points, you will see a positive slope.

807
01:42:27,756 --> 01:42:32,916
But that doesn't mean that a true association material is linear as well.

808
01:42:33,276 --> 01:42:36,336
So that's the reason why we have to interpret this with care.

809
01:42:36,696 --> 01:42:40,056
So we feel to reject this I'm sorry we read this.

810
01:42:40,296 --> 01:42:44,226
That does mean that no association. This has to be linear.

811
01:42:44,706 --> 01:42:52,806
And this simply means that if there is a strong increasing trend, which is why an X as illustrated by the SAT report here.

812
01:42:53,016 --> 01:42:56,916
So we have to interpret this with some care.

813
01:42:59,886 --> 01:43:03,456
Okay. So let me finish this this like before we.

814
01:43:04,686 --> 01:43:11,606
So another thing is that when we fail to reject the null hypothesis, well, we feel the rejection.

815
01:43:13,266 --> 01:43:17,166
Well, here, for example, I mean, this plot.

816
01:43:17,676 --> 01:43:23,826
Yeah, we see that there is some clear association between block x.

817
01:43:24,516 --> 01:43:28,326
However, if we see a linear regression model library,

818
01:43:29,616 --> 01:43:38,706
this is this is why we get likely if there is there is no increasing or decreasing trend because the hidden by this look at this by this curvature.

819
01:43:39,276 --> 01:43:45,636
So so in this case if we simply pass this we will likely not reject this model

820
01:43:45,636 --> 01:43:50,006
as but the thumb mean that there is no strong association between planets.

821
01:43:50,946 --> 01:43:59,216
So so these two examples, I mean, this slide in this slide, we're trying to use them to show that we have to interpret the results.

822
01:43:59,226 --> 01:44:03,156
We can either reject,

823
01:44:03,306 --> 01:44:12,545
either we reject this is or this is simply we do not want to overinterpret those results

824
01:44:12,546 --> 01:44:18,036
because there are more cases like this illustrated by this particular and this slide here.

825
01:44:18,396 --> 01:44:23,546
There are cases where the association may not be the linear out of the true association.

826
01:44:23,556 --> 01:44:28,686
If we talk about a linear facility in our model, that the true association is hidden.

827
01:44:30,076 --> 01:44:33,286
So let's leave the problem here.

828
01:44:33,486 --> 01:44:38,596
Okay. So I think we will complete the original photographs along with some here for you.

