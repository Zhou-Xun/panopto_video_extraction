1
00:00:00,980 --> 00:00:26,459
I think do think that people are using the common good at the moment so that we've all got and with

2
00:00:26,460 --> 00:00:43,350
Lewis Hamilton of all people get from 10109 text at least is no controversy over the spending plan.

3
00:00:44,310 --> 00:00:48,450
But that's okay.

4
00:00:49,110 --> 00:00:53,820
Good afternoon. And so how's your second homework going?

5
00:00:54,510 --> 00:01:02,100
It's long. I'm thinking of bit that time, but I know that it will take quite a bit of time, process and data, right?

6
00:01:02,130 --> 00:01:13,860
Yeah. So I received a couple of emails that request a extension I think is a very reasonable request given that you have spend a lot of time,

7
00:01:13,860 --> 00:01:18,420
process and data and how much extra time you need.

8
00:01:18,570 --> 00:01:21,970
That's my question. We have a footprint.

9
00:01:21,990 --> 00:01:25,380
Yes. I mean, we just left the class afterwards and give us like a week.

10
00:01:25,680 --> 00:01:28,990
A week. Okay. That's at least one.

11
00:01:29,360 --> 00:01:36,470
I know about everyone else. Can the week like an 18th or.

12
00:01:36,660 --> 00:01:41,460
Well, a week from there because we don't meet up for like the 2020s.

13
00:01:41,910 --> 00:01:45,120
Yeah. Okay. Because we're not one is 20 year old.

14
00:01:45,320 --> 00:01:47,820
Okay. You think you can. Okay.

15
00:01:48,360 --> 00:02:06,780
The fine with this is, of course, that, you know, we can have flexible this two days and and is okay 20th we switch to the new world deadline.

16
00:02:07,400 --> 00:02:14,760
You know one thing I'm just sure you is that after a with data that the data will be used in your midterm.

17
00:02:14,790 --> 00:02:20,129
Okay. So so you're not wasting time only for homework two and you will save a lot of

18
00:02:20,130 --> 00:02:28,030
time for your midterm where would do some some other analysis of the data and on.

19
00:02:31,110 --> 00:02:38,280
Okay. Oh, okay. And we're going move.

20
00:02:43,920 --> 00:02:51,299
So I've been talking about doing parameter estimation and inference and I have been

21
00:02:51,300 --> 00:02:59,160
focused on the ESR model and you know this most computer model in the past week.

22
00:02:59,850 --> 00:03:03,060
Now I want to switch back to this space based model.

23
00:03:03,450 --> 00:03:08,310
The reason I want to talk a little bit more in the primary mission universe,

24
00:03:08,820 --> 00:03:14,990
in the context of space based model, is that this model is really good for us to add more covers, right?

25
00:03:15,020 --> 00:03:23,969
So this model has its strengths that namely that allows us to accommodate a lot of covariates

26
00:03:23,970 --> 00:03:34,650
in our study of the sort of surveillance data and time source data for C and also today,

27
00:03:34,650 --> 00:03:40,379
I will show you that this kind of model can be extended to do with some spatial data as well.

28
00:03:40,380 --> 00:03:51,480
So because of the complex system of the, you know, spatial and temporal dynamics or correlations long time,

29
00:03:51,480 --> 00:04:05,040
it's very hard to specify actually a per with distribution and the likelihood method is very hard to, you know, to propose and to validate.

30
00:04:05,340 --> 00:04:11,550
So people take a sort of alternative mass using estimating question approaches.

31
00:04:11,940 --> 00:04:18,959
So estimating equation approaches are somewhat a slightly more and advanced message than mass movements

32
00:04:18,960 --> 00:04:26,130
because we already see that we can use method moments in the cyber model context to estimate,

33
00:04:26,130 --> 00:04:32,610
you know, the basic transmission parameters in most comparable models.

34
00:04:33,090 --> 00:04:38,940
But now we have a little bit more sophisticated model to model the, you know,

35
00:04:39,690 --> 00:04:44,640
association between your outcome like number of confirmed cases or number of deaths.

36
00:04:44,970 --> 00:04:55,830
With some of the covers there, it's hard to work out likelihood this such a high dimensional entity that we need to do a lot of numerical computation.

37
00:04:56,160 --> 00:05:04,200
Numerical burden is really big so that people trying to find a way to simplify that compulsive likelihood is the one.

38
00:05:04,200 --> 00:05:13,050
I'll talk about the previous lecture that we project high domains you likely to lower one the lower like two objects.

39
00:05:13,650 --> 00:05:18,360
I will come back to that compulsive likelihood in this spatial data analysis

40
00:05:18,360 --> 00:05:23,730
because compulsive likely has been extensively used in the spatial data analysis.

41
00:05:24,180 --> 00:05:31,470
That's the second half of this course that we're talking about, how we're deal with a spatial record, correlate data.

42
00:05:32,430 --> 00:05:38,759
But back to this state space model, you know, you have this observed process, a leading process.

43
00:05:38,760 --> 00:05:42,930
You have a model that takes a lot of culverts into it.

44
00:05:42,930 --> 00:05:50,759
Now, how you're going to, you know, do estimation and I hope I have already introduced this kind of model.

45
00:05:50,760 --> 00:06:01,390
Now let's just focus on parameter estimation today and see how we can do this kind of job of assistance is to,

46
00:06:01,690 --> 00:06:06,570
as a statistician, to use the data to figure out of the model parameters.

47
00:06:06,810 --> 00:06:09,990
Yeah. So I would focus on two things.

48
00:06:09,990 --> 00:06:19,830
One is the the date gee, I forgot put here so I want to do $0.03.

49
00:06:19,830 --> 00:06:28,440
Why is t generalize estimate in question this approach proposed by Scott Zager from

50
00:06:28,440 --> 00:06:36,170
Johns Hopkins and then I talk about the M algorithm and I talk about common estimate,

51
00:06:36,270 --> 00:06:43,260
the equation. Okay. So this are the three methods that are being sort of familiar with.

52
00:06:44,010 --> 00:06:49,740
So, so I give you an introduction of this method that being in the literature for a while.

53
00:06:49,920 --> 00:07:01,410
Okay? So basically we're talking about this model of the structure so that you have you observe process.

54
00:07:02,310 --> 00:07:11,280
Why T which could be a factor, right? And could be number of confirmed cases, number of deaths, a number of recoveries and so on.

55
00:07:11,280 --> 00:07:13,050
So this could be a factor.

56
00:07:13,380 --> 00:07:24,060
Or if you look at this, you know, vector as a you focus on one type of outcome, for example, number of confirmed cases or a number of death.

57
00:07:24,390 --> 00:07:30,420
But they are each component is is a number of deaths from one county.

58
00:07:30,930 --> 00:07:36,150
So, for example, you know, in Michigan, we have 84 counties or 87 counties.

59
00:07:36,480 --> 00:07:40,620
You can make the Y t to be a seven dimensional.

60
00:07:40,680 --> 00:07:47,670
Factor. Each element of that factor requires the number of tests of a particular country.

61
00:07:47,760 --> 00:07:53,670
Right. So that you can make that to be a factor or you can have different types of there's variables

62
00:07:54,060 --> 00:08:00,930
that recovers or where other infections or depends on what you probably would like to study.

63
00:08:00,930 --> 00:08:10,680
But anyway, so you can have this way, which could be a multidimensional vector as it requires of some surveillance data that you have,

64
00:08:10,680 --> 00:08:14,640
and now you have this latent process.

65
00:08:14,760 --> 00:08:21,420
This is essentially the, you know, the peak driven sort of dynamics.

66
00:08:22,170 --> 00:08:26,280
There are something in fashion, in fascist dynamics going on in the population.

67
00:08:26,280 --> 00:08:36,000
You cannot directly observe it. What you observe is some past snapshots are, you know, collected from a existing surveillance system.

68
00:08:36,480 --> 00:08:41,180
So you want to understand what's going on in this system, right?

69
00:08:41,190 --> 00:08:53,040
So that when this moves forward and from the past, so you have the data collected, right?

70
00:08:53,040 --> 00:08:57,569
So and so, so force.

71
00:08:57,570 --> 00:09:06,209
And then you figure out that this system, not only just the association, but also you like to make a prediction using this system.

72
00:09:06,210 --> 00:09:12,990
And so you need to really work out the the values of parameters in this modeling system.

73
00:09:13,260 --> 00:09:22,680
So that's the essential model we're trying to figure out, as I say, that we will focus on the T as generalized estimate equation.

74
00:09:22,680 --> 00:09:29,960
And, and Ziegler had the paper 1988 to the Parametric a paper out.

75
00:09:30,450 --> 00:09:40,109
He proposed this generalized estimating equation approach to estimate the parameter in this system and then the Delta and the Jasa paper 1997.

76
00:09:40,110 --> 00:09:46,200
And that proposed a monte Carlo algorithm to estimate parameter in this system.

77
00:09:47,430 --> 00:09:58,589
So this is part of my dissertation 1989 that we work on the common estimate equation your references randomizer because

78
00:09:58,590 --> 00:10:06,930
I had a time I still remember that the first time I talked to my advisor that he's a Danish who is from Europe.

79
00:10:06,930 --> 00:10:11,430
And about my dissertation I see Peter.

80
00:10:11,430 --> 00:10:15,840
All the publications between you and me in the future will be alphabetical order.

81
00:10:16,680 --> 00:10:20,160
So that means he has a j. I have s.

82
00:10:20,520 --> 00:10:30,570
So he is always the first author. We published about seven or eight papers together and then you know, he is always the first author because he has J.

83
00:10:30,930 --> 00:10:38,549
So that's why I'm very interested, including people, these last names deceased or something like that, or anyway,

84
00:10:38,550 --> 00:10:45,840
but I always for my students as the first author because I feel that this syllabus to answer like I made like

85
00:10:45,840 --> 00:10:52,740
90% of even the less than that but anyway more than half of the contribution I did not give the first author.

86
00:10:53,040 --> 00:11:01,410
Right. But you know, that's what he said. I mean, after I'd be calling for Professor, I think the last people published it was Tucson eight.

87
00:11:01,740 --> 00:11:11,100
I awarded the group professor and I joke with him, I, I follow the rule we set up long time ago, like more than ten years ago.

88
00:11:11,100 --> 00:11:16,590
And your view? First author? I'm the last author. I mean, I'm in the alphabetical order.

89
00:11:16,980 --> 00:11:20,820
But anyway, that's that's a funny story that between me and my advisor.

90
00:11:20,820 --> 00:11:33,150
But anyway, published a paper in by Mexico where we do this comma estimating equation and some details in the chapter 612 in my book.

91
00:11:33,300 --> 00:11:36,300
Yeah and yeah.

92
00:11:36,300 --> 00:11:43,230
So talk about a possible extension and, and so on and so okay, let's go on.

93
00:11:45,870 --> 00:11:49,019
So here is a model that we can consider.

94
00:11:49,020 --> 00:11:59,700
I, like you have a time to state out like why t is the vector of, you know, outcomes of interest.

95
00:12:00,030 --> 00:12:10,470
Okay. And is time varying covers could be a baseline, could be baselines, not an unnecessary time, varying covariance.

96
00:12:10,920 --> 00:12:17,760
But if you're looking out some of time varying covariance, then this model can accommodate that.

97
00:12:17,880 --> 00:12:25,770
Okay. So suppose why t is d dimensional response vector of course in this particular case, so we can look at number of,

98
00:12:26,640 --> 00:12:34,500
you know, confirmed cases, number of that, so number of recovers from hospital or number of hospitalizations.

99
00:12:35,730 --> 00:12:40,440
So so whatever the outcomes, they are interesting to study.

100
00:12:41,000 --> 00:12:46,379
A jauntily from your database and then you can put two here and there.

101
00:12:46,380 --> 00:12:57,420
You make an assumption that according to this based belt model, the comp structure, you assume that for each.

102
00:12:58,800 --> 00:13:02,220
So you have the component for each component, right?

103
00:13:03,720 --> 00:13:14,910
So you'll need to have a conditional plus on distribution where the late in process is state i t so so you can imagine that

104
00:13:15,930 --> 00:13:26,460
so this is let's see if you have a non another white he I so for example this I say this is this first one this is the.

105
00:13:31,190 --> 00:13:38,809
Suppose this is the number of hospitalizations and this one false one.

106
00:13:38,810 --> 00:13:44,330
This is something. Then you have the underlying process.

107
00:13:44,490 --> 00:13:51,190
If you have this, then you could have, number one, the number of deaths sort of like not process going on.

108
00:13:51,230 --> 00:14:03,440
See the two teams minus one and y2t and you know, that's C, the two teams, so on, so forth.

109
00:14:03,890 --> 00:14:13,790
So we have this sort of the process where each y t component can be driven by its own underlying process.

110
00:14:14,900 --> 00:14:24,650
So in this case that you could have a dimensional theory on t christina t could be a factor of d dimensional rate.

111
00:14:24,660 --> 00:14:32,420
So 1t0, you know, you have the costs of this, right?

112
00:14:32,420 --> 00:14:38,040
So t from zero one number to a certain kind of t right.

113
00:14:38,060 --> 00:14:48,860
So you could have a vector of that. Okay. So, so you know that, of course, that the correlation happens among there's latent forces.

114
00:14:49,010 --> 00:14:53,930
Right. T to keep plus one also, of course.

115
00:14:55,280 --> 00:15:10,430
Y two plus one. So so you have the the com structure of Mount V for each component, but you know, you have D many of this copies.

116
00:15:10,730 --> 00:15:20,310
But there is a correlation along the lithium process because number of hospital the um, right number of hospital I mean the, you know,

117
00:15:20,600 --> 00:15:28,400
the lithium process that drives the number of the hospitalization should be clearly the length and process that drives our number of deaths.

118
00:15:28,520 --> 00:15:38,809
Okay. So, so, so you have this correlation copies and delay in process that that's the construction that you like to

119
00:15:38,810 --> 00:15:48,460
use to model the dynamics of the system so that your in the same time you have your covariance process.

120
00:15:48,470 --> 00:15:51,320
This could be also a component specific.

121
00:15:51,920 --> 00:16:02,180
Okay, so here you can put the vaccination, you can put on control measures, you can put some assistance qualities, whichever you think is appropriate.

122
00:16:02,270 --> 00:16:06,140
Okay. So of course that you have your Alpha I parameter.

123
00:16:06,440 --> 00:16:13,400
This is related to, you know, something about the particularly thing, right?

124
00:16:13,730 --> 00:16:19,730
So, so for example, you can have this treatment in part of this process.

125
00:16:19,730 --> 00:16:27,200
For example, on the the deaths, if the IV component is related to number of death,

126
00:16:27,740 --> 00:16:33,140
then you ought to really, you know, use the the treatment as part of this.

127
00:16:33,200 --> 00:16:44,690
Right. So and, you know, whether or not the the the breath assistance equipment would be helpful for people to recover from severe,

128
00:16:45,400 --> 00:16:53,570
you know, sometimes I como I como is the device that people use in hospital to help people to you know,

129
00:16:53,570 --> 00:17:05,389
breath and whether or not equable will be something useful to reduce the mortality rate for people with severe COVID symptoms or other symptoms.

130
00:17:05,390 --> 00:17:13,040
Right. So infectious disease. So that's the variable maybe you can interpret for this particular component.

131
00:17:13,040 --> 00:17:21,859
When I component refers to the year, the number of the, you know, the category of death that you try to study.

132
00:17:21,860 --> 00:17:31,340
So in principle you could have a different exi intradermal or according to different types of the, the component you're looking at.

133
00:17:31,700 --> 00:17:38,720
Okay, so, so for this kind of model than you assume that for the identify ability issue,

134
00:17:39,140 --> 00:17:50,870
you put this initial position as you know with mean one because otherwise the intercept term in the RFA would not be identifiable.

135
00:17:52,520 --> 00:17:56,360
Okay, that's the powerful, the observed process in the calm structure.

136
00:17:56,360 --> 00:17:58,820
And now, you know,

137
00:17:58,850 --> 00:18:13,610
have to specify this dynamics according to this Markov chain of process so that you have the latent process that are chi chi temporally dependent,

138
00:18:13,730 --> 00:18:23,360
follow certain distribution t that, you know, whatever the distribution you think is of should bo to describe this lethal process.

139
00:18:23,360 --> 00:18:28,710
As I already mentioned that the the advantage of this based.

140
00:18:28,820 --> 00:18:31,790
Always that you could, you know,

141
00:18:32,150 --> 00:18:44,000
formulate a p lacked process into a one lacked a latent process by using the factorization sort of technique streak that I demonstrate.

142
00:18:44,160 --> 00:18:51,590
Okay. So this is essentially the model set up where you have dimensional time series of counts.

143
00:18:51,710 --> 00:19:02,520
You'll want to join them, all of them. Okay. Now, so you have parameters coming from this observe process.

144
00:19:02,550 --> 00:19:06,710
They are for ice only. It will be the parameter of interest you like to study.

145
00:19:07,070 --> 00:19:10,550
And also we have a parameter coming from your latent process, right?

146
00:19:11,030 --> 00:19:24,980
So that whichever the Markov model that you use to describe this temple dynamics or evolution of this infection situations and you,

147
00:19:25,050 --> 00:19:29,090
there are some parameters that can arise from here.

148
00:19:29,240 --> 00:19:37,730
Okay. So now let's talk about the scale Zeus work on generalize estimating equation.

149
00:19:39,020 --> 00:19:44,899
Well, I think that, you know, basically it's just that, you know, let's just do a simple analysis.

150
00:19:44,900 --> 00:19:53,420
And the complication here is largely related to the latent process.

151
00:19:53,430 --> 00:20:05,060
If you really want to work out the latent process, then the the specification of latent process are fact whole dynamics.

152
00:20:05,600 --> 00:20:14,490
And if you look to observe process, this is just a positive model which is relatively simpler than the latent process.

153
00:20:14,510 --> 00:20:18,410
Leatham process. The Markle process can be very complex.

154
00:20:19,040 --> 00:20:27,020
And but the observed process is really just a possible regression model, which relatively easy to deal with.

155
00:20:27,530 --> 00:20:37,760
So his idea for the for the use of g or generalized equation is that I really want to estimate the RFA.

156
00:20:38,210 --> 00:20:38,530
Okay.

157
00:20:39,140 --> 00:20:48,590
By treating all the lead impulses as a nuisance structure or a nuisance parameter or anything in the lead in process will be regarded as nuisance.

158
00:20:48,590 --> 00:20:50,899
I'm like, second, our interest I,

159
00:20:50,900 --> 00:21:00,500
my interest is really to figure out how to estimate our fall so that I know how the X covers or the fact the, you know,

160
00:21:00,500 --> 00:21:10,880
the, the average number of the counts or number of the average, the rate of, you know, average rate of death or arbitrary rate of hospitalization.

161
00:21:11,090 --> 00:21:19,219
So also force I'm interested in the association parameter in the process model by treating all

162
00:21:19,220 --> 00:21:25,940
the late instructor as a nuisance sort of parameter or nuisance part of this joint dispute,

163
00:21:25,970 --> 00:21:29,900
of course that the whole system involves both processes.

164
00:21:29,930 --> 00:21:37,339
Right. The observe process, follow the conditional process distribution that you have a markov process latent.

165
00:21:37,340 --> 00:21:43,310
But this one, if you I'll take this one step sort of simpler.

166
00:21:43,580 --> 00:21:47,750
So once the back to simplify this whole process, then you say, gee,

167
00:21:47,750 --> 00:21:56,329
I just want to estimate our form and treating the even process as nuisance that I could probably have

168
00:21:56,330 --> 00:22:03,440
a fast and stable algorithm to really estimate the parameter of interest in this kind of model.

169
00:22:04,310 --> 00:22:11,930
But I, I think that kind of point of view is good in the sense that if you are only interested in association,

170
00:22:12,110 --> 00:22:15,470
okay, how this covariate affects your outcome.

171
00:22:15,860 --> 00:22:23,900
But if you really want to do prediction, right, if you really want the prediction that this latent process will be evolved.

172
00:22:24,350 --> 00:22:29,690
So you're really on the understand how they lay them passes, evolves to the future time points.

173
00:22:30,080 --> 00:22:38,660
You'll really need to figure out somewhat some kind of detail of the latent process in order to make a good prediction to the future data point.

174
00:22:40,430 --> 00:22:48,530
The G does not really provide that kind of detail because between the leading process as nuisance structure or prioritization.

175
00:22:49,220 --> 00:22:55,840
And so that prediction becomes a little bit trickier in the a g context.

176
00:22:56,090 --> 00:23:01,309
So that's something I try to solve. Will know I was working my my dissertation.

177
00:23:01,310 --> 00:23:07,100
I really want to do common theater so I'm not going to do generalized estimation function.

178
00:23:07,100 --> 00:23:12,290
I do comma estimate function so that I can bring common future into this because now you see

179
00:23:12,290 --> 00:23:17,600
the common futurity has have learned allows you to really utilize the Markov structure,

180
00:23:17,990 --> 00:23:25,340
not the distribution performance, the first moments of this Markov process to predict something in the future.

181
00:23:25,340 --> 00:23:28,490
Time So that I can really do prediction. So that's.

182
00:23:29,120 --> 00:23:38,480
One of the primary consideration why I want to do something different from TV at that time when I work on asthma data is,

183
00:23:39,020 --> 00:23:43,220
you know, the air pollution on the asthma hospital admissions.

184
00:23:43,350 --> 00:23:48,440
Okay. Anyway, so let's just go go over the key idea.

185
00:23:48,470 --> 00:23:54,170
Okay. She is very, very popular is the method that have been widely used in analysis of longitudinal data.

186
00:23:54,410 --> 00:24:00,110
If you have taken 653, you this is not something new to you at all.

187
00:24:00,800 --> 00:24:07,310
Right. So it is quite a popular method to deal with longitudinal data.

188
00:24:07,310 --> 00:24:21,530
And Ziegler also developed a method to deal with this dynamic on sort of modeling a structural framework that to estimate parameters.

189
00:24:21,740 --> 00:24:27,740
Okay. So what's the difference between longitudinal data and this data?

190
00:24:28,160 --> 00:24:34,850
Well, the difference here is longitudinal error is more many, many short time stories.

191
00:24:34,880 --> 00:24:39,650
Right. So, you know, longitudinal you have taken 653.

192
00:24:39,660 --> 00:24:43,630
Right. So longitudinal data is you have all short time streams.

193
00:24:44,660 --> 00:24:52,430
All right. So so you have to pick one, two, three, four by the end.

194
00:24:53,120 --> 00:24:56,600
So everybody has like two or three visit, like ten days.

195
00:24:56,780 --> 00:25:01,100
You have many, many short time stories. That's basically the longitudinal data.

196
00:25:01,610 --> 00:25:16,520
So so you want to really summarize the information cross subjects in order to understand the sort of time of varying sort of dynamics or pattern.

197
00:25:17,510 --> 00:25:24,790
So this statistical inference, right, in longitudinal data is really coming with angles to.

198
00:25:25,970 --> 00:25:33,530
So that is if I added more and more subjects in my study, each one contribute, each one contributing one short time measures.

199
00:25:33,830 --> 00:25:37,860
What is the relationship between my outcome and my callback?

200
00:25:38,390 --> 00:25:46,700
That's longitudinal data. So if I ask so these efficient users is based on the number of subjects goes to infinity.

201
00:25:47,060 --> 00:25:53,810
So if I add more and more subjects in my study, what is the relationship between the outcome and my covariance?

202
00:25:54,050 --> 00:25:58,640
That's longitudinal. So what is this modeling the model we're were talking about?

203
00:25:59,060 --> 00:26:04,230
So here we talk about the long time studies.

204
00:26:05,060 --> 00:26:11,900
We only have field. Right. So we have we don't have many of this long time process.

205
00:26:12,500 --> 00:26:19,970
So we only have a few of them. Number of hospitalizations, number of deaths, number of confirmed cases.

206
00:26:20,150 --> 00:26:23,600
Right. So we don't have very field.

207
00:26:24,680 --> 00:26:29,510
So aim is small if you think end this number.

208
00:26:29,720 --> 00:26:30,570
Number of times.

209
00:26:31,610 --> 00:26:44,390
So where is the information arises in this context where whole time you have really long between this number of data with number of measurements.

210
00:26:44,840 --> 00:26:48,830
So this is short time series.

211
00:26:49,580 --> 00:26:55,100
This is longitudinal, mainly short time.

212
00:26:55,220 --> 00:27:00,710
So that's longitudinal data. So you have field long time source.

213
00:27:04,910 --> 00:27:10,770
So that's the data structure in the infectious disease model where you have, oh,

214
00:27:11,900 --> 00:27:17,060
you are able to use the surveillance system to model like daily data and weekly data.

215
00:27:17,090 --> 00:27:26,870
You have a lot of data over time. So here the information comes from the input of this, along with the increase of subjects here,

216
00:27:26,870 --> 00:27:31,040
information comes from the accumulation of their time series data.

217
00:27:31,370 --> 00:27:34,340
So basically inference coming from Peterson Field.

218
00:27:35,300 --> 00:27:43,520
So your extend your lens, you follow this of sort of a measurement of the number of hospitalization.

219
00:27:43,520 --> 00:27:54,500
And more and more, more, more, more, more. So your information is really coming from the situation one number of measurements over time.

220
00:27:55,460 --> 00:27:59,750
Okay. Goes to infinity. So so this statistic for us is completely different.

221
00:27:59,840 --> 00:28:03,990
Here you have such an exclusive infinity. Here you have time goes to infinity.

222
00:28:04,010 --> 00:28:10,340
Okay. So that's the fundamental difference between the longitudinal data and the modeling I'm talking about here.

223
00:28:10,560 --> 00:28:19,930
Okay. So sometimes I use the a little actually is capital T.

224
00:28:20,320 --> 00:28:27,340
The aim here is not number of subjects is different from the traditional notation.

225
00:28:27,340 --> 00:28:31,180
We're using statistical loops, you know, for everything.

226
00:28:34,050 --> 00:28:45,580
Okay, let's put this way. So here a is number of the, you know, the repeated measurements, four times three like you knew.

227
00:28:46,030 --> 00:28:51,310
So this is a the length of time, stress, not number of subjects.

228
00:28:51,570 --> 00:28:54,580
Okay. So fundamentally, they have little difference.

229
00:28:54,610 --> 00:28:58,030
Okay. Then you understand better structure. Okay. That's very important.

230
00:28:58,030 --> 00:29:08,920
We do estimation formulate like you whatever to to to really find out how you are going to do your statistic.

231
00:29:09,130 --> 00:29:14,650
This statistic, I mean, put it very general, this sense is really just, you know,

232
00:29:15,220 --> 00:29:23,950
the information from many data points to get your statistics or statistics kind of summary of certain randomness, right?

233
00:29:24,370 --> 00:29:31,090
So where the summary can be done, what are the information you should use to create summary information?

234
00:29:31,720 --> 00:29:36,940
In this context, we're using the country's long time source.

235
00:29:37,180 --> 00:29:37,420
Okay.

236
00:29:37,690 --> 00:29:48,820
The length of time strokes gives you the information that you are supposed to summarize your data over the are the time not over number of subjects.

237
00:29:48,880 --> 00:29:59,860
Okay. So that's you have that's something you need to understand because for surveillance data, we have very limited, limited access to individuals.

238
00:30:00,130 --> 00:30:08,530
You are working on a homework tool, right? You look at a data, it's very hard to even find age group from 12 to 18.

239
00:30:08,530 --> 00:30:11,710
They're looking at their desperate rates. They don't give you.

240
00:30:11,770 --> 00:30:19,090
Of course, if you want, I'll come to see. Do you see? Maybe you have it. But for an outsider you can only have this aggregated thing.

241
00:30:19,360 --> 00:30:22,450
What do you have? Is really rich information over time.

242
00:30:23,110 --> 00:30:27,010
Okay, you don't have individual data, usually don't get it.

243
00:30:29,140 --> 00:30:35,830
So g for estimate the equation you can interpret estimate the increasing as method moments.

244
00:30:36,040 --> 00:30:44,019
Okay. In economics, right? Economics people do not really use as the main equation.

245
00:30:44,020 --> 00:30:50,720
They most of time use method moments, so it's roughly equivalent.

246
00:30:50,740 --> 00:31:01,450
LC Okay. And yeah, I think statisticians are not very good at creating aims like estimating equation.

247
00:31:01,480 --> 00:31:08,260
It may be mathematically meaningful, but it refutes people don't use this terminology.

248
00:31:08,380 --> 00:31:17,040
Okay. And anyway, so maybe machinery people call this learning process, right?

249
00:31:17,230 --> 00:31:24,670
Learning this is called whatever reinforcement learning or D learning whatever.

250
00:31:24,970 --> 00:31:28,480
So they don't really call this estimating equation.

251
00:31:28,870 --> 00:31:35,859
Well, but anyway, so this is a equation that will be used to do estimation.

252
00:31:35,860 --> 00:31:44,110
So that's called estimating equation. And this estimate question will be formed based on the first two marginal movements.

253
00:31:44,770 --> 00:31:50,139
Okay. So estimate equations are formed by movements, margin lies and latent process.

254
00:31:50,140 --> 00:31:54,940
Why you want to marginalize latent process? Because you don't want to deal with it, right?

255
00:31:54,940 --> 00:32:01,030
So when you marginalize lethal processes, you just kill the latent process, right?

256
00:32:01,270 --> 00:32:08,620
If you if you have a joint distribution with three variables, you don't want to consider the third variable, what do you do?

257
00:32:08,650 --> 00:32:16,210
You do modulation, right? You just, you know, marginalize the third variable that's super variable will be gone from your distribution.

258
00:32:16,570 --> 00:32:24,060
So, so marginalization is the way to simplify your model to do this aggregation of

259
00:32:24,700 --> 00:32:30,370
so you marginalize the latent process and then you can have this marginalized.

260
00:32:30,790 --> 00:32:38,859
So the first two moments that will allow you to form estimating questions or give you more than conditions that you can form,

261
00:32:38,860 --> 00:32:42,880
you're estimating the equations to estimate a parameter.

262
00:32:43,870 --> 00:32:54,190
So in other words, that late in process is not estimated empty by the writer being treated as source of dependance about the mark of ten.

263
00:32:54,580 --> 00:32:59,110
So you understand that there is a latent Markov process.

264
00:33:00,520 --> 00:33:04,389
They contribute to temple dependance, but you don't want directly.

265
00:33:04,390 --> 00:33:10,750
Do you deal with that and you just marginalize that? Marginalization will allow you to generate some dependance, all of it.

266
00:33:11,760 --> 00:33:15,540
As I said, that the primary interest here is the parameter of our fall.

267
00:33:16,080 --> 00:33:22,390
That's really something you focus on energy. So what is the first moment?

268
00:33:22,440 --> 00:33:26,520
For first moment, of course, is the mean, marginal meaning of your life.

269
00:33:26,550 --> 00:33:35,760
So. So Y to the super m means that you have this what, the end dimension of vector, right?

270
00:33:36,870 --> 00:33:40,950
That so that you know you can't have the mean and then.

271
00:33:41,310 --> 00:33:47,310
And each position you have a dimension. The marginal mean parameter, which is the function of you are fact.

272
00:33:47,640 --> 00:33:57,990
That's easy to understand, right? This is meanwhile coming from your problem distribution and each at a each time the mean the meaning

273
00:33:57,990 --> 00:34:07,200
of this moment right is a three dimensional and each one of this is the marginal meaning of that.

274
00:34:07,530 --> 00:34:10,919
Okay, but there is a typo. There should be.

275
00:34:10,920 --> 00:34:17,000
W I'm sorry. W Okay. Okay.

276
00:34:18,060 --> 00:34:28,200
So and each time you're wide eyed eye as ice component of your data.

277
00:34:28,290 --> 00:34:36,570
Right. So that's a dimension of factors for why one team and why the team.

278
00:34:36,870 --> 00:34:43,800
Right. So that's basically the the data and you calculate the meaning of i.t.

279
00:34:45,090 --> 00:34:49,700
Right. Then you use double expectation argument.

280
00:34:49,710 --> 00:34:55,160
Why it. There's a type of there should be double expectation given your feedback.

281
00:34:56,010 --> 00:35:05,780
I.t. Right. And and this one is small as part of distribution.

282
00:35:05,780 --> 00:35:11,650
Right this way. It follows conditionally conditional on weight and price follow up, possible distribution.

283
00:35:11,660 --> 00:35:14,660
That's your model, right? So that's the model.

284
00:35:14,660 --> 00:35:24,380
This is a I t see that I t because profit distribution has the mean of that according to pricing model.

285
00:35:25,190 --> 00:35:31,099
And it is a, it's not random.

286
00:35:31,100 --> 00:35:41,930
It's we fix all recovery process, right? So we have a fixed design in this case, especially if you want, you can add to really get access to it.

287
00:35:42,170 --> 00:35:49,100
I guess for simplicity, I just assumed that all the exit prices are fixed or consider fixed design just like in normal.

288
00:35:49,100 --> 00:35:58,310
Right? So now we calculate exploitation of t i t which are assumed to be one because this is by construction.

289
00:35:58,670 --> 00:36:05,420
So I wanted the the Oracle parameter to be identifiable in the observer observe process.

290
00:36:05,420 --> 00:36:08,900
I need to put the delete in process this one equal to one.

291
00:36:10,310 --> 00:36:16,610
So that's basically calculation you can see here, use the double expectation document.

292
00:36:17,420 --> 00:36:25,460
So that's if you take the log of that, then essentially you have locking in your model, right?

293
00:36:25,580 --> 00:36:35,450
So I t is modeled as the exponential of, you know, of X, ITV pulls of R5.

294
00:36:36,590 --> 00:36:44,870
And then if you take log of that, basically the log of log of this is log of a key.

295
00:36:45,710 --> 00:36:52,670
Essentially log of IP is really just a XIV transpose on our own.

296
00:36:53,750 --> 00:36:58,660
So this is log in or model so that it's easy to answer that.

297
00:36:58,790 --> 00:37:04,790
Okay. So next part that you need to work on is the second moment of this process.

298
00:37:05,360 --> 00:37:11,360
So we d know the covariance matrix of y and so what is the Y?

299
00:37:11,360 --> 00:37:15,110
And you can imagine that Y is a matrix in this case.

300
00:37:15,110 --> 00:37:23,870
Right. So, so y since this wiring is what is the divide in matrix.

301
00:37:24,470 --> 00:37:35,350
So you can see that at the time y you have a dimensional vector and, and this is the time one time to you have the dependency vector and so forth.

302
00:37:35,360 --> 00:37:40,250
So force at time end you collect another dimension vector, right?

303
00:37:40,550 --> 00:37:47,780
So you have a time, series of dimensional time sure is collected here and then you have Devi M Matrix.

304
00:37:48,260 --> 00:37:51,830
So you are talking about the covariance matrix of this matrix.

305
00:37:54,980 --> 00:38:06,770
So, so what we are trying to do here is really set up a after you get the two to after you get this to host the conditional moment,

306
00:38:07,340 --> 00:38:11,540
then you can create this estimate equation.

307
00:38:11,870 --> 00:38:17,510
Okay, so how you're going to find out the the estimate of our fun.

308
00:38:18,200 --> 00:38:25,400
Well this is the zeekr suggests that you're trying to find the root of this equation.

309
00:38:26,300 --> 00:38:34,250
Okay. So the equation here involves your data, which is two Y and also part of x imparted in amu.

310
00:38:35,120 --> 00:38:40,249
Okay. Oops, the amu is your meaning.

311
00:38:40,250 --> 00:38:48,560
Meaning of course this function of your covered. So our forecast, the parameter RFA and the the covers here.

312
00:38:49,370 --> 00:38:53,330
So you just find the root of this estimate equation.

313
00:38:53,810 --> 00:39:03,560
Okay, so here is the, the covariance of this residual universe of the covers of the residual,

314
00:39:03,890 --> 00:39:10,430
and here is the force of the derivative of this mean parameter with respect to the parameter of interest.

315
00:39:11,270 --> 00:39:15,770
Okay, there's a argument why you want to put this whole factor.

316
00:39:20,380 --> 00:39:26,290
So if you'll go back to the the original estimate in Funky Theory.

317
00:39:33,080 --> 00:39:37,590
You say what? Why you have this very ad hoc way to specify this, right?

318
00:39:38,330 --> 00:39:44,180
So a British statistician named Crawford.

319
00:39:53,040 --> 00:39:58,980
He studied this problem. So if I want to ask too many questions.

320
00:40:00,420 --> 00:40:03,480
So. So from here, you can solve this.

321
00:40:04,020 --> 00:40:07,980
You certainly can solve your word, your root.

322
00:40:08,370 --> 00:40:15,690
There, you can put a set of arbitrary images to solve here.

323
00:40:17,310 --> 00:40:25,350
So the question here is, what is the ultimate aim such that the solution will be efficient?

324
00:40:26,070 --> 00:40:27,330
Okay, that's this question.

325
00:40:28,260 --> 00:40:36,120
So you can put all of different pay in front of this residual to create a measurement, a different type of estimate, an equation to estimate.

326
00:40:36,480 --> 00:40:42,750
So you are right. This is often my case.

327
00:40:43,230 --> 00:40:49,170
So the question here is, if you choose different pay here, you have different type of estimate of our fund.

328
00:40:49,690 --> 00:40:57,510
Now, what is type of a at which that the the difference of standard URL of the estimate would be smaller?

329
00:40:57,840 --> 00:40:59,580
What will be most efficient.

330
00:41:00,570 --> 00:41:15,510
So what what is the that the optimal gauge that gives you better efficiency and improved improved in theory the optimal a is inverse the universe,

331
00:41:15,510 --> 00:41:19,350
the variance, the y times the derivative of this.

332
00:41:22,560 --> 00:41:33,460
So this is the optimal. So it's not from Zeiger, it is from quarter margin quarter.

333
00:41:34,180 --> 00:41:39,430
He proved that if you want to make this kind of estimate increase,

334
00:41:39,970 --> 00:41:46,030
what's the best the coefficient you can have based that coefficient in terms the optimal

335
00:41:46,030 --> 00:41:53,590
efficiency and this coefficient is the the first order of derivative basic gradient.

336
00:41:54,070 --> 00:42:03,940
This is gradient of your parameter function times the dependance inverse of dependance of a position matrix of your way.

337
00:42:03,970 --> 00:42:04,470
Okay, that,

338
00:42:04,510 --> 00:42:17,770
that's that's all coming from Zynga and do you just use this existing now was 9084 paper of by mining product but anyway so improve this and you'll

339
00:42:17,770 --> 00:42:31,450
see that of course that when you calculated years of the Y Y is not only coming from the observe process the Y is driven by the latent process theta.

340
00:42:31,960 --> 00:42:37,840
So when you catch the marginal variance of your Y and the latent process will

341
00:42:37,840 --> 00:42:43,719
play the role and fire will be the primary coming from the latent process,

342
00:42:43,720 --> 00:42:55,330
whichever it is. But there is a kind of parameter unknowns in the late in process dot b o that will be

343
00:42:55,330 --> 00:43:01,270
evolving the of the margin variance of your Y we marginalized group latent process.

344
00:43:01,660 --> 00:43:10,600
Okay. This is a nuisance parameter in this context because this estimate increasing is trying to solve or estimate

345
00:43:10,600 --> 00:43:18,940
the parameter of our for this estimate equation does not allow you to estimate the parameter nuisance

346
00:43:18,940 --> 00:43:26,499
parameter five this is null estimate inflation for you to estimate counter fi only allow you to estimate

347
00:43:26,500 --> 00:43:32,140
the function estimating parameter view so that you basically treat the fire as a nuisance parameter.

348
00:43:32,650 --> 00:43:35,770
No fire is unknown. How do you deal with that?

349
00:43:35,770 --> 00:43:46,120
When you want to solve this for our fire, then you can plotting any squirt of like squirt of squirt with consistent estimate of anything.

350
00:43:47,650 --> 00:43:53,500
We can prove that. Then you can get consistent estimate our fun and enjoy efficiency.

351
00:43:53,500 --> 00:44:02,889
So in the software, what you're really working on here is the the estimate of your alpha using this estimate

352
00:44:02,890 --> 00:44:08,950
increasing where this nuisance primer has to be replaced by a good and consistent estimate.

353
00:44:09,250 --> 00:44:24,520
Okay, that's what happens. So you practice that reverting this thing is quite challenging and so so you have to deal with and deeper in the matrix.

354
00:44:24,850 --> 00:44:34,540
Okay. So in practice the way you deal with the multi factor here is that you stack them together, right?

355
00:44:34,540 --> 00:44:38,620
So you move this second bar here and third bar here.

356
00:44:39,010 --> 00:44:46,419
So you create a anti dimension or grant of being vector.

357
00:44:46,420 --> 00:44:55,990
So you stack them together one by one to create a huge vector so that you can work on this end debate and the covariance matrix.

358
00:44:56,440 --> 00:45:00,380
Of course, this can be very high dimensional when you deal with it.

359
00:45:01,060 --> 00:45:04,330
It will take a lot of computing resources to deal with it.

360
00:45:04,600 --> 00:45:09,879
Okay. But anyway, that's what Skull Ziglar suggested.

361
00:45:09,880 --> 00:45:14,400
And, and, and okay.

362
00:45:14,800 --> 00:45:16,420
Under the this space model.

363
00:45:16,510 --> 00:45:30,790
Okay, we can you know, this model we just specified here write this model structure and we can work on what called it marginal covariance matrix.

364
00:45:32,380 --> 00:45:40,960
So the margin covariance matrix or equal to the conditional expectation condition variance plus the virtual conditional mean.

365
00:45:42,040 --> 00:45:51,550
So for this conditional part, right. So we know based on distribution so that.

366
00:45:55,970 --> 00:46:02,000
It becomes the. Right.

367
00:46:02,360 --> 00:46:14,510
The first part you look at the divergence of and okay, so that will be e0y and give in to that and.

368
00:46:14,880 --> 00:46:21,560
Right. So, so this this one would be under the possible distribution.

369
00:46:21,560 --> 00:46:30,020
This will be on a T and I'll see I t you can you imagine that for, for,

370
00:46:30,330 --> 00:46:37,610
for a generic terms here that is the variance of the parcel distribution will be same as the mean.

371
00:46:38,410 --> 00:46:43,940
Then you take the expectation of this that really gives you the meaning of C,

372
00:46:43,940 --> 00:46:52,790
the R is one plus equal to one, so that essentially you get the diagonal matrix for this one.

373
00:46:53,090 --> 00:46:57,470
Okay. And now you're looking at the variance of that.

374
00:46:57,920 --> 00:47:15,800
Okay. The second term under this model, states be small so that the expectation the Y and the end will be I t you know,

375
00:47:16,100 --> 00:47:24,860
say that, you know, this is big vector, right for a long term like this and then you take the variance of that, right?

376
00:47:25,430 --> 00:47:32,900
Take a variance of that. And then, as I say, down all the lead in process are correlated.

377
00:47:33,650 --> 00:47:44,570
So the covariance matrix of this will be equal to the, uh, the, the, the, a part, you know,

378
00:47:45,410 --> 00:47:53,930
this part times the covariance matrix of theta and the, the coefficient part of, of, of that.

379
00:47:54,520 --> 00:47:58,300
Okay. So that will be, you can work out that.

380
00:47:58,310 --> 00:48:07,100
So that gives you this kind of uh, of sort of sandy sandwich form that you have a G and you have the,

381
00:48:07,670 --> 00:48:17,390
a part that happened over the middle part and also the covariance matrix of the or C the end and then the transpose of this data.

382
00:48:17,780 --> 00:48:26,150
That's what you have. Mm hmm. So now what you need to do here is we want to take a this is your V, right?

383
00:48:26,690 --> 00:48:36,979
This is your V. Okay, so, so in in this matrix, this is a V, then you need to get E, you know, inverse matrix of this V,

384
00:48:36,980 --> 00:48:44,150
then you certainly need to work out a universe matrix of gamma, which is the covariance matrix of the theta process.

385
00:48:44,660 --> 00:48:48,500
And then it could be a very computation intensive.

386
00:48:49,190 --> 00:48:52,430
Okay. So here is the contribution from Xga. Okay.

387
00:48:53,540 --> 00:49:00,800
Basically, the idea of TV is he said, well, the gamma this gamma theta is so complicated.

388
00:49:01,280 --> 00:49:05,630
Can I do a can I discarded?

389
00:49:05,900 --> 00:49:10,550
Can I just ignore it? And that's what are you trying to do?

390
00:49:10,640 --> 00:49:13,880
He has this idea of working covariance matrix.

391
00:49:14,360 --> 00:49:21,200
I don't want to use the covariance matrix obtained by marginalization of your states based model.

392
00:49:21,740 --> 00:49:28,070
I just want to impose a alternative covariance matrix that I can easily compute.

393
00:49:28,430 --> 00:49:34,550
Of course, if you stay away from the actual actually covariance matrix you're doing,

394
00:49:34,790 --> 00:49:39,650
you'll miss specify your cause matrix under whatever model is specified, right?

395
00:49:40,010 --> 00:49:43,860
Your covariance matrix no longer the covariance matrix general.

396
00:49:43,860 --> 00:49:51,409
If your model, you propose a alternative covariance matrix that's easy to compute, that's misplaced,

397
00:49:51,410 --> 00:49:59,059
but transmits heat and also misfires misses I that convinced me so he's just a working commitment matrix I

398
00:49:59,060 --> 00:50:06,200
just wanna replace this with this this complicated stuff I just want to replace this conflicting stuff.

399
00:50:06,200 --> 00:50:16,429
I know that you're gonna feel, you know, this matrix is very complex, this coming from your latent Markov process.

400
00:50:16,430 --> 00:50:23,240
I don't want to deal with it. I just want to replace this thing by some simpler and easy can compute.

401
00:50:24,080 --> 00:50:30,130
Well, is this legal? Well, that's really the question about this is really the contribution of G.

402
00:50:32,780 --> 00:50:35,870
So these are suggested replacing this complex things.

403
00:50:35,960 --> 00:50:41,990
These are work. It creates matrix, this much simpler structure that's easy to compute and to estimate the nuisance parameter.

404
00:50:41,990 --> 00:50:51,020
But that's the basic idea. He just wanted to do something different, not just the kind of the original matrix generate from the model specified.

405
00:50:51,680 --> 00:50:56,180
So he won't approximate this and this is the way we proposed.

406
00:50:56,390 --> 00:51:00,410
Okay. So I want that to be to be specified this.

407
00:51:01,760 --> 00:51:15,620
Okay. That's up to his choice. And here the deal is that the time, the matrix of this and the karma is star is a matrix of error.

408
00:51:15,620 --> 00:51:19,850
One structure. Okay. That's what he tried to do.

409
00:51:19,910 --> 00:51:28,130
He said, Look, I want to replace this thing by a simple matrix.

410
00:51:28,700 --> 00:51:34,880
This matrix is a, for example, altering trig auto regressive L1 structure.

411
00:51:35,090 --> 00:51:38,150
Of course, this is not the same as the previous one.

412
00:51:38,900 --> 00:51:44,300
I mean, that's the whole purpose. You have a working correlation structure or concavity structure,

413
00:51:44,840 --> 00:51:50,630
and that's basically gives you the advantage of, you know, easier computing and so on.

414
00:51:51,170 --> 00:51:58,790
Of course, you will lose the efficiency, but you can do computation efficiency whilst you do this.

415
00:51:59,090 --> 00:52:02,749
Okay. What are you trying to do here is okay. You have sigma.

416
00:52:02,750 --> 00:52:07,010
Where is the the over dispersion parameter.

417
00:52:09,040 --> 00:52:11,420
This this is over dispersion parameter. Okay.

418
00:52:11,840 --> 00:52:19,400
Essentially what Ziegler suggested in this context, this is essentially the negative binomial distribution.

419
00:52:19,550 --> 00:52:24,890
Okay. So if you consider over dispersion in this case due to the marginalization.

420
00:52:25,790 --> 00:52:34,339
So when you have plus one comma distribution in the homework, you know that you get this negative binomial distribution based over dispersion.

421
00:52:34,340 --> 00:52:41,659
He said, okay, let's work on this marginal mean of marginal variance which has this over dispersion

422
00:52:41,660 --> 00:52:47,660
phenomenon and put this covariance matrix correlation matrix in a simpler form.

423
00:52:48,380 --> 00:52:59,990
And then I can enjoy the simple numerical sort of calculation and that he and others like this can replace Gamma Star by a

424
00:53:00,050 --> 00:53:10,550
simple error one structure and estimate the parameter of this as of operation parameter using external method moments method.

425
00:53:10,940 --> 00:53:18,440
And then plotting here you get could estimate equation and the estimate from the beta.

426
00:53:18,950 --> 00:53:25,370
Okay, so while what we prove here is that this one,

427
00:53:26,240 --> 00:53:35,210
the alpha hat as a solution from this of the root of this estimate equation under some might be the very condition

428
00:53:35,720 --> 00:53:43,940
would have would be would be a consistent estimate and sometime the normal risk and sandwich covariance matrix.

429
00:53:44,100 --> 00:53:51,680
Okay. That's basically g method. So what are we trying to what what if we follow his idea?

430
00:53:51,710 --> 00:54:01,550
Right. The idea here is that he he basically did this surgery for this very structure before.

431
00:54:01,550 --> 00:54:06,959
The variance structure is very complex. He just replaced the variance here, the inverse.

432
00:54:06,960 --> 00:54:10,130
The variance by something simpler. Okay.

433
00:54:10,940 --> 00:54:17,530
And this simplification for this variance structure will allow you to have, you know,

434
00:54:17,540 --> 00:54:23,119
a fast numerical computing, a moore's numerical stability, but an end of the.

435
00:54:23,120 --> 00:54:27,320
You still have consistent estimate for your estimate, our fall,

436
00:54:27,890 --> 00:54:35,960
which is the parameter a primary interest to be a asymptotically unbiased estimate by conciseness.

437
00:54:36,530 --> 00:54:42,500
And also you have a part in normality for this estimate or so that you can do statistic inference.

438
00:54:43,580 --> 00:54:46,310
Okay. So. So that you ask.

439
00:54:46,760 --> 00:54:55,580
Well, sounds too good to be true that what is the loss in this simplification or approximation of your covariance matrix efficiency?

440
00:54:56,540 --> 00:55:01,520
Okay. So if you want. If you use original method,

441
00:55:03,200 --> 00:55:14,090
if you want to follow this or you events like you may need the 500 subjects to reach like 80% power or something like that.

442
00:55:14,540 --> 00:55:25,250
Now you feel you do still method. You may need 8800 subjects so your confidence interval is wider for 440 method.

443
00:55:25,730 --> 00:55:31,160
You need more data point in work to get the shorter confidence interval.

444
00:55:31,490 --> 00:55:44,510
The same. Same same values of confidence about you know, in comparison to the one obtained by the original model requires more data points to do that.

445
00:55:44,720 --> 00:55:48,860
So that's basically loss of support, right? So yeah.

446
00:55:48,870 --> 00:55:58,399
So you'll enjoy the consistency and some popularity of these, a simple numerical calculation,

447
00:55:58,400 --> 00:56:03,559
but you need more data points in order to achieve the same level of statistical

448
00:56:03,560 --> 00:56:08,110
efficiency or status power if you compare it to the one you used to achieve.

449
00:56:09,020 --> 00:56:18,080
Okay. So so the question here is really this sacrifice issue where this loss is something desirable.

450
00:56:18,500 --> 00:56:24,880
I mean, the big data out there, like so we have a lot of data sample size, maybe this is not a very big deal,

451
00:56:24,890 --> 00:56:30,400
but in the clinical study, every data point is very expensive to collect.

452
00:56:30,410 --> 00:56:34,309
So, so, you know, I mean, I don't know in the universities,

453
00:56:34,310 --> 00:56:44,150
this may be the field that this sample size may not be a issue because we can, you know, collect data all the time.

454
00:56:44,150 --> 00:56:53,270
But on our hand, that the more data probably are subject to, you know,

455
00:56:53,480 --> 00:56:57,410
the measurement error, because there are a lot of underreporting issue, data quality issue.

456
00:56:57,410 --> 00:57:08,190
And I don't know, like there's always a issue like how, how, how the data quality of fact in results is more data points.

457
00:57:08,190 --> 00:57:12,209
The better or more data points bring more noise in the text.

458
00:57:12,210 --> 00:57:18,920
That is analysis. There's always a issue of not a clear one side, a winning.

459
00:57:18,920 --> 00:57:28,400
I mean, more complex than that. So if you use error one structure, of course, this is the you know, the error or one dependent structure.

460
00:57:28,610 --> 00:57:35,120
Okay. So T and S is the distance between to time point.

461
00:57:35,600 --> 00:57:38,900
So T could be the first thing and S could be the third thing.

462
00:57:39,050 --> 00:57:47,560
Right? So you have this. If you're right it all right.

463
00:57:49,480 --> 00:57:57,880
So let's say in our one hour correlation structure causation matrix.

464
00:58:00,240 --> 00:58:03,780
So what is that? Let me just work on a four by four matrix. Okay.

465
00:58:04,950 --> 00:58:08,970
So if you work on a four by four matrix, then you have, of course, this correlation matrix.

466
00:58:09,630 --> 00:58:15,270
The dialog elements must be one. So we have row, row, square and killed it.

467
00:58:16,020 --> 00:58:23,010
But no, here I use say. Okay.

468
00:58:23,020 --> 00:58:29,620
Here is Baci. And this is sign, say, square by square sign and.

469
00:58:33,090 --> 00:58:40,020
Cupid. So this is a four by four nature like called of course want matrix.

470
00:58:40,380 --> 00:58:43,610
You can generalize this core issue matrix to end.

471
00:58:44,430 --> 00:58:49,320
Right. So. So of course, this is the counter that you need to estimate.

472
00:58:50,770 --> 00:58:54,930
So in t we do not estimate this portion.

473
00:58:55,410 --> 00:59:01,950
This is called nuisance parameter, all the quotation parameter. So this is the parameter in the working condition structure.

474
00:59:02,310 --> 00:59:09,330
So in the g you do not provide any estimating function to estimate this.

475
00:59:09,780 --> 00:59:21,660
Right. You just use a simple r you know, the normal conditions because this is autocorrelation so that you can just use this autocorrelation.

476
00:59:22,170 --> 00:59:30,360
So foci is the auto correlation of like one dependance so that you have y t this

477
00:59:30,360 --> 00:59:36,599
residual times one lakh difference and adding up has scale by this dispersion parameter

478
00:59:36,600 --> 00:59:41,850
and add them together across all the D components because you assume that they're

479
00:59:41,850 --> 00:59:49,470
all the same under this old in the latent process or to have this same A1 structure.

480
00:59:50,160 --> 00:59:53,700
And then you just standardize by the variance, right?

481
00:59:54,360 --> 00:59:58,740
The variance of this is the mean because you have proximity distribution.

482
00:59:59,520 --> 01:00:02,590
So you can estimate this, uh,

483
01:00:03,030 --> 01:00:12,090
all the correlation function or condition parameter because this is lack one time dependance of same time you can estimate the dispersion parameter.

484
01:00:12,250 --> 01:00:22,709
Okay. That's easy. So people say, well, how about I just told you, you know, operation timber operation.

485
01:00:22,710 --> 01:00:28,740
I just do not want to deal with the latent process that that's also a possibility.

486
01:00:28,750 --> 01:00:32,850
You can use independence or concordance structure.

487
01:00:33,570 --> 01:00:39,960
So essentially what you're trying to see here is this your y structure would have say equal to zero.

488
01:00:40,890 --> 01:00:48,660
I have a dependance, right? So I don't have any out because of course, this tool run, you know, the data are temporally recorded,

489
01:00:48,660 --> 01:00:56,730
but I just want them specified by correlation using correlation structure that's mathematically legal.

490
01:00:56,790 --> 01:00:59,790
You can do that, but you're losing efficiency anyway.

491
01:00:59,800 --> 01:01:03,629
I mean, you use error y, you lose probably less efficiency.

492
01:01:03,630 --> 01:01:07,170
But are you out of independence correlation structure?

493
01:01:07,260 --> 01:01:11,310
I know I lose efficiency, but I still get a consistent estimate.

494
01:01:11,430 --> 01:01:15,390
I still have a some tidy, unbiased estimate and right.

495
01:01:15,480 --> 01:01:20,610
If I use the course. So this is what people do and your Mobutu is going to do.

496
01:01:21,480 --> 01:01:27,900
You're going to do this in homework two as well. Okay. I just put this correlation.

497
01:01:27,900 --> 01:01:31,410
All the working conditions. Zero. I ignore dependance.

498
01:01:32,010 --> 01:01:48,470
I just run this. I just run this estimating question with this working condition to be identity matrix.

499
01:01:48,910 --> 01:01:52,360
So this is what this is essentially an organized regression.

500
01:01:53,470 --> 01:01:56,490
Show me. All data points are independent. Okay.

501
01:01:56,800 --> 01:02:02,620
Numerically, numerically. So you can use our package your to run this.

502
01:02:03,130 --> 01:02:07,900
If you tweet this working dependent structure to be identity matrix.

503
01:02:08,650 --> 01:02:15,219
Okay it's working is not too quick is something I choose to run the data analysis using

504
01:02:15,220 --> 01:02:20,560
this identity matrix under the independent working independent working structure.

505
01:02:21,010 --> 01:02:29,110
This estimate question is just the scored equation from your logging in a regression model that you can use our package to run it.

506
01:02:30,490 --> 01:02:38,500
Okay. So these are coming in the self dependance estimation log RFA is consistent, no problem.

507
01:02:39,340 --> 01:02:50,350
But of course not. They still have some target normality, but you just don't get, you know, good efficiency are your recoveries.

508
01:02:50,350 --> 01:03:00,840
The real problem is pretty wide. So under this situation, right, it it is equivalent to fitting a conventional gear in our package here.

509
01:03:00,850 --> 01:03:10,210
And under the assumption of independent sample where point estimates are valid but standard error all are not valid.

510
01:03:10,990 --> 01:03:18,610
Why? Because the correct standard error should be calculated from the sandwich covariance matrix rather than the scope,

511
01:03:18,850 --> 01:03:25,720
you know, rather than the feature information. So Fisher information is just part of this sandwich estimate.

512
01:03:25,870 --> 01:03:42,009
No sandwich, right? In six or two.

513
01:03:42,010 --> 01:04:01,150
You have seen this Bartlet identity, right? I it the Bartlett identity.

514
01:04:01,300 --> 01:04:07,780
What was the borough identity? Basically it says you have this school function.

515
01:04:08,710 --> 01:04:14,050
This is going on. This is like block likelihood. First of all, the door of the block like is your score, right?

516
01:04:15,370 --> 01:04:27,410
So the variance of this score minus is equal to minus the minus of of the of the first or the second order of the relative.

517
01:04:31,550 --> 01:04:42,260
Under likelihood. Likelihood. This is the famous identity you call Bartlet identity.

518
01:04:42,280 --> 01:04:50,919
Right. So. So the obvious. The variability of this score or function is equal to the expression minus second order.

519
01:04:50,920 --> 01:04:56,260
Drove the log like. This is one feature information as I write.

520
01:04:57,100 --> 01:04:59,020
This is your feature information.

521
01:05:02,860 --> 01:05:10,689
So, you know, likelihood inference you can either use Fisher information based on this second world or derivative log like or never.

522
01:05:10,690 --> 01:05:15,370
You can do this yourself. The first one to make it 602 you'll learn this identity.

523
01:05:15,460 --> 01:05:23,610
You must prove this way. There's a famous, famous identity, but where you have misplaced different model in this particular case,

524
01:05:23,620 --> 01:05:29,590
you missed this site the second moment because you know that independence is now too.

525
01:05:29,620 --> 01:05:33,640
You know that you're miss specified the second moment, right?

526
01:05:34,120 --> 01:05:41,200
So you use independence second movement runner to a true independence structure

527
01:05:41,410 --> 01:05:48,820
to to make estimation so this one doesn't hold so bar identity doesn't hold.

528
01:05:49,630 --> 01:05:56,920
So what is the sandwich for? Sandwich form is appearance.

529
01:05:59,590 --> 01:06:05,350
Of this inverse. Inverse official information is your end times.

530
01:06:08,230 --> 01:06:13,620
Transpose and. Okay.

531
01:06:14,010 --> 01:06:22,440
This is the sandwich form. So under the bonnet identity, this 12 becomes identity matrix.

532
01:06:25,310 --> 01:06:28,950
Okay. On the likelihood of contacts.

533
01:06:29,930 --> 01:06:37,950
The because of borrower identity this two things are the same then the becomes identity matrix.

534
01:06:37,950 --> 01:06:42,200
So this is the official information. Okay.

535
01:06:43,590 --> 01:06:56,450
And and then if you do not have part identity, that's the case of modem specification, then you have to use this as your asymptotic covariance matrix.

536
01:06:57,020 --> 01:07:02,150
Okay. So this is called Go Dandi Information for diabetes.

537
01:07:02,300 --> 01:07:07,760
Indian statistician and he was a student of our Fisher Berkeley UK.

538
01:07:10,770 --> 01:07:15,660
And he's the first person who found this sandwich form later on.

539
01:07:15,960 --> 01:07:19,470
Originally from Harvard, who used to be the faculty in the art department.

540
01:07:19,890 --> 01:07:23,760
But then he moved to Harvard and earned your way.

541
01:07:25,740 --> 01:07:34,260
He said to me that the most surprising thing, the most prompt think he did, is that this is named as the sandwich estimate.

542
01:07:34,350 --> 01:07:40,050
I said, I don't believe it. It was the eighties advisor when Daniel was accused here.

543
01:07:40,260 --> 01:07:42,720
They were fiercely working on survival.

544
01:07:43,710 --> 01:07:51,690
And he said the most improbable thinking in his career is name this thing a sandwich estimate, or I said, gee, I will call it goddamn you for this.

545
01:07:51,760 --> 01:07:58,160
And so so I did recall I mean, I formally in my book just call this goddamned information.

546
01:07:58,170 --> 01:08:10,920
We have to respect the original contribution in front of them in 1954 paper I think that's the first time GOULD And he found that if you do

547
01:08:10,920 --> 01:08:21,360
not have this model identity having this me specify the model working on estimating equation you are some part of various pieces like this.

548
01:08:21,730 --> 01:08:34,290
Okay, so I did not write it out, but I'm just telling you that in the g m our package, right?

549
01:08:35,220 --> 01:08:41,610
Because here in our package clearly t r and it's used under the likelihood framework.

550
01:08:42,090 --> 01:08:46,530
So they only calculate the Fisher inputs only calculus this part.

551
01:08:47,250 --> 01:08:54,450
But you miss me, specify the case. You do now have you know this, you have this.

552
01:08:54,450 --> 01:09:05,900
The can do cannot cancel all this two things so that this was sandwich form and of course g e package g you calculate this thing, right?

553
01:09:06,120 --> 01:09:15,720
So but if you want us consider the working dependency the point estimate no problem.

554
01:09:15,960 --> 01:09:20,820
So the point estimate no problem. You still get parameter estimates correctly.

555
01:09:21,270 --> 01:09:30,180
It is. How do you calculate standard error? All that is different from from this this situation and the package.

556
01:09:30,270 --> 01:09:38,889
Right. So the choice of independent working corporation is particularly undesirable in analysis of

557
01:09:38,890 --> 01:09:43,960
long time stories where underlying tempo transition is part of the primary interest in theater.

558
01:09:44,910 --> 01:09:53,200
As such, transition is essential to add additional knowledge of time course evolution of this process given risk data collect over time.

559
01:09:53,230 --> 01:09:58,870
That's essentially my argument. When I started work on my dissertation to work on Common Future, I said,

560
01:09:59,230 --> 01:10:06,940
You have so much information over time why you gave up modeling the time course dynamics.

561
01:10:06,940 --> 01:10:11,499
You can learn a lot from that, right? Of course not from proactive point of view.

562
01:10:11,500 --> 01:10:15,549
You do not want to really bothered by this late in process.

563
01:10:15,550 --> 01:10:18,070
But what if you want to do protection,

564
01:10:18,580 --> 01:10:26,290
you want to do good prediction and you really need to work on build it and processes you work to achieve that could improve

565
01:10:26,290 --> 01:10:33,489
status power while better modeling tempered dependance rather than working up space about working conditions structure.

566
01:10:33,490 --> 01:10:37,569
You don't know how close the working structure to the actual one, right?

567
01:10:37,570 --> 01:10:44,469
So just simply for the consideration of numerical simplicity, which is not very good a way of thinking.

568
01:10:44,470 --> 01:10:50,290
But I should not criticize Ziglar too much, but I just say that, you know,

569
01:10:51,370 --> 01:11:04,870
the choice of the masses should be based on the purpose of your analysis and and you know how much you want to give it and the data.

570
01:11:05,020 --> 01:11:11,860
Okay. So of course that do you want to make a forecast and quantify the uncertainty of forecasting?

571
01:11:12,100 --> 01:11:24,040
Okay. So I, I think a rigorous method is useful and you can implement in the package G together estimate.

572
01:11:25,570 --> 01:11:35,770
Okay. So additional remarks, the G method only cares about the memo, namely the neither plus the model and parameters in the model,

573
01:11:35,770 --> 01:11:40,719
namely our film parameter consistency is ensure about estimating efficiency

574
01:11:40,720 --> 01:11:45,250
will may be generally compromised by using the working quotation structure.

575
01:11:46,090 --> 01:11:54,810
The latent temperature transition is approximate, perhaps greatly simplified by some much simpler self dependance like error,

576
01:11:54,820 --> 01:11:59,889
one in which latent process now fully used in estimating and inference late

577
01:11:59,890 --> 01:12:05,620
in process treat as kind of nuisance part in the model does not estimate it,

578
01:12:06,550 --> 01:12:11,950
but marginalized by avoiding is marginalized by voice.

579
01:12:11,980 --> 01:12:19,059
I deal with this nuisance part is certainly not useful to figure out position of flying

580
01:12:19,060 --> 01:12:24,580
object because you really need to figure out lead process like the one I talked about,

581
01:12:24,610 --> 01:12:28,540
the original state space model where you stayed up this late in process.

582
01:12:28,540 --> 01:12:36,790
That's actually the thing you want to learn what this represent, the true position of the rocket in the sky.

583
01:12:36,790 --> 01:12:44,259
You really want to recall that thing to you are to quantify the position and primary interest comma fierce

584
01:12:44,260 --> 01:12:53,799
motor and okay so that's one thing that 98 Zager did and now later all people trying to improve that of

585
01:12:53,800 --> 01:13:07,360
course and and so the first thing is as this so the M algorithm is this very natural idea to do this kind

586
01:13:07,370 --> 01:13:13,900
of better method using EMR algorithm because you can treat that late latent process as missing data,

587
01:13:14,650 --> 01:13:21,850
right? So, so M algorithm is the algorithm that allow us to deal with missing values, mission data.

588
01:13:22,270 --> 01:13:26,469
And if you treat latent process as missing data, then you can easily,

589
01:13:26,470 --> 01:13:33,790
naturally build up a EMR algorithm in the estimation of this kind of statement model.

590
01:13:35,740 --> 01:13:46,390
So Chair now doctor from University of Iowa, Iowa State, I'm sorry, they started to, you know, develop this method.

591
01:13:48,580 --> 01:13:50,920
You published a paper in Gaza. Okay.

592
01:13:51,700 --> 01:13:59,109
So as I say, that they treat the latent processes and recent data and calculate the outcome in likelihood based on augmented data.

593
01:13:59,110 --> 01:14:07,959
So, you know, you have observe process, you have leading processes, you have argument data consisting of both observe process and let them process.

594
01:14:07,960 --> 01:14:11,890
Then you work out on the east of Amster.

595
01:14:12,460 --> 01:14:25,660
So the in the east that so unfortunately in this nonlinear dynamic model, it has no close formal expression for expectation of sufficient statistic.

596
01:14:26,320 --> 01:14:31,220
So that Chan and Dalton suggested that you can you.

597
01:14:31,290 --> 01:14:36,780
Merkel evaluated the expectations of a sufficient statistic by Monte Carlo method.

598
01:14:37,620 --> 01:14:47,730
That's I will give a little more detail later and this suggests the keep sampling scheme to General Monte Carlo samples from space based model.

599
01:14:48,300 --> 01:14:56,840
That's the advantage of space based model. You have this hierarchical structure and you have this hierarchical structure.

600
01:14:56,850 --> 01:15:03,959
You have this Markov structure. So this is very, very nice generative modeling machinery.

601
01:15:03,960 --> 01:15:08,850
And people will call this generative model. You can easily simulate theta from this system.

602
01:15:09,510 --> 01:15:13,760
Okay. Just like our model, right? Walmart mode compare model.

603
01:15:13,770 --> 01:15:19,440
You have this differential equations and that allow you to easily simulate data.

604
01:15:20,160 --> 01:15:29,160
Okay. And this space based model is also a hierarchy model with the Markov structure, then you can easily simulate data from this system.

605
01:15:29,490 --> 01:15:39,840
So there's no problem that you can easily implement the Monte Carlo simulation to simulate, you know, the data, to calculate the expectation.

606
01:15:40,710 --> 01:15:48,570
Of course, there's always a simulation error for the month column as to how many Monte Carlo samples are that are essential.

607
01:15:48,600 --> 01:15:53,680
Right. Maybe I don't have much time.

608
01:16:07,330 --> 01:16:13,720
So just just to give you a brief idea, I don't know how much you know I'm above them to up.

609
01:16:14,260 --> 01:16:22,540
Okay. So you want to do the integration of X, the X, let's say, from zero one.

610
01:16:23,000 --> 01:16:26,190
Oh, just for simplicity. Right. You want to do the integration?

611
01:16:26,220 --> 01:16:45,820
Yes. Well, you can turn this into affects G, X times Q, X x, where G is a density like uniform distribution between zero one.

612
01:16:48,670 --> 01:16:58,210
I'm not telling you this mathematically. Right. So then this becomes expectation of X.

613
01:17:00,370 --> 01:17:04,330
Max. You agree?

614
01:17:05,320 --> 01:17:09,740
I would before. I have nothing to do with statistics or probability.

615
01:17:09,760 --> 01:17:12,890
I just want to take a look at Interpol.

616
01:17:12,910 --> 01:17:22,720
This is hard to compute. But I say I forcefully insert his g x into this and g is a density.

617
01:17:23,500 --> 01:17:28,440
So then I mean, this is the definition of expectation or function.

618
01:17:28,480 --> 01:17:33,610
Right. So this. So now what I need to do is launch number.

619
01:17:34,240 --> 01:17:39,940
If I can generate x, x and I d from g x.

620
01:17:41,900 --> 01:17:50,860
Right. Then this whole exploitation will be approximated by law of last number of lives.

621
01:17:52,180 --> 01:17:57,790
One over an iPhone one two in the sample mean to approximate this.

622
01:17:59,060 --> 01:18:04,880
Oops. Right.

623
01:18:04,900 --> 01:18:08,710
That's. That's what I want to do. Okay. So my integral.

624
01:18:12,510 --> 01:18:18,990
Can be approached from about this and this will convert to this one and those infinite firewalls, large number.

625
01:18:20,220 --> 01:18:30,670
So that's why we say when you use Monte Carlo method to calculate expectation and then once the number of data points in here didn't,

626
01:18:31,020 --> 01:18:38,790
the larger did better because of large number, but maybe larger will not give you much improvement.

627
01:18:38,850 --> 01:18:40,760
You need to find a way to stop, right?

628
01:18:40,770 --> 01:18:50,980
So anyway, so there's always a one column approximation here due to the choice of data point to proceed with this.

629
01:18:51,450 --> 01:18:55,770
So that's basic idea of Monte Carlo to calculate integral.

630
01:18:57,480 --> 01:19:03,629
So in this step, because of the that you show that you do not have close form expression,

631
01:19:03,630 --> 01:19:10,170
you have to use this Monte Carlo method to based on the law of large number to approximate expectation.

632
01:19:11,100 --> 01:19:19,950
So m established relative as true for you just run a log in regression model once the expectations of sufficient statistics are calculated.

633
01:19:20,850 --> 01:19:28,379
Okay. So this approach does use the late in process in the power of estimation because you need the general

634
01:19:28,380 --> 01:19:34,170
Monte Carlo samples from this system late in process two will be involved in the data generation.

635
01:19:34,750 --> 01:19:41,100
Okay. So however you m algorithm is no for slow conversions.

636
01:19:41,100 --> 01:19:53,430
Right. We know that M algorithm has this new newer convergence also that of the the whole C that the,

637
01:19:55,650 --> 01:20:03,990
the, uh, so yeah, my algorithm is trying to find this solution of your likelihood method.

638
01:20:04,710 --> 01:20:20,340
Okay. Let me put this way. There is a kind of policy debating about the method convergence and word became so 1976, right.

639
01:20:20,340 --> 01:20:31,139
The first paper just to be from three Harvard professors, I think Dempster, Leonhardt and Dan Rubin, they published the paper of EMR algorithm.

640
01:20:31,140 --> 01:20:36,770
Now it's very, very important paper. Wow. This most influential papers in statistics, right?

641
01:20:37,180 --> 01:20:42,150
EMI was in 1976 or 77. I forgot exactly the year.

642
01:20:43,170 --> 01:20:48,000
Unfortunately, the proof of a group method conversions that was wrong in that paper.

643
01:20:48,690 --> 01:20:54,780
Oh, so different. Who used to be a chair of the department statistic here?

644
01:20:55,590 --> 01:21:04,860
1982 published a paper in A.O. of statistics that provided correct mathematical proof for the convergence of the EMI algorithm.

645
01:21:05,550 --> 01:21:13,110
And it's funny that Geoff, who himself is working on design factor desired of all these four designs,

646
01:21:13,170 --> 01:21:16,260
are working on the industry statistics, essentially.

647
01:21:16,740 --> 01:21:24,389
But this paper I think, is five or six pages and or so statistic as most cited paper in his career you can go to

648
01:21:24,390 --> 01:21:33,930
look at was sort of Google scholar and the EMI algorithm convergence is the proof 1982

649
01:21:33,930 --> 01:21:38,580
analysis did the paper is actually lost sight of paper but he's now working at EMI algorithm

650
01:21:38,910 --> 01:21:45,930
is working on design experiments you can see how important was them in the statistic,

651
01:21:45,950 --> 01:21:52,280
right? If you look back the whole history of statistic in the past three decades, right?

652
01:21:52,890 --> 01:22:02,550
What are the most important milestones and contribution statistical or algorithm bootstrap m m CMC loss?

653
01:22:02,600 --> 01:22:12,710
All right, you can name it. And most important advancement statistic in past four decades or so all algorithms right from bootstrap

654
01:22:12,720 --> 01:22:20,790
M I just talk about and then then and CMC I'm also going to cover it and lasso the whole algorithm,

655
01:22:21,520 --> 01:22:32,310
right. So, so that I think the statistics are very, very numerical and so that's why we need to work on a lot of numerical programing nowadays.

656
01:22:32,910 --> 01:22:42,250
And so and my time working s plus now you work on.

