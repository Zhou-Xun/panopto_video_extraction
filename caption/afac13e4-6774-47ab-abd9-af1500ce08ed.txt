1
00:00:09,960 --> 00:00:18,090
So I know that today is the last day for Indra.

2
00:00:18,780 --> 00:00:25,709
So and Wednesday is the due date for the first homework.

3
00:00:25,710 --> 00:00:28,800
So a couple of instructions.

4
00:00:28,800 --> 00:00:41,190
So some of you might have gotten a automatic rating if you have submitted a hallmark on Friday.

5
00:00:41,190 --> 00:00:47,670
So there will be another automated grading tonight, another to make reading Tuesday night.

6
00:00:48,240 --> 00:00:55,500
And after that, there will be automatic that there will be final grading for Wednesday night.

7
00:00:56,340 --> 00:01:04,530
So and homework zero if you haven't made sure you must submit homework Cherokee the homework one.

8
00:01:05,010 --> 00:01:08,880
So that's, uh, that, that's important.

9
00:01:11,220 --> 00:01:23,520
Okay. So, and I guess some of you may have questions and I, I always strongly encourage you to work on the homework problems as early as possible.

10
00:01:23,760 --> 00:01:33,239
But I understand that some of you might have practical issues in implementing the homework correctly, and that debugging takes a long time.

11
00:01:33,240 --> 00:01:36,330
Sometimes it's hard to figure out what's what went wrong.

12
00:01:36,540 --> 00:01:54,329
So I can give you individual list help, but the GSI will give you a lot more time to help you individually, so please make use of those office hours.

13
00:01:54,330 --> 00:02:02,280
We have office hours right after the class until 1030 for my office, our GSA office hours available tonight.

14
00:02:02,670 --> 00:02:09,330
7 to 9, I think. And there's another office hour tomorrow, 5 to 6.

15
00:02:09,780 --> 00:02:14,819
So you can utilize them if you have any questions.

16
00:02:14,820 --> 00:02:19,160
You can also send a message to me or you can send a private.

17
00:02:19,260 --> 00:02:28,260
So if you if your question contains any code, send send a private message or email or including me in GSI all the time.

18
00:02:29,430 --> 00:02:39,030
And if you have a yeah, if you have a general question, you can you can ask a question the broadly.

19
00:02:39,390 --> 00:02:46,660
But any question that contains the code you need to you need to send individually.

20
00:02:46,770 --> 00:03:00,089
Okay. So those are the general background to help help you complete accomplish the homework successfully.

21
00:03:00,090 --> 00:03:03,180
If you have any further questions.

22
00:03:03,780 --> 00:03:06,990
Feel free to ask me now before story.

23
00:03:09,390 --> 00:03:21,270
It's not you know, let's continue from the last lecture where we learned about the taught and now we're learning about the emerging sort.

24
00:03:21,810 --> 00:03:26,640
We went through the basic algorithms of the merger, so if you can recall.

25
00:03:27,150 --> 00:03:30,299
So this is a type of divide and conquer algorithm.

26
00:03:30,300 --> 00:03:38,370
So you divide a problem of sorting problem into sub problems where you actually

27
00:03:38,370 --> 00:03:43,560
solve the problem of sorting the first half and second half of the array.

28
00:03:44,220 --> 00:03:49,470
Okay. So that's that, that's good.

29
00:03:49,470 --> 00:03:57,030
But when you try to sort of source something, if you sort the first half and if it sorted the second half,

30
00:03:57,030 --> 00:04:01,280
it doesn't automatically make the problem solved.

31
00:04:01,300 --> 00:04:07,020
You still have the two distinct array that is not not completely sorted yet,

32
00:04:07,470 --> 00:04:15,900
and you need to somehow combine those two solutions to come up with the actually sorted array.

33
00:04:16,290 --> 00:04:20,420
And that's the part that is different from anything from Hightower.

34
00:04:21,090 --> 00:04:25,620
You know, it's a fairly trivial to solve to combine those two solutions.

35
00:04:25,620 --> 00:04:37,410
If you know how to how to move a minus one disk in the tile problem, then, you know, solving the larger problem is easy, very, very easy.

36
00:04:37,590 --> 00:04:46,530
Just a three steps. In this case, you need to come up with another algorithm to come to combine those two sorted sequences.

37
00:04:46,530 --> 00:04:49,640
And those are the combi steps. So divide and conquer and combine those.

38
00:04:49,770 --> 00:04:58,739
Those are steps. So we looked at the code and I explained this part before,

39
00:04:58,740 --> 00:05:06,000
but I think some of some of you haven't exactly understood what, what, what it what it is doing.

40
00:05:06,000 --> 00:05:09,930
So first of all, the body so targeting itself is very, very simple.

41
00:05:09,930 --> 00:05:18,240
So it's actually easier to understand because if you divide the array into two sub sub arrays, sub vectors.

42
00:05:18,540 --> 00:05:24,809
And after that, you, you return the sorted race and you combine them.

43
00:05:24,810 --> 00:05:34,410
And these merge algorithm should return a of the largest margin is sorted array between those two already sorry array.

44
00:05:34,410 --> 00:05:41,040
So assumption here is that is multi algorithm is assuming that both A and B are already sorted.

45
00:05:41,250 --> 00:05:48,930
But you need to decide whether you should come up with a solid array that combines the both elements together.

46
00:05:49,300 --> 00:05:54,270
Okay, so how does it do this? So let me explain a little bit slowly again.

47
00:05:54,270 --> 00:06:06,750
So if you have a or a a of length of the length of the two lengths of so or A and B, and if you have, you need to create A.

48
00:06:06,930 --> 00:06:14,399
So we are creating a large array with a combined length and we maintain two in the

49
00:06:14,400 --> 00:06:21,630
in this case I and J and IAG in this is for tracking the element for A and A and B.

50
00:06:22,350 --> 00:06:31,229
So what we are trying to do is we fill in the element for each of those combined array.

51
00:06:31,230 --> 00:06:37,140
So one through R. So our is the length of our is the size of a combined array.

52
00:06:37,680 --> 00:06:42,240
So we are deciding whether to copy from a or a copy from B.

53
00:06:42,840 --> 00:06:48,060
So in which cases do we copy from? A There are three there are two cases.

54
00:06:48,990 --> 00:06:56,420
One is that if A of AI is similar, then P of J if this happens, okay.

55
00:06:56,970 --> 00:07:02,610
And then we copy from A because we want to fill in the smallest element first.

56
00:07:03,330 --> 00:07:12,210
But there are other cases. So I n j iterating each of the two a and b a vector.

57
00:07:13,140 --> 00:07:17,340
So when you iterate them in mate.

58
00:07:17,340 --> 00:07:21,690
So you mean you may already have iterated everything in the B.

59
00:07:21,930 --> 00:07:25,200
Okay, so if you already reach the end of the B,

60
00:07:25,200 --> 00:07:32,190
you have nothing to copy from B then actually you cannot even compare them because a b of J doesn't exist.

61
00:07:32,400 --> 00:07:39,030
So you need to check the boundary condition in the case that if I already scanned everything from B,

62
00:07:39,030 --> 00:07:44,760
I always have to copy from A that that is checking this.

63
00:07:45,480 --> 00:07:54,959
Okay. So this means that if J is greater the length of V meaning that J already went out of bounds for the B,

64
00:07:54,960 --> 00:07:59,070
so I don't have anything to copy from B, so I don't even need to compare.

65
00:07:59,070 --> 00:08:03,570
I just need to copy from me. Okay. And the same thing.

66
00:08:04,230 --> 00:08:08,280
So if a of I.

67
00:08:09,190 --> 00:08:20,860
Which means that I when I went out of town. So I, you know, I don't have anything to copy from a then you automatically copy from B in this case,

68
00:08:21,520 --> 00:08:29,350
you, you use this to make sure that you have something to copy from a.

69
00:08:29,380 --> 00:08:39,130
So there's three conditions here actually. So here, check first check whether there is a something copy from B, okay.

70
00:08:39,340 --> 00:08:46,690
Otherwise, if, if this is false. Okay, so, so if this is true,

71
00:08:47,830 --> 00:08:58,299
then you automatically automatically copy from a even without checking from checking these these logic so all those lazy variations.

72
00:08:58,300 --> 00:09:05,500
So if this is true and this is the or that doesn't check it doesn't even check the the second condition at all.

73
00:09:05,570 --> 00:09:16,330
Okay, so and if this is false, that means that now I have something to cut, something to copy from B then let's check this.

74
00:09:16,480 --> 00:09:23,930
Okay, so first check twice. Do I have something to copy from a and then okay.

75
00:09:24,430 --> 00:09:31,360
Then compare and copy them only if what if and only if when a of eyes unless they be okay.

76
00:09:31,630 --> 00:09:34,990
So that's in that case you you have copy from eight.

77
00:09:35,500 --> 00:09:48,160
If not so if neither of them are true. So if I don't have anything to copy from A or if a B of B of J is smaller than equal or smaller than A of I,

78
00:09:48,790 --> 00:09:52,840
then I go this part and a copy from B, okay,

79
00:09:53,410 --> 00:10:03,370
so and you increment the index of I if you copy from a copy from increment the index of J,

80
00:10:03,640 --> 00:10:15,640
if you copy from B because I am supposed to keep track of the current index to copy from from each of A and A and B, so that's the majority.

81
00:10:16,000 --> 00:10:24,550
Okay. So this is not super complicated, but writing the right condition like this requires some additional thoughts.

82
00:10:25,780 --> 00:10:32,760
Okay. So that's what we learned last time.

83
00:10:33,720 --> 00:10:40,920
So if you run this. And this.

84
00:10:43,950 --> 00:10:53,220
Then this. Sorry. I should have run the earlier part because this part create x.

85
00:10:53,460 --> 00:11:01,230
Okay. So if I run it again, it returns a solid list of X.

86
00:11:03,690 --> 00:11:08,040
So so that's that that's the that's this part.

87
00:11:08,370 --> 00:11:15,059
I'm just I know I went through this part, but just wanted to make because I went a little bit fast.

88
00:11:15,060 --> 00:11:19,020
I just wanted to make sure that everyone understood what the merger would be with us.

89
00:11:19,370 --> 00:11:27,120
Okay. Okay. So then how fast this merger more research time, would you mean?

90
00:11:27,150 --> 00:11:30,180
So. So that's the next thing we wanted. We wanted to learn.

91
00:11:30,750 --> 00:11:35,930
Okay, so, uh. So what the.

92
00:11:40,490 --> 00:11:59,110
Okay. This is very. Sorry.

93
00:12:00,650 --> 00:12:03,710
Okay. Yeah. Okay. So.

94
00:12:06,520 --> 00:12:14,620
Uh, so what. What I wanted to try here is that once that figured out,

95
00:12:14,620 --> 00:12:20,800
figuring out the time complexity of murder sort so to remember what the time complexity insertions or twice.

96
00:12:23,120 --> 00:12:27,380
It's weird and scared. All right, so this this we went through this.

97
00:12:27,980 --> 00:12:35,870
So this you can you can just count how many you know, how much time each of a step takes.

98
00:12:35,870 --> 00:12:42,290
And this is a unnecessarily complicated calculation, but the bottom line is that this is basically a double loop.

99
00:12:42,500 --> 00:12:48,140
So if you think about the worst case, you have to, you know, compare everything.

100
00:12:48,680 --> 00:12:53,540
So and you need to go to the double loops. So this is a lovely square language.

101
00:12:53,990 --> 00:12:58,150
Okay. So imagine sort of what is the time complexity?

102
00:12:58,160 --> 00:13:09,050
So the time complex analysis is a little just the more it is a little messier if you have the number of elements that are odd.

103
00:13:09,290 --> 00:13:17,290
So let's just assume just for time, complexity sake, let's assume that the number elements are exactly two to the the power of two.

104
00:13:17,300 --> 00:13:29,240
But this can be easily generalized if the number of elements is not parallel to if you just do the proper rounding for it in each of the step.

105
00:13:29,510 --> 00:13:40,580
Okay. So then because the element is if you take the power of two, whenever you use split divide, the problem is a problem.

106
00:13:40,790 --> 00:13:45,560
The each of the parts exactly have the same number of elements that that makes life easier.

107
00:13:46,400 --> 00:13:53,690
So let's assume that if you have only one element, let's say you have a certain constant amount of type C, okay?

108
00:13:54,830 --> 00:13:59,330
Then if you have greater than one element,

109
00:13:59,510 --> 00:14:09,650
what you need to do is you need to pay for the price paid for the cost for sorting the some elements of surprise.

110
00:14:10,010 --> 00:14:13,310
So each of each that you have a two parts and over two.

111
00:14:13,340 --> 00:14:18,860
So you're paying you're paying the time complexity for the time cost for sorting this part.

112
00:14:19,400 --> 00:14:22,820
In addition, these are within the merge algorithm.

113
00:14:22,820 --> 00:14:27,140
We just went through it just linearly scanning to arrays.

114
00:14:27,260 --> 00:14:31,280
So it is a, it is just the the one.

115
00:14:33,680 --> 00:14:41,690
So it's just a single loop. So this is a sum sum constant times the proportional to the size.

116
00:14:42,440 --> 00:14:49,200
Okay. So that's the term complex. You need to pay for it. So then how can you do this?

117
00:14:49,220 --> 00:14:59,270
So basically, then what you can you can do is a you're you're basically adding the C times ends step.

118
00:14:59,970 --> 00:15:07,280
So if you think about this, you have a certain element.

119
00:15:07,280 --> 00:15:20,180
Let's say you have like 60 elements initially, then you need to split this to eight elements and eight elements here.

120
00:15:20,510 --> 00:15:24,440
Okay. And four elements and four elements. Four elements and four elements.

121
00:15:24,710 --> 00:15:27,710
Okay, two elements. Two elements. Two elements.

122
00:15:27,710 --> 00:15:31,880
Two elements. So on. Right. And each of them.

123
00:15:32,060 --> 00:15:43,639
One, one, one, one, one, one, one, one. Right. So the the the very bottom step has a basically it takes a constant time and there's a 16 element.

124
00:15:43,640 --> 00:15:47,270
So it's constant times 16. So that's that's clear.

125
00:15:48,530 --> 00:15:52,090
This part is basically takes two.

126
00:15:52,190 --> 00:16:03,049
So so this is what each each of them if you two to a certain you need a to see right and there's a part of them two should times eight.

127
00:16:03,050 --> 00:16:09,680
So these two also takes the 16 C so this time each each of the stories that for C

128
00:16:09,950 --> 00:16:17,959
and times force is 16 C so you basically each of them has a 16 C time complex.

129
00:16:17,960 --> 00:16:26,660
You need to sort each of the step, which means that you you need to see times n times of height of the three.

130
00:16:28,100 --> 00:16:34,070
Okay. If you divide them into like a binary tree, what is the height of the tree?

131
00:16:35,630 --> 00:16:39,320
So. So M is a here at height of the height of the tree.

132
00:16:39,320 --> 00:16:52,490
Because of this, if you have two to the m elements that the height of three is m so m is exactly two over n,

133
00:16:52,490 --> 00:16:58,280
if you if you consider the special case of M equal to two the M.

134
00:16:58,640 --> 00:17:06,800
So in this case it's easier to show that this time complex and times log two of N which is and again time complexity.

135
00:17:08,270 --> 00:17:14,980
Okay. So is a slogan faster than quadratic as good?

136
00:17:16,450 --> 00:17:34,180
Yes. How fast is it? Well the Fed is a factor in the fact of NDB available when a lower one and doesn't make much of a difference if any 16.

137
00:17:34,870 --> 00:17:46,120
But if it's 1 billion, let's say that low 20 is 30 and 30, 33 versus 1 billion is a big improvement.

138
00:17:46,150 --> 00:17:50,110
So if you have a more and more element, the difference becomes a larger and larger.

139
00:17:51,040 --> 00:17:58,179
Okay. So for arbitrary end, you can make a similar kind of equation.

140
00:17:58,180 --> 00:18:04,749
So I'm not going to go into detail, but basically you can bound this into a certain range.

141
00:18:04,750 --> 00:18:10,120
And because this is just the talking about time complexity that talks about the highest order,

142
00:18:10,120 --> 00:18:14,500
it's very easy to show that this still is energy and time complexity.

143
00:18:16,560 --> 00:18:30,030
Okay. You can just use the upper bound. Okay. So now let's compare how these how fast and slow each of the algorithm is.

144
00:18:30,060 --> 00:18:35,340
I already showed some spoiler, but I'm going to do it again.

145
00:18:36,630 --> 00:18:40,770
So let's say you create a 10,000 random variables.

146
00:18:42,960 --> 00:18:51,070
So let me just give me a second. So.

147
00:18:51,520 --> 00:18:54,550
Excuse me. So.

148
00:19:00,240 --> 00:19:09,520
Give me a second. Okay.

149
00:19:10,620 --> 00:19:18,040
Okay. Okay. It's better. Okay. Sorry. Uh, if you have a we create a 10,000 and the values.

150
00:19:18,440 --> 00:19:22,440
Okay. And let's run the insertion.

151
00:19:22,460 --> 00:19:25,600
Certain these takes some time.

152
00:19:25,990 --> 00:19:31,870
Okay. So let's assume that this argument works correctly.

153
00:19:32,080 --> 00:19:37,180
We don't need to show the correctness again. This takes about 4 seconds in terms of elapsed time.

154
00:19:38,050 --> 00:19:45,730
If you run more sort, this takes about, you know, almost 40, almost 35 times faster.

155
00:19:46,390 --> 00:19:53,620
So you see that if you try to run this target, this larger number of elements, there is a substantial differences.

156
00:19:53,770 --> 00:20:02,170
Okay. So if you the reason why I didn't increase to 10 to 5, obviously, then this takes like more than 5 minutes.

157
00:20:02,410 --> 00:20:07,540
So you can but you can try. You will see that, you know, this really scale.

158
00:20:07,660 --> 00:20:12,280
So I can probably do it through this because it'll probably take like 15 seconds.

159
00:20:12,880 --> 00:20:19,300
Okay. So 20,000 then it'll scale linear, the quadratic.

160
00:20:19,620 --> 00:20:26,080
So you will it'll take about, I think lawfully roughly 15 seconds I think if it's four times.

161
00:20:26,620 --> 00:20:31,140
So let's check it. Okay. Previous is a 3.9.

162
00:20:31,150 --> 00:20:34,810
Now it's almost 16 seconds. So the skills, quadratic.

163
00:20:35,090 --> 00:20:39,459
You can experiment with it. So this should scale almost linearly.

164
00:20:39,460 --> 00:20:44,380
So this should be still pretty fast, which is the case.

165
00:20:44,500 --> 00:20:47,090
So almost twice. Right? So it does.

166
00:20:47,770 --> 00:20:56,860
So even though the thought is hard to understand that you, you, you may not completely understand why this is much faster,

167
00:20:56,860 --> 00:21:05,470
but basically divide and conquer together is a good way to make the time,

168
00:21:05,890 --> 00:21:13,300
reduce the time complexity about apparently more and more so compared to the high bar equation that uses uses a loop.

169
00:21:13,690 --> 00:21:23,260
So this is what we do. However, just this is not related to divide and conquer algorithm, but this is still not as fast.

170
00:21:23,500 --> 00:21:28,720
So if you use a so target from the just the improvement in R is much faster.

171
00:21:28,750 --> 00:21:32,469
There are some inefficiencies that are involved in a very short.

172
00:21:32,470 --> 00:21:33,970
There are relatively two parts.

173
00:21:34,300 --> 00:21:45,880
One is that this is so using R, so it's a certain sort of function is using some built in function to implement a C++ or a Fortran, usually Fortran.

174
00:21:46,900 --> 00:21:50,229
Then that algorithm is very well optimized.

175
00:21:50,230 --> 00:21:53,330
It's much, much faster than just learning by the hour.

176
00:21:53,380 --> 00:21:58,600
So that's the main region. The other part is that these are what is self.

177
00:21:58,870 --> 00:22:01,540
If you implement in Python or other languages,

178
00:22:01,540 --> 00:22:08,960
could it be faster because this more sort of algorithm is is creating an array and it copies every time.

179
00:22:08,980 --> 00:22:12,340
So it's it's not necessary to do that.

180
00:22:12,340 --> 00:22:18,570
But the our structure to our function is to hard to modify the argument.

181
00:22:18,610 --> 00:22:24,429
So if you pass an argument, it's a hard to modify that you need to use some sort of global assignment.

182
00:22:24,430 --> 00:22:30,110
And those things are little, you know, I you know, those are those are not a good practice.

183
00:22:30,510 --> 00:22:35,980
So so here you have to copy because the limitation in our languages.

184
00:22:36,370 --> 00:22:40,809
So some of the inefficiencies could be improved if you implement a similar thing in

185
00:22:40,810 --> 00:22:50,350
Python or C++ or other languages that allow us to passing the argument as a reference,

186
00:22:50,350 --> 00:22:57,010
not as value. And we haven't learned the core body value and COBOL reference yet, but we're going to get to that later.

187
00:22:57,340 --> 00:23:01,830
Okay. So.

188
00:23:02,640 --> 00:23:06,459
So that's that's the end of the this this lecture.

189
00:23:06,460 --> 00:23:15,700
I probably don't have to spend the 5 minutes. I'll just give you 2 minutes to discuss about this aspect and if there is any anything that was unclear.

190
00:23:16,660 --> 00:23:22,190
Feel free to discuss with your colleagues and ask the question afterwards.

191
00:23:22,190 --> 00:23:28,660
So I'll give you 2 minutes to discuss and you can think about these three questions or what other?

192
00:23:29,980 --> 00:23:33,129
What other? Yeah. Why are you such a salty slur?

193
00:23:33,130 --> 00:23:36,160
What is the possible weakness of the current implementation?

194
00:23:36,310 --> 00:23:45,670
Is it you know. Yeah. So we, I already discussed that I'd probably give you a spoiler, but still good to discuss with your colleague.

195
00:23:46,030 --> 00:24:01,990
Okay. Oh.

196
00:24:27,890 --> 00:24:32,400
You. The first.

197
00:24:35,870 --> 00:24:44,580
You. Know.

198
00:24:46,930 --> 00:24:52,180
One. Is.

199
00:24:56,240 --> 00:25:00,250
So. I thinking.

200
00:25:12,520 --> 00:25:21,670
Or. The fact of the matter is that still.

201
00:25:32,220 --> 00:25:53,880
That. Okay.

202
00:25:56,790 --> 00:26:00,060
So any question about this discussion items?

203
00:26:01,260 --> 00:26:08,600
Why is it such a search floor? Y y y is insertions are slower.

204
00:26:12,930 --> 00:26:18,930
If you if you add in the tours is because of the time complexity.

205
00:26:19,050 --> 00:26:28,700
Oh, sorry. So, yeah, it's all because of time complexity and the more distant agendas intended answer here, you know,

206
00:26:29,280 --> 00:26:36,150
there could be many different weaknesses, but the intended answer here is that just involves the copying.

207
00:26:36,690 --> 00:26:41,820
Oh, but have you heard about how many people heard about kick start?

208
00:26:42,960 --> 00:26:58,110
Okay. Okay, good. So quicksort is is an algorithm that is not necessarily faster than modest sort, but often times it does.

209
00:26:58,350 --> 00:27:07,979
And the if you wanted to more advanced answer so we didn't learn quicksort but the reason why

210
00:27:07,980 --> 00:27:13,440
quicksort is faster than more resources that merges sorts is still requires a copy inside.

211
00:27:13,470 --> 00:27:18,540
So when you try to do the merge you need to create, you need to have to erase and you need to copy them.

212
00:27:18,540 --> 00:27:24,400
That's unavoidable. But the quicksort, you can actually you don't need to create a copy.

213
00:27:24,420 --> 00:27:29,190
You can just do the in in in a in place swapping.

214
00:27:29,190 --> 00:27:33,270
You can only use that operation. It's still divide and conquer algorithm.

215
00:27:34,020 --> 00:27:42,089
We can learn about that. You know, if you if you wanted to really learn about the kick sorted, that there is a lot of good resources out there.

216
00:27:42,090 --> 00:27:46,229
You can learn about this. So it's a good, good thing to try and see what quicksort is.

217
00:27:46,230 --> 00:27:48,570
Much more complicated to understand, by the way.

218
00:27:49,950 --> 00:27:58,290
But it's a very good algorithm and therefore sort of faster because, you know, it's implemented that way.

219
00:27:58,290 --> 00:28:02,640
So it's a time complex is not everything. There are other, other factors.

220
00:28:02,880 --> 00:28:07,680
You learn more about the matrix operations, actually. Okay.

221
00:28:08,910 --> 00:28:14,460
So that's the end of the lecture tree. Okay. Let's move on to lecture for any other questions.

222
00:28:19,930 --> 00:28:29,340
Okay. So. So matrix operators computation of this part is practically important.

223
00:28:29,380 --> 00:28:39,640
I, I like this part and I would like you to understand this part very well because you're going to deal with a lot of metrics,

224
00:28:39,640 --> 00:28:42,640
computation and understanding.

225
00:28:42,640 --> 00:28:46,750
Those details are important. Okay. The why metrics matters.

226
00:28:46,750 --> 00:28:51,700
So metrics matters in statistics because you are using metrics a lot.

227
00:28:51,820 --> 00:28:56,080
Okay? So a lot of a lot of cases, linear regression, logistic is a mixed model.

228
00:28:56,080 --> 00:29:04,930
You're just using it alone. Okay. And if you make the metrics computation efficiently, that makes a big, big difference.

229
00:29:05,350 --> 00:29:15,399
And sometimes there are specific features or there is just overall there are specific feature that involves a metric.

230
00:29:15,400 --> 00:29:22,270
Sometimes you have a some loop implementation that could takes a long time, especially art is very slow in the loop.

231
00:29:22,960 --> 00:29:32,620
But if you if you think or think differently, you can avoid the loop and convert them into metrics operation that makes a lot faster.

232
00:29:32,620 --> 00:29:42,460
So those tricks we would like learn. So I'm going to go this part very quickly because this is interesting but not practical, important.

233
00:29:42,550 --> 00:29:49,120
So I'm going to go call this part. So if you didn't understand the stress in our way, then I'm going to go through stress integration.

234
00:29:49,120 --> 00:29:54,670
Now, that's okay. But this is interesting.

235
00:29:54,820 --> 00:30:02,070
Wait. Okay. So you can actually make the matrix operation faster using divide and conquer.

236
00:30:02,540 --> 00:30:06,579
So you have these divide and conquer schemes.

237
00:30:06,580 --> 00:30:10,780
So divide you divide the problem, solve problems and combine them.

238
00:30:11,470 --> 00:30:17,980
That's what you need to do, though. The the problem we're going to solve is matrix multiplication.

239
00:30:18,550 --> 00:30:22,320
So when you try to do the math, this multiplication, this is how you do it, right?

240
00:30:22,330 --> 00:30:30,520
So you calculate. So you need to you have an end by in metric and by matrix and by so another.

241
00:30:30,910 --> 00:30:38,590
So and by p matrix and p by matrix. And you can generate end by matrix to calculate the each of these elements in them.

242
00:30:38,590 --> 00:30:44,319
By m matrix, you need to do the linear linear addition summation.

243
00:30:44,320 --> 00:30:48,660
So this takes the order of p time complexity.

244
00:30:48,670 --> 00:30:56,860
So total time complex is the end times m, m times p and if you are talking about the square demand faces,

245
00:30:57,520 --> 00:31:08,530
what is the time complexity so and cubic because each of them each of calculating each of a cell takes order of end time.

246
00:31:08,530 --> 00:31:13,690
So this requires and cubic time to fill in all and square elements.

247
00:31:15,640 --> 00:31:19,030
So but you can do that better.

248
00:31:19,240 --> 00:31:25,290
Okay, so there are method that can do better than cubic.

249
00:31:25,660 --> 00:31:32,139
Okay, so I'm not going to go into detail.

250
00:31:32,140 --> 00:31:38,709
The stress in algorithm is a we're going to learn it so this you can this this

251
00:31:38,710 --> 00:31:44,590
may not look impressive and into the cubic to enter the 2.8 something but

252
00:31:44,950 --> 00:31:57,370
this is practical is still fastest algorithm if you wanted to just if you are interested in just reducing the time complexity self you can do better.

253
00:31:57,370 --> 00:32:04,060
So there are algorithm that goes like two to the 2.376 to 2.37373.

254
00:32:04,570 --> 00:32:16,059
But those are volumes are not as efficient because it is attached with a huge overhead with the addition of constant term.

255
00:32:16,060 --> 00:32:19,990
So that that that's, that's that.

256
00:32:20,440 --> 00:32:23,829
So but these are very complicated algorithm.

257
00:32:23,830 --> 00:32:29,770
But if you implement this algorithm that is a first best known algorithm, these can be much faster,

258
00:32:29,770 --> 00:32:39,700
especially if you are dealing with a very large matrices, but still not fast enough to to do the operation with billion by beta matrix, for example.

259
00:32:40,570 --> 00:32:48,430
Okay. So you can also prove the lower bound that in a lower bound is very easy to show

260
00:32:48,430 --> 00:32:52,809
that this is should not be faster than scary because you are filling in as a limit.

261
00:32:52,810 --> 00:32:59,680
So it cannot be faster and scare, but also with certain assumption you can show that the lower bound is in scale again.

262
00:32:59,680 --> 00:33:08,919
So there is a limitation how fast you can do the metrics are the multiplication operation and then we're going to talk about the stress algorithm.

263
00:33:08,920 --> 00:33:16,270
So what we are going to do here is that you have a two squared matrices here and a to make it easier.

264
00:33:16,270 --> 00:33:21,850
Let's assume that this is the matrix size is two to the K in this particular example.

265
00:33:22,010 --> 00:33:28,479
It doesn't have to be. You can. Tweak the algorithm slightly by filling in zero in additionally.

266
00:33:28,480 --> 00:33:33,430
So this can be easily generalized with arbitrary size square metrics.

267
00:33:34,090 --> 00:33:39,550
But now, just to make it easy to explain, let's assume that any is due to the power of a K.

268
00:33:40,720 --> 00:33:47,530
Then what these are going to is trying to do is partitioning each of matrix into a four parts.

269
00:33:48,070 --> 00:33:57,969
Oh, okay. And if you if you partition into for parts equal parts, you can I mean,

270
00:33:57,970 --> 00:34:02,470
it's a it's easier to figure out what what's the relationship between the A and B and C,

271
00:34:03,220 --> 00:34:12,490
but instead of the two calculating the c11, C, one, two directly, you can actually calculate the into intermediate value like this.

272
00:34:12,670 --> 00:34:16,690
Okay. So M1 is defined like this.

273
00:34:16,690 --> 00:34:18,640
M two is defined like this and so on.

274
00:34:18,940 --> 00:34:30,880
So y you know, you can, you can you will know soon, but you have seven different quantities you can calculate instead of eight.

275
00:34:32,620 --> 00:34:39,770
Then that helps because if you calculate this seven quantities, you can calculate the c11.

276
00:34:39,790 --> 00:34:43,600
So you want to see two, one, c two, two as a function of M on to M seven.

277
00:34:43,930 --> 00:34:52,800
Like this. Good. So and the proving, the correctness between those two are relatively trivial.

278
00:34:52,810 --> 00:34:56,500
I'm not going to go into the details, but you can show that.

279
00:34:57,490 --> 00:35:00,940
So basically, you calculate seven different elements.

280
00:35:01,150 --> 00:35:11,979
Okay. And you can you can represent those the matrix of operation of large larger element,

281
00:35:11,980 --> 00:35:18,580
the c11 the larger element of a C as a function of M one to M seven.

282
00:35:19,600 --> 00:35:31,690
That Y is important. Seven is important. Well, then, if you try to calculate the time complexity here, you can you can argue this way.

283
00:35:31,960 --> 00:35:40,840
So I need to calculate. So basically I'm doing the calculation of the smaller problem and over two.

284
00:35:41,050 --> 00:35:44,920
Okay. And but I need to do that problem seven different times.

285
00:35:45,040 --> 00:35:50,470
Right. So because you you are calculating, but these are all matrix multiplications.

286
00:35:50,500 --> 00:35:54,580
Right. So you are doing this the matrix multiplication over seven times.

287
00:35:54,760 --> 00:35:59,890
Okay. And for smaller, smaller matrices.

288
00:36:00,340 --> 00:36:07,720
And after that, to combine these some problems into this larger problem, you just need the matrix addition.

289
00:36:08,050 --> 00:36:10,870
May this addition is just scared of it. Right.

290
00:36:11,350 --> 00:36:19,750
So you're doing the matrix of multiplication of seven time and do some matrix addition then that is the time complexity.

291
00:36:20,110 --> 00:36:25,290
So that that is the time of the calculating for larger element.

292
00:36:25,510 --> 00:36:31,540
Okay. And you can prove that this mess method theorem we are going to go through.

293
00:36:31,540 --> 00:36:39,790
But if you use this method theorem, basically what it says is that you do this global law with two of a seven.

294
00:36:40,060 --> 00:36:48,340
Okay. So this time complex they will become of order of the bigger notation of and and to the law of two of seven,

295
00:36:49,030 --> 00:36:53,440
which is 2.8 or seven and to the 2.8 or seven.

296
00:36:53,450 --> 00:36:56,470
So this is a faster than and to begin with.

297
00:36:57,800 --> 00:37:03,640
Okay. So then if you see this, you know.

298
00:37:06,390 --> 00:37:15,690
Yeah. My my reaction when I saw this first time, two, two reactions was, oh, this is so cool because you can use divide and conquer,

299
00:37:15,690 --> 00:37:22,830
which is a very is a very sophisticated idea if you think about this and you can actually make it faster than cubic.

300
00:37:23,520 --> 00:37:27,060
That is my first reaction. Okay. Hold on a second thought.

301
00:37:27,870 --> 00:37:34,019
You know, a second thought is like, who cares? It's your your your reducing the time complex.

302
00:37:34,020 --> 00:37:38,370
It's a very tiny amount. Looks like it doesn't look like a very impressive improvement.

303
00:37:38,520 --> 00:37:41,880
Okay. So those are the two reaction I had. Okay.

304
00:37:42,360 --> 00:37:48,540
So let's set that thought into side and let's try to figure out how they approve this.

305
00:37:48,960 --> 00:37:57,180
So if you have this inductive formula, then how do you show that this is the total time complexity?

306
00:37:57,180 --> 00:38:00,370
So this is just mad proof.

307
00:38:00,480 --> 00:38:05,580
So I'm not. So, again, this is this is not a big deal if you don't know it.

308
00:38:05,580 --> 00:38:10,140
But this is a useful, uh, useful fact.

309
00:38:10,830 --> 00:38:20,510
You might want to know if you have this recursive formula, then you can easily come up with the general.

310
00:38:21,990 --> 00:38:27,300
The formula for, for, for representing t of n is as a function of n okay.

311
00:38:28,080 --> 00:38:34,620
So a lot of these divide and conquer style algorithms have these recurrence equations.

312
00:38:34,620 --> 00:38:40,680
So you have a key of n dividing this problem into b into some B difference.

313
00:38:40,950 --> 00:38:47,609
So some problems and you're solving the problem at different times and there is a additional,

314
00:38:47,610 --> 00:38:51,390
additional cause to combine those solutions into something else.

315
00:38:51,480 --> 00:39:01,560
Okay. So any societal problem is the number of such problems and these are such of such problems and therefore n is the cost of the cost of the work.

316
00:39:01,710 --> 00:39:09,150
Okay. So then well you can so then method theorem is that there is a three cases.

317
00:39:09,420 --> 00:39:16,530
Okay, so first case is that. So it's all about how what, what is what is appropriate.

318
00:39:16,680 --> 00:39:27,690
Okay. So first of the cases that if f of when is the polynomial where the C is less than this low B of a,

319
00:39:27,970 --> 00:39:34,560
okay, then this means that this F end part is not substantial.

320
00:39:34,560 --> 00:39:37,980
So let me let me just go three then explain this.

321
00:39:38,340 --> 00:39:48,000
So sometimes when you try to solve in the divide and conquer way, sometimes solving these problems may take a long time.

322
00:39:48,420 --> 00:39:53,370
But the the combining the solution F of when could be very trivial, right?

323
00:39:53,760 --> 00:39:56,820
So sometimes F of when is very, very quick.

324
00:39:57,330 --> 00:40:08,460
And but this may take take larger more time in the matrix of this multiplication, let's say you have dividing this into eight different problems.

325
00:40:08,670 --> 00:40:22,230
Okay. So and then if you solve this into eight instead of seven, if it's eight or nine, it actually it's worse than cubic.

326
00:40:22,320 --> 00:40:27,510
Okay. So I mean, equal or worse than cubic. So there is no no benefit.

327
00:40:27,840 --> 00:40:33,720
Right? So sometimes if if this takes it takes a large amount of time.

328
00:40:33,990 --> 00:40:37,469
You actually, even if you combine, takes very short of time.

329
00:40:37,470 --> 00:40:42,600
This doesn't necessarily help much. Okay. So there are cases like that.

330
00:40:42,600 --> 00:40:45,600
Sometimes you can divide into some problems.

331
00:40:45,600 --> 00:40:52,560
Some problem is so easy to solve, it's so quick. But combining takes a long time than if the combine takes a long time.

332
00:40:52,770 --> 00:40:57,540
This divided conquer may not work. So that's the that's the thing we would like to learn.

333
00:40:57,540 --> 00:41:05,939
So one cases that f of when is so fast so is faster than low B of A than f over and doesn't matter.

334
00:41:05,940 --> 00:41:13,410
It doesn't contribute the highest order. So in that case a t of MN can be represented just in this way.

335
00:41:13,620 --> 00:41:16,620
Okay, so this is exactly the case of a stress in an algorithm.

336
00:41:17,070 --> 00:41:21,870
Be equal to equal sevens or low and the low to two of seven.

337
00:41:21,870 --> 00:41:32,370
Because a because a low of two over seven is a 2.8 and the F of n is the quadra was two, so CS two and this is 2.8 or seven.

338
00:41:32,370 --> 00:41:40,560
So the, the, the constant part is not the substantial part contributing to the time complexity.

339
00:41:40,860 --> 00:41:45,149
And in general you can generalize that equation like that.

340
00:41:45,150 --> 00:41:56,940
So let's say here N is like this, for example, then then you can, you can plug the plug them in so low to a low of a the three.

341
00:41:56,940 --> 00:42:00,809
This part is two. So then three is a larger than two.

342
00:42:00,810 --> 00:42:04,020
So this this falls into this particular case.

343
00:42:04,020 --> 00:42:11,280
That means that this is a. And this time complexity is the data to add to the queuing.

344
00:42:11,370 --> 00:42:21,689
Okay. So and you can you can actually calculate this exact recurrence and that is you can you can prove that this is into the cubic.

345
00:42:21,690 --> 00:42:27,030
But if you just wanted to know the time complexity, you don't need that kind of calculation.

346
00:42:28,950 --> 00:42:38,280
So Master Theorem two is that if C and no B of A is exactly the same.

347
00:42:38,710 --> 00:42:47,510
Okay, let's see if this is exactly the same. And then you have the time complex.

348
00:42:47,520 --> 00:42:55,409
It looks like this. It's a little, little complicated, but basically this is sort of a put it to end end types of order,

349
00:42:55,410 --> 00:43:01,410
which you have a longer term is, uh, is involving here.

350
00:43:01,620 --> 00:43:06,030
Okay. So in fact, the multi sort is this kind of case.

351
00:43:06,180 --> 00:43:10,499
Okay. So what the. So this is similar to modern sort of case, right?

352
00:43:10,500 --> 00:43:17,489
So you have you have the you solve you divide a problem into different problem.

353
00:43:17,490 --> 00:43:23,240
And each of them takes the two, two times to two times of these.

354
00:43:23,250 --> 00:43:35,330
So you need to solve the twice that this is a log to of A to B to an equal to the row talk to is one and for when is also degree of one then means

355
00:43:35,370 --> 00:43:44,339
that means that each of the f of any to solving the problem and combine the problem has about the exact the about the same level of time complexity.

356
00:43:44,340 --> 00:43:50,989
So each of them are competing to each other, so which means that it not be the case.

357
00:43:50,990 --> 00:43:54,390
So this is a linear time. We know that each of them linear time.

358
00:43:54,750 --> 00:44:01,140
So each of them you have to pay for the linear time to combining them and the.

359
00:44:01,920 --> 00:44:07,890
So this is a proportional to the height of this tree. When you when you divide the problem, the same problem.

360
00:44:07,900 --> 00:44:17,130
So that's that's what it takes here. So then this this in this recursive formula,

361
00:44:17,580 --> 00:44:31,530
you will end up having some and low end types of time complexity where actually the this polynomial times some low function in the lastly if you

362
00:44:32,340 --> 00:44:46,470
if f always larger then load p of a so let's say in this case combining let's say combining takes and quadratic then of then instead of linear,

363
00:44:47,100 --> 00:44:55,710
then combining actually takes more time. So each of them and each of each of them is n and and and secured.

364
00:44:55,770 --> 00:45:02,759
Right? So there is really no benefit of dividing the problem into a sub problem.

365
00:45:02,760 --> 00:45:11,220
In this case. This is and and cubic and then this is a and squared times two pieces and squared times four.

366
00:45:11,490 --> 00:45:18,000
So for so this might be a little bit taking longer thanks to solving the biggest problem is combining

367
00:45:18,000 --> 00:45:24,900
takes take so long time in this case just F of the end dominates and that that's the time complexity.

368
00:45:24,990 --> 00:45:29,670
So because at the end of the top level combining the problem takes a really long time.

369
00:45:30,720 --> 00:45:44,010
So in this case, if a equal to be equal to an F of is and secure the total time complex just as I get to them, and so that that's what it is.

370
00:45:44,220 --> 00:45:47,310
Okay. So depending on the what was the relationship,

371
00:45:47,310 --> 00:45:57,360
it was aimed and see you can it falls into this category and you can easily guess what the time complex should be if it's a formulated by the way.

372
00:45:57,540 --> 00:46:04,139
So instead of like really writing the algorithm, you can think about, oh, what's my time?

373
00:46:04,140 --> 00:46:11,330
Complex, lovely. When I, when I solve this problem in the divide and conquer like style and I have this kind of recursive formula,

374
00:46:11,340 --> 00:46:14,940
I have this much time, this might this many times I need to solve this.

375
00:46:14,940 --> 00:46:18,989
The problems I combining the solution will take this much time.

376
00:46:18,990 --> 00:46:23,160
Then you can you plug in this equation to see whether this actually helps or not.

377
00:46:24,040 --> 00:46:32,490
There mixes so with before really implementing them and realizing that this is actually slower than the you know, the night implementation.

378
00:46:34,670 --> 00:46:39,090
Okay. Any questions so far? Yeah.

379
00:46:39,800 --> 00:46:43,080
Assuming based you are on the system. Yeah.

380
00:46:43,110 --> 00:46:46,640
So based two logs if the.

381
00:46:46,820 --> 00:46:54,740
So this one, I think this is a base B logs, but doesn't matter because this is time complex.

382
00:46:54,740 --> 00:47:01,550
They're linear. Linear. This just goes on. Okay. Okay.

383
00:47:02,150 --> 00:47:09,880
So so next thing I want to talk about is that, you know, time complex is not really everything because of it.

384
00:47:10,520 --> 00:47:18,410
As I said, you know, who cares if the time compressed into the three to the end to end to the 2.8?

385
00:47:18,710 --> 00:47:22,460
Actually, if you calculate how much gain you're getting, it's it's not substantial.

386
00:47:22,460 --> 00:47:28,850
You can you can do the calculation unless you're dealing with like billions of billion metrics, which is not possible.

387
00:47:28,910 --> 00:47:37,910
Right. So time complexity is only talking about the relative time compared to when you when you have a larger,

388
00:47:37,910 --> 00:47:45,080
larger, larger percentage goes larger and larger, this becomes progressively more and slower and slower.

389
00:47:45,080 --> 00:47:47,480
So it's very important to understand that trend.

390
00:47:48,230 --> 00:47:56,059
But if we are going to be self takes a lot larger overhead for solving any side of problem you need to solve that overhead you step.

391
00:47:56,060 --> 00:48:04,820
So relative time complex is important, but absolute computational time is also may depend on different things, especially implementation details.

392
00:48:04,820 --> 00:48:13,250
So whether you wrote this or what you mean are a python or C or for Fortran, actually that matters a lot of times.

393
00:48:14,210 --> 00:48:21,890
For example, if you're using loop in R, the R just needs to attach it with a lot of additional check with each step in the loop.

394
00:48:21,890 --> 00:48:25,370
So it takes really a lot of time, so it slows down the implementation.

395
00:48:25,940 --> 00:48:29,450
So that's something you may want to know about.

396
00:48:29,810 --> 00:48:36,080
Okay. So with that, let's try to work with the practical examples.

397
00:48:38,090 --> 00:48:42,140
Okay. So. Uh.

398
00:48:42,370 --> 00:48:49,780
Sorry. So this is the algorithm over the matches, the multiplication.

399
00:48:49,850 --> 00:48:53,020
We we're not going to implement the stress on average here. Okay.

400
00:48:53,530 --> 00:48:57,370
Because that that that is faster than the neighbor algorithm,

401
00:48:57,370 --> 00:49:07,810
but that's not faster than the actual metrics of this multiplication you're going to use in R So we're going to just show some examples.

402
00:49:08,740 --> 00:49:16,090
So here where we just implement a triple loop implementation where you have three.

403
00:49:16,090 --> 00:49:19,930
So this is a general metrics multiplication. It doesn't have to be security metrics.

404
00:49:19,930 --> 00:49:23,989
So you just have end time and end by p p, by m matrix.

405
00:49:23,990 --> 00:49:27,640
Then you return and by metrics.

406
00:49:27,850 --> 00:49:33,249
Okay. And it goes the this is the time implementation.

407
00:49:33,250 --> 00:49:38,649
So each of them you calculate this term and keep adding them.

408
00:49:38,650 --> 00:49:43,030
So that's the exact implementation, the summation. So that's, that should be fine.

409
00:49:43,450 --> 00:49:52,360
And let's see whether this algorithm is correct. So in this case, A is so A is one.

410
00:49:52,450 --> 00:49:59,670
So if you see this, this is a what you need to know that this is you know, this the field of column first.

411
00:49:59,770 --> 00:50:07,179
One, two, three, four. So first column is a one, two, 3/2 economies of two, three, four, four, five, six and so on.

412
00:50:07,180 --> 00:50:13,090
And if you if you wanted to do it the other way around, you can do the by role equal through.

413
00:50:13,150 --> 00:50:22,670
So this is a basic ah ah syntax. But when you, when you create a matrix ah does column first, python does row four.

414
00:50:22,720 --> 00:50:25,310
So it's very complicated, very confusing. Okay.

415
00:50:26,950 --> 00:50:39,070
So if you do this, this is a multiplication with two matrices in if because you know that a built in an implementation is available.

416
00:50:39,340 --> 00:50:42,430
You can just compare that and that gives the right answer.

417
00:50:42,460 --> 00:50:49,220
So this is a list, correct? Okay. So then how fast is this divergence?

418
00:50:49,220 --> 00:51:00,280
So let's try to run it. Okay. So this algorithm, if you talk about 100, 100 times 200 matrix, this is going to take about 0.2 seconds.

419
00:51:01,270 --> 00:51:06,550
And, uh, built in matrix, operation takes 0 seconds.

420
00:51:06,730 --> 00:51:11,180
Okay. So let's make it a little bit bigger. So hundred.

421
00:51:11,200 --> 00:51:15,520
So let's, let's calculate hundred times 100 is a point 2 seconds.

422
00:51:15,730 --> 00:51:22,750
So if you the 300 by 300, this is until we got to Williams so times 27 may take a lot of time.

423
00:51:22,960 --> 00:51:29,290
So let's see whether whether this is true. So if it's like, could it be like 5 seconds or something?

424
00:51:29,740 --> 00:51:33,430
Okay. So which is true. It takes 5 seconds.

425
00:51:33,610 --> 00:51:41,320
Okay. So it takes 320 by 300 is not a large matrix, but also already it's very, very slow.

426
00:51:41,330 --> 00:51:45,790
But, you know, built in this multiplication is really, really fast.

427
00:51:46,420 --> 00:51:51,550
And if you think about stress in algorithm stress, an algorithm fit is not supposed to make this fast.

428
00:51:51,910 --> 00:51:59,440
So there must be something else. Right. So and that's the implementation inside the implementation.

429
00:51:59,450 --> 00:52:05,560
So let's go beyond the time complexity. And sometimes time complexity matters, but sometimes other things matter.

430
00:52:06,280 --> 00:52:09,880
So let's try to change this implementation a little bit.

431
00:52:10,210 --> 00:52:19,030
Okay. So instead of doing the triple loop less the W, but I cheated because it's double loop.

432
00:52:19,030 --> 00:52:23,500
But the double of this is another loop is the just summation of two vectors.

433
00:52:24,220 --> 00:52:33,490
But there's a we know that R has some function. So let's, let's use the some function because there's no really need to implement the, the full loop.

434
00:52:33,700 --> 00:52:38,140
And I just did it for convenience, not even for efficiency purpose.

435
00:52:38,530 --> 00:52:41,890
And let's see what happens. Good. So this should be equivalent.

436
00:52:42,250 --> 00:52:47,620
Okay. So do the same thing. Contribute 100. Okay, then time is.

437
00:52:47,620 --> 00:52:54,760
There's quite a bit difference in time though. I just change it to the third loop into just some function.

438
00:52:55,240 --> 00:53:00,910
And this became faster. Interesting. Let's try to do the 300 by 300.

439
00:53:01,630 --> 00:53:08,270
Okay. And this is what you have.

440
00:53:08,540 --> 00:53:13,790
So this is a 27 times and a lovely and this a point of five to.

441
00:53:14,210 --> 00:53:17,720
So this is a this is not quite 20 something times.

442
00:53:17,720 --> 00:53:22,280
This is like, uh, it's a it's a little faster.

443
00:53:22,280 --> 00:53:25,550
So it's interesting. So it's a 16 times. Right.

444
00:53:26,180 --> 00:53:33,210
So the reason is that just some operation itself is faster.

445
00:53:33,250 --> 00:53:38,059
There's a, there's any built in functioning. Ah, you don't realize.

446
00:53:38,060 --> 00:53:44,800
But our developers put a lot of effort to make this run as fast as possible, especially if it's open.

447
00:53:44,990 --> 00:53:48,830
The function that is used a lot of times. So that's why.

448
00:53:49,160 --> 00:53:54,170
Okay, so now we're not so let's 2000 by 2000.

449
00:53:54,200 --> 00:53:59,389
If you think about a thousand by 2000, then this will take like a 2 minutes or something.

450
00:53:59,390 --> 00:54:09,590
So let's, let's do, let's not do the first function, just comment it out, but let's compare the second function and third function.

451
00:54:10,490 --> 00:54:16,030
Now, you know, because this is a this is ten times larger.

452
00:54:16,040 --> 00:54:19,120
It was a .0.05 before.

453
00:54:19,130 --> 00:54:22,250
So it could take like almost 50 seconds, I think.

454
00:54:22,400 --> 00:54:24,830
But in the 50 seconds it takes 20 seconds.

455
00:54:24,830 --> 00:54:32,810
So it's not it's still like a or there is a range of the becoming in cubic, but slightly faster than you can imagine.

456
00:54:33,200 --> 00:54:36,859
So it didn't really improve the time complexity.

457
00:54:36,860 --> 00:54:45,349
But the reason why this is a slightly faster than cubic is just because of the summation, because last loop is becoming more and more efficient.

458
00:54:45,350 --> 00:54:50,329
When you try to do the summation across a lot of elements because there there is

459
00:54:50,330 --> 00:54:56,840
some parallel edition it's a is called like the it's not really generalization,

460
00:54:56,840 --> 00:55:02,600
but it's a speed up when you try to do the vector operation that is that is implemented in most of that interpretation.

461
00:55:03,830 --> 00:55:09,250
Okay. So even though we learned this trusted algorithm,

462
00:55:09,430 --> 00:55:20,570
the reason why this this procedure became faster is mostly because mostly because these are improvised,

463
00:55:20,590 --> 00:55:23,860
a more efficient way to do the basic operation.

464
00:55:24,160 --> 00:55:29,380
So using built in function actually helps helps the patients.

465
00:55:31,390 --> 00:55:34,870
Later in the lecture, we're going to learn slightly about the recipe.

466
00:55:35,200 --> 00:55:38,739
Okay. You don't have to run asleep.

467
00:55:38,740 --> 00:55:43,090
We're when we're not going to give you any homework that requires our sleep implementation.

468
00:55:44,380 --> 00:55:56,310
But I do recommend to learn how to sleep because that can sometimes change a lot regarding your research.

469
00:55:56,320 --> 00:56:04,240
So if you have some part that is computationally very intensive using our sleep, you can make a lot more efficient.

470
00:56:04,240 --> 00:56:08,979
So you still can in all the C++ part of the six or seven,

471
00:56:08,980 --> 00:56:17,350
if you haven't learned C++ and if you learn our sleep through that way because you just need to learn basics

472
00:56:17,350 --> 00:56:24,800
of the C++ and it's very Onshape is actually I think it's better than easier than learning C++ by yourself.

473
00:56:24,820 --> 00:56:26,080
So that's,

474
00:56:26,080 --> 00:56:35,080
that's what I think because you have already quite a bit of knowledge in R and you just need to learn some of the C++ syntax and how to debug in that.

475
00:56:35,080 --> 00:56:42,610
That's what you need. So I strongly encourage you to run learn RCP more even though we we don't we don't cover them that much.

476
00:56:43,360 --> 00:56:49,150
Okay. And you're more than welcome to do your project in RCP and that it will be always

477
00:56:49,960 --> 00:56:54,790
appreciated and there'll be that will be considered the more challenging,

478
00:56:56,080 --> 00:57:03,760
you know, way to do the do the assignment. So that will be that there will be a plus point if you use RCP in your project.

479
00:57:05,380 --> 00:57:14,330
Okay. So that's, that's the part for the for the metrics and paging methods, multiplication.

480
00:57:15,160 --> 00:57:24,040
This just a one slide I just wanted to mention. So how many of you learned to learn this term before class playback?

481
00:57:24,400 --> 00:57:28,840
How many of you heard about this before? See that?

482
00:57:28,900 --> 00:57:37,660
There is a very few people heard about this. Okay, so it's a it's it's totally fine if you haven't learned about it.

483
00:57:37,660 --> 00:57:45,750
And you probably don't need to learn about it. If you don't, you don't have to use the plus and LPAC knowing me.

484
00:57:46,840 --> 00:57:53,680
But you are using this a lot when you do your research because this is the software

485
00:57:53,680 --> 00:57:58,780
libraries that are used for design use for expediting this matrix operations.

486
00:57:59,840 --> 00:58:09,670
So R Python and Julie are pretty much every languages are actually not implementing matrix operation themselves.

487
00:58:09,910 --> 00:58:15,910
They are using these libraries and there are actually multiple version of these plus analytic libraries.

488
00:58:16,060 --> 00:58:23,770
Okay. So depending on the what kind of library, what, what kind of library, it's using it for the matrix operation.

489
00:58:24,070 --> 00:58:27,640
You actually see a lot of differences in terms of efficiency.

490
00:58:27,670 --> 00:58:35,830
So for example, there is at least three, three plus in light packed libraries that are that is widely available.

491
00:58:35,860 --> 00:58:41,920
One is the venue blessing light pack, which is default pretty much out for for everything.

492
00:58:42,470 --> 00:58:49,000
There's something open B+ which is usually faster than default and there's an Intel M tail button is even faster.

493
00:58:49,510 --> 00:58:53,640
But you know, not, not every environment to support that.

494
00:58:53,650 --> 00:59:04,780
So those are those are very are three very popular I would have sometimes if you do very complex complicated matrix operations.

495
00:59:05,230 --> 00:59:10,600
Some implementation takes like, you know, 100 times better than the other.

496
00:59:10,600 --> 00:59:15,700
Sometimes it happens. So yeah, that's something you may want to keep in mind.

497
00:59:17,200 --> 00:59:24,550
So you can check whether what kind of class and latest version it's using by using just session info.

498
00:59:24,920 --> 00:59:29,590
Okay. Then it says what kind of blessing light peg is being used in.

499
00:59:29,590 --> 00:59:37,300
This are the first first few lines. So this is a if you see this, this says open, bless.

500
00:59:37,570 --> 00:59:44,830
This is open, bless. So this is using. So the it doesn't mean that Open Bless is installed in your machine.

501
00:59:44,950 --> 00:59:51,189
This means that Open Bless is installed in the machine that is used as a backend for this Google collapse.

502
00:59:51,190 --> 00:59:58,479
So Google, pretty much every machine that I set up for CoLab is using open blast because they know open less is faster.

503
00:59:58,480 --> 01:00:02,800
They use less of their CPU. So that's why that's why they didn't get.

504
01:00:04,840 --> 01:00:08,840
You know, Intel employees, it's probably harder to use for them.

505
01:00:10,130 --> 01:00:14,240
Okay. So but these are the implementations.

506
01:00:14,250 --> 01:00:20,720
So if you these are going to default, open blessing, Internet kill, there's some outliers in other other versions.

507
01:00:21,380 --> 01:00:25,940
But you just need to understand that there's a multiple version and that's that's what you need to know.

508
01:00:25,940 --> 01:00:34,490
You don't need to know what's the difference in each of them. And usually my general advice is that if you have to use the default, that's okay.

509
01:00:34,850 --> 01:00:41,060
But if you have a chance to switch it up, there's an interim call that's better if you don't know how to,

510
01:00:42,080 --> 01:00:45,290
you know, your sysadmin should help a bit, should be able to help.

511
01:00:45,290 --> 01:00:48,290
For example, in the Great Lakes, the two version of AH,

512
01:00:48,770 --> 01:00:57,020
there's a R version that uses a default going to a pleasant light pack and there's a R open blast which is actually using open place.

513
01:00:57,230 --> 01:01:02,240
You probably didn't know that so far, but if you're using some of the universe cluster,

514
01:01:02,690 --> 01:01:06,520
you can load the version of AH that is using open place and make it a lot less.

515
01:01:08,270 --> 01:01:13,190
Okay, so that's that, that's let's move on.

516
01:01:15,700 --> 01:01:19,930
Okay. So now talk about the matrix inversion.

517
01:01:20,320 --> 01:01:24,100
Okay. So now we talk about the metals multiplication.

518
01:01:24,100 --> 01:01:28,210
I understand that's too big. And get get can get slightly faster.

519
01:01:28,840 --> 01:01:31,750
Matrix inversion basically is kind of very similar.

520
01:01:32,380 --> 01:01:47,920
So there is an algorithm that does, uh, that uses this kind of recursive formula to reduce the time complexity into a, uh.

521
01:01:49,930 --> 01:01:55,270
Yeah. Into. So. Into this. Uh.

522
01:01:57,400 --> 01:02:00,800
Okay. So this is a little,

523
01:02:00,850 --> 01:02:09,300
little confusing for when is a time complex before the matrix inversion and a T is the time complex for the matrix multiplication.

524
01:02:09,310 --> 01:02:18,650
So this is the recursive formula because you need a combination of matrix inversion to meet this inversion and six matrix multiplication.

525
01:02:18,660 --> 01:02:29,950
So if you see here, so to to calculate the matrix inversion of this, you need to do the inverse and you need to the inverse.

526
01:02:30,070 --> 01:02:35,860
So you basically have two to matrix inversion and a lot of matrix multiplications.

527
01:02:36,370 --> 01:02:41,420
So I'm not going to the detail of this because the formula, again, this is interesting.

528
01:02:41,440 --> 01:02:46,329
And if you wanted to learn about this, it's it's it's definitely interesting.

529
01:02:46,330 --> 01:02:50,880
But this is not the main part where you can make the huge amount improvement.

530
01:02:51,940 --> 01:02:58,329
But but but because this is a recursive formula, you can prove that if you if someone implement this recursive way,

531
01:02:58,330 --> 01:03:10,630
which actually belongs in Apex does already, then you can make the time complexes end to the 2.3373 instead of 382.2.8.

532
01:03:10,750 --> 01:03:15,639
For the stretching, I would say it's a little bit faster and it's easier to make a faster inversion

533
01:03:15,640 --> 01:03:20,260
algorithm than a person multiplication algorithm in terms of time complexity.

534
01:03:21,340 --> 01:03:24,850
But the lower bound is still an and scale over.

535
01:03:26,490 --> 01:03:30,459
So so you're welcome to implement this algorithm yourself.

536
01:03:30,460 --> 01:03:33,400
But that's not the main part we would like to learn.

537
01:03:35,260 --> 01:03:45,250
So calculating the matrix determinant also, and pretty much every matrix operation for the squared metrics is in cubic.

538
01:03:46,390 --> 01:03:53,590
But there are also recursive way to make make the matrix determinant calculation faster.

539
01:03:53,890 --> 01:03:58,420
And the fastest argument it's known is also N2, the 2.2373.

540
01:03:58,840 --> 01:04:05,110
Okay. But you don't need to worry about implementing that because you just are, you know,

541
01:04:05,500 --> 01:04:15,640
the usually just going to use the lesson like pick library instead of making those low level matrix multiplication, inversion or determinant yourself.

542
01:04:16,270 --> 01:04:19,580
Okay. Okay.

543
01:04:20,090 --> 01:04:29,030
So now let's talk about this. Let's say you have a sum.

544
01:04:29,930 --> 01:04:33,530
So matrix, matrix and vector. So here you is a vector.

545
01:04:34,070 --> 01:04:43,400
A is a matrix. And to calculate this, okay, let's say every vector, every matrix, the size of N, so size of end vector was scared matrix.

546
01:04:44,270 --> 01:04:48,110
Okay. What is the time complexity to calculate this?

547
01:05:17,090 --> 01:05:22,070
So let's say X is a vector, size and vector.

548
01:05:22,940 --> 01:05:35,340
Okay. Why is a silent actor kept up as side and why and matrix and BS and mayhem antics.

549
01:05:35,640 --> 01:05:40,650
Okay. And let, let's, let's think about the basic things first.

550
01:05:41,760 --> 01:05:45,930
If you do the back to everybody, back for multiplication, what's the complexity?

551
01:05:50,510 --> 01:06:00,830
This time, complex and linear. Great. Okay. What is the time complexity for, uh, matrix times vector?

552
01:06:05,640 --> 01:06:14,660
Any idea? Just the neighborhood. And scared and scared.

553
01:06:15,050 --> 01:06:20,480
Very good. Why was I scared? You need to calculate the different elements.

554
01:06:20,660 --> 01:06:26,140
Right. So because of the outcome is vector and different than each of the elements.

555
01:06:26,300 --> 01:06:30,260
And different elements just requires that summation of ten different elements.

556
01:06:30,470 --> 01:06:34,610
So this isn't scared if you have an eight kind of speed.

557
01:06:37,550 --> 01:06:50,060
Uh, that's, uh, we learned that this is for the order of and Cuba and this is,

558
01:06:50,990 --> 01:07:00,480
this can be further expedited with a 2.8 or seven if you use stress in a way.

559
01:07:01,070 --> 01:07:05,149
Okay. So that's the time complexity.

560
01:07:05,150 --> 01:07:11,050
So let's forget about this. Let's I say this is and and and it's good and quick.

561
01:07:11,450 --> 01:07:17,240
Okay. That what is the what is time complexity of computing this?

562
01:07:21,810 --> 01:07:24,810
You transpose ab3.

563
01:07:38,870 --> 01:07:42,620
Okay Radiohead if you think this can be quite good the radio time.

564
01:07:46,140 --> 01:07:50,090
Raise your hand if you think at this time you calculate the quadratic term complexity.

565
01:07:52,990 --> 01:07:56,400
Raise your hand if you think this can be calculated cubic time complexity.

566
01:07:58,150 --> 01:08:03,910
Okay. If it needs quite well, because I did.

567
01:08:03,940 --> 01:08:07,690
Oh, you're. You're just trying to do this, right? So by a typo.

568
01:08:07,870 --> 01:08:12,700
Okay, so if you think a type of is an end to the end to the fourth.

569
01:08:14,530 --> 01:08:17,560
Okay, so what? So many.

570
01:08:18,010 --> 01:08:22,089
So the majority of you don't have an opinion now. You need to.

571
01:08:22,090 --> 01:08:25,480
You need to make a make up your mind. Okay.

572
01:08:28,400 --> 01:08:32,930
So the answer is you can do this or this.

573
01:08:33,680 --> 01:08:38,210
Okay. Both of them. Why?

574
01:08:38,570 --> 01:08:43,340
Okay. So let's say I calculate this eight times to be first.

575
01:08:44,210 --> 01:08:49,370
Then what? So let's say this is older than eight times would be.

576
01:08:49,370 --> 01:08:54,470
Well, we will return make the return a matrix so it'll take the order of an cubic.

577
01:08:55,310 --> 01:09:03,880
Then you calculate the you times you transpose another matrix which is a quadratic time and after that it's a vector, ten vector.

578
01:09:03,890 --> 01:09:09,920
So it's either way. Okay, so this is an end to the cubic overall.

579
01:09:10,820 --> 01:09:14,510
Right. But what if you just do this?

580
01:09:16,430 --> 01:09:19,700
Then this will return a vector like this.

581
01:09:20,060 --> 01:09:23,330
And this is the quadratic operation.

582
01:09:23,540 --> 01:09:27,530
This will return. Return the vector again. So quadratic equation.

583
01:09:27,830 --> 01:09:32,840
Oh, you to quadratic operation. And the last one is of order of N.

584
01:09:33,800 --> 01:09:38,510
So then there is no cubic operation. Okay. So you can.

585
01:09:38,510 --> 01:09:41,960
You can calculate this into the the quadratic operation overall.

586
01:09:42,470 --> 01:09:46,580
So depending on the which order you calculate, you can change the complexity.

587
01:09:47,270 --> 01:09:55,420
Okay. So it's important to understand this because you at least that you need to know what's is a lot of time complicated.

588
01:09:55,870 --> 01:10:05,230
Okay. So, uh, so how about the further adding multiplication?

589
01:10:05,440 --> 01:10:09,880
So let's say you are calculating this.

590
01:10:11,290 --> 01:10:30,130
Okay, then. So when you calculate this, okay, this, this, the time complexity of calculating this element is or if you if you do this way,

591
01:10:30,820 --> 01:10:37,030
I want to calculate this part the first order one squared and the calculate the vector times vector with a variance.

592
01:10:37,030 --> 01:10:42,429
So this is the otherwise. Okay, so you cannot change the time complexity.

593
01:10:42,430 --> 01:10:49,389
This is the best, the best you can do, but you can actually improve the running time differently.

594
01:10:49,390 --> 01:10:53,140
Even the time complexity is saying, okay, what do I mean?

595
01:10:53,380 --> 01:11:00,970
Okay, so first of all, instead of doing this two separate operation, you can do this.

596
01:11:01,690 --> 01:11:09,970
Okay? So first, the way to do it is you can just make it a single double loop at once, okay?

597
01:11:10,510 --> 01:11:18,820
Because of this, this is equivalent to this. So you can just do the multiplication all together within one loop that's arguably faster.

598
01:11:20,770 --> 01:11:27,579
And another case, which is a probably more complicated example, let's say,

599
01:11:27,580 --> 01:11:38,950
you know this a what the A's and let's say you happen to know what's the the the lower triangular matrix that looks like this.

600
01:11:38,950 --> 01:11:52,150
It is actually a sticky decomposition, basically. Then let's say you have a decomposition of this a as the the multiplication two matrix.

601
01:11:52,900 --> 01:12:06,219
Then what you can do is that you basically have x this an l l transpose x because this is this is

602
01:12:06,220 --> 01:12:16,299
what else looks like then this is basically x times well l times at all times x transpose transpose.

603
01:12:16,300 --> 01:12:21,160
So you basically have something transpose and something, okay?

604
01:12:21,640 --> 01:12:33,760
So you can calculate this part first and make, make them as to make them as just vector multiplication up the vector multiplication.

605
01:12:33,760 --> 01:12:40,840
So in this case, so this is a very special case where you actually can pre calculate this, this, this value.

606
01:12:41,440 --> 01:12:49,060
But if you have a way to pre calculate this, you could change this time complexity to this case.

607
01:12:49,210 --> 01:12:57,760
Okay. So it's a sometimes linear time, right? But obviously this is a cheating because they assumed that this was pre quite related.

608
01:12:58,420 --> 01:13:03,069
But this happens a lot because of this quadratic equation is used in various calculations.

609
01:13:03,070 --> 01:13:13,780
Sometimes you have you have access, you have an excess of this value and apparently you can't calculate this value of it previously.

610
01:13:14,080 --> 01:13:22,090
So so you can calculate pre compute this value and you can use them in using this operation.

611
01:13:22,090 --> 01:13:34,870
So in this case, if you can, you store some value and if you can use them, sometimes you can make some of the quadratic complexity equations,

612
01:13:35,560 --> 01:13:41,070
equations you can you can do do the computation in the linear time with some clustering.

613
01:13:41,320 --> 01:13:49,820
Okay. Okay. So and now let's let's get into the more complicated case.

614
01:13:51,200 --> 01:13:55,220
So let's let's now talk about the solving linear systems.

615
01:13:55,340 --> 01:14:02,990
Okay? If you are solving linear system, a lot of cases you have solving this equation A, X equals B, right?

616
01:14:03,800 --> 01:14:11,160
So then how do you solve this? Solve this? So obviously you calculate the inversion and you calculate this, right?

617
01:14:11,570 --> 01:14:18,440
How much time does it take? Well, I need to calculate the inverse, which takes N and then cubic in the neighbor region.

618
01:14:18,710 --> 01:14:24,470
So in cubic and I need to do the math to expect a multiplication.

619
01:14:24,470 --> 01:14:28,430
So another quadratic. So this is cubic. Okay, good.

620
01:14:29,660 --> 01:14:33,890
So. Well, this works. It works fine. But there are two problems.

621
01:14:34,010 --> 01:14:40,670
One is that, well, the calculating inverse inversion takes time, but that's not the main problem.

622
01:14:41,540 --> 01:14:44,510
Sometimes the calculating version is not numerically stable.

623
01:14:46,370 --> 01:14:53,930
Okay, so you may not have dealt with a lot of matrix inversion, but some matrix is not immutable.

624
01:14:54,020 --> 01:15:05,149
Right? And even if invariable, if the the matrix, the smallest that and value of the matrix is very small, that is it is not very stable.

625
01:15:05,150 --> 01:15:11,930
If there is a vertical linear in the in your matrix, you can be very unstable.

626
01:15:12,200 --> 01:15:25,550
Okay. So, so instead of using actual matrix inversion, there are a lot of ways to solve this using matrix decomposition.

627
01:15:25,820 --> 01:15:29,690
Okay. I'm going to go through multiple ways to do this.

628
01:15:29,840 --> 01:15:34,460
Okay. So hopefully I can cover these two. Still this in our. Okay.

629
01:15:35,760 --> 01:15:38,810
So what do I mean? Okay.

630
01:15:39,830 --> 01:15:43,870
So first one is using LDA decomposition.

631
01:15:44,270 --> 01:15:50,179
So well, let me let me check those two because I assume that you know the basics of linear algebra,

632
01:15:50,180 --> 01:15:53,690
but just wanted to check how many of you know what the value decomposition is.

633
01:15:57,220 --> 01:16:02,530
Okay. Is it is it true? So how many people you don't know what the [INAUDIBLE] you would accomplish?

634
01:16:04,660 --> 01:16:08,680
Okay. How many people don't know what the cure, the competition is?

635
01:16:11,150 --> 01:16:17,570
So even so with so many people, no more college campuses in the area to compete.

636
01:16:17,600 --> 01:16:23,750
That's good. How about Solecki, the competition? How many of you don't know what the tallest thing in the competition is?

637
01:16:25,340 --> 01:16:28,670
Okay, so that that's not more common. So that's fine.

638
01:16:29,450 --> 01:16:34,669
How about the again, the competition. Everyone knows what I get.

639
01:16:34,670 --> 01:16:37,970
The question is how? How many people don't know what I get?

640
01:16:38,000 --> 01:16:41,510
I guess the competition is about the singular value the competition is.

641
01:16:42,320 --> 01:16:45,950
If you don't know what the single reality competition is. Okay.

642
01:16:46,670 --> 01:16:57,380
So those are the five the competition. I'm assuming that, you know, because this is the basic, basic knowledge of what the linear algebra is required.

643
01:16:58,700 --> 01:17:03,830
If you don't know, you need to go to Wikipedia or whatever just to understand what the concept is.

644
01:17:04,050 --> 01:17:12,260
Okay. I'm glad that many people know what I get the competition because other parts are, you know, less complicated to understand.

645
01:17:13,190 --> 01:17:16,610
But that's that's what I'm assuming you to know.

646
01:17:17,060 --> 01:17:19,780
Okay. Before going into this this thing. Okay.

647
01:17:21,440 --> 01:17:28,820
So if you don't know the matrix, the competition is not a not a big deal because we're not it's not a linear algebra class.

648
01:17:29,060 --> 01:17:32,870
But sometimes if you don't know, you just need to know.

649
01:17:32,980 --> 01:17:38,100
Okay. So you need to learn that. Okay. Okay.

650
01:17:38,330 --> 01:17:41,420
So what is the value of the competition? Okay. All new.

651
01:17:41,420 --> 01:17:49,610
The competition is l usually when you have some images, the competition when you have l, that means a lower triangular matrix.

652
01:17:50,390 --> 01:17:53,420
When you have a U, that's an upper triangular matrix.

653
01:17:53,690 --> 01:17:59,030
Okay. So you can make a square.

654
01:17:59,390 --> 01:18:02,450
So assume that a is a square and invariable.

655
01:18:03,200 --> 01:18:13,040
Then you can make a square in whatever matrix is a multiplication of l draw triangle matrix and the times of multiple triangular matrix.

656
01:18:13,220 --> 01:18:24,200
Okay. So this makes U of x equivalent to the.

657
01:18:26,340 --> 01:18:35,520
So so let's let's do this a equals l u then we need to solve a x equals v, right?

658
01:18:36,150 --> 01:18:45,510
And this is l l u x in x is v right.

659
01:18:46,050 --> 01:18:53,580
And if you multiply l'inverse because the lower triangular matrix there, they're easy to find the inverse.

660
01:18:53,970 --> 01:18:58,380
So you have this so that that's what it means.

661
01:18:58,500 --> 01:19:04,559
Okay, so well, what, why is this?

662
01:19:04,560 --> 01:19:10,470
So this is important. Okay, so that's probably what, what you need to know.

663
01:19:10,920 --> 01:19:16,440
Okay. So basically, LMC is a u u inverse.

664
01:19:22,120 --> 01:19:26,680
Alley bursts and B right. So you get you can quite get the this way.

665
01:19:30,040 --> 01:19:34,210
So why why this earlier? The composure is useful.

666
01:19:34,540 --> 01:19:45,910
Well, if you have a lower triangular matrix or upper triangular matrix, that is easier to invert because you.

667
01:19:46,180 --> 01:19:52,030
So it's a thought if you if you think about the inverting event emerging events that are that are a little bit triangular.

668
01:19:52,840 --> 01:19:58,840
So you it's a it's a pre-built so trivial to solve the one the first equation.

669
01:19:58,990 --> 01:20:03,459
Secondly patient once you know the series in the first equation it's a it's easy to solve.

670
01:20:03,460 --> 01:20:11,860
So it is a if you have a triangular matrix, finding inverses about your problem is a quadratic problem rather than a cubic problem.

671
01:20:12,700 --> 01:20:18,310
So you basically once you have this decomposition, this becomes a quadratic problem.

672
01:20:18,770 --> 01:20:24,250
The different finding the composition is obviously could be a problem, but this is one way to solve it.

673
01:20:24,340 --> 01:20:27,969
Okay. Because a it's a wicket.

674
01:20:27,970 --> 01:20:32,140
Ours is just to I'll just go back go back to this examples.

675
01:20:32,140 --> 01:20:41,770
But basically, this one is not a recommended way to solve to solve the linear system because this is also numerically not stable.

676
01:20:41,770 --> 01:20:47,200
So that that's where I would stop and we'll will revisit this part from the next lecture.

