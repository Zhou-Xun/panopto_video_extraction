1
00:00:02,000 --> 00:00:06,080
So. Couple of things.

2
00:00:09,380 --> 00:00:13,820
So, um, so I mentioned that we,

3
00:00:13,820 --> 00:00:24,860
it looks like it would be down to the size where we can sort of move back to individual projects rather than group projects.

4
00:00:27,950 --> 00:00:35,540
However, I want to put that to a vote. So if you go under quizzes, you can find a vote.

5
00:00:35,840 --> 00:00:39,430
So you can vote whichever way you'd like. We want to keep the group projects.

6
00:00:39,920 --> 00:00:47,059
I can assemble two groups. We fix up the ones we have.

7
00:00:47,060 --> 00:00:53,709
Or we can. We could move back to the individual. So we do individual.

8
00:00:53,710 --> 00:01:02,350
Obviously we'll have less time for presentations and I'll have to think a little bit about the best ways to do that until we get some feedback,

9
00:01:02,410 --> 00:01:08,350
because I'm happy to take the vote anonymously.

10
00:01:08,350 --> 00:01:15,450
But if people have comments they want to make to the group. I'm kind of indifferent.

11
00:01:15,450 --> 00:01:18,959
Either way, I'm at the place of good projects as you get to work with somebody else,

12
00:01:18,960 --> 00:01:28,080
it's not all you and it's a good chance to practice it that the plus on the individual project is that,

13
00:01:28,080 --> 00:01:32,520
you know, it's easier, it's a waste of person to work with things on your own.

14
00:01:32,520 --> 00:01:39,290
So. And if people have come to get back to me, it's going to.

15
00:01:40,730 --> 00:01:51,170
So I think the deadlines will stay the same end of October for the brief proposal and of course, the end of the class with the action.

16
00:01:53,750 --> 00:02:02,899
And I do feel a project so good for this class. It's a good way to kind of get into the idea of causal inference beyond the context

17
00:02:02,900 --> 00:02:07,120
of something that you've actually got a chance to work on that you put your hands on.

18
00:02:07,130 --> 00:02:13,790
So. And again, thanks for the homework.

19
00:02:13,800 --> 00:02:17,460
I will try to get that back within a week.

20
00:02:19,200 --> 00:02:23,920
So that's early for your next week's news or don't get too far behind.

21
00:02:24,510 --> 00:02:38,370
Also spend some time going over the last. All right.

22
00:02:42,070 --> 00:02:45,340
So let's just kind of finish up.

23
00:02:47,670 --> 00:02:52,020
With a little wrap up fun.

24
00:02:55,190 --> 00:02:58,950
Estimation with matching time.

25
00:03:00,620 --> 00:03:04,340
But before I do that, any questions and important things that people want to bring up?

26
00:03:14,930 --> 00:03:24,860
Okay. So. Okay.

27
00:03:25,070 --> 00:03:32,030
I guess just to go over this again, but this came through quickly to catch the end, which we didn't quite finish.

28
00:03:32,780 --> 00:03:39,259
So we were looking at the booster seat estimation where we have about 2000 kids

29
00:03:39,260 --> 00:03:42,980
that were in booster seats and 5000 that were not in the SAGE 4 to 7 group.

30
00:03:44,210 --> 00:03:53,360
We did a 1 to 1 match based on a set of covariates relating to the age and type of vehicle severity of the crash and so forth.

31
00:03:54,170 --> 00:03:59,570
And we got this, this measure here using the know how long business really algorithm.

32
00:04:00,470 --> 00:04:05,900
But of course, this is only among the treated and it's kind of perhaps a little a little

33
00:04:05,900 --> 00:04:10,310
inefficient that it's really only sort of doing the computer one match with it.

34
00:04:11,180 --> 00:04:21,200
So so we then did the full case again.

35
00:04:21,200 --> 00:04:29,929
Now we want to match both sides. We want to match among the treated that among the controls and we allowed for multiple matching.

36
00:04:29,930 --> 00:04:37,370
So it wasn't just 1 to 1. We sort of took everybody that was sort of within this range of of one in terms of the caliper match.

37
00:04:37,370 --> 00:04:43,100
And so a lot of those individuals be perfectly matched or they might be off by one out of the various categories.

38
00:04:45,800 --> 00:04:50,150
And so we got a little different measure for the treated group.

39
00:04:50,150 --> 00:04:56,150
And then the people that were in car seats, we actually got a measure for them as well.

40
00:04:56,930 --> 00:05:04,520
We combine those, we get a point estimate that was pretty similar to what we saw with just among the treated the variance estimators.

41
00:05:04,520 --> 00:05:07,880
A little is a little smaller, so it actually comes out to be significant.

42
00:05:08,450 --> 00:05:13,370
And we had a handful of treatment and control cases that could not be matched because there was

43
00:05:13,370 --> 00:05:19,910
simply nobody that was close enough on the other group to meet this an obvious distance criterion.

44
00:05:24,130 --> 00:05:30,130
So I think this is kind of where we left it.

45
00:05:31,690 --> 00:05:33,040
I had put in some code here.

46
00:05:35,860 --> 00:05:50,409
So basically we're going through basically just sort of trying to get the on working on the treatment side here to get this treatment effect.

47
00:05:50,410 --> 00:05:57,610
So this is very similar to what we did before, but now we're we're matching everybody.

48
00:05:58,660 --> 00:06:07,270
First of all, we're doing the math on this distance to to estimate the difference between for our treatment and among all the controls.

49
00:06:08,410 --> 00:06:13,090
And then we're to take everybody who fits that criteria.

50
00:06:14,740 --> 00:06:30,670
And then we're going to look at. The treatment outcome rate weakness is y y y of one for the individual.

51
00:06:31,810 --> 00:06:34,260
And then we're going to look at the mean of the matches, right?

52
00:06:34,270 --> 00:06:44,530
So this is the sum of the individuals in the control group, know the seatbelt group, if you will,

53
00:06:45,880 --> 00:06:50,260
who meet the matching criteria divided by the total number to meet that mentioned criteria.

54
00:06:53,140 --> 00:07:02,670
So. So that gives us this y one minus y zero within each individual.

55
00:07:05,220 --> 00:07:10,320
And then we're going to. Compute the mean there.

56
00:07:13,700 --> 00:07:20,189
Dropping the non matched cases. So this this M one is keeping track of the number of match controls.

57
00:07:20,190 --> 00:07:23,600
So if it's zero, we didn't have anybody to match.

58
00:07:23,640 --> 00:07:28,310
We want to get rid of those individuals and then we're gonna compute the variance.

59
00:07:32,370 --> 00:07:38,880
Using the kind of simple, very insistent manner here.

60
00:07:46,020 --> 00:07:49,080
Right this experiences to meet her here.

61
00:07:52,420 --> 00:08:04,920
We're just in the match group, but with multiple matches. And I just take it to mean the number of masked individuals.

62
00:08:04,920 --> 00:08:08,160
So this is kind of an approximation. If this was constant, then it would be. Exactly.

63
00:08:10,990 --> 00:08:18,730
And then we went standard error rate. So we need to divide by the total number of observations we're using.

64
00:08:22,850 --> 00:08:34,330
So that gives a sort of variances to matters of oops sorry to down here to talk to your tell tell

65
00:08:34,340 --> 00:08:43,850
one treatment effect treated sorry the question to that point that kind of gets that first piece.

66
00:08:44,550 --> 00:08:47,930
Now we're doing multiple matches so it's a little bit different. That's for me to.

67
00:08:55,700 --> 00:09:00,020
So then we play the same game in the control side. Everything just turns around now.

68
00:09:01,160 --> 00:09:04,880
And so for a control outcome, we observe that.

69
00:09:05,720 --> 00:09:16,850
And then our y1i hat that is going to be the mean of the match and y one's using this no matching criteria.

70
00:09:18,970 --> 00:09:24,530
And we can just compute. Tells the tale.

71
00:09:24,530 --> 00:09:29,330
City tells you cups of tea tells your hotels subsea.

72
00:09:30,020 --> 00:09:33,650
Sorry, but I mean of those.

73
00:09:33,710 --> 00:09:37,280
Why? One eyes and sky. Zero eyes.

74
00:09:44,000 --> 00:09:52,250
And then we combine them just by the fraction that are in each group to get the overall treatment effect.

75
00:09:58,240 --> 00:10:04,090
So but as I mentioned before, there's potentially quite a bit of correlation here that we're ignoring,

76
00:10:04,240 --> 00:10:09,580
which probably suggest or variance is pretty underestimated. So I did use a bootstrapping approach.

77
00:10:14,730 --> 00:10:17,880
To get at these values here.

78
00:10:20,480 --> 00:10:27,440
And so that gave us quite a bit larger standard error at the bootstrapping.

79
00:10:29,180 --> 00:10:33,240
So just to remind everybody, if you haven't seen this before,

80
00:10:34,790 --> 00:10:44,900
the basic idea in bootstrapping is you re sample reconfigure s to meter point estimator and you just do that again and again.

81
00:10:45,920 --> 00:10:51,979
Then you take the resulting variance of that as an estimate of the variance of your point.

82
00:10:51,980 --> 00:10:55,460
Estimate of interest. So here.

83
00:11:04,440 --> 00:11:10,920
And I did that 200 times is sort of a minimum number to get an estimate of the variance to do more.

84
00:11:10,950 --> 00:11:18,300
It does take a little while to do this. Each one of these this whole thing took 7 hours to run on my on my desktop.

85
00:11:18,570 --> 00:11:25,260
So it's not it's not trivial. The coding isn't really the coding for it's fairly straightforward.

86
00:11:25,350 --> 00:11:33,810
Just the one time takes a little bit. So as I said, I'm going to re sample within treatments and within controls.

87
00:11:35,100 --> 00:11:41,850
So to do that, I'm using the sample command norm and use replace it with true.

88
00:11:43,170 --> 00:11:52,469
If you replace equals false and you re sample, you're going to get exactly the same set of subject as you started with.

89
00:11:52,470 --> 00:12:00,209
But when you replace equals true you essentially get been getting the details of what this is.

90
00:12:00,210 --> 00:12:01,890
It's sort of a multi mobile, right,

91
00:12:01,890 --> 00:12:13,560
with one over in sub one or one or in some zero probability selection for each one of the individuals done and one or not times.

92
00:12:13,620 --> 00:12:17,970
So and I'm just using this, right, just to sort of.

93
00:12:20,780 --> 00:12:30,830
Compute. The the row numbers for each of them to keep track of their own numbers for each of the individuals and treatment and control.

94
00:12:32,480 --> 00:12:41,630
So. So I've got vectors of axes and vectors of outcomes here for treatment control.

95
00:12:42,350 --> 00:12:50,240
And I'm just going to pull off the ones that show up and the control group or the treatment resampling.

96
00:12:51,350 --> 00:12:56,810
So essentially, some individuals are being dropped. Other individuals are appearing multiple times.

97
00:12:57,200 --> 00:13:01,940
You can also do this with weights where you use weights equal to the number of individuals in our sample.

98
00:13:02,660 --> 00:13:06,050
But but here, I'm just going to do it more directly.

99
00:13:07,130 --> 00:13:12,110
And this is a vector I mean, this is a vector of outcomes. This is a matrix of my.

100
00:13:14,690 --> 00:13:19,400
My predictors, so what I'm going to use for the match.

101
00:13:19,580 --> 00:13:26,090
So then it goes to go through to the same game. But now I'm using just the re sampled data set.

102
00:13:27,560 --> 00:13:35,820
So. Right.

103
00:13:37,070 --> 00:13:41,870
So then we go through and the same things we did before.

104
00:13:45,410 --> 00:13:52,070
And then I compute my overall estimate of the use for each one of the bootstrap samples.

105
00:13:53,460 --> 00:13:57,350
I just do this to keep track of them and I compute the variance.

106
00:13:58,790 --> 00:14:02,870
And that's how I got. These coyotes here.

107
00:14:04,730 --> 00:14:12,180
That's right. You're. You're. To them because they are separate, straight.

108
00:14:12,190 --> 00:14:26,490
I don't have to worry about. But the correlation of crosses so I can just take using the formula variance of it over the.

109
00:14:29,010 --> 00:14:32,969
Independent quantities are the sum of the variances,

110
00:14:32,970 --> 00:14:40,890
and the variance of a constant times around variable is going to be that constant squared times in terms of random variable.

111
00:14:42,310 --> 00:14:49,100
So. Okay. So that was that little piece I just finished up.

112
00:14:51,290 --> 00:15:00,110
And the question is this. So we needed to account for the correlation because in this case we were like reusing like the mattress, right?

113
00:15:00,560 --> 00:15:06,110
If we use like one of those like earlier methods where there wasn't any room for using, we don't need to do this.

114
00:15:06,110 --> 00:15:09,440
Right. Those seemed to support that problem.

115
00:15:20,670 --> 00:15:25,970
Okay. So now we're going to switch gears a little bit and we're going to introduce this idea of

116
00:15:25,980 --> 00:15:32,730
causal association within the context of what's often termed principle stratification.

117
00:15:34,860 --> 00:15:43,500
So this just sort of pushes us out into a to a new area where we now want to account for post-treatment variables.

118
00:15:44,400 --> 00:15:49,920
So for variables that we're trying to bring into our analysis that we measure after treatment has occurred.

119
00:15:52,830 --> 00:16:04,739
So so our previous work in this area and I'm with this, we're going to use this idea and I'll say it twice here.

120
00:16:04,740 --> 00:16:12,390
So the basic idea in this is that we come with this concept of principal strata, where we essentially take these post-treatment variables,

121
00:16:13,860 --> 00:16:21,240
consider their potential outcome, turn them into picture outcomes as well, and then we stratify based on those potential outcomes.

122
00:16:22,110 --> 00:16:30,330
So we've essentially created a pretreatment variable as a function of potential outcomes which allow which are the stratification,

123
00:16:31,020 --> 00:16:34,550
term principles, freedom and the resulting causal.

124
00:16:35,040 --> 00:16:41,040
You can then do estimate effects conditional.

125
00:16:41,070 --> 00:16:45,690
These principal strata then have the same kind of cause interpretation that we have.

126
00:16:45,810 --> 00:16:51,390
In our previous analysis. We were going with pretreatment variables because we created this kind of pseudo pretreatment variable.

127
00:16:52,320 --> 00:16:58,410
So, so it's going to be a little abstract briefly here and then I'm going to kind of get to a specifics.

128
00:16:58,410 --> 00:17:07,620
And I think that'll become clearer as to as to what the do with some of that some of the more abstract concepts become clear.

129
00:17:08,550 --> 00:17:14,970
So again, so our discussion around causal inference from randomized experiments to observational data,

130
00:17:15,690 --> 00:17:19,169
I think has provided a firm foundation for inference from randomized experiments,

131
00:17:19,170 --> 00:17:23,370
but it hasn't really broken a lot of new ground for causal infants and more general settings.

132
00:17:24,010 --> 00:17:29,220
And particularly, we've really assumed that all of our coverage of interest groups are pretreatment, right?

133
00:17:29,790 --> 00:17:37,890
So, so like in the given with the observational data, with our car crash data set,

134
00:17:38,910 --> 00:17:43,370
everything we were matching on was something that was sort of before the crash occurred, right?

135
00:17:43,440 --> 00:17:51,570
It was the type of vehicle who was driving the car and in those those kinds of effects.

136
00:17:52,410 --> 00:18:03,380
So so if we're interested in post-treatment covariates, you know, then if we need to do that, they can be impacted by treatment assignment.

137
00:18:06,210 --> 00:18:12,990
So one solution to this is just consider the joint distribution of potential outcomes of the post-treatment variable.

138
00:18:13,920 --> 00:18:18,960
So now we have a pretreatment variable that we can condition on the same manner as any other pretreatment variable.

139
00:18:20,250 --> 00:18:24,330
And so this is this idea of principle straight up. So.

140
00:18:27,070 --> 00:18:39,920
So if we have some post-treatment variable se if we partition the units so that the outcome that post-treatment for the outcome for that

141
00:18:39,920 --> 00:18:49,670
post-treatment variable under control is as not and under treatment as s one or more generally that occur across a number of possible treatments.

142
00:18:51,620 --> 00:19:01,600
Then if they all have that same value, then that basically becomes this this principle strategy, right?

143
00:19:01,640 --> 00:19:07,850
It's a potential outcome can think about the potential outcome under treatment level of S.

144
00:19:08,360 --> 00:19:12,700
S of a. So.

145
00:19:15,110 --> 00:19:21,800
So friend Gorka said Ruben this is sort of their and their have their papers with this 22 biometrics paper where they

146
00:19:22,460 --> 00:19:28,130
define this and they go under to fine principle fixes the comparison potential outcomes within a principal stratum.

147
00:19:29,750 --> 00:19:33,379
So right if we sort of think about a binary.

148
00:19:33,380 --> 00:19:39,630
Yes. And two treatment levels. Here in terms of working.

149
00:19:50,220 --> 00:20:02,580
So this principles foundation concerned. Basically the people on orbit.

150
00:20:04,620 --> 00:20:18,640
Right. And. The principal said that the Wall Street number could be zero or one under the treatment or control.

151
00:20:19,580 --> 00:20:33,890
But she said. Zero.

152
00:20:39,610 --> 00:20:45,680
I'd be zero control or treatment.

153
00:20:50,280 --> 00:20:54,720
One. You control your own country.

154
00:20:58,240 --> 00:21:11,750
One, two, three, four. For that.

155
00:21:11,900 --> 00:21:16,890
To be clear, if it appears so, they can call.

156
00:21:18,170 --> 00:21:29,690
So these are essentially four different possible for you. So it's one where our tax treatment variable is zero.

157
00:21:29,840 --> 00:21:33,709
During the treatment control two was zero.

158
00:21:33,710 --> 00:21:39,650
To control the treatment, in three words 100 zero zero.

159
00:21:39,650 --> 00:21:43,670
In the treatment and prevention for which one?

160
00:21:43,880 --> 00:21:46,250
No matter what treatment was.

161
00:21:56,060 --> 00:22:04,130
So we can then think about comparing potential outcomes to treatment and control conditional on being in one of those strata.

162
00:22:07,350 --> 00:22:14,249
Or more generally. We have more than two potential outcomes we can prepare, you know, among that.

163
00:22:14,250 --> 00:22:19,260
So I mean more than two to levels of treatment that we can prepare among different levels of treatment.

164
00:22:19,650 --> 00:22:24,320
Again, conditions trigger other potentially more than for straight event because you know,

165
00:22:24,330 --> 00:22:30,420
if you think about the potential outcomes under more than two treatments.

166
00:22:32,770 --> 00:22:35,890
So. But the concept is the same.

167
00:22:40,120 --> 00:22:45,070
And I guess the bottom line then, because these principles trade are not affected by treatments if.

168
00:22:45,430 --> 00:22:53,200
Right. These are things that are sort of back to being three pretreatment variables that these principal effects are indeed causal effects.

169
00:22:59,960 --> 00:23:03,200
So one area and this is how we're going to focus on,

170
00:23:03,200 --> 00:23:07,579
for example one key area was you could see this been used is in situations

171
00:23:07,580 --> 00:23:11,570
where you may have a randomized trial but you may not have perfect compliance.

172
00:23:13,100 --> 00:23:22,820
So so even though the assignment, if you will, is randomized, if you think about how they comply,

173
00:23:22,850 --> 00:23:28,370
that becomes s that's now something that's observed post-treatment.

174
00:23:29,470 --> 00:23:36,730
It's not necessarily a randomized variable anymore. So you might want to think about.

175
00:23:40,150 --> 00:23:46,750
Causal effects given different types of compliance, right?

176
00:23:46,840 --> 00:23:48,790
So you may have some subjects that comply.

177
00:23:48,790 --> 00:23:55,130
They take the treatment if they're assigned or they don't if they're not, may have some said they just can't or won't take the treatment.

178
00:23:55,150 --> 00:24:00,879
So they're in a different strain and conceivably could have even potentially the reverse where people

179
00:24:00,880 --> 00:24:05,620
always take the treatment if they somehow can access it on the control side or or where they never were,

180
00:24:05,620 --> 00:24:10,299
they sort of defy that would be kind of an odd situation that would be conceivable,

181
00:24:10,300 --> 00:24:16,780
where people do the opposite of what they're requested to do again, if they can sort of access treatment on either side.

182
00:24:17,950 --> 00:24:20,980
So so once you conditioned back on this, though,

183
00:24:21,530 --> 00:24:29,680
you think about this sort of compliance behavior is something that's a pre treatment characteristic of an individual.

184
00:24:30,340 --> 00:24:35,530
Once you conditional on those types of individuals, you're back now to having a causal effect.

185
00:24:35,530 --> 00:24:39,000
Whereas if you simply look at what they took then,

186
00:24:39,630 --> 00:24:44,110
then there may be systematic differences between individuals who take the treatment and those who don't.

187
00:24:45,220 --> 00:24:50,320
So some of these potential outcome differences may have matched the observed differences.

188
00:24:53,490 --> 00:25:01,730
Okay. So. And again, we'll get at that more detail, get through the example.

189
00:25:01,940 --> 00:25:06,020
So conceptually, people started to sort of get the idea a little bit.

190
00:25:08,350 --> 00:25:10,060
It takes a little while.

191
00:25:11,830 --> 00:25:24,910
So you get the idea here is that we've we've converted this pretreatment variable, this this post-treatment variable to a pretreatment variable.

192
00:25:26,950 --> 00:25:32,340
But if the price, the fact that we can expect these same problems, causal problem,

193
00:25:32,350 --> 00:25:40,329
we can't necessarily fully observe under certain under certain conditions or restrictions, we may at least partially observe it.

194
00:25:40,330 --> 00:25:48,430
We can never fully observe it because we can only see an individual under one treatment arm.

195
00:25:48,430 --> 00:25:55,630
So and thus we only see there. This s the potential outcome responding to treatment on which they see.

196
00:25:56,920 --> 00:26:00,960
The exposure to which they think.

197
00:26:03,010 --> 00:26:14,389
So. Okay. So again, so I guess what I'm just repeating what you said there,

198
00:26:14,390 --> 00:26:21,800
that we're always going to have some some missing data for these unobserved treatment rooms.

199
00:26:23,060 --> 00:26:33,020
So, again, one of the things in this class is, if you're seen already, is that this in many ways, there's a lot of application to missing data theory.

200
00:26:33,920 --> 00:26:41,660
So we're going to hit sort of touch with Bayesian stuff in a more he talked about the an algorithm which you may or may not have seen or heard of.

201
00:26:42,300 --> 00:26:49,800
I'm not expecting everybody to be an expert here in in these kind of things, but I'm going to give a little touch on the theory behind it.

202
00:26:49,810 --> 00:26:57,110
And and I think the the application piece, the basic idea is fairly straightforward to take away from.

203
00:26:57,110 --> 00:27:03,440
So you can't play some of this hopefully with straightforward settings, but, you know, we're going to look at.

204
00:27:04,760 --> 00:27:10,729
So but the very general form here, right.

205
00:27:10,730 --> 00:27:17,830
We're back to this idea of we have a likelihood that's based on observed data, right?

206
00:27:17,870 --> 00:27:27,110
What people were assigned and how they behaved with respect to an outcome and a post-treatment variable of interest s and then treatment assignment.

207
00:27:29,570 --> 00:27:35,390
So we can kind of write this out as a set of conditional distributions.

208
00:27:36,290 --> 00:27:47,510
So if we think about integrating over missing data, we now have sort of complete data for Y and S an SNP here means this principle stratification.

209
00:27:48,830 --> 00:27:56,960
So if we break it out so that we have some treatment, that may depend on an outcome in the principal straight of the treatment assignment,

210
00:27:58,160 --> 00:28:04,330
an outcome that's a function of principal strata in some parameter.

211
00:28:05,450 --> 00:28:10,520
And then the principal stratum assignment, which is a function of different set of parameters.

212
00:28:11,180 --> 00:28:14,090
And we integrate that out of respect to the missing data minus.

213
00:28:15,500 --> 00:28:20,240
So things help a lot if we have randomized treatment, and that's actually the application I think about here.

214
00:28:20,750 --> 00:28:25,640
So that whole thing becomes just day so we can go outside the interval.

215
00:28:25,790 --> 00:28:28,070
It doesn't depend on missing data.

216
00:28:28,970 --> 00:28:44,390
So so then are that we can just we can just focus on this combination of the, the distribution of, of the full complete data,

217
00:28:44,990 --> 00:28:57,470
including missing data for the for the potential outcomes conditional on the reputation and then the distribution of the principal stratification.

218
00:28:58,490 --> 00:29:07,220
And then sort of this continues to play a role here because A helps define the distribution of these missing data elements.

219
00:29:09,990 --> 00:29:25,820
So. So without some additional assumptions, this likelihood often doesn't have unique maximum sort of perplex identification.

220
00:29:26,960 --> 00:29:32,330
So there may be some constraints, like some principal straight. It may be empty based on the nature of the data.

221
00:29:32,810 --> 00:29:34,250
The problem that you're addressing.

222
00:29:35,690 --> 00:29:42,470
Alternatively, we could fix, not identify parameters of different plausible values and conduct sensitivity analyzes.

223
00:29:43,310 --> 00:29:50,480
Or we could use proper priors to kind of wear some kind of Bayesian ID because of proper periods of pop or posterior.

224
00:29:51,050 --> 00:29:59,330
And so we'll always have everything working. You know, everything will be coherent.

225
00:29:59,870 --> 00:30:08,870
But of course, if there's not a lot of data to estimate some of these parameters, then you'll be highly sensitive to the choice, of course.

226
00:30:10,160 --> 00:30:13,840
So, you know, hopefully once you release.

227
00:30:17,430 --> 00:30:23,490
Okay. So I want to get out of the abstract and get into the concrete here a little bit.

228
00:30:23,740 --> 00:30:24,810
And again, as I mentioned,

229
00:30:25,320 --> 00:30:31,440
we're going to focus on this idea of randomization failure where subjects do not take the treatment to which they're assigned.

230
00:30:31,750 --> 00:30:34,770
It's called Broken Experiment's compliance issues and so forth.

231
00:30:48,410 --> 00:30:55,250
So. Okay. So here again. Right, so treatment, it's going to be randomized and I assume that the treatment taken is not.

232
00:30:57,710 --> 00:31:00,050
And talk a little bit about this idea of intent to treat.

233
00:31:00,950 --> 00:31:10,279
So, of course, if you ignore the treatment taken and just focus on the randomization, that's a meaningful quantity too.

234
00:31:10,280 --> 00:31:14,570
And of course, that can be estimated even in the usual randomized distributors.

235
00:31:15,920 --> 00:31:22,850
But if you want to think a little bit about what's the treatment effect among people who can take the treatment,

236
00:31:23,330 --> 00:31:28,700
and particularly those who comply with the treatment, if there's a possibility of of sort of going both ways.

237
00:31:31,060 --> 00:31:42,310
Four subjects on the alarm. And so that will lead us to this idea of a compliance classes which correspond precisely to the principal strata.

238
00:31:42,970 --> 00:31:50,050
But in this context, for compliance classes, there are some restrictions to sort of based on the nature of health.

239
00:31:50,740 --> 00:31:58,060
The treatment with the treatment is how it can be accessed and so forth that it may or may not be reasonable to use.

240
00:31:58,810 --> 00:32:06,770
And finally, I'm going to take a little bit from my own work, this physician encouragement literature area.

241
00:32:06,910 --> 00:32:16,270
So this is kind of a special case that has some interesting properties in terms of compliance that sort of is a pretty rich this problem to work with.

242
00:32:17,980 --> 00:32:22,810
So. Okay. All right.

243
00:32:23,320 --> 00:32:27,130
So so get into a little more detail here.

244
00:32:28,750 --> 00:32:32,460
Again, an experimental settings. There may be noncompliance to treatment assignment.

245
00:32:32,500 --> 00:32:36,130
So even if you've been able to randomize treatment,

246
00:32:36,640 --> 00:32:42,040
there may be side effects where some people in clinical trials can't take the treatment, even if they're assigned to it.

247
00:32:44,260 --> 00:32:51,700
And in these encouragement designs, which I'm going to talk about in a minute, as this is a specific example, the treatment control lecture available,

248
00:32:51,700 --> 00:32:59,620
everybody those assigned to the treatment are encouraged to treat the treatment and specifically actually the physician encouragement,

249
00:32:59,620 --> 00:33:05,770
setting the physicians or assigned to treatment or control and those in the encouragement are encouraged to give their patient as a treatment.

250
00:33:06,220 --> 00:33:09,870
It's very common in mental health trials. It was in the application, it was working.

251
00:33:10,960 --> 00:33:11,300
I mean,

252
00:33:12,310 --> 00:33:20,530
it's also common in economic initiation experiments where subjects randomized to receive additional assistance may either forgo the assistance.

253
00:33:20,530 --> 00:33:28,510
Right. So they've sometimes, you know, there may be a setting where you can get access to extra job training or extra welfare benefits.

254
00:33:29,800 --> 00:33:34,480
But for whatever reason, people, particularly in the job training thing, they don't want bother with the job training.

255
00:33:34,480 --> 00:33:39,280
So they they don't do it. Conversely, so these randomized control might get assistance for other means.

256
00:33:39,280 --> 00:33:47,140
So maybe they're still able to get into the job training program even though they they were it was not necessarily available to everyone.

257
00:33:48,040 --> 00:33:54,190
And the people in the treatment, say, are sort of automatic. And so so things kind of go in different directions.

258
00:33:56,680 --> 00:34:05,520
So so then in the most general setting, right, we have this treatment assignments randomized, but the treatment taken, yes.

259
00:34:05,530 --> 00:34:16,660
Is post randomization. So we can no longer assume those who take the treatment or a random sample of the entire population or that it was the

260
00:34:17,740 --> 00:34:23,440
treatment effect that we get in those that subsample will be the same as the treatment affected in the larger population.

261
00:34:24,850 --> 00:34:38,230
So so there are a couple of things we can do without, without sort of, you know, reaching out to specific.

262
00:34:39,730 --> 00:34:43,240
Well, before I only just ask, was this was this all clear?

263
00:34:43,240 --> 00:34:46,890
I think it's fairly straightforward, but it's something.

264
00:34:49,030 --> 00:34:59,830
So there's a couple of things we can do in terms of that without sort of needing to go beyond anything we've we've we've talked about so far,

265
00:35:00,910 --> 00:35:07,690
and particularly with respect to these sort of principle stratification is the first one of these is the intent to treat effect, right?

266
00:35:08,140 --> 00:35:17,629
So basically, if we don't worry about the compliance with maybe another way to think about it,

267
00:35:17,630 --> 00:35:22,720
if we sort of absorb failure to comply into the overall effect.

268
00:35:23,740 --> 00:35:29,889
Right, we can just use are usual estimate rate the mean subjects assigned to treatment minus

269
00:35:29,890 --> 00:35:35,500
the mean assigned a control which is the expected value of of of the difference,

270
00:35:36,610 --> 00:35:43,090
say in the population of the of the.

271
00:35:45,940 --> 00:35:50,290
The causal effects of the individual averaged across the population.

272
00:35:50,290 --> 00:36:00,040
So that's our that's our ace and this is called attempt to treat because essentially it's intention, right?

273
00:36:00,040 --> 00:36:05,349
So you don't worry about how effective it was if there were people that couldn't take the treatment.

274
00:36:05,350 --> 00:36:05,589
Well,

275
00:36:05,590 --> 00:36:13,960
maybe they don't even so they're mixed in here right along with the subjects that may or may not have the ability to take the treatment over here.

276
00:36:14,500 --> 00:36:17,330
And so we're just going to look at this sort of overall, overall average.

277
00:36:17,350 --> 00:36:21,670
And for a lot of, you know, policy things that may be important measure by itself.

278
00:36:22,930 --> 00:36:25,320
But you also might want to know, well, maybe I mean,

279
00:36:25,330 --> 00:36:33,280
because potentially this s could be changed with a maybe a different structure and maybe you took away some of the side effects.

280
00:36:34,570 --> 00:36:42,459
You know, maybe if there was a different incentive to like to actually people wanted the treatment, maybe it would be it could look different.

281
00:36:42,460 --> 00:36:53,170
So you might want to sort of look among the individuals that that that were able to take the treatment and sort of say what's what happened with them,

282
00:36:53,170 --> 00:37:00,700
right? So now instead of a condition or now you're going to conditioned on this to take the individuals who took the treatment,

283
00:37:00,700 --> 00:37:03,700
take the mean of those when it's the subjects who didn't.

284
00:37:05,020 --> 00:37:08,050
The problem, however, now is this is no longer causal because.

285
00:37:13,030 --> 00:37:19,630
The subjects who took who didn't take the treatment dropped over here.

286
00:37:20,530 --> 00:37:25,000
Right. They might have had some. They've got a white one that we're not observing.

287
00:37:25,810 --> 00:37:34,320
And because it's no longer lines of. Exactly. With a then we're not able to kind of get back to this piece here and B and

288
00:37:34,360 --> 00:37:37,509
rely on the fact that it is randomized because this is no longer randomized.

289
00:37:37,510 --> 00:37:41,380
Or we can think about this in terms of the indicators of essence today.

290
00:37:42,010 --> 00:37:53,740
And if you work out our previous piece on this, you can start to see that that s is going to be this is going to kind of muck things up.

291
00:37:53,950 --> 00:38:01,659
Right. Because we're no longer assume that as this is just a randomized variable with probability

292
00:38:01,660 --> 00:38:09,819
equal to in one over in of being equal to one could depend on their covariates under cover.

293
00:38:09,820 --> 00:38:19,990
It's all kinds of other factors. So, so, so while this is, you know, potentially an interesting measure,

294
00:38:19,990 --> 00:38:24,940
it's not so it'd be causal and this is causal, but maybe not the most important measure.

295
00:38:26,770 --> 00:38:38,649
So so this idea of going back to principle 3 to 3 basically allows us to to return back to causal

296
00:38:38,650 --> 00:38:43,660
inference to as the potential outcomes of the treatment taken into the two treatment assignments.

297
00:38:45,190 --> 00:38:49,990
So if we look to s of zero as the treatment taken,

298
00:38:50,020 --> 00:38:57,099
if assigned a control arm and s of one of the treatment taken assigned treatment arm kind of are back to the same four settings.

299
00:38:57,100 --> 00:39:02,470
But now we have a bit more of a specific interpretation for each of these strata.

300
00:39:03,910 --> 00:39:10,930
So in particular, if a subject assigned to treatment doesn't take the treatment and if they're assigned a control, they don't take the treatment,

301
00:39:11,320 --> 00:39:18,280
then we call these subjects never takers if they're assigned to the control and don't take it,

302
00:39:18,280 --> 00:39:22,390
but they do take it under treatment, then they're considered compliant.

303
00:39:25,360 --> 00:39:27,669
You could flip things around, right?

304
00:39:27,670 --> 00:39:35,079
So you could have subjects who take the treatment only if they're not assigned to it and then subjects who don't take it if they are assigned to it.

305
00:39:35,080 --> 00:39:37,510
And you might consider that perverse, I suppose.

306
00:39:38,230 --> 00:39:45,280
And but if it did happen, you could think of two fires and then somebody should take the treatment no matter what.

307
00:39:46,660 --> 00:39:49,780
And as we'll see, the second dependent have the treatment, which is a drug trial.

308
00:39:50,560 --> 00:39:59,160
Maybe pretty hard to have these last two groups because maybe there's no way to access the treatment if you're on the controller.

309
00:39:59,620 --> 00:40:06,190
So that's one way where these is going to start to get a little more manageable computationally

310
00:40:07,480 --> 00:40:13,990
because the last two strains are basically have to be empty or even if you can do it,

311
00:40:13,990 --> 00:40:17,080
maybe even if you think that the fire straight, it doesn't make a lot of sense.

312
00:40:17,410 --> 00:40:23,350
There's probably three, three or four, but at the most general settings within a figure,

313
00:40:23,350 --> 00:40:28,749
this is what the normal variable for categories T and probability is.

314
00:40:28,750 --> 00:40:31,960
Now in a population or small population,

315
00:40:32,530 --> 00:40:39,150
the difference we're doing here corresponding to the proportion that are never taken as proportion of the players proportionate to fires.

316
00:40:39,610 --> 00:40:45,720
Proportion or you take those. So that makes sense.

317
00:40:55,350 --> 00:40:58,520
So this is sort of the conceptual piece here plays a big role.

318
00:40:58,560 --> 00:41:02,940
Yes, it's in the notation as I q what does the P stand for?

319
00:41:03,150 --> 00:41:08,030
So that's that's where I'm basically that's going to correspond. Oh, you're right.

320
00:41:08,040 --> 00:41:12,150
I probably should. You see here. So SIPC is basically the same variable.

321
00:41:12,660 --> 00:41:22,860
You're both trying to identify the. The principal.

322
00:41:22,870 --> 00:41:39,150
Sorry. So. So I think that when trying to move or to see for the rest of this, but you could take it as kind of equivalent to this IP.

323
00:41:45,480 --> 00:41:54,450
So the resulting intent to treat estimates conditional on principles treated thus every calls interpretation.

324
00:41:56,100 --> 00:42:04,259
So you might call this the intent to treat effect among employers might really be the most interesting

325
00:42:04,260 --> 00:42:08,790
one as it shows the impact of the treatment among those who actually comply with the treatment.

326
00:42:10,100 --> 00:42:15,819
So. So why some zero here?

327
00:42:15,820 --> 00:42:22,650
It corresponds to the outcome when they don't take the treatment and why sub one corresponds to outcome they do take the treatment.

328
00:42:22,950 --> 00:42:29,910
Because for the fact they are compliant. So if we are looking at this difference among never takers.

329
00:42:30,630 --> 00:42:34,080
Right. Basically both of these subjects.

330
00:42:35,520 --> 00:42:38,820
But the outcome would be under not taking treatment.

331
00:42:39,600 --> 00:42:42,749
Now, it's possible that those you might assume that's going to be zero.

332
00:42:42,750 --> 00:42:45,180
And indeed, that's often the assumption that's made. But it could be there.

333
00:42:45,190 --> 00:42:54,110
Something about the assignment mechanism itself, the fact the person's been assigned it could potentially have some impact on the outcome.

334
00:42:54,120 --> 00:43:01,260
So it may not just be the treatment. The assignment itself might be playing a role, in which case these things might be non-zero.

335
00:43:02,100 --> 00:43:09,720
And similarly, most always takers, right? So these would be the outcomes of undertaking treatment that aren't but different assignments.

336
00:43:11,370 --> 00:43:18,180
So it's possible these might or may not be of interest, but we can.

337
00:43:22,770 --> 00:43:27,480
No. We have the option of overestimating. Okay.

338
00:43:28,120 --> 00:43:35,560
So now on to inference. So this is where things are going to get a little bit the little bit more curious.

339
00:43:37,960 --> 00:43:41,330
So our complete data, like it's pretty simple, right?

340
00:43:41,350 --> 00:43:47,770
We have essentially a mixture model. So for each individual, some individuals who are independent,

341
00:43:49,300 --> 00:44:02,830
it's basically their potential joint potential outcome given that are in the principle stratum times a probability.

342
00:44:02,830 --> 00:44:05,890
B then that's freedom summed up. Overall, it's freedom.

343
00:44:07,630 --> 00:44:15,320
So in the absence of other assumptions, we're a little tricky here because we don't actually observe, see directly William's survey in house.

344
00:44:16,450 --> 00:44:19,750
So it's going to turn out you really can't fully identify this non parametric look,

345
00:44:20,560 --> 00:44:29,170
but you might build this parametric identification if you make specific assumptions about F and then some,

346
00:44:30,090 --> 00:44:35,620
you know, there may be some issues about the particular strain in particular.

347
00:44:35,620 --> 00:44:43,030
Some of them might be known to be empty essentially because of the way the study is designed.

348
00:44:43,690 --> 00:44:47,990
So. So that could help to. Okay.

349
00:44:48,320 --> 00:44:52,010
So and of course, we have perfect implants.

350
00:44:53,060 --> 00:45:03,980
Everything becomes really simple because basically your compliance variable corresponds to your assignment variable.

351
00:45:05,090 --> 00:45:11,820
So you now automatically know that those two are the same, in which case everybody is a compliant.

352
00:45:12,920 --> 00:45:25,700
And we essentially can go back to our likelihood now as a function just of or our potential outcomes, and we can just choose to.

353
00:45:26,360 --> 00:45:33,110
Well, if you if you sense the sort of likelihood based setting, you might want to use more of a model based inference.

354
00:45:33,110 --> 00:45:37,940
We talk about randomized trials, so if you do have covariance, you could bring them in.

355
00:45:37,970 --> 00:45:41,050
Otherwise essentially it's going to map right back to them.

356
00:45:41,630 --> 00:45:48,760
The usual difference at least. Now.

357
00:45:50,080 --> 00:45:55,540
So let's go to our first sort of non-trivial setting where control subjects are not access treatment.

358
00:45:57,310 --> 00:46:02,980
So so this is we're going to need to sort of put your conceptual head on here and do this a little bit.

359
00:46:04,330 --> 00:46:07,860
So this is often very standard, right?

360
00:46:07,900 --> 00:46:10,120
So assumption a control arm can access the treatment,

361
00:46:10,120 --> 00:46:15,459
but there's still going to be noncompliance because subjects on treatment may not be able to use.

362
00:46:15,460 --> 00:46:17,050
The treatment could be side effects.

363
00:46:17,170 --> 00:46:23,080
If you have trouble accessing the treatment, if it's not like just a drug or something, you're doing it through a physician.

364
00:46:24,580 --> 00:46:25,990
So there could be other factors.

365
00:46:27,100 --> 00:46:35,560
So in this setting, of course, since you can't get the treatment if you're assigned a control that you know that's going to be zero,

366
00:46:35,620 --> 00:46:41,079
you're basically going to be on the control. Or if they've been assigned, they're taking the control.

367
00:46:41,080 --> 00:46:45,459
If they were assigned a control. So you can't have the fires are never taker's.

368
00:46:45,460 --> 00:46:50,820
Right. So. These subjects exist.

369
00:46:52,590 --> 00:46:56,410
So. So down to Australia.

370
00:46:57,200 --> 00:47:15,679
But and in addition, we observed this as a P or C for subjects on the treatment or so if they were assigned to treatment and they took the treatment,

371
00:47:15,680 --> 00:47:20,060
then we know that they are going to be compliant.

372
00:47:21,590 --> 00:47:27,620
And if they were assigned a treatment but didn't take it, then we know they're going to be never taken.

373
00:47:28,040 --> 00:47:41,120
Right, because we know that it's zero zero. So if I said one is one, then their compliance is if it's a1000, then they're they've never taken group.

374
00:47:42,350 --> 00:47:49,040
So the only sort of little component that we can observe here is subject to the controller, right?

375
00:47:50,000 --> 00:47:53,270
So we don't know what we know.

376
00:47:53,270 --> 00:47:57,380
Of course, if they're under control, armed, the risks that they're going to be as zero is going to be zero.

377
00:47:57,590 --> 00:47:59,810
But we don't know what they would have done had they been on treatment.

378
00:48:00,860 --> 00:48:04,790
So essentially those subjects to control a mixture of compliance and they were takers.

379
00:48:06,170 --> 00:48:12,650
But we can tell just from their behavior with respect to the treatment, which one they are.

380
00:48:14,450 --> 00:48:17,570
Yes, I can explain that three, four times, right?

381
00:48:18,680 --> 00:48:28,090
Sure. So basically so we know we know of no risk.

382
00:48:28,160 --> 00:48:32,320
Zero. And, of course, members.

383
00:48:33,250 --> 00:48:37,710
These are never tinkers players.

384
00:48:38,850 --> 00:48:45,170
Players. These are always people's. So we know that.

385
00:48:48,330 --> 00:48:51,570
You assume that people I met on the program could not access the treatment?

386
00:48:55,390 --> 00:48:58,670
As. That can't happen.

387
00:48:59,600 --> 00:49:03,520
So these are sort of the only acts. So.

388
00:49:05,210 --> 00:49:19,630
So if we. So if you see one, that means we observe as one.

389
00:49:19,720 --> 00:49:26,290
Right. Because. Inside the treatment.

390
00:49:27,250 --> 00:49:35,940
So. If it's just one equals one, then we know they're up here.

391
00:49:36,070 --> 00:49:39,330
Right. That's a fear of zero. That's a warning sign.

392
00:49:39,330 --> 00:49:48,170
So we know that they're in play. Similarly.

393
00:49:52,450 --> 00:49:57,610
They don't take the treatment. We know as 1000.

394
00:49:58,620 --> 00:50:02,490
So that means they are.

395
00:50:16,800 --> 00:50:33,710
Questions. So so our complete data likelihood then is going to be this mixture model.

396
00:50:35,860 --> 00:50:40,300
So it's going to be.

397
00:50:43,600 --> 00:50:48,040
So if we assume we know the compliance tests.

398
00:50:51,030 --> 00:50:59,840
Then. Then they're basically if we if they're a supplier,

399
00:51:01,220 --> 00:51:09,830
then we have their distribution for potential outcomes given a set of parameters that will be associated with compliance terms,

400
00:51:09,830 --> 00:51:16,790
the probability of being a compliance plus, the probability of being a never taker, which is just one minus a probability.

401
00:51:17,320 --> 00:51:20,000
Right? Because these parties have to submit to one.

402
00:51:20,000 --> 00:51:32,780
There's only two possible buys times their distribution of potential outcomes given they're never taker and and status.

403
00:51:32,900 --> 00:51:38,360
And then we have an indicator here. The problem, though, is that we don't have as exclusively observed in the controller.

404
00:51:38,960 --> 00:51:41,060
So for the subsequent treatments, we can plug them.

405
00:51:41,060 --> 00:51:47,780
We know if they took the treatment, they're in here so possible to see if they didn't try to take the treatment.

406
00:51:47,780 --> 00:51:51,680
They're over here. But for the solution, the control arm, we don't know that.

407
00:51:52,520 --> 00:52:01,060
So we're going to need to use we have different thing. We could use a Bayesian approach, but I'm going to introduce this expectation maximization,

408
00:52:01,400 --> 00:52:09,229
which is very similar actually a lot of the Bayesian ideas. It's it's just it doesn't use the stochastic process.

409
00:52:09,230 --> 00:52:13,910
It's it's it's deterministic in terms of, of the way the algorithm runs.

410
00:52:15,290 --> 00:52:19,250
So okay. So to make things a little more concrete,

411
00:52:19,970 --> 00:52:24,800
we're going to consider an outcome really that we're going to consider situation with the outcomes currently distributed.

412
00:52:26,210 --> 00:52:34,970
And so that are potential outcomes under treatment in our control are independent with with equal variances.

413
00:52:36,200 --> 00:52:41,480
So but we're going to allow them to have potentially different means.

414
00:52:42,980 --> 00:52:54,680
So if you're a compliant and you got treatment assigned to treatment, then you have theta one some is your mean you were assigned to control,

415
00:52:54,680 --> 00:53:02,749
you have 3 to 0 subsea so I guess theta sub one C theta subzero C and if you

416
00:53:02,750 --> 00:53:06,740
were assigned if you were assigned to a treatment and you're a never taker,

417
00:53:07,370 --> 00:53:12,520
then you have three theta data.

418
00:53:12,770 --> 00:53:17,960
So one n for your mean. And if you were assigned a control, you're never taker, you're theta zero.

419
00:53:19,400 --> 00:53:24,290
And we're going to to account variances, right. So easier it's pretty straightforward to extend that.

420
00:53:25,280 --> 00:53:32,360
And then the last little part of the model, right. We know these probabilities, probability of being a compliant property being never taker.

421
00:53:32,450 --> 00:53:35,180
Right. So they have some the ones that's really just want to know parameter.

422
00:53:36,860 --> 00:53:39,790
So there's obviously some things you might want to consider further here, right.

423
00:53:39,860 --> 00:53:50,509
You might assume that if if essentially different assignments don't if the assignment

424
00:53:50,510 --> 00:53:54,290
itself has no impact on on the on the outcome at all about taking the treatment,

425
00:53:54,830 --> 00:54:02,149
it might be reasonable to assume these are equal to each other. Right. So essentially there's the i.t.

426
00:54:02,150 --> 00:54:07,280
And is zero, right? There's no effect of treatment assignment.

427
00:54:08,030 --> 00:54:14,390
You don't take the treatment. You also, I guess, might think that it's possible if you're not,

428
00:54:14,480 --> 00:54:19,070
since this group is also not taking the treatment that this might be equal to one or both of these.

429
00:54:19,070 --> 00:54:38,680
But is that is sensible an assumption? So we made it not want to think my and so not see as it were quite nice night mean for example.

430
00:54:41,140 --> 00:54:56,090
Neither one got the treatment. Well, maybe that's reasonable.

431
00:54:56,360 --> 00:55:03,290
Can someone defend why? It would be. And if not, they may not be.

432
00:55:24,520 --> 00:55:31,600
Back in the days of this was really tough because you had your cameras off and I can look you in their hand if you drink my coffee like that.

433
00:55:32,780 --> 00:55:41,800
No, seriously, let's think about a little bit here. Why Wayne might not, you know, know women at this stage to not see this state they know in.

434
00:55:44,820 --> 00:55:48,600
What about these individuals? What are we separating these people?

435
00:55:48,750 --> 00:56:11,700
Why are these different? It's great to be. I mean, the difference here is that these are employers and these are never takers.

436
00:56:18,490 --> 00:56:26,210
So. She? I thought I had an idea.

437
00:56:26,390 --> 00:56:30,049
Okay. I think they're just different kinds of B.S.

438
00:56:30,050 --> 00:56:34,400
Exactly right here. Hear the people.

439
00:56:34,400 --> 00:56:37,510
This, this this is not a randomized very clear.

440
00:56:37,520 --> 00:56:40,879
This C is not randomized. Right. So it might be.

441
00:56:40,880 --> 00:56:44,480
And this is just an outcome, right? This is the outcome, the absence of treatment.

442
00:56:44,780 --> 00:56:48,319
But it might be I don't know if this is a mental health measure.

443
00:56:48,320 --> 00:56:55,240
It might be the people that are in compliance have sort of a different kind of baseline behavior than the people that are never takers.

444
00:56:55,250 --> 00:57:01,550
Or it could be if you're having issues with side effects, that's related to something that's also tied to the outcome of interest.

445
00:57:02,360 --> 00:57:11,419
And so the people who can who could take the drug successfully, even if they didn't,

446
00:57:11,420 --> 00:57:13,760
might sort of be systematically different from those who couldn't.

447
00:57:15,930 --> 00:57:20,549
So I mean, when in some situations you might potentially make an argument where these are things,

448
00:57:20,550 --> 00:57:28,520
that would be a very strong argument and sort of defeat the need for the potential of further components behavior from the plants.

449
00:57:28,530 --> 00:57:34,230
It's just random. It's just that you didn't then these might be equal.

450
00:57:34,950 --> 00:57:42,059
And then but also at the same time, you not need to worry about conditions on it for you to make assessments.

451
00:57:42,060 --> 00:57:47,160
So usually we're in a setting here. We do think there's some reason to think these individuals might be different.

452
00:57:52,550 --> 00:57:57,770
Because again, a person is randomized.

453
00:57:58,070 --> 00:58:01,100
So experimenter has control over that.

454
00:58:01,970 --> 00:58:05,720
Yes, that's a subject for security people.

455
00:58:05,720 --> 00:58:10,860
But if you work in a medical environment. Is.

456
00:58:11,280 --> 00:58:17,280
But somehow an autonomous individual can make decisions about how they're going to take the treatment or not.

457
00:58:19,410 --> 00:58:26,430
Or that there's some external force that has is independent of the society mechanism.

458
00:58:28,040 --> 00:58:36,450
It's doing that. So.

459
00:58:39,140 --> 00:58:43,130
So basically the only missing data here is a compliance that is in the controller.

460
00:58:44,000 --> 00:58:49,310
So I mention this M algorithm. I'm going to spend just a few minutes doing a little touch on that.

461
00:58:51,380 --> 00:58:57,230
So anybody who has seen this before. One to every couple sort of.

462
00:58:57,230 --> 00:59:01,960
Yeah. All right. Q Yes. So okay.

463
00:59:01,970 --> 00:59:07,760
So I think we have time to kind of get through this and maybe a little bit to the example, which is as far as it can go today anyway.

464
00:59:08,690 --> 00:59:17,630
So so it's basically this, this kind of an algorithm basically determines likelihood or posterior modes conditional on observed data.

465
00:59:18,770 --> 00:59:22,430
And it capitalize on the fact that parameter estimation would be easy if the data were not missing.

466
00:59:22,670 --> 00:59:26,300
Right. So if we knew Pi subsea. Right.

467
00:59:26,390 --> 00:59:33,880
Or if we, if we knew C and all the individuals. Right. So if we knew compliance status in everybody, this would be a super simple problem, right?

468
00:59:34,160 --> 00:59:40,700
You would just get the means within the individuals that were in compliance or never takers.

469
00:59:41,210 --> 00:59:46,010
You would then, because again, it's randomized within that assignment, it's randomized within that.

470
00:59:46,010 --> 00:59:52,550
And then you would just get the proportion that are compliant and never takers. And that would be you place a team place event that would be done.

471
00:59:53,060 --> 00:59:59,209
But the problem is, for those people on control, we don't know what they would have done in the treatment.

472
00:59:59,210 --> 01:00:02,900
They're a mixture of both. So we're going to have to.

473
01:00:06,020 --> 01:00:09,020
So the algorithms in natural thing do this because the real simple problem,

474
01:00:09,410 --> 01:00:13,850
if we were to know that and we don't, but the analysis allows us to get around that.

475
01:00:14,900 --> 01:00:18,920
All right. So there's a little bit of noisy, somewhat dense notation here.

476
01:00:20,240 --> 01:00:26,060
I don't expect you to totally grasp this the first time through. And it's actually not really necessary to to apply it.

477
01:00:27,410 --> 01:00:35,330
But the basic idea here is that if you think about your complete data,

478
01:00:35,330 --> 01:00:39,050
you can factor it into observed data and the missing data give an observed data.

479
01:00:39,260 --> 01:00:46,820
Right. So in our example, this would be sort of the ideal case where we we know compliance that's for everybody.

480
01:00:47,390 --> 01:01:00,410
So we could write that down as a function of the observations or we, we have observed compliance status and then the sort of missing status is that.

481
01:01:01,700 --> 01:01:09,229
So if we take the log of both sides, of course this product now becomes a sum and so we can rewrite it so that you

482
01:01:09,230 --> 01:01:13,190
can also see there is a bit of logical intuition here that the likelihood,

483
01:01:13,190 --> 01:01:20,540
given the observed data, is a likelihood given the full data minus this conditional likelihood of missing the skipping observed.

484
01:01:20,540 --> 01:01:26,719
And this is a little vague and not true. I mean, it's it's sort of harder to sort of make this clearly concrete.

485
01:01:26,720 --> 01:01:35,240
But conceptually, you can think about this. As you know, we'd have really the larger the likelihood value is, the more information you have.

486
01:01:35,240 --> 01:01:40,639
Right. So the best situation would be is if we had the complete data, we don't have that.

487
01:01:40,640 --> 01:01:45,380
So we sort of have to subtract off to the sort of portion of the information that's missing.

488
01:01:46,220 --> 01:01:49,580
And that gives us a likelihood that's based on the observed data.

489
01:01:51,670 --> 01:01:58,840
So so this idea here is that we're going to take expectation with respect to y

490
01:01:58,840 --> 01:02:04,010
Ms. given the observed data and its incurred estimate of of theta theta city.

491
01:02:05,860 --> 01:02:12,459
So, um, so we basically are conditioning on, on the observed data.

492
01:02:12,460 --> 01:02:24,250
So the left hand side doesn't change, but the right hand side can be rewritten as the expectation of theta given both observed it and missing data.

493
01:02:24,250 --> 01:02:27,100
Right. Why is both observed and missing here the complete data.

494
01:02:27,760 --> 01:02:36,610
So we want to condition of the observed data take expectation aspect of the missing data given the observed data and some value theta t right.

495
01:02:36,610 --> 01:02:44,440
So that's just right expectation is just going to be integrating out this log likelihood component.

496
01:02:45,850 --> 01:02:59,830
With respect to this, this, this, uh, this expectation which is the missing giving observed theta t t so for this piece appear right the same game.

497
01:03:00,880 --> 01:03:10,900
So except this is basically the log of, of my missing data given my observed data and theta,

498
01:03:11,860 --> 01:03:15,970
right that part drops down there just like this part down here.

499
01:03:16,660 --> 01:03:21,370
And again, with what I'm taking, expectation is missing data during observing.

500
01:03:22,720 --> 01:03:27,700
So you're going to call this function F and I'm going to take these two products and

501
01:03:27,700 --> 01:03:36,339
call them F and G and basically with a little bit of use of Jensen's inequality,

502
01:03:36,340 --> 01:03:47,320
which basically says that if I have the the log of some random variable here.

503
01:03:50,980 --> 01:03:58,210
The expectation of that respect to Gene is always going to be less than the law that the expectation of that thing with respect to Jean.

504
01:03:59,350 --> 01:04:02,860
So and the actually writing this as if over G times,

505
01:04:02,860 --> 01:04:13,000
Jean you could see that basically this these two G's cancel once I'd take the law outside and then I just have the,

506
01:04:13,240 --> 01:04:18,040
you know, the live f respect to itself, which is always one and that blog about is zero.

507
01:04:20,110 --> 01:04:24,010
And but of course, that's also right.

508
01:04:25,540 --> 01:04:29,650
But the blog of one is zero. So I can rewrite it like this.

509
01:04:30,340 --> 01:04:36,070
So basically now I've got I've rewritten this form to be less than or equal to this piece here.

510
01:04:37,240 --> 01:04:47,830
They're both provided by G. So within the interval, I can just essentially multiply back by G and I'm left with B looks very much like this.

511
01:04:49,600 --> 01:05:11,280
So basically. So whatever value of theta I plug in here.

512
01:05:14,330 --> 01:05:19,220
Dysfunctions are always going to be less than if I plugged in theta t so I plug theta t in here for theta.

513
01:05:20,360 --> 01:05:23,900
Right. I will get something that's less than theta t.

514
01:05:25,740 --> 01:05:32,820
So basically then if I'm choosing theta T to maximize this first piece, right,

515
01:05:33,030 --> 01:05:37,830
the expected value of this complete data livelihood conditional on the observed data,

516
01:05:38,880 --> 01:05:45,540
then I'm going to increase the log likelihood because by definition the first part right, that I picked this day to do that.

517
01:05:47,160 --> 01:05:52,830
And then the second part, because I'm plugging in some theta that's not theta T is always going to be less than zero.

518
01:05:54,390 --> 01:06:01,200
So essentially this sort of trick allows us to always be sort of climbing up the hill toward the optimal value of theta.

519
01:06:03,420 --> 01:06:05,489
So basically you kind of bounce back and forth.

520
01:06:05,490 --> 01:06:13,290
And this is this is where I do want you to sort of try to get the basic concepts here, which is basically there's sort of two steps to this.

521
01:06:14,220 --> 01:06:22,690
So you take your conditional, your complete data log likelihood and you take the expectation of that conditional observed data, right?

522
01:06:22,710 --> 01:06:30,540
So basically any components that are observed, right, the expected value of that conditional event is just the value itself.

523
01:06:31,440 --> 01:06:38,400
And for the ones that aren't observed, you replace them with their conditional expected value.

524
01:06:38,880 --> 01:06:41,340
Given the observed data and your most recent estimate theta.

525
01:06:43,110 --> 01:06:50,549
Then you take that and you pretend that's actually the data you have and you go through and then maximize that, right?

526
01:06:50,550 --> 01:06:54,360
Because it's really easy to do it, assuming you can do the maximization really easy.

527
01:06:54,360 --> 01:06:57,900
That's with the complete data. It's to real simple problem, right?

528
01:06:58,020 --> 01:07:03,120
So once if we knew theta and we knew yes.

529
01:07:03,990 --> 01:07:11,520
Or at least expected value of this given the observed data and theta we just go through and then we can re re compute that.

530
01:07:12,720 --> 01:07:13,300
So a little.

531
01:07:13,390 --> 01:07:22,230
This is actually from the original paper on this which I referenced up there, sort of a complete data example, but it's sort of an example here.

532
01:07:22,230 --> 01:07:29,250
If we've got let's say we have a complete data model that's small to normal for categories like that,

533
01:07:31,100 --> 01:07:35,340
exactly the same, but it is actually somewhat similar.

534
01:07:36,000 --> 01:07:43,500
And we're going to assume we have so probabilities that out of two one with some unknown theta.

535
01:07:44,640 --> 01:07:55,290
And so basically the second and third cells are are going to be one fourth times data.

536
01:07:56,400 --> 01:08:01,890
So that adds up to one half theta. Third cell probability is one half minus one half theta.

537
01:08:02,700 --> 01:08:05,700
Right? So you have these all up here, one half in the last.

538
01:08:05,700 --> 01:08:10,739
So probability Wilson is just known to be one half. So these are counts basically.

539
01:08:10,740 --> 01:08:16,800
So I don't know, we have 100 observations that get thrown into one of these cells with these probabilities.

540
01:08:17,250 --> 01:08:24,540
These are the counts of the cells. Now, in our observed data, for whatever reason, we can't separate three and four.

541
01:08:25,590 --> 01:08:29,040
So we know they're in category three are they're a Category four, but we don't know which one.

542
01:08:29,040 --> 01:08:36,809
So we just sort of look at those together. So it's obvious the problem, but you can see the issue, right?

543
01:08:36,810 --> 01:08:44,940
So basically if we get all these probabilities to be real simple, we would just plug in, you know,

544
01:08:45,870 --> 01:08:55,230
they just solve for theta by using the distributions of y one or two and y three over the over the sum of all four to get to get the estimates.

545
01:08:55,980 --> 01:09:00,090
But because we can't separate way three and with four, we're a little bit stuck.

546
01:09:01,500 --> 01:09:06,420
But not to start because we actually have to know the probability of being like four is one half.

547
01:09:07,020 --> 01:09:11,610
So it actually doesn't contribute anything like that. Right? That's just a just a constant.

548
01:09:11,610 --> 01:09:15,540
So the likelihood really involves only one, one, two, three.

549
01:09:16,830 --> 01:09:23,399
Right. Than what normal it's. One half times one might say.

550
01:09:23,400 --> 01:09:33,540
The Race to the Power White 1143. The Way to the Power Way to win four three to race the power away three in there way four reached the power half.

551
01:09:34,260 --> 01:09:39,540
But that is an evolving unknown. So we can kind of just love that off and then we take the log.

552
01:09:39,540 --> 01:09:47,700
And so we get we get out of this part here. And as a matter of fact, these these these functions here.

553
01:09:47,930 --> 01:09:53,479
Right. The system can read the log of one half times.

554
01:09:53,480 --> 01:09:59,420
One might say that is the log of one half, plus or minus theta multiplied by y one.

555
01:09:59,420 --> 01:10:04,400
So y one times the log of one have got a constant. Ignore it.

556
01:10:05,480 --> 01:10:09,230
So we're just really going to boil it down to this piece here.

557
01:10:11,060 --> 01:10:19,700
So. It makes sense to the more than normal likelihood taking the long drop and all the things that are constants.

558
01:10:23,690 --> 01:10:28,550
So again, if this was fully observed, I could then compute theta.

559
01:10:28,880 --> 01:10:41,680
Right. I just differentiates. But the theta get y one over one minus theta and take the derivative of that and again, minus one, minus one.

560
01:10:42,590 --> 01:10:47,990
And then this is just one over theta. I'm just part here. So I set that zero.

561
01:10:49,300 --> 01:10:59,120
Then when I do cross multiplication, I should get something like one minus three over theta equals y, one over y, two plus three.

562
01:11:00,650 --> 01:11:07,790
And of course one minus theta over theta is just one of the theta minus one.

563
01:11:07,790 --> 01:11:10,910
So I can push that one over there.

564
01:11:11,400 --> 01:11:17,840
Okay. You come to the denominator, get to y one plus y two plus y three and y one over y,

565
01:11:17,840 --> 01:11:22,670
two plus y, three plus y to his wife, three over y to cost both reason that sums up.

566
01:11:23,780 --> 01:11:27,110
So and then I just flip it around. So basically my theta hat,

567
01:11:27,740 --> 01:11:37,639
if I were to know this is just the proportion of observations that are in y to y three over one plus y to plus y three is to dynamic sense, right?

568
01:11:37,640 --> 01:11:47,600
Because sort of one half of the of this sort of conditional probability could arise in this way.

569
01:11:47,600 --> 01:11:51,170
Two of my three group and the other half is is in y one.

570
01:11:51,320 --> 01:11:56,960
So you kind of just take. Essentially every 2 hours to so.

571
01:11:58,850 --> 01:12:07,020
But with Theta and BP. Another thing that doesn't been updated is that people can sort of.

572
01:12:08,310 --> 01:12:11,310
You've got proportional. So.

573
01:12:13,040 --> 01:12:18,710
Okay. So that's it. If I knew why, three would be all done.

574
01:12:18,720 --> 01:12:23,300
So what we're going to do here is we're going to replace wait three with its expected conditional expectation.

575
01:12:24,530 --> 01:12:30,050
So basically so I mean, if you sort of write this all out,

576
01:12:31,280 --> 01:12:37,310
then what we what we have here is is expected value of Y three given the observed data and theta.

577
01:12:38,570 --> 01:12:49,790
And so our observed data is why three and why for given theta t and if I knew theta, right?

578
01:12:51,350 --> 01:12:58,089
Because. It would then be way three.

579
01:12:58,090 --> 01:13:01,020
The probability being my three would be one fourth of a pie.

580
01:13:01,810 --> 01:13:07,840
So I won four times by the probability of being in why three or why four would be one fourth.

581
01:13:08,170 --> 01:13:11,360
I'm sorry. One for theta plus one half. Yeah.

582
01:13:12,400 --> 01:13:21,030
So. So the portion that are in white, three out of white, three in white four, and this one for theta, the current estimate of theta.

583
01:13:21,870 --> 01:13:27,930
And then of course, the condition probably one or the other is going to be one half plus 140.

584
01:13:30,430 --> 01:13:39,820
So we can essentially bounce back and forth. We can re estimate theta replacing by three with the expected value here.

585
01:13:40,930 --> 01:13:47,410
And then we can we estimate this expected value of y three with our partners to meet of theta from this quantity.

586
01:13:50,560 --> 01:13:53,570
So you write that. All right.

587
01:13:54,340 --> 01:14:02,950
Updated theta T plus one is y two plus the expected value of one, three or y one plus weigh two plus expected 8y3.

588
01:14:06,820 --> 01:14:13,270
And then to get that updated value of y three, you plug in the city.

589
01:14:15,300 --> 01:14:21,750
So you can't actually solve this analytically, but usually that's not doable.

590
01:14:21,780 --> 01:14:26,310
So we can just kind of like go turn the crank so you just keep updating.

591
01:14:27,190 --> 01:14:30,090
We had some initial data equal to 0.5,

592
01:14:31,230 --> 01:14:39,450
so when you plug that in to give you an updated estimate of 6.2 probability and again, given these values of of.

593
01:14:43,690 --> 01:14:52,450
What one might do in the summer and get 6.2 for two, keeps on it, stabilizes it at this maximum value.

594
01:14:52,510 --> 01:14:57,610
So when you got this value here, essentially theta t doesn't change.

595
01:15:02,910 --> 01:15:07,650
So all right. So that was a quick introduction to the team.

596
01:15:08,790 --> 01:15:18,790
But the only thing that makes it slightly mean sort of the original intuition was that you estimate missing values,

597
01:15:18,790 --> 01:15:22,590
just me parameters using those estimate, we just mean those parameters and so forth.

598
01:15:23,040 --> 01:15:27,120
So that actually works exactly. Fine if your likelihood is is.

599
01:15:29,280 --> 01:15:34,530
It's sort of linear, but it's not linear that you kind of have to do this expectation piece, right?

600
01:15:34,530 --> 01:15:40,740
So if you have like an x squared, you have to take the expectation of x squared, which is the variance of experts expect that basically squared.

601
01:15:41,640 --> 01:15:44,660
So those kinds of things are going on here. This is pretty simple. So, so.

602
01:15:45,500 --> 01:15:57,230
So. So it's this is very close to the to the sort of ad hoc pre 1974 idea.

603
01:15:58,300 --> 01:16:09,410
But but how. So questions about this all poor example.

604
01:16:12,650 --> 01:16:33,220
Jump in and. Okay.

605
01:16:36,590 --> 01:16:40,319
So. All right.

606
01:16:40,320 --> 01:16:45,640
So we have. Awkward timing.

607
01:16:50,110 --> 01:16:53,980
I think rather than try to race through this, I'm going to stop a couple of minutes early.

608
01:16:54,820 --> 01:16:58,600
So I want to sort of get a big bite and get this all the way through.

609
01:16:59,520 --> 01:17:05,919
Mean everybody here 10 minutes late in English and through it. So, so, all right.

610
01:17:05,920 --> 01:17:13,290
So basically what we're going to do is we're going to go through and work out the expected value of these essence.

611
01:17:13,300 --> 01:17:17,110
Well, we don't know them, right? In many cases, then the treatment side will know them.

612
01:17:17,320 --> 01:17:22,690
So there's no mystery there. What you do is the probability of being the player or the probability of an endeavor taker,

613
01:17:23,230 --> 01:17:27,460
given that you're one control side from your observed data.

614
01:17:28,450 --> 01:17:35,290
And then once we have that, we're going to just plug that right back into the usual kind of simple math in your estimation,

615
01:17:36,200 --> 01:17:40,809
and basically doing a little bit of replacing zeros and ones with weights for

616
01:17:40,810 --> 01:17:43,969
the control people with the way to the probabilities of being in this one.

617
01:17:43,970 --> 01:17:49,420
I assume that's back for the course and then we get the answer.

618
01:17:50,440 --> 01:17:54,560
So we'll do a little simulated example and.

619
01:17:55,930 --> 01:18:00,940
Right. So I guess the last little thing in this is that often there's no way there's there

620
01:18:00,940 --> 01:18:07,329
are actually analytical solutions that are involved sort of working out information,

621
01:18:07,330 --> 01:18:10,810
statistic functions, taking squares and it can get a little ugly.

622
01:18:11,500 --> 01:18:15,639
But so usually this bootstrap is often our institution studying these cases.

623
01:18:15,640 --> 01:18:19,060
But we'll come back to that, go through it in detail.

624
01:18:20,470 --> 01:18:24,090
Wednesday. Okay.

625
01:18:25,740 --> 01:18:30,700
Thank you all very much. I'll be around for hours after.

