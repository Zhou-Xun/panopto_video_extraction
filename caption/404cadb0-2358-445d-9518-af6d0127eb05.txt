1
00:00:02,160 --> 00:00:17,250
Yeah. I mean, you took all your.

2
00:00:28,380 --> 00:00:36,270
Scary. Yeah, I can. You know what I mean by that?

3
00:00:36,270 --> 00:00:40,740
I mean, you can look except for everything else.

4
00:00:43,850 --> 00:00:48,480
Yeah, I think. I.

5
00:00:55,910 --> 00:00:59,240
Oh. Yeah.

6
00:01:15,800 --> 00:01:22,460
How are you? All right, everybody.

7
00:01:22,480 --> 00:01:32,870
We're going to get started. Thank you. So last time we got almost all the way through the first set of slides for the review.

8
00:01:33,110 --> 00:01:36,290
The only stuff we didn't cover was the computing stuff.

9
00:01:36,290 --> 00:01:39,320
And that's what I wanted to take a few minutes to talk to you all about.

10
00:01:39,380 --> 00:01:47,810
This morning I met with the GSA yesterday, and it seems that some people are having some trouble getting started in air.

11
00:01:48,740 --> 00:01:55,010
So I just wanted to give you some encouraging words and and let you know that it is a bit of a learning curve.

12
00:01:55,520 --> 00:02:03,259
But I think you can all you can all get there. So just use the resources of the GSA eyes, ask them for help.

13
00:02:03,260 --> 00:02:06,650
They're very they're very experienced. They're very good teachers.

14
00:02:07,640 --> 00:02:14,120
So I think what we're going to do is they're first, we're setting up the T.A. office hours now.

15
00:02:14,120 --> 00:02:18,290
So I think what we're going to plan for is that their first office,

16
00:02:18,290 --> 00:02:25,070
our session will be kind of a how to use our how to get our running sort of session.

17
00:02:25,400 --> 00:02:30,830
So if you all are having trouble with things like just booting into our loading data,

18
00:02:30,830 --> 00:02:35,900
stuff like that, then please do come to the first office hours for the Tas.

19
00:02:36,830 --> 00:02:43,100
They will certainly be able to cover other topics as well if in their session nobody needs that kind of help.

20
00:02:43,820 --> 00:02:47,270
But I wanted to make that available to people if they're happy,

21
00:02:47,480 --> 00:02:55,440
if you're struggling with with some of those kinds of sort of programing things, because the course is really sort of twofold, right?

22
00:02:55,460 --> 00:03:02,180
We're learning the statistical concepts, but then implementing them, we need to use the programing languages of our or SAS.

23
00:03:02,900 --> 00:03:05,750
So, so that's that's kind of where we are with that.

24
00:03:06,170 --> 00:03:13,340
Does anybody have any concerns or thoughts about office hours or computing or anything else they want to share now?

25
00:03:13,960 --> 00:03:18,750
Yes. Really that's that's nothing to off for.

26
00:03:21,890 --> 00:03:27,070
So that's. I. Thank you.

27
00:03:27,100 --> 00:03:33,159
Yes. Yes. So actually, I'm glad you brought that up, because when I was talking with the GSEs yesterday, they mentioned that as well.

28
00:03:33,160 --> 00:03:35,469
So I think at least for the first one,

29
00:03:35,470 --> 00:03:44,260
they will do it in person and I will bring up with them if the possibility if some of them could be in person as well, just kind of going forward.

30
00:03:45,280 --> 00:03:50,920
Yeah, that's a thank you for that suggestion. Anybody else concerns or.

31
00:03:51,060 --> 00:03:55,390
Yes. Okay.

32
00:03:56,930 --> 00:04:02,870
And. We're asking as two separate question.

33
00:04:06,520 --> 00:04:11,630
Mm hmm. Something crushing was probably going to. Right.

34
00:04:12,320 --> 00:04:17,540
And I felt that those two were the same questions.

35
00:04:17,840 --> 00:04:24,410
That is a very astute conclusion and not to give away the answer, but you are very much on the right track with that.

36
00:04:24,800 --> 00:04:32,150
So it's not supposed to be a trick question. It's just, if you think about it, the regression model and we'll talk about regression today.

37
00:04:32,160 --> 00:04:37,730
So I don't want to foreshadow too much, but the regression model is essentially working on the same data that's in the scatterplot.

38
00:04:38,270 --> 00:04:42,260
So what we'll talk more about that today. Any other any other?

39
00:04:42,290 --> 00:04:45,530
Yes. This is slightly unique ish. Sure.

40
00:04:48,750 --> 00:04:52,490
Mm hmm. Take 520. Mm hmm. Know how to code?

41
00:04:52,580 --> 00:04:55,729
Are there any resources for you? So this is.

42
00:04:55,730 --> 00:05:03,530
This is what I'm talking about with. I think the GSI will be very helpful for you, and they can probably share with you and I can look up as well.

43
00:05:03,530 --> 00:05:07,100
So I'm sure there's a lot of Internet resources for R especially.

44
00:05:07,640 --> 00:05:12,680
So I will I will look into that. I will talk to the GSEs and make sure that they're aware of this as well.

45
00:05:13,310 --> 00:05:19,490
But yeah, what I'll try to do then is because it sounds like there's a lot of students who may be in that same that same boat.

46
00:05:20,360 --> 00:05:23,230
So I'll try to find some sort of there because I know they're out there.

47
00:05:23,240 --> 00:05:33,320
It's been a while since I look for them, but I will try to post on canvas a set of, you know, websites where that's that's kind of the focus is.

48
00:05:33,560 --> 00:05:39,140
Yeah. Thank you for that. Other questions before we jump into today's lecture.

49
00:05:42,990 --> 00:05:50,310
Yes. You can.

50
00:05:50,340 --> 00:05:55,690
You mean formally or just temporarily? So it depends.

51
00:05:55,690 --> 00:06:01,750
You can you. What you should do is check with the the GSI, whose labs section you wanna switch to and see if they have space.

52
00:06:02,200 --> 00:06:07,040
And if they have space. I think it's okay. Anybody else.

53
00:06:10,260 --> 00:06:12,630
Okay. We'll get started then because we've got a lot to cover.

54
00:06:13,680 --> 00:06:22,860
So as I said, we we last time we got all the way up through the end of our review session and we talked about some univariate analysis.

55
00:06:22,860 --> 00:06:35,220
We looked at a one sample t test. Right. So today we're going to get into the whole notion of linear regression in the simple linear regression case.

56
00:06:35,550 --> 00:06:38,850
And simple just means one covariate.

57
00:06:39,510 --> 00:06:46,860
I'll use the term covariates to mean predictors, explanatory variables, independent variables.

58
00:06:46,860 --> 00:06:54,870
These are all going to mean the same thing. But in the simple linear regression setup there there's one X variable and one Y variable.

59
00:06:55,830 --> 00:07:00,000
So we're going to talk about a lot of things today.

60
00:07:00,000 --> 00:07:05,100
We're going to talk about the setup, we're going to talk about stochastic models and what that means.

61
00:07:05,460 --> 00:07:12,480
And I think I brought up the idea of deterministic in the last lecture estimation and interpretation of parameters,

62
00:07:13,050 --> 00:07:20,010
the four key assumptions of linear regression and then the variance and again the code

63
00:07:20,010 --> 00:07:24,390
that's at the end of the slides is really intended to kind of give you examples.

64
00:07:25,050 --> 00:07:31,560
I have a feeling we're not going to have time to go through it in class generally, but again, we can adjust as needed.

65
00:07:32,790 --> 00:07:34,499
So I think we talked,

66
00:07:34,500 --> 00:07:45,510
we alluded in the past classes to the importance of regression because it's the idea of how do you relate multiple variables to each other, right?

67
00:07:45,540 --> 00:07:50,909
If you have a treatment, how do you determine the effect of that treatment on some outcome variable?

68
00:07:50,910 --> 00:07:55,020
Right. And it could be a continuous treatment. It could be a dose of a drug.

69
00:07:55,020 --> 00:07:56,909
It doesn't have to be treatment or treatment.

70
00:07:56,910 --> 00:08:05,310
B The linear regression and the regression framework more generally is able to accommodate both of those cases as well as many others.

71
00:08:05,860 --> 00:08:09,450
And we'll look at some of those examples because, again,

72
00:08:09,450 --> 00:08:14,160
this this idea of linear regression is just sort of the basics, but regression is much more general.

73
00:08:14,670 --> 00:08:20,579
You don't have to have linear regression, you can have non-linear regression, you can have regression with non normal outcomes.

74
00:08:20,580 --> 00:08:24,120
That's what we're going to look at for logistic regression when we get to that.

75
00:08:24,240 --> 00:08:33,610
And a lot of these techniques will extend. So simple linear regression is the scatterplot, two dimensional scatterplot regression.

76
00:08:33,620 --> 00:08:38,659
So it's essentially when we have an X variable or explanatory variable on the x

77
00:08:38,660 --> 00:08:46,600
axis and a Y variable or outcome on the Y axis and the data looks like this,

78
00:08:46,610 --> 00:08:48,410
the data is the points here.

79
00:08:48,770 --> 00:08:58,940
And then the goal with linear regression is to fit a straight line to these points and we'll talk about how that's done as well.

80
00:08:58,940 --> 00:09:04,860
But the idea is to quantify the relationship between this one covariate again,

81
00:09:04,880 --> 00:09:11,990
explanatory variable predictor and the response Y So the data come in as pairs.

82
00:09:12,440 --> 00:09:19,700
So each pair X and Y constitutes a data point and you need to have these pairs because you need to know which

83
00:09:19,730 --> 00:09:26,030
X goes with which why that's important because otherwise it's going to be hard to discern that relationship.

84
00:09:27,590 --> 00:09:33,770
So the straight line is essentially a a model equation.

85
00:09:34,130 --> 00:09:39,380
So what the model here represents is remember, e is the expectation operator.

86
00:09:39,650 --> 00:09:47,000
So expectation of Y given X is equal to beta, not plus beta one times x.

87
00:09:47,690 --> 00:09:53,050
So this is the expected value of Y given a value of x.

88
00:09:53,840 --> 00:10:00,290
So the idea of conditional expectation is really key to to the idea of regression.

89
00:10:00,950 --> 00:10:08,089
Because now we've talked about in the last class, we talked about the example of weights of male Detroit residents.

90
00:10:08,090 --> 00:10:14,450
Right? That was kind of a marginal analysis in the sense that we were just looking at the weights.

91
00:10:14,450 --> 00:10:17,450
We weren't looking at their dependance on another factor.

92
00:10:17,900 --> 00:10:23,990
So now what we're doing is we're looking at the dependance of the Y variable on X.

93
00:10:24,650 --> 00:10:35,840
So the mean value of Y here depends on X through this particular mathematical flaw, and it's linear regression because this is a linear relationship.

94
00:10:36,320 --> 00:10:43,130
So if I talk about a linear combination of the covariance, that's going to be something that looks like this.

95
00:10:44,330 --> 00:10:51,799
So again, what we have here is the sum of the parameter beta, not which is the intercept plus beta one,

96
00:10:51,800 --> 00:11:02,540
which is the slope parameter times x and beta one is the the parameter that captures that relationship between X and Y.

97
00:11:02,810 --> 00:11:11,060
So if we look at the line here, you all remember back to algebra that a line is determined by by two values, right?

98
00:11:11,060 --> 00:11:16,440
Its slope and its intercept. So the intercept is beta not and then the slope is beta one.

99
00:11:16,790 --> 00:11:23,930
So the steeper the slope kind of the steeper, the higher the value of beta one, the steeper this slope will be.

100
00:11:25,110 --> 00:11:32,010
So the example here is the outcome of systolic blood pressure and the explanatory variable is age.

101
00:11:32,400 --> 00:11:37,950
Right. So what we're trying to see is what happens to systolic blood pressure as as somebody ages.

102
00:11:39,060 --> 00:11:49,560
So what you can see here is that at least visually, as age increases, there seems to be a modest increase in systolic systolic blood pressure.

103
00:11:50,520 --> 00:11:55,589
So, for example, when we have a linear regression model, we can answer questions like,

104
00:11:55,590 --> 00:12:00,450
what is the mean blood pressure in an individual who is 85 years old?

105
00:12:00,810 --> 00:12:10,890
So that would be this expression here. And what we would do to find that value out is after we fit our model, which is obtaining this line,

106
00:12:11,250 --> 00:12:21,810
we go to 85 on the x axis and just kind of draw a line up to the line and then across and it looks like it's going to be around 118 or something,

107
00:12:22,380 --> 00:12:29,150
right? So that is kind of how the the linear regression framework works.

108
00:12:29,160 --> 00:12:33,209
And I want to emphasize the importance of the scatterplot here.

109
00:12:33,210 --> 00:12:40,170
I think the first step, whenever you're dealing with a data set of any sort, is to look at it graphically,

110
00:12:40,440 --> 00:12:44,339
because that's the way you can identify some of these this like this value out here.

111
00:12:44,340 --> 00:12:48,570
It's kind of like it's kind of high, right? Relative to everything else. This one's kind of low.

112
00:12:48,780 --> 00:12:57,420
So those may or may not be outliers, right? So visually, it can be easy to or easier to identify outliers, for example.

113
00:12:57,660 --> 00:13:03,060
And it's also easier to pick up on other kinds of relationships that may not be linear.

114
00:13:03,600 --> 00:13:07,800
Um, it's, the problem is when we get to multiple regression.

115
00:13:09,110 --> 00:13:16,390
It's not going to be as easy to visualize. So the set up here.

116
00:13:16,510 --> 00:13:22,630
So the Y value is going to have to be essentially continuous.

117
00:13:23,620 --> 00:13:29,920
And there are many cases where it can be treated as continuous, even though it might be, for example, a count variable.

118
00:13:30,280 --> 00:13:33,489
Right. So for example, we have a Likert scale.

119
00:13:33,490 --> 00:13:37,420
So those are scales that are like very poor to excellent things like that.

120
00:13:37,420 --> 00:13:41,680
So you can ordinarily sort them into like one, two, three, four, five.

121
00:13:42,100 --> 00:13:46,240
So sometimes you can treat those as continuous for the purposes of linear regression.

122
00:13:46,600 --> 00:13:53,740
Similarly, with things like number of hospitalizations, what we'll want to be careful of is when we get to the assumptions,

123
00:13:54,250 --> 00:14:01,240
because a lot of times the assumptions for linear regression may be violated by some of these different kinds of variables.

124
00:14:01,690 --> 00:14:10,900
So, so kind of the classical examples of continuous variables are things like blood pressure or cholesterol levels, lab values, things like that.

125
00:14:12,520 --> 00:14:21,219
BMI is another good one. So, you know, we've talked a little bit about what defines a continuous variable and what defines discrete variables.

126
00:14:21,220 --> 00:14:23,049
But again,

127
00:14:23,050 --> 00:14:32,440
this the Y here has to be either continuous or can be treated as continuous because it takes on a large number of unique values potentially,

128
00:14:34,000 --> 00:14:41,890
because really what we're interested in doing is estimating the conditional mean of that variable given the covariates.

129
00:14:42,580 --> 00:14:46,629
We'll talk more about that. So the that's the Y variable.

130
00:14:46,630 --> 00:14:51,370
The outcome then the X is we're we're only going to look at a single X at first.

131
00:14:51,370 --> 00:14:57,640
But in the general framework you can have many predictors or covariates is the same term here.

132
00:14:58,740 --> 00:15:05,010
So we're interested in the independent effect of each covariate on the outcome.

133
00:15:05,370 --> 00:15:13,169
So this is when we can talk about the effect of smoking on low birth weight adjusted for age, right?

134
00:15:13,170 --> 00:15:20,370
In that example, when we say adjusted for age, we just mean that age is another ex in the model.

135
00:15:21,690 --> 00:15:27,920
So the, the one of the points here is that we don't have to assume anything about the Xs.

136
00:15:27,930 --> 00:15:32,879
They can have any form that we would care to include in our model because the

137
00:15:32,880 --> 00:15:38,250
model that we're fitting doesn't depend on a particular distribution for the Xs,

138
00:15:38,760 --> 00:15:43,680
because it's conditional on the Xs at their attained values, essentially.

139
00:15:44,430 --> 00:15:47,870
So you can have continuous variables like age, right?

140
00:15:47,880 --> 00:15:53,520
We talked about age. The smoking variable for the birth weight example was pax per day.

141
00:15:53,820 --> 00:16:01,680
And that's again something that can be you might argue that it's discrete but you can also treat is continuous because you can have a half pack a day,

142
00:16:01,680 --> 00:16:06,300
right? You can have somebody reporting that they smoke a pack and a half a day, something like that.

143
00:16:07,620 --> 00:16:09,090
So that can be treated as continuous.

144
00:16:09,330 --> 00:16:18,780
Again, it's in the in the case of of the X values, it's less important how what category or sorry, what group these are in it.

145
00:16:19,110 --> 00:16:22,770
But it does become important when we when we think about interpretation.

146
00:16:23,310 --> 00:16:30,510
So in terms of kind of the assumptions and making the whole setup work, we don't care about the distribution.

147
00:16:30,810 --> 00:16:39,630
But depending on whether we have the X treated as continuous or categorical, it will change the interpretation of the coefficient estimate.

148
00:16:39,640 --> 00:16:45,960
So those are the betas. So deterministic versus stochastic.

149
00:16:45,970 --> 00:16:56,350
So so the whole idea of linear regression is to apply methods for line fitting lines to data when there is error in the data.

150
00:16:56,350 --> 00:16:58,600
So random noise in the data.

151
00:16:59,050 --> 00:17:09,190
So that is and that's a stochastic model because in real life in science, there's going to be always random noise in the data.

152
00:17:09,520 --> 00:17:16,930
So this is going to be contrasted with the idea of a deterministic model, which is just based on kind of scientific fact.

153
00:17:17,320 --> 00:17:22,840
And the two examples here are from I guess, physics probably.

154
00:17:23,110 --> 00:17:30,189
So the temperature relationship between Fahrenheit and Celsius is exactly this relationship, right?

155
00:17:30,190 --> 00:17:38,710
So you take to get Fahrenheit, you take 32 and add nine over five times the Celsius temperature.

156
00:17:39,160 --> 00:17:46,840
So if you think about it, this is the same form of the linear regression equation we looked at with the data not made of one.

157
00:17:47,140 --> 00:17:52,420
So we still have an intercept and a slope, but there's no noise in this model.

158
00:17:52,420 --> 00:18:02,620
This is an exact relationship. So all of the statistical tools that we are going to use are going to rely on a particular form of

159
00:18:02,620 --> 00:18:09,460
random noise and in linear regression that's going to be Gaussian noise or normally distributed noise.

160
00:18:10,510 --> 00:18:14,649
The other example here is just a units conversion, right?

161
00:18:14,650 --> 00:18:20,290
I guess that's the same as the temperature in this case. It's weights, so it's KG £2.

162
00:18:20,650 --> 00:18:23,830
So in this case, there's no intercept, there's just a slope.

163
00:18:24,130 --> 00:18:29,110
So to get the weight in KG, you take the weight in pounds and multiply by 0.45.

164
00:18:29,440 --> 00:18:38,380
So these are exact relationships in the sense of every point that you could care to to plot would fall exactly on the line.

165
00:18:38,740 --> 00:18:44,530
So if you remember from our example that we looked at a moment ago, the blood pressure point fell all over the place.

166
00:18:44,530 --> 00:18:49,810
They were not just on that line. So that represents stochastic noise.

167
00:18:51,530 --> 00:19:00,890
So for example, if in this deterministic setup, if we were if we didn't have that model for temperature and we just had these data points here,

168
00:19:01,190 --> 00:19:08,690
we can use our knowledge of algebra to compute the slope and the intercept of the line using any two of these points.

169
00:19:08,990 --> 00:19:09,380
Right.

170
00:19:09,560 --> 00:19:18,740
Because it's an exact relationship, you only need two points to determine the entire line and then the rest of the points will just fall on that line.

171
00:19:19,130 --> 00:19:23,840
So you can calculate the slope in this way and calculate the intercept in this way.

172
00:19:24,230 --> 00:19:31,070
So this is just kind of going back to some basics from from algebra and plotting things in the coordinate space.

173
00:19:31,850 --> 00:19:35,570
So what we're doing with with stochastic models, by contrast,

174
00:19:35,840 --> 00:19:42,110
is generalizing this this notion of fitting a line to points to win the points

175
00:19:42,110 --> 00:19:46,940
don't just fall on that line that there's random noise that's been added in.

176
00:19:48,340 --> 00:19:55,479
So probabilistic just means, again, that there is some randomness that's been included in the model.

177
00:19:55,480 --> 00:19:57,190
And there's again in science,

178
00:19:57,190 --> 00:20:05,290
we almost always have these situations where there is randomness that's been included or that's part of the phenomena under study.

179
00:20:05,530 --> 00:20:07,780
Right. We talked about sampling last time.

180
00:20:07,780 --> 00:20:16,269
We talked about when you have a random sample of people, that's going to give you different estimates than a random sample of different people.

181
00:20:16,270 --> 00:20:18,340
If you have different people included, right.

182
00:20:19,720 --> 00:20:27,430
In the example with systolic blood pressure and age, you're going to have different blood pressure values from one sample versus another.

183
00:20:28,720 --> 00:20:33,130
And there's also the sort of within individual variation, right?

184
00:20:33,160 --> 00:20:36,760
Because somebody's blood pressure could vary over time for various reasons.

185
00:20:37,960 --> 00:20:46,210
So there's there's just a lot of sources of random variation that can be encountered in statistical studies.

186
00:20:47,350 --> 00:20:51,850
So we want to in our in our approach to modeling,

187
00:20:51,850 --> 00:21:00,220
we want to be able to account for that randomness and make sure that when we estimate parameters from the data,

188
00:21:00,520 --> 00:21:09,220
that those the uncertainty in those parameters is correctly reflected in things like our confidence intervals and our P values.

189
00:21:09,810 --> 00:21:18,480
Right. So if we go back to our regression model equation, we have y equals beta, not plus beta one plus E.

190
00:21:19,020 --> 00:21:25,140
So what's different about this? Does anybody notice anything that's different about this equation from the other one?

191
00:21:27,080 --> 00:21:32,140
What do you guys notice? Yeah.

192
00:21:32,150 --> 00:21:38,930
There's an error term here. Exactly. So this is a different way of writing the same regression model.

193
00:21:38,990 --> 00:21:46,740
There's also no expectation of operator over here because what we have here is the the actual value.

194
00:21:46,760 --> 00:21:51,680
So no longer are we saying the expected value of Y is equal to blah, blah, blah.

195
00:21:51,980 --> 00:21:58,040
We're saying the value of Y, the attained value of Y is equal to the linear.

196
00:21:58,400 --> 00:22:03,230
Of course, the linear predictor is the some of the betas and the axes.

197
00:22:03,920 --> 00:22:07,760
It's the linear predictor here, plus the error term.

198
00:22:08,090 --> 00:22:12,600
So this is what I keep talking about when I say random noise. This is the the error here.

199
00:22:12,620 --> 00:22:24,080
We're going spend a lot of time talking about how to estimate the variance of this error and how that affects the precision of our estimated betas,

200
00:22:24,380 --> 00:22:25,190
for example.

201
00:22:26,300 --> 00:22:37,040
So a side kind of a side note here, the X's in practice can be can be random in the sense of like the ages in systolic blood pressure examples,

202
00:22:37,370 --> 00:22:44,479
or they can be fixed in an experimental design. So you might encounter some studies where you're,

203
00:22:44,480 --> 00:22:53,360
you're like an agricultural study where you have different plots and you're applying a different level of nutrients to each plot,

204
00:22:53,360 --> 00:22:57,050
and you've chosen those. So then that would be deterministic.

205
00:22:57,260 --> 00:23:04,640
It doesn't really matter whether the axes are deterministic or random because we're conditioning on them in the model.

206
00:23:05,000 --> 00:23:13,940
So they are just considered to be fixed in the model regardless of how they're actually obtained, if that makes sense.

207
00:23:15,500 --> 00:23:20,420
So the the only random part in this equation is the E here.

208
00:23:20,810 --> 00:23:29,420
Right. So if we took an expectation of this whole term, if we play our operator over here and our operator over here,

209
00:23:29,630 --> 00:23:34,580
remember where the expectation of a sum is equal to the sum of the expectations.

210
00:23:35,090 --> 00:23:42,160
So then the expectation would be kind of pass through and it would just kind of catch on the E here, right?

211
00:23:42,170 --> 00:23:46,999
Because the beta, not beta one x, those are all considered constants.

212
00:23:47,000 --> 00:23:51,979
So we can take those out of the expectation and we just end up with expectation

213
00:23:51,980 --> 00:23:57,680
of y is equal to the sum of these guys plus the expectation of of E here.

214
00:23:57,990 --> 00:24:04,790
And what we're going to assume is that the expectation of E, given the covariates, is going to be equal to zero.

215
00:24:06,690 --> 00:24:12,180
So this is a picture of the linear regression framework.

216
00:24:12,270 --> 00:24:19,620
Right. So and this is going to show what we're talking about when we're talking about this noise term e here.

217
00:24:19,920 --> 00:24:24,690
Right. And this is what distinguishes this model from a deterministic model.

218
00:24:24,720 --> 00:24:30,540
Right. So the the the the line here is the regression model.

219
00:24:31,050 --> 00:24:34,110
Right. And that's going to be the data. Not here.

220
00:24:34,470 --> 00:24:40,050
That's the intercept. So that's where the the line intersects the the y axis.

221
00:24:40,930 --> 00:24:44,640
Um, and then beta one is the slope.

222
00:24:45,360 --> 00:24:53,820
So. So that's the the the way this works and.

223
00:24:55,230 --> 00:25:07,440
Let's see here. Um, the the lines here, the vertical lines here represent the distance between the the regression line and the data points.

224
00:25:07,560 --> 00:25:12,209
Right. So that's going to be the error that we're observing.

225
00:25:12,210 --> 00:25:15,590
So the larger these values, the more error we're seeing.

226
00:25:16,110 --> 00:25:20,700
And that's going to be a way that we can quantify the randomness in the data.

227
00:25:21,220 --> 00:25:29,790
Um, so the, the, the, the way that we quantify that really is kind of by the, we talked about the variance, right?

228
00:25:29,790 --> 00:25:37,380
And as a measure of spread, so what we're going to have in the linear regression framework is kind of a residual variance.

229
00:25:38,460 --> 00:25:45,840
So that's this, that's this idea of Y minus its expected value given X, right?

230
00:25:46,260 --> 00:25:49,889
So that's the residual the residual term here.

231
00:25:49,890 --> 00:25:53,790
We we call it an error. We call it a residual in the regression framework.

232
00:25:54,570 --> 00:26:00,780
And it's the random part of the model. So the mean is just the I call the the linear predictor.

233
00:26:00,780 --> 00:26:07,259
So that's bad enough. Plus Beta one and we'll talk more about the interpretation of beta.

234
00:26:07,260 --> 00:26:11,900
No, here, I think I might have misspoken here. I don't think this beta vendor knows where it intersects the y axis.

235
00:26:11,910 --> 00:26:18,000
I think it should be where it intersects the X axis. But we'll talk about the the interpretation of data not as well.

236
00:26:19,230 --> 00:26:25,980
So back to our example from the weights of the Detroit residents.

237
00:26:26,580 --> 00:26:34,650
We're we're we're encountering a situation here where we've already used a linear regression framework without realizing it.

238
00:26:34,980 --> 00:26:38,190
Right. So when we modeled the weight in the Detroit residence,

239
00:26:38,370 --> 00:26:47,010
we can write that as equal to the weight is equal to the mean, which we were trying to estimate, plus a random term.

240
00:26:47,460 --> 00:26:51,090
And then this would be what we would call an intercept only model.

241
00:26:51,570 --> 00:26:54,570
So this is still within the framework of linear regression.

242
00:26:54,570 --> 00:26:56,370
We just didn't treat it in those terms.

243
00:26:56,670 --> 00:27:06,570
And that can be a very helpful device to realize that something that we've analyzed is sort of a special case of a more general model, right?

244
00:27:06,720 --> 00:27:13,470
So this is the same this this is exactly the same as analyzing the W's as random, right?

245
00:27:13,470 --> 00:27:17,310
Because, again, if we take the variance of each side of this equation,

246
00:27:17,550 --> 00:27:24,150
this fixed term is going to drop out and we're going to have the variance of W is equal to the variance of E, right?

247
00:27:24,150 --> 00:27:35,280
So all we've done here is we've decomposed W into a fixed and a random term for the purposes of explicating the linear regression model.

248
00:27:35,850 --> 00:27:44,250
And it's an intercept only model because there's no covariance in the model, there's no predictors, so there's no slope.

249
00:27:44,910 --> 00:27:50,460
So, so I want to make sure we distinguish between the idea of coefficients and covariance.

250
00:27:50,790 --> 00:27:55,170
So coefficients are the betas, covariates are the axes.

251
00:27:56,130 --> 00:28:00,840
So the slope here would be the coefficient of a predictor.

252
00:28:01,050 --> 00:28:10,940
So that's the terminology there. Typically, of course, we we don't need to think about this kind of example in linear regression terms,

253
00:28:11,870 --> 00:28:21,770
although if you go on to further study of repeated measures models, then it does become helpful when you think about the idea of a paired T test.

254
00:28:22,010 --> 00:28:29,930
If you have missing values that can become when you think about it's called a linear, mixed model, that can be a helpful way to analyze that.

255
00:28:30,170 --> 00:28:39,230
So there's there's value in realizing that something that we've analyzed in a more simple way actually fits into a broader framework.

256
00:28:39,500 --> 00:28:41,329
And we'll see other examples of that as well.

257
00:28:41,330 --> 00:28:49,280
When we talk about things like ANOVA, because that sort of arose independently historically of the idea of linear regression,

258
00:28:49,280 --> 00:28:52,850
but they all fit under the same, the same general framework.

259
00:28:54,560 --> 00:29:05,480
So, so when we do have a slope or a set of covariates that just is saying that the mean then is allowed to depend on those covariates, right?

260
00:29:05,480 --> 00:29:14,030
So when we have the blood pressure example, we're saying that the mean blood pressure can depend on age.

261
00:29:14,770 --> 00:29:22,450
Right. So that's why we had this notation of expectation of why give in conditional on X.

262
00:29:23,050 --> 00:29:28,390
So we're allowing the mean to depend on other variables.

263
00:29:29,180 --> 00:29:32,920
Any questions so far? Okay.

264
00:29:34,520 --> 00:29:37,820
So back to our back to our example here with blood pressure.

265
00:29:37,830 --> 00:29:44,990
So so the scenario here is that a group of clinicians are trying to quantify the impact of age on systolic blood pressure.

266
00:29:45,020 --> 00:29:49,460
This is the same example that we talked about before. Different data here.

267
00:29:49,850 --> 00:29:56,419
So the collected data are in this scatterplot. So again, what we want to look at when we when we see this data.

268
00:29:56,420 --> 00:29:58,850
So first, we want to plot the data right?

269
00:29:58,850 --> 00:30:05,600
Whenever we get data that we're going to we're going to analyze, we want to plot it and see what does it look like visually.

270
00:30:05,870 --> 00:30:10,459
It's especially easy when there's only one predictor like this. So what do we see here?

271
00:30:10,460 --> 00:30:14,930
We see kind of an increasing slope, right, as we talked about before.

272
00:30:15,170 --> 00:30:19,220
Now, there's no line that's been applied to this yet because we're going to have to fit that line.

273
00:30:19,580 --> 00:30:23,360
So the other thing to notice is there's an outlier here.

274
00:30:23,360 --> 00:30:28,280
And again, we're not going to talk about that yet, but I want you to be aware of outliers,

275
00:30:28,280 --> 00:30:32,629
because they can be kind of the bane of your existence when you're analyzing data,

276
00:30:32,630 --> 00:30:38,600
because they can if you don't plot the data, you might not know they're there and you can get the answers that that don't make sense.

277
00:30:39,830 --> 00:30:43,730
So so let's think about trying to fit a line to this data.

278
00:30:43,880 --> 00:30:50,600
Right. What would we want to consider as a measure of kind of a good line?

279
00:30:51,800 --> 00:30:56,000
We want it to. We want it to go. Does he have any thoughts about that?

280
00:30:56,030 --> 00:31:00,110
Like what would be a good line to fit to this day? What criteria could you think about?

281
00:31:01,270 --> 00:31:07,709
Yes. To go through as many. Exactly.

282
00:31:07,710 --> 00:31:10,440
Yes, you are foreshadowing the exact criteria we use.

283
00:31:10,440 --> 00:31:20,940
We want to minimize the sum of the squared residuals, actually, but the idea is to minimize the distance between every point and that line.

284
00:31:21,510 --> 00:31:22,990
So. So that's the key, right?

285
00:31:23,010 --> 00:31:29,880
Because we could fit any line here, you know, but there's only going to be one line that we're going to consider the line of best fit.

286
00:31:30,270 --> 00:31:39,480
And it's the least squares line because we use the minimizing values for the betas that minimize the least squares criterion.

287
00:31:40,290 --> 00:31:45,749
So, so that's how we choose the best line. And the data is just kind of written out here.

288
00:31:45,750 --> 00:31:48,360
So this is the paired format we talked about.

289
00:31:48,360 --> 00:31:55,560
So each Y is associated with a single X and then you can plot them because each value is kind of paired with another.

290
00:31:57,000 --> 00:32:04,170
So estimation. Right. So we talked about estimating means and variances before.

291
00:32:04,440 --> 00:32:07,180
So now we're going to go a little bit further than that.

292
00:32:07,200 --> 00:32:17,940
We're going to estimate a conditional mean relationship and a variance because we still have this term e here that we need to account for.

293
00:32:18,240 --> 00:32:22,680
Right. Because when we had a single variable, we had a univariate analysis.

294
00:32:22,950 --> 00:32:25,080
We were interested in estimating the mean and the variance.

295
00:32:25,410 --> 00:32:32,850
Now we're interested in estimating the regression parameters which determine the mean and the residual variance.

296
00:32:32,850 --> 00:32:35,880
Right, because e here is the residual or the error term.

297
00:32:36,450 --> 00:32:42,360
So the mean we've talked about is determined by the betas and the Xs.

298
00:32:42,750 --> 00:32:53,790
Right. So once you know the values of the betas, you can find the mean of Y for any X by plugging in an X value to this to this equation.

299
00:32:54,300 --> 00:33:00,600
Right. So one of the things to think about is interpolation or extrapolation, right?

300
00:33:00,930 --> 00:33:09,300
Because we say we have our data set here where we have age going from 20 ish to 70 years.

301
00:33:10,260 --> 00:33:16,260
Do you think it would be a good idea to predict somebody's blood pressure, who is who is 100 years old from this data?

302
00:33:16,350 --> 00:33:19,560
Does anybody think that would be a good idea? No, it wouldn't.

303
00:33:19,590 --> 00:33:23,870
Why not? What's that?

304
00:33:25,910 --> 00:33:30,890
Yeah. We don't have any data, right? There's no data out there that we can base that extrapolation on.

305
00:33:31,070 --> 00:33:35,660
So we want to be very careful. This is a powerful relationship that we have here.

306
00:33:35,900 --> 00:33:42,290
But we need to be careful about not plugging in X values that are too far from what we observed.

307
00:33:42,830 --> 00:33:49,040
Right. And another case that you might encounter is where you have sort of clusters of data and then nothing in the middle.

308
00:33:49,370 --> 00:33:54,650
Right. So it doesn't have to be extrapolation. There can be times when interpolation is a problem as well.

309
00:33:54,980 --> 00:34:01,340
If, for example, we had data kind of going up to age 30 and then a big gap and then age 60,

310
00:34:01,550 --> 00:34:04,880
we wouldn't really know what was going on in the middle here. Right.

311
00:34:05,000 --> 00:34:12,080
We want to have we want to make predictions only in areas of the data where we actually have data points,

312
00:34:12,410 --> 00:34:17,420
because that's where we have information on what that relationship looks like.

313
00:34:17,930 --> 00:34:26,990
Right. So so this is the mean. And then the errors are the difference between the observed value of Y and then the the mean.

314
00:34:27,410 --> 00:34:32,240
Right. So that's the distance between the observed value of Y and the regression line.

315
00:34:33,030 --> 00:34:36,899
So if we have estimates for Baidu and Beta one, we can do what I was just saying.

316
00:34:36,900 --> 00:34:44,340
We can predict y, right? So we can predict the value of Y for each observed value of x.

317
00:34:44,760 --> 00:34:50,520
And the idea of interpolation and extrapolation is you can also predict values for values of X that you didn't observe.

318
00:34:50,880 --> 00:34:54,630
But again, we talked about Y that may not be a good idea in some cases.

319
00:34:55,590 --> 00:34:58,590
And then the residuals are the estimated errors.

320
00:34:58,800 --> 00:35:04,090
And that's going to be the difference between the observed Y and the predicted Y.

321
00:35:04,110 --> 00:35:12,270
Right. So the hats mean predicted. So when you plug in beta hat and beta had not in beta one hat, you get a y hat.

322
00:35:12,300 --> 00:35:19,710
Right? So that's the predicted value of y. So think back to our Detroit male resident weight example.

323
00:35:20,760 --> 00:35:24,570
What would be what would be the predict? What would be the y hat for that?

324
00:35:25,170 --> 00:35:40,820
Does anybody want to take a guess? You can just shout it out.

325
00:35:46,580 --> 00:35:51,170
It's to me, it's just to me there's one value. There's one mean value for everybody.

326
00:35:51,170 --> 00:35:55,010
So the y hat for an intercept only model is just the intercept.

327
00:35:55,280 --> 00:36:01,999
So everybody gets the same prediction, right? And we can already think about for that example why that might not be a good idea.

328
00:36:02,000 --> 00:36:05,720
Right? What is what does weight depend on very strongly in the population.

329
00:36:08,010 --> 00:36:11,400
Height. Yes, height. Exactly. Among many other things.

330
00:36:12,060 --> 00:36:17,910
But there's a very strong relationship with height. So that would be something that we would want to include in a model for weight if

331
00:36:17,910 --> 00:36:23,130
we were really trying to capture what is going on scientifically with weight.

332
00:36:23,580 --> 00:36:27,420
So but when we have an intercept only model, everybody gets the same prediction.

333
00:36:27,630 --> 00:36:36,240
And that's an important notion because we're going to end up comparing all of our regression models to an intercept only model.

334
00:36:36,450 --> 00:36:43,710
So that's kind of our we'll call it a null model. So it's the model where if we didn't know anything else, what would we predict?

335
00:36:43,920 --> 00:36:47,820
Right. So the residuals, again,

336
00:36:48,480 --> 00:36:58,050
that's what we're that's what we're going to focus on in terms of minimizing or in terms of optimizing our choices of beta and beta one.

337
00:36:58,380 --> 00:37:04,380
Right. We want to, again, somehow minimize the distance between y and y hat.

338
00:37:05,010 --> 00:37:07,770
Right. So between the observed and predictive value.

339
00:37:07,980 --> 00:37:16,290
So again, the predicted values are going to fall along this line and the observed values are going to be at the ends of these these vertical lines.

340
00:37:16,560 --> 00:37:19,800
So we want to minimize that distance. Right.

341
00:37:20,070 --> 00:37:23,500
And there's a number of ways that you conceive of distance.

342
00:37:24,660 --> 00:37:29,340
When we talk about the square distance here, that's what we're doing in linear regression.

343
00:37:29,760 --> 00:37:36,050
But there, for example, another notion of distance would be the absolute value of these residuals.

344
00:37:36,060 --> 00:37:39,180
Does anybody know what happens when we minimize the absolute value?

345
00:37:40,630 --> 00:37:46,980
What estimate do we get then? We've talked about this, but we didn't talk about it in this form.

346
00:37:48,870 --> 00:37:53,890
Just the median. If we minimize the absolute values of the errors, we get the median.

347
00:37:53,900 --> 00:37:58,880
And there's actually another form of regression that uses that criterion to get estimates.

348
00:37:59,150 --> 00:38:04,130
In linear regression, we're using the notion of squared errors.

349
00:38:04,460 --> 00:38:15,530
So we'll talk about sums of squares. We're going to see this a lot sum of squared errors or C is the sum of the of the E hats here.

350
00:38:15,530 --> 00:38:22,190
Right. And e hat means that we've plugged in y and y had and squared that difference.

351
00:38:22,340 --> 00:38:28,790
Right. So that's the sum of squared errors. We minimized that to get this this line.

352
00:38:28,940 --> 00:38:32,060
So why do we do why do we square it, though? Does anybody know?

353
00:38:35,400 --> 00:38:37,260
Yeah. You need to deal with the negatives, right?

354
00:38:37,380 --> 00:38:42,450
Like, otherwise, you know, you're going to get some weird answer because the the errors can be positive.

355
00:38:42,600 --> 00:38:49,580
Yes. Question. I understand. Wow.

356
00:38:50,540 --> 00:38:54,770
So the absolute value gives you the median and it gives you what's called quantile regression.

357
00:38:55,130 --> 00:39:02,810
If you use it in a regression framework, the it gives you a different result because it's a different function, right?

358
00:39:02,810 --> 00:39:06,740
Because the absolute value is the square root of the squares.

359
00:39:06,980 --> 00:39:14,060
Right. So if you take the square of the errors and then the square root, that brings you to the absolute value, that's a different criterion.

360
00:39:14,480 --> 00:39:18,170
So because it's just different mathematically, that's why you end up with a different estimate.

361
00:39:19,100 --> 00:39:23,450
It's a good question. And again, there's there's other there's other choices as well.

362
00:39:23,450 --> 00:39:26,569
And we'll talk about when we get to logistic regression.

363
00:39:26,570 --> 00:39:30,680
That'll be another criterion that we'll use to obtain estimates.

364
00:39:30,680 --> 00:39:39,409
And actually the criterion we use in logistic regression turns out to be a different version of the criterion for for linear regression.

365
00:39:39,410 --> 00:39:42,500
So there's a relationship there. Other questions?

366
00:39:47,200 --> 00:39:51,240
Okay. So we're minimizing the sum of squared error.

367
00:39:51,250 --> 00:39:58,330
So we're ending up with the least squares estimates. So the least squares means we've minimized the sum of squares.

368
00:39:58,780 --> 00:40:05,379
So to find these, we can chug through some calculus and we can get because we're minimizing something, right?

369
00:40:05,380 --> 00:40:08,620
We're minimizing this function. That's the sum of the squared errors.

370
00:40:08,920 --> 00:40:14,800
And that sum of squared errors can be written as a function of the betas here.

371
00:40:14,860 --> 00:40:21,280
Right. So that's the way that we obtain the beta hat's beta hat one and beta hat.

372
00:40:21,280 --> 00:40:24,310
Not so bad hat one has this form here.

373
00:40:24,610 --> 00:40:25,000
Right.

374
00:40:25,180 --> 00:40:36,440
So the numerator is the sum of we can think of these kind of cross products, but the difference between X and the mean of X and Y and the mean of Y.

375
00:40:36,470 --> 00:40:40,540
So that's the product and we call that sub x y.

376
00:40:40,930 --> 00:40:47,170
And then the denominator is the sum of the deviations between X and its mean squared.

377
00:40:47,260 --> 00:40:53,950
So that's sub x. So I know this is this doesn't look very intuitive and it probably isn't.

378
00:40:53,950 --> 00:40:57,070
But we'll talk more about why this form looks like it does.

379
00:40:58,390 --> 00:41:08,710
And then for the beta or not, you can just use the means y bar and x bar and the beta hat, one that you plugged in from this to get the intercept.

380
00:41:09,970 --> 00:41:16,600
So you might see some X, Y and s of x excuse me, written in these different forms.

381
00:41:18,130 --> 00:41:27,940
I don't want to belabor any of that, but again, this is this is the way that we arrive at these estimates is minimizing that sum of squared errors.

382
00:41:28,810 --> 00:41:37,720
And one thing to know is that the regression line that we've said using this method is always going to go through the point X bar, y bar.

383
00:41:37,990 --> 00:41:43,840
Right. So that's if you find the mean the X's on the x axis, the mean of the Y's on the Y axis,

384
00:41:44,140 --> 00:41:50,080
the point where those two lines intersect, that's where the regression line is always going to go through.

385
00:41:50,860 --> 00:41:54,130
And you can kind of see that from manipulating some of these expressions.

386
00:41:55,730 --> 00:42:04,910
So for the age and systolic blood pressure example, this is the fitted line that we would achieve by applying the least squares criterion.

387
00:42:04,910 --> 00:42:14,930
So this is the line that minimizes the sum of squared errors between the fitted values and the and the observed values.

388
00:42:15,140 --> 00:42:18,940
Right. So if you look over here, we have some quantities here.

389
00:42:18,950 --> 00:42:22,940
We're going to talk in more detail about things like R squared.

390
00:42:23,510 --> 00:42:31,159
That's a measure of goodness, of fit of the line. So that's going to be and then this Ramsey here, we're going to talk about that, too.

391
00:42:31,160 --> 00:42:39,410
That's an estimate of the error variance or the error standard deviation, but this is the fitted line that we would achieve here.

392
00:42:40,670 --> 00:42:45,739
And one thing to think about is we talked about this being kind of an outlier.

393
00:42:45,740 --> 00:42:51,830
Right. Does anybody think that this is really influencing this line that much?

394
00:42:54,040 --> 00:43:04,160
Yes. It depends on how many data points there are, and it also depends on where this line falls in the X sorry, where this point falls in the X space.

395
00:43:04,460 --> 00:43:09,140
If this point fell more to the right, it might pull that line up more so.

396
00:43:09,140 --> 00:43:16,940
So the idea of an outlier is kind of fluid. And we'll talk about some some diagnostic measures that will help you determine

397
00:43:17,180 --> 00:43:22,340
if a what looks like an outlier is actually having undue influence on the line.

398
00:43:22,580 --> 00:43:27,530
To my eye, it doesn't seem like this point is having too much of an effect on this line,

399
00:43:27,740 --> 00:43:36,649
and it's probably because it's toward the middle of the distribution of the axes, so it's not really able to pull things one way or the other.

400
00:43:36,650 --> 00:43:41,210
And that's when we talk about leverage. That's kind of what we're talking about.

401
00:43:42,500 --> 00:43:45,980
So parameter interpretation, right? This is key.

402
00:43:46,280 --> 00:43:49,930
So let's look at this example of regression output.

403
00:43:49,970 --> 00:43:55,940
So when you fit a regression model some, you've done this already for the homework you're going to see in simple in your regression,

404
00:43:55,940 --> 00:44:03,290
you're going to see a table that has two rows. There's one row for the inner self and one row for your slope parameter.

405
00:44:03,710 --> 00:44:08,540
So in this case, the intercept is 98.7.

406
00:44:08,900 --> 00:44:18,080
So the interpretation of the intercept is going to be the mean of the outcome when the covariate is equal to zero.

407
00:44:18,970 --> 00:44:25,720
Right. So this is the estimated mean systolic blood pressure when age is equal to zero.

408
00:44:26,520 --> 00:44:32,610
Right. So so that's that's the the interpretation of The Intercept.

409
00:44:32,940 --> 00:44:38,849
And we'll talk about something called centering the variables and a bit.

410
00:44:38,850 --> 00:44:44,370
And that involves essentially creating a more interpretable intercept.

411
00:44:44,370 --> 00:44:47,970
Right. So what's the problem with this intercept in terms of interpretation?

412
00:44:49,830 --> 00:44:55,320
Think about what I was talking about with extrapolation. What's that?

413
00:44:57,450 --> 00:45:01,669
Somebody said something. Because because we don't have data.

414
00:45:01,670 --> 00:45:07,130
They're right. We don't have any data at age zero. Our lowest data point is, is age 20.

415
00:45:07,460 --> 00:45:12,650
So so we can get a more interpretable intercept by censoring our covariance.

416
00:45:13,400 --> 00:45:19,470
So we'll I'll talk more about that. So these are the estimates and these are the standard errors.

417
00:45:19,490 --> 00:45:22,400
So the standard error, remember, we talked about what the mean.

418
00:45:22,880 --> 00:45:30,620
Standard error is equal to the standard deviation divided by the square root of N right in the linear regression context.

419
00:45:30,890 --> 00:45:35,510
The standard error has a little bit of a different form, but the idea is the same.

420
00:45:35,810 --> 00:45:43,730
We're trying to ultimately do hypothesis testing and confidence interval construction, and that's what we get with this T statistic.

421
00:45:43,740 --> 00:45:52,970
So remember we talked about T statistics being equal to a parameter estimate divided by its estimate, the standard error.

422
00:45:53,810 --> 00:45:59,780
And then we talked about this idea of critical values and actually carrying out the hypothesis test.

423
00:46:00,200 --> 00:46:05,240
And that's the idea of inference. So you can get all of that from this table.

424
00:46:05,960 --> 00:46:14,930
But before before we move on, I wanted to to explain what the interpretation of the age coefficient is, because this is a little bit more complicated.

425
00:46:15,290 --> 00:46:26,540
So the age coefficient here is 0.97. So what that saying is that for each one unit increase in the x value, so each one year increase in age,

426
00:46:27,020 --> 00:46:33,560
the systolic blood pressure is going to increase by almost one point per year of age.

427
00:46:33,800 --> 00:46:41,240
So that's how this is interpreted. If this was negative, then it would be a decrease associated with each one year increase.

428
00:46:41,510 --> 00:46:45,950
So that would be a negative relationship. What we saw was a positive relationship.

429
00:46:46,640 --> 00:46:50,720
So if we think about correlation coefficients is going to be a positive correlation.

430
00:46:51,380 --> 00:46:58,220
So so if we write out the estimates here we have beta one had is 0.97 we have beta not had is 98.7.

431
00:46:58,640 --> 00:47:06,860
So if we want to write the model based on this output, this is if I ask you to do this on a exam, this is what I would want to see.

432
00:47:07,130 --> 00:47:11,240
I would want to see either the expectation of why,

433
00:47:11,240 --> 00:47:17,690
given X is equal to the estimated intercept plus the estimated slope times x or

434
00:47:18,200 --> 00:47:22,990
Y is equal to the estimated intercept plus the slope times x plus the error.

435
00:47:23,020 --> 00:47:27,080
Remember, these are two ways of writing the same model.

436
00:47:27,350 --> 00:47:35,460
So when we have the expectation here, we don't need the error term because the error terms expectation is zero, right?

437
00:47:35,480 --> 00:47:42,140
So when we take the essentially this form on the left is the form on the right after we take expectations.

438
00:47:43,010 --> 00:47:48,579
Right. They made me questions about writing the model. Because this is really important.

439
00:47:48,580 --> 00:47:52,870
It's really important that you are able to write out the models that you're fitting because that's

440
00:47:52,870 --> 00:47:57,130
going to be how you're going to know how to interpret the results and the parameter estimates.

441
00:47:59,470 --> 00:48:04,690
So we did this a little bit. Right. Right. So the slope is the second line in that output, right?

442
00:48:04,690 --> 00:48:10,959
So in the in the in the model output, it's always going to label the intercept as the intercept.

443
00:48:10,960 --> 00:48:16,900
And everything else in the model is going to be a slope and it's going to be the slope associated with the covariate that's listed.

444
00:48:17,140 --> 00:48:22,390
So in this case, we had age listed, right? Because age is our only covariate in this model.

445
00:48:23,020 --> 00:48:26,899
So we have a .97 mm.

446
00:48:26,900 --> 00:48:33,640
Of Mercury estimated mean change is the dog blood pressure per one year increase in age.

447
00:48:33,970 --> 00:48:40,290
So this is the interpretation that I want you all to think about whenever you see a regression model, right.

448
00:48:40,450 --> 00:48:51,730
So the regression coefficient beta one is the estimated mean change in the outcome per one unit increase in the X variable.

449
00:48:52,030 --> 00:48:58,089
So if it's negative, remember, if beta one is negative, it's going to be a decrease in the outcome.

450
00:48:58,090 --> 00:49:03,370
If it's positive, it's an increase of the outcome. So we talked about the intercept two, right?

451
00:49:03,610 --> 00:49:11,739
So Beta not had is 98.7. So this is the best made it mean blood pressure for a newborn and hope.

452
00:49:11,740 --> 00:49:15,040
I'm hoping that that we can all see why this is the case.

453
00:49:15,040 --> 00:49:21,080
Right. Because if you plug in X here, X equals zero here or maybe it's there and look at it over here.

454
00:49:21,100 --> 00:49:28,990
So if you plug in expectation of Y, given X equals zero, you can see this zeros this multiplied by zero zero.

455
00:49:29,140 --> 00:49:36,940
You just end up with 98.7. So that's why the intercept is the mean value for somebody with an X value of zero.

456
00:49:37,330 --> 00:49:44,709
So in this case, it's a newborn and this is maybe not meaningful because we didn't have any newborns in the data.

457
00:49:44,710 --> 00:49:55,870
Right. Our data source stops around age 20. So is the estimated mean change in blood pressure for a one year increase in age from 34 to 35,

458
00:49:56,200 --> 00:50:00,070
different from that in the change of age from 46 to 47.

459
00:50:01,270 --> 00:50:06,340
And the answer is no. And the reason is because we have a linear model here.

460
00:50:06,370 --> 00:50:15,580
So if we write this out algebraically, we can write the expected value of y given age equals x plus one and x is arbitrary here.

461
00:50:15,850 --> 00:50:20,770
Right. That's what we're trying to show minus the expected value of y given h equals x.

462
00:50:21,070 --> 00:50:28,630
So what we've written here is the difference in mean blood pressure, given a one year difference in age.

463
00:50:29,110 --> 00:50:35,870
So if we write this out, we can write made it up plus beta one times x plus one minus beta not plus beta one times x.

464
00:50:36,460 --> 00:50:39,850
Shining through the algebra here you end up with just beta one half.

465
00:50:40,150 --> 00:50:49,719
So because x was arbitrary, we know that it doesn't matter if you're looking at the difference between 34 and 35 or 46 and 47,

466
00:50:49,720 --> 00:50:53,140
it's the same mean change regardless.

467
00:50:53,470 --> 00:50:59,920
And again, this is another reason why we want to be wary of extrapolating, right?

468
00:50:59,920 --> 00:51:06,550
Because it's possible that the data out in as somebody gets to age is 80, 90 or older.

469
00:51:06,940 --> 00:51:15,700
That might not be a linear relationship anymore. Then it might depend where you start to where you to what that effect is for the change.

470
00:51:15,700 --> 00:51:19,470
So that's when we get into the idea of a nonlinear relationship.

471
00:51:20,350 --> 00:51:27,339
And I'll talk more about nonlinear X values versus nonlinear models in a bit, but I just want to foreshadow that.

472
00:51:27,340 --> 00:51:35,620
That's something that can happen, is that a relationship can be linear over some range of the axes and not linear over other ranges.

473
00:51:36,340 --> 00:51:43,030
Okay. So so what is the expected difference in blood pressure between a 45 versus a 55 year old person?

474
00:51:43,420 --> 00:51:45,250
Right. So we do the same thing here.

475
00:51:45,400 --> 00:51:55,210
We write out the predicted value of blood pressure for somebody age 55, subtract that predictive value for somebody age 45.

476
00:51:55,540 --> 00:52:00,700
And what do we end up with when we go through the algebra here? It's beta one times ten.

477
00:52:01,680 --> 00:52:08,400
So when we want to look at a difference that's greater than one year or whatever the difference may be,

478
00:52:08,580 --> 00:52:12,570
we just multiply that value times the estimated coefficient.

479
00:52:13,660 --> 00:52:17,229
So if it was a five year difference, then it would be five times better.

480
00:52:17,230 --> 00:52:21,140
What had. So everybody follow that question so far.

481
00:52:27,240 --> 00:52:36,380
Yes. Model or. When you're writing out the model, you can if you'd like.

482
00:52:36,390 --> 00:52:41,580
It's again it's these forms are kind of these forms are kind of interchangeable here.

483
00:52:41,730 --> 00:52:46,020
You can either write it this way with with the hat, or you can write it as an expectation.

484
00:52:46,500 --> 00:52:49,840
Without it you have. So either way.

485
00:52:49,840 --> 00:52:53,350
Either way is fine. I often. I mean.

486
00:52:54,920 --> 00:52:58,850
Strictly speaking, there's an additional specification of the amount this is.

487
00:52:58,850 --> 00:53:05,330
This model is not completely specified yet. And what's what's missing here is anybody want to take a guess at what's missing here?

488
00:53:07,170 --> 00:53:12,760
There's one parameter that we haven't talked about that we haven't specified is not a regression parameter.

489
00:53:12,780 --> 00:53:25,130
What is? What do we not what do we not know about E!

490
00:53:28,310 --> 00:53:31,360
It's variants. We don't know the variants here.

491
00:53:31,370 --> 00:53:35,630
So to completely specify this model, we would need the error variants.

492
00:53:35,930 --> 00:53:45,240
So oftentimes what's nice is to write out this form and then you can add on each is distributed normally with mean zero and variants.

493
00:53:45,260 --> 00:53:50,540
Sigma Hat, we'll talk about Sigma hat in a bit, but yeah, either those forms are acceptable.

494
00:53:52,780 --> 00:53:58,329
All right. So the key assumptions there, several key assumptions for linear regression.

495
00:53:58,330 --> 00:54:04,780
And these are just the assumptions that allow us to kind of go through and make our inferences on the beta.

496
00:54:04,780 --> 00:54:08,679
Hads So this is the setup again.

497
00:54:08,680 --> 00:54:15,790
So this sigma squared is what I was talking about with E, so this here, this is a completely specified model.

498
00:54:16,240 --> 00:54:23,950
So this is saying that the Y is the symbol means is distributed as Y is distributed

499
00:54:23,950 --> 00:54:30,279
as normal with mean beating up plus beta one times X and variance sigma squared.

500
00:54:30,280 --> 00:54:35,559
So this is a completely specified probability model now because we know the

501
00:54:35,560 --> 00:54:39,160
mean and we know the variance and for a normally distributed random variable,

502
00:54:39,310 --> 00:54:42,460
that's all you need to know to completely specify it.

503
00:54:42,940 --> 00:54:46,630
So this is a specified a fully specified probability model.

504
00:54:48,080 --> 00:54:54,740
So what we have here graphically is this linear relationship between and acts that we've talked about,

505
00:54:54,980 --> 00:55:01,940
and you can picture these little Gaussian distributions at each point along the line here,

506
00:55:02,210 --> 00:55:06,440
because conditional on the value of X, we have a different mean.

507
00:55:06,770 --> 00:55:13,400
But these these if you look closely, these little Gaussian distributions look the same, right?

508
00:55:13,880 --> 00:55:17,660
They all have the same spread, although they have different means.

509
00:55:17,900 --> 00:55:21,200
So the mean is different because X appears in this expression.

510
00:55:21,410 --> 00:55:27,170
But there's only one variance, right? So this is the constant variance assumption.

511
00:55:27,650 --> 00:55:38,020
Right? That's the that's saying that the variance in the outcome cannot depend on anything, especially not on the X values.

512
00:55:38,180 --> 00:55:47,330
Right. Because if we had if these little Gaussian curves were getting wider as X increased, that would be Hatteras get assessed.

513
00:55:47,480 --> 00:55:52,670
And that's one of the problems that we need to address sometimes in linear regression,

514
00:55:53,270 --> 00:55:57,980
because that can that can cause issues with our inference for our model.

515
00:55:58,640 --> 00:56:02,300
So that's that's Thomas Cadastre. See, that's assumption number two here.

516
00:56:02,510 --> 00:56:06,590
Assumption number one is just that this is a linear relationship, right?

517
00:56:06,710 --> 00:56:14,930
So we have only one X here and we're just we're just saying that this relationship is linear is kind of the the basic assumption.

518
00:56:14,990 --> 00:56:20,690
Right. So there's not a curving relationship here. There's not like a step function or something.

519
00:56:21,320 --> 00:56:23,899
We will talk about ways to model things like that.

520
00:56:23,900 --> 00:56:31,970
But for now, we are just assuming that the relationship between X and Y has this linear shape, right?

521
00:56:33,290 --> 00:56:41,780
So so that we talked about one and two here. Normality is specifically saying that these errors have the normal distribution.

522
00:56:42,440 --> 00:56:48,830
Right. So we can talk about we can think about examples of applying linear regression.

523
00:56:49,370 --> 00:56:52,220
I mentioned things like a Likert scale, right?

524
00:56:52,520 --> 00:57:02,150
That's something where it's only taking on a discrete set of values so it becomes harder to conceive of a normal distribution for the residuals.

525
00:57:02,450 --> 00:57:07,819
And that's what we when we talk about normality here, we're talking about the residuals, and we'll go to more detail about this.

526
00:57:07,820 --> 00:57:11,420
But this is just kind of foreshadowing and then independence.

527
00:57:11,420 --> 00:57:14,810
So we haven't talked too much about statistical independence yet,

528
00:57:15,110 --> 00:57:27,320
but that's just the idea that the these data that X1 and X1 as sorry X1 of y one as a pair are not statistically dependent on any other x and y pair.

529
00:57:27,710 --> 00:57:32,900
So there's no another way to think about this, is there? There's no clustering in the data.

530
00:57:33,200 --> 00:57:37,489
There's no association in subgroups of the data.

531
00:57:37,490 --> 00:57:42,970
So a violation of that would be something like if you were sampling families, right,

532
00:57:42,980 --> 00:57:46,850
and you were looking at an outcome variable measure on individuals and families,

533
00:57:47,150 --> 00:57:53,360
you would expect that the values of these variables within a family would be correlated with one another.

534
00:57:53,510 --> 00:57:59,360
So that would violate this independence assumption. So so a little bit more on each of these assumptions.

535
00:57:59,370 --> 00:58:07,370
So so the linearity assumption is really just a statement of this mathematical form here, right?

536
00:58:07,370 --> 00:58:12,980
The conditional mean and why given X is equal to beta plus beta one times x.

537
00:58:13,370 --> 00:58:16,460
So the model just needs to be linear in the parameters.

538
00:58:17,300 --> 00:58:20,340
So this is a non linear relationship here, right?

539
00:58:20,360 --> 00:58:25,490
You can tell that there's this kind of curving association between Y and X.

540
00:58:26,870 --> 00:58:36,740
And again, we can we can look at ways to to handle this kind of model, but it's going to be so it could be something like this, right?

541
00:58:36,980 --> 00:58:45,920
So beaten up this beta one times X squared, that's still linear in the parameters even though it's not linear in the X's.

542
00:58:47,000 --> 00:58:52,430
So so that's an example of a way to handle this kind of relationship, right?

543
00:58:52,430 --> 00:58:56,900
Because this could be part of a parabola, right? This could be some part of a parabola.

544
00:58:57,860 --> 00:59:03,050
And to fully specify a parabolic relationship, you'd probably also want X itself.

545
00:59:03,470 --> 00:59:14,150
Right? You want to have beta, not times plus beta one times X plus beta two times X squared, for example, to fully specify a quadratic relationship.

546
00:59:14,690 --> 00:59:20,549
Right. And then this kind of multiplicative relationship is a little bit different.

547
00:59:20,550 --> 00:59:25,890
We'll talk more about that. We'll talk about transforming the outcome and things like that.

548
00:59:26,490 --> 00:59:33,780
But that's a little bit of a different scenario. But what you should what you should remember from this is that you can have X's in any

549
00:59:33,780 --> 00:59:41,339
form as long as they're linearly combined in to get the the conditional mean right.

550
00:59:41,340 --> 00:59:50,880
So you can have logs of x, you can have X squared, you can have square roots of x as long as they're all combined in this linear form.

551
00:59:51,120 --> 00:59:55,530
It's a linear model in the parameters. They may have questions about this.

552
00:59:59,630 --> 01:00:04,760
Okay. Moving on to the variance constant variance assumption, this is a little bit, I think, more straightforward.

553
01:00:05,120 --> 01:00:11,030
So this is just saying, again, that the that the variance of Y doesn't depend on X.

554
01:00:11,360 --> 01:00:19,370
So this is an example here of as I was talking about, the variance of Y is increasing with X here.

555
01:00:20,030 --> 01:00:24,230
So you can see that there's this kind of fan pattern as X increases.

556
01:00:24,650 --> 01:00:33,380
The fan spreads out. So this is a this is a case where that that assumption would be violated.

557
01:00:33,590 --> 01:00:37,850
And there's ways to deal with that in our linear regression framework that we'll talk about.

558
01:00:38,120 --> 01:00:43,220
But again, this is one of the things that you can glean from plotting the data, right?

559
01:00:43,370 --> 01:00:51,079
So when you plot the data, you can see that when things like this are happening and what the one way that this can come up is,

560
01:00:51,080 --> 01:01:00,530
is when Ys are, for example, count variables, a lot of times this will also come up when Y's are just kind of just kind of positive.

561
01:01:01,430 --> 01:01:07,640
So positive values are values that are restricted. To be positive often just increase the variance as they're larger.

562
01:01:08,270 --> 01:01:10,549
So this is something that actually comes up a lot this,

563
01:01:10,550 --> 01:01:18,110
this non constant variance and in particular this form that you see here where the variance increases with the mean basically.

564
01:01:19,930 --> 01:01:30,400
So normality. Again, this is this is also, I think, more straightforward to understand the the conditional distribution of why given X is normal.

565
01:01:30,580 --> 01:01:35,690
Right. So why itself does not have to be normal.

566
01:01:36,870 --> 01:01:37,540
That makes sense.

567
01:01:37,600 --> 01:01:47,560
Why give an X does and this I know this is a little bit weird the equivalent statement to this is that the residuals are normally distributed.

568
01:01:48,130 --> 01:01:53,740
So why I mean, we can think of an example where Y is not normally distributed, right?

569
01:01:54,430 --> 01:01:59,560
Because Y looked at marginally, could have some weird form.

570
01:01:59,590 --> 01:02:04,090
Right? Again, the example you could consider is heights in the population.

571
01:02:04,930 --> 01:02:14,460
And if we're regressing height on sex, male or female, then if we look at it marginally, it's going to be bimodal, right?

572
01:02:14,470 --> 01:02:17,470
On average, women have a shorter mean height than men.

573
01:02:17,830 --> 01:02:26,200
So that would be that would look not normal marginally, but it could still be normal conditionally on sex.

574
01:02:26,200 --> 01:02:29,410
So if you know someone's sex, you predict their height.

575
01:02:29,620 --> 01:02:36,430
Those residuals are the difference between the predicted height and the observed height that might be normally distributed.

576
01:02:36,790 --> 01:02:45,040
So that's all we're saying with this normality assumption, is that the errors need to be normally distributed, not the y's themselves, right?

577
01:02:45,280 --> 01:02:50,200
So the Y conditional on x is the same as saying the residual is normal.

578
01:02:50,410 --> 01:03:01,540
They just have a different mean here. So one of the ways to deal with this, not being this assumption, not being fulfilled is to transform Y, right?

579
01:03:01,540 --> 01:03:10,960
So one of the things that we can do is we can take the log of y and that can give you more normally distributed errors.

580
01:03:11,260 --> 01:03:18,100
The problem with that is that that then changes how you interpret the parameters, right?

581
01:03:18,100 --> 01:03:28,720
Because you can kind of write out this model and see that now the mean log of y is equal to beta plus beta one times x.

582
01:03:29,080 --> 01:03:33,280
So if you want to interpret on the original scale, you have to go through a few hoops,

583
01:03:33,280 --> 01:03:40,780
you have to jump through a few hoops to get to that interpretation. So that can be the issue with with transforming the the outcome.

584
01:03:42,790 --> 01:03:47,170
Any questions about this? Okay.

585
01:03:48,070 --> 01:03:51,280
So the last assumption, again, I think is a fairly straightforward one.

586
01:03:52,120 --> 01:03:59,650
So this is the independence assumption. So this is a this is the Ys are statistically independent of one another.

587
01:03:59,830 --> 01:04:04,120
So you can't predict knowing one way what any other way is going to be.

588
01:04:04,480 --> 01:04:11,650
So that's what we mean by independent, right? So knowing why I doesn't give you any information on why J.

589
01:04:11,830 --> 01:04:16,750
J not equal to y beyond their covariate values.

590
01:04:17,050 --> 01:04:24,010
Because the very values can give you a prediction. But knowing Y is not going to tell you anything else about why j.

591
01:04:24,810 --> 01:04:29,190
And we talked about an example here. When you sample families, right.

592
01:04:29,340 --> 01:04:35,640
So family unit, the individuals within their family are going to be correlated with each other very likely.

593
01:04:36,300 --> 01:04:42,720
And that's going to give you a structure where you violate this independence assumption because those family units,

594
01:04:42,720 --> 01:04:49,620
the Y ise in those family units are going to be more similar to each other than they are to units outside that family.

595
01:04:50,040 --> 01:04:55,020
So we thought that's an example of cluster data. If we have repeated measures data over time.

596
01:04:55,320 --> 01:04:59,640
So when we if we, for example, measure heights on children over time,

597
01:04:59,880 --> 01:05:05,160
that would be time order data and there would likely be correlation over time in that data.

598
01:05:05,340 --> 01:05:10,050
So it's the same individual, it's data over time. That's another case where this arises.

599
01:05:12,150 --> 01:05:19,650
So I don't wanna spend too much time on this because it's kind of foreshadowing some, some stuff that you may go on to if you take further coursework.

600
01:05:19,650 --> 01:05:28,830
But the examples of other regression models we talked about was on data and binomial data.

601
01:05:28,830 --> 01:05:35,489
We will look at binomial data and the reason that we have to look at it in a different kind of

602
01:05:35,490 --> 01:05:41,940
framework is because the variance depends on the mean for binomial and for plus on or count data.

603
01:05:42,330 --> 01:05:46,410
And some examples of these things are our number of hospitalizations, etc.

604
01:05:46,710 --> 01:05:50,340
I think you all have probably encountered the percentage distribution before,

605
01:05:51,030 --> 01:05:57,419
but there is a regression model framework that allows you to essentially do what we're doing with the linear regression model.

606
01:05:57,420 --> 01:06:05,480
But for count data and this is just another way of dealing with things like non constant error variance, right?

607
01:06:05,490 --> 01:06:09,330
Because in this case the error variance depends on the mean, right?

608
01:06:09,330 --> 01:06:12,000
So the mean and the variance, the Poisson are equal.

609
01:06:13,360 --> 01:06:18,520
So this is different from the normal distribution where the mean and the variance are two separate parameters.

610
01:06:20,520 --> 01:06:26,880
And again, binary data. I don't want to belabor this too much. We're going to talk a lot more about this when we get to the logistic units.

611
01:06:27,780 --> 01:06:33,120
But it's just another case where the mean and the variance are linked to each other.

612
01:06:33,360 --> 01:06:37,110
So this is a violation of that homeless get ethnicity assumption.

613
01:06:39,050 --> 01:06:45,170
So the regression variance now, right. We've talked a lot about the error terms of the residuals.

614
01:06:45,530 --> 01:06:53,600
So now we want to characterize the variance of the residuals because this is going to tell us how much noise is in the data.

615
01:06:53,750 --> 01:07:00,680
How far on average are our data points from the fitted line are from their fitted values.

616
01:07:01,040 --> 01:07:06,260
So how essentially kind of how noisy are our predictions for the data?

617
01:07:06,950 --> 01:07:08,899
So the model setup is the same.

618
01:07:08,900 --> 01:07:17,690
Here we have the Y is conditional and the X is a distributed normally beta, not just beta one x is the mean and then the variance is sigma squared.

619
01:07:17,690 --> 01:07:20,750
So this is the error variance, the residual variance.

620
01:07:21,080 --> 01:07:26,510
So the estimated predicted mean is beta, not hapless beta one half times x.

621
01:07:27,050 --> 01:07:31,040
So what do we do to estimate the variance on the regression line?

622
01:07:31,400 --> 01:07:36,140
So the variance estimate here is has a simple form that's going to look very familiar

623
01:07:36,500 --> 01:07:42,020
because it's basically the same thing that we've seen in the sample variance.

624
01:07:42,440 --> 01:07:50,330
But for two differences, right? There's two different things that are going on here from the sample variance, right?

625
01:07:50,600 --> 01:07:56,870
So one of them is this term, right. Instead of y bar we now have y I had.

626
01:07:57,380 --> 01:08:03,140
So the mean is not constant. It depends on I threw this relationship.

627
01:08:03,560 --> 01:08:08,390
So that's one difference. What's the other difference if I spotted in this is this expression.

628
01:08:11,160 --> 01:08:15,240
Minus two. Yeah. So we're subtracting two from end in the denominator.

629
01:08:15,570 --> 01:08:22,920
And we talked about the subtraction of one of the denominator to the sample variance to account for the fact that we've estimated the mean.

630
01:08:23,520 --> 01:08:27,840
Now we're subtracting two here. Does anybody want to guess why?

631
01:08:29,130 --> 01:08:36,280
Yes. The degrees of it is there's an additional degree of freedom that comes from estimating this beta one, right.

632
01:08:36,520 --> 01:08:40,840
So now the mean doesn't just depend on one parameter, it depends on two.

633
01:08:41,560 --> 01:08:50,590
So we have to subtract two from the denominator because we've lost two degrees of freedom in fitting this additional parameter to the data.

634
01:08:51,670 --> 01:08:56,790
So this is the this is the variance estimate or just this is just some different ways to write it.

635
01:08:56,800 --> 01:08:59,820
Here we can talk about the mean squared error.

636
01:08:59,830 --> 01:09:05,050
We'll talk about that. And this is the sum of squared errors divided by in minus two.

637
01:09:05,320 --> 01:09:11,560
So, so and I want to say a little bit more about this idea of degrees of freedom, because it'll be used in different, different ways.

638
01:09:12,040 --> 01:09:17,680
So so the in minus two, that value is the residual degrees of freedom.

639
01:09:18,160 --> 01:09:22,390
So the degrees of freedom that wind to the model are to here.

640
01:09:22,600 --> 01:09:26,049
And there's really, depending on kind of how you're looking at this,

641
01:09:26,050 --> 01:09:33,100
you can think about it as really just one for the model and then one for the intercept, because you're always going to have that mean intercept value.

642
01:09:33,100 --> 01:09:35,230
We talked about the intercept only model.

643
01:09:35,590 --> 01:09:43,060
We're always going to be comparing our regression models with access to a model that only contains the intercept.

644
01:09:43,450 --> 01:09:47,439
So there's kind of one implicit degree of freedom in the model,

645
01:09:47,440 --> 01:09:53,530
and then there's the additional one in simple linear regression for the for the beta one.

646
01:09:56,060 --> 01:10:02,780
So this is the again, the least squares fit for our residual for our regression model.

647
01:10:03,080 --> 01:10:05,960
And this is the intercept only fit, right?

648
01:10:06,170 --> 01:10:18,320
So, so what we're doing here is we are hopefully using the information in the X variable to improve our predictions of the wise.

649
01:10:18,770 --> 01:10:25,700
So if we look at the average distances on this plot and compare them with this one, I know it's a little bit messy,

650
01:10:25,970 --> 01:10:31,850
but there's the the average distances are going to be greater when we don't have that X value.

651
01:10:32,980 --> 01:10:36,850
So this is kind of a a trait that will we'll return to.

652
01:10:37,930 --> 01:10:45,940
That is the notion of when you add a predictor to the model, you're never going to do worse at predicting the outcome.

653
01:10:46,360 --> 01:10:51,090
Even if it's just a an unimportant predictor, it's not going to make you do worse.

654
01:10:51,100 --> 01:10:55,240
You'll do basically the same than if you had a noisy predictor.

655
01:10:55,480 --> 01:11:02,889
But if it's a predictor that has in any way an association with the outcome, then you're going to improve your predictions.

656
01:11:02,890 --> 01:11:07,060
And by improve the predictions, we mean decrease the mean squared here.

657
01:11:07,690 --> 01:11:11,950
So the mean squared error is kind of a measure of the quality of your predictions, right?

658
01:11:12,310 --> 01:11:21,220
So the smaller the main squared error, the better your estimates are, the closer your predictions are to the observed values.

659
01:11:21,760 --> 01:11:27,850
Right. So this is the again, the right hand panel is just estimating the mean value for everybody.

660
01:11:27,850 --> 01:11:33,280
So everybody in this case will get the same prediction. This is like the Detroit weight scenario, right?

661
01:11:33,280 --> 01:11:40,900
Everybody would get predicted at the mean value. But if we had people's height, we would see something like this, right?

662
01:11:41,020 --> 01:11:45,040
We can have a regression model for weight given height and you would likely see a

663
01:11:45,040 --> 01:11:49,780
positive association and you would be able to predict people's weights better.

664
01:11:50,200 --> 01:11:53,440
So that's kind of the notion here. So any questions here?

665
01:11:56,540 --> 01:12:01,449
Okay. So I guess we have a few a few minutes here.

666
01:12:01,450 --> 01:12:04,510
We can go through some code. I think this is probably helpful for everybody.

667
01:12:05,590 --> 01:12:15,430
So and some of you probably already done this, but I think the the SAS code is given here for kind of reading in the data looking at histograms.

668
01:12:15,430 --> 01:12:24,129
And so this is kind of a univariate summary. So we've talked about a bit some plotting done here, more planning.

669
01:12:24,130 --> 01:12:30,820
So the project means the PROC regs. So this is the one where we're going to be that we've talked about today, right?

670
01:12:31,030 --> 01:12:36,400
So you have your model statement that's going to have the outcome equal sign and then the predictors.

671
01:12:37,450 --> 01:12:43,120
And then that's really kind of all you need to specify. The model, the the software will do the rest.

672
01:12:43,810 --> 01:12:52,090
So we'll look at a few of the outputs here. So the means output is just going to give you some of the summary statistics we've talked about, right?

673
01:12:52,090 --> 01:12:59,110
We've talked about mean standard deviations. It's going to give you the minimum the maximum key here is it gives you the end.

674
01:12:59,350 --> 01:13:02,550
Right. So that's really important to remember is what's the sample size?

675
01:13:02,560 --> 01:13:04,510
And you have to look at this for your homework as well,

676
01:13:05,200 --> 01:13:11,860
because what can happen is you can end up with a different number of observations used if there are missing values.

677
01:13:11,950 --> 01:13:20,950
Right. And you have this in your homework. So so this is the proc rag output and the sum of the things that we're talking about here.

678
01:13:21,310 --> 01:13:24,490
Beta not hat is here, beta one is here.

679
01:13:24,490 --> 01:13:28,990
So this is the, the table of the estimated regression parameters.

680
01:13:30,070 --> 01:13:33,460
There's a lot of stuff here that we aren't we haven't looked at yet. So don't worry about it.

681
01:13:33,760 --> 01:13:39,370
The root emcee is just going to be this the sigma squared hat or sorry, sigma hat.

682
01:13:39,670 --> 01:13:45,610
This is sigma squared hat. We'll talk more about what some of these other values mean in future.

683
01:13:47,130 --> 01:13:52,610
So in ah it's a little bit of a, it's a similar table here.

684
01:13:52,620 --> 01:14:02,339
Right. The, the syntax for the model is just elm is the function we use outcome tilde side and then predictors and then you

685
01:14:02,340 --> 01:14:09,460
have to tell ah what the data is that you're plugging in and then you use summary on that model object to get the,

686
01:14:09,480 --> 01:14:20,400
the summary table here. Right. So we have a summary of the residuals which again is what we look at to when we diagnostically go in to determine,

687
01:14:21,120 --> 01:14:24,750
are the residuals normally distributed? Right. Because that's one of our assumptions.

688
01:14:25,080 --> 01:14:26,940
So that's one thing we want to look at here.

689
01:14:27,270 --> 01:14:35,880
And what we see here is there's a very large residual here and that could be a tip off that there are potentially outliers.

690
01:14:36,060 --> 01:14:39,930
And that may or may not be important. But that's one thing that you can glean from from this.

691
01:14:40,440 --> 01:14:48,750
Then you have the essentially the same table here that you have in the SAS output where we have the estimated betas,

692
01:14:49,170 --> 01:14:57,330
beta, not beta one standard errors, T values and P values associated with those T statistics.

693
01:14:58,410 --> 01:15:07,229
And we'll talk more in the next class about how to get like the degrees of freedom for these tests and things like that.

694
01:15:07,230 --> 01:15:09,220
And we've already alluded to some of that, right?

695
01:15:09,510 --> 01:15:16,500
So we've seen that the degrees of freedom in the residual in the case of simple linear regression is going to be equal to N minus two.

696
01:15:16,890 --> 01:15:21,210
So that's typically what we're going to see as the degrees of freedom for these T tests,

697
01:15:21,570 --> 01:15:27,840
because we've estimated two parameters in predicting this mean and then this residual standard error.

698
01:15:28,140 --> 01:15:34,890
It's the same as the root of the MSI. There's a lot of there's a lot of equivalent terms floating around.

699
01:15:34,890 --> 01:15:41,460
So make sure if you if you have confusion about that, please ask either in class or on the discussion boards,

700
01:15:42,030 --> 01:15:47,640
because I want to make sure everybody's on the same page regarding terminology, because I know that can be that can be difficult.

701
01:15:48,390 --> 01:15:51,540
So this is the mean squared error root mean square there.

702
01:15:51,690 --> 01:15:59,430
And then it also tells you the residual degrees of freedom here, right, says 28, because we had 30 data points minus two.

703
01:16:00,270 --> 01:16:02,040
So we had a question on the homework.

704
01:16:02,040 --> 01:16:09,510
Right, about the number of data points that goes into the regression model versus the number of data points on the scatterplot.

705
01:16:10,620 --> 01:16:15,440
And the question was kind of art does the same, right?

706
01:16:15,450 --> 01:16:20,459
And I think that's really good intuition because really what we're doing with this regression

707
01:16:20,460 --> 01:16:24,900
model and with any linear regression model is we are characterizing a scatterplot.

708
01:16:25,260 --> 01:16:28,380
So it should be the same right when we plot the data.

709
01:16:28,710 --> 01:16:34,620
That's going to be when we plot the scatterplot, that's going to be the data that we're applying our model to.

710
01:16:34,830 --> 01:16:42,630
So there should not be a discrepancy between the number of data points going into the regression and the number of data points going into the plot.

711
01:16:42,990 --> 01:16:48,480
And what you'll find when we get to more, when we get to multiple regression,

712
01:16:48,840 --> 01:16:54,120
is that missing values become a little bit more complicated because somebody can be missing a value.

713
01:16:54,120 --> 01:17:01,020
Let's think about the birth weight and smoking example, right? So we have three variables age, birth, weight and smoking.

714
01:17:01,410 --> 01:17:05,850
Somebody could be missing a value for smoking, but not for age.

715
01:17:06,690 --> 01:17:12,300
And then but they would be excluded from a model that includes both age and smoking.

716
01:17:12,840 --> 01:17:17,040
Right. So they only have one missing value, but they get excluded because they're not a complete case.

717
01:17:17,700 --> 01:17:22,050
Right. So if we think about complete cases, that's somebody who has full data.

718
01:17:22,320 --> 01:17:28,620
So when we start to get into more complicated models, we need to be aware of different missing data patterns.

719
01:17:28,800 --> 01:17:36,190
Okay. And then the ID are you have to go one more step to get this the ANOVA table.

720
01:17:36,190 --> 01:17:45,820
We'll talk a lot more about ANOVA in future, future lectures, but that's essentially a ANOVA is a way to partition the variance in our data.

721
01:17:46,660 --> 01:17:51,460
And this is this is done by basically computing sums of squares and mean squares.

722
01:17:52,540 --> 01:17:55,779
And then an F statistic. We'll talk more about this as well.

723
01:17:55,780 --> 01:18:02,410
But if you recall, an F statistic is going to be a ratio of sums of squares divided by the degrees of freedom.

724
01:18:02,800 --> 01:18:08,410
So we'll talk about that. And then this is another way to compute the residual variance.

725
01:18:09,520 --> 01:18:15,280
So I think we're going to stop there. We'll we'll wrap up and we will continue on Tuesday.

726
01:18:30,370 --> 01:18:39,880
Hey. How's it going? Right.

727
01:18:40,420 --> 01:18:46,260
Right. And so we'll we'll look at that.

728
01:18:46,260 --> 01:18:49,140
So what it comes down to is a scatterplot of the residuals.

729
01:18:49,350 --> 01:18:55,020
So what you want to see is when you plot the residuals on the Y axis and the X axis, the predictor,

730
01:18:55,320 --> 01:19:00,600
you want to see that that looks both as a cluster of zero and eventually as you go away from.

731
01:19:02,310 --> 01:19:06,550
I don't have an example here, but I will see an example.

732
01:19:06,870 --> 01:19:10,390
But that is the scale. Hey.

733
01:19:11,620 --> 01:19:16,090
Sure. Sure. Okay.

734
01:19:21,560 --> 01:19:38,950
Okay. But you need to have a line where you vote.

735
01:19:39,700 --> 01:19:43,750
You need to have, like, a real. Yeah. You need to actually pull it inside.

736
01:19:44,080 --> 01:19:47,130
So there needs to be a line. It's like, read death in your script.

737
01:19:48,010 --> 01:19:54,790
Cause otherwise what happens, happens, but renders it starting. It doesn't even show.

