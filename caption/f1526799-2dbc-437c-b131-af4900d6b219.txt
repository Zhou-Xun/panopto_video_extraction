1
00:00:02,400 --> 00:00:16,620
Morning, guys. Well, we have a few things to say about the midterm, but until next week, I think that's weighed into the second half.

2
00:00:17,370 --> 00:00:26,970
You know, we have to have a little bit more on the media as well.

3
00:00:28,490 --> 00:00:32,400
That's why these remind me if I forgot.

4
00:00:33,060 --> 00:00:39,390
I think I will not if I don't think we would have a little bit more about the midterm, too, after the break.

5
00:00:41,490 --> 00:00:47,760
Okay. So now let's go back to the last example we have been talking about.

6
00:00:51,580 --> 00:01:06,700
You know, we have been talking about interaction and we are looking at this example where you have we are you have each.

7
00:01:09,420 --> 00:01:14,730
Well, we're not response of while the response of interest is at an HDL.

8
00:01:14,970 --> 00:01:21,420
You're not you have to be rebels. Wait, wait. Is continues and the group is an to have three categories.

9
00:01:22,170 --> 00:01:28,980
As I mentioned, now you're able to feed three lines separately because we have three groups.

10
00:01:30,180 --> 00:01:38,750
So you could imagine that if you separate this three groups into three different datasets and then for each dataset,

11
00:01:38,790 --> 00:01:45,300
you think you've got an enormous symbol in your region to see how HDL depends on the weight within that group.

12
00:01:45,660 --> 00:01:57,480
Then you will get to the three separate separated lines, and by doing this, you are allowing the intercepts and results to differ with different.

13
00:01:57,870 --> 00:02:01,880
They could differ across both of the three groups.

14
00:02:03,840 --> 00:02:14,970
So last time we we mentioned that while actually writing the three groups, the three Sabry lines, they could be written as a drawing model.

15
00:02:15,270 --> 00:02:28,169
Now let's go through them very quickly and then I'm going to make a slide comment, a small comment on when these two are equivalent,

16
00:02:28,170 --> 00:02:36,090
because last time we started to include okay, so for the first group now this is line for the first group.

17
00:02:36,600 --> 00:02:44,610
Now for the first group we have well, this is just a rewriting the model in terms of measures.

18
00:02:44,730 --> 00:02:49,410
So here in the first line, we have intercept beta one and slope beta four.

19
00:02:50,940 --> 00:03:00,569
So that's why here we have intercept of one slope beta four and this is intercept and this is wait for the individual from the first order,

20
00:03:00,570 --> 00:03:05,460
from, you know, from 1 to 8. These are the first eight subjects.

21
00:03:06,450 --> 00:03:11,040
And for the second line, the intercept is beta to slow the spread of five.

22
00:03:11,880 --> 00:03:14,040
And so we have intercept, we have the wedge.

23
00:03:14,400 --> 00:03:21,390
But now these ways they are looking for the second, you know, for the second eight subjects in the in the second group.

24
00:03:23,370 --> 00:03:31,230
Now similar for the last group, we can write it as we know that this is the intercept, this is a slope and this corresponds to intercept.

25
00:03:31,410 --> 00:03:37,530
And this is not these are the weights for the ten individuals, you know, for the ten individuals from the third group.

26
00:03:38,100 --> 00:03:43,980
And so within each group we are able to write the model in this matrix for.

27
00:03:45,350 --> 00:03:53,179
And then if we stack all these commissions and Congress on top of each other,

28
00:03:53,180 --> 00:03:59,059
so because we have six parameters corresponding to three groups in group, I have an intercept and swaps.

29
00:03:59,060 --> 00:04:01,220
In total we have six betas.

30
00:04:01,730 --> 00:04:11,650
Now if we stack all these six betas until about each other, then for a group one now because group one the slope is beta one.

31
00:04:11,660 --> 00:04:16,070
Oh, sorry, the intercept is intercept. It's been a long a slow but it's been up four.

32
00:04:16,550 --> 00:04:27,130
So we have to write the intercept into the column ones and the column weights correspond to the columns in the middle.

33
00:04:27,140 --> 00:04:32,050
These are zeros so that the parameters in the middle there will not be picked right away.

34
00:04:32,060 --> 00:04:39,620
Right this way. We do it as measured by location. So this is just a rewriting of of what?

35
00:04:39,650 --> 00:04:44,110
Using matrix notation. And similarly, we can write group two, right?

36
00:04:44,330 --> 00:04:50,210
But again, group A2 and in group do corresponds to intercept beta two, kind of slow beta five.

37
00:04:50,540 --> 00:04:57,879
So you would need to pull to the column ones and columns of weights into the corresponding columns.

38
00:04:57,880 --> 00:05:08,480
Right. So in order to, to, to refer them to be probably modified by beta two and beta five and similarly and before for the last group.

39
00:05:09,110 --> 00:05:20,930
Okay. So once we, once we rewrite the three groups using matrix notation and then what we do is,

40
00:05:21,260 --> 00:05:27,260
okay, we just write the whole thing into a single expression.

41
00:05:27,260 --> 00:05:33,200
So what we do is we put these three matrix, the three measures is.

42
00:05:34,430 --> 00:05:36,440
Until we try it, and that's our design.

43
00:05:36,650 --> 00:05:43,610
These are metrics because we have an individual from the first group, an individual from the second group, and ten individual from the last group.

44
00:05:44,060 --> 00:05:57,220
So we're a stack them on top of each other. So that if you look at the notation here, if we use that matrix notation, then this y this is a 26.

45
00:05:57,260 --> 00:06:02,390
There's a two plus eight plus plus ten. There's a 28 dimension measure.

46
00:06:02,930 --> 00:06:11,540
Similarity. This is a 26 dimension. So the error of absolute metric and EBITA is two of six to measure because in total we have six meters.

47
00:06:12,290 --> 00:06:16,350
And then this X, the design matrix, it has three.

48
00:06:16,370 --> 00:06:25,730
You can consider this to be three blocks. The three blocks, these three blocks are just the three blocks here on this slide.

49
00:06:25,940 --> 00:06:32,450
You're standing on top of each other together, right? So so then you will have this x matrix.

50
00:06:33,980 --> 00:06:40,520
So that's how you write the, the thing in a in a, in a drawing and a matrix notation.

51
00:06:40,970 --> 00:06:45,110
But of course, you don't have to do that explicitly.

52
00:06:45,110 --> 00:06:51,440
I mean, if you are really familiar with matrix notation, the familiar with the linear regression model,

53
00:06:52,040 --> 00:07:00,170
you can drag the right right down this, this, this matrix that is part of this matrix notation.

54
00:07:00,950 --> 00:07:07,100
So but here we went over that just to show you guys that now if you write three separate models,

55
00:07:07,130 --> 00:07:12,470
then you can actually write them together as a drawing model and vice versa.

56
00:07:12,470 --> 00:07:16,700
If you have the drawing model equal, separate them into three models for the three different groups.

57
00:07:18,340 --> 00:07:28,780
And then for this matrix notation, it's if you want to write it as well, not using the matrix notation but just to use scalar.

58
00:07:30,010 --> 00:07:33,220
And then this, this is the model. So these are the same models.

59
00:07:33,580 --> 00:07:41,560
I these are the same model. It's just one written using matrix matrix notation, not another one does not in the information.

60
00:07:41,920 --> 00:07:46,900
So if we look at this model, sometimes it's easier is a lot easier to look at this model.

61
00:07:47,590 --> 00:07:57,640
So if you look at this model here, we have because we have three groups and we have defined as three variables, indicators for those three groups.

62
00:07:58,030 --> 00:08:01,270
And this model includes these three dummy variables.

63
00:08:06,340 --> 00:08:12,670
So for example, let's say for group one, for group one, then do 23 are both equal to zero.

64
00:08:13,120 --> 00:08:19,300
Empty one is equal to one. So what is left then is better and better to a authorities up here.

65
00:08:20,350 --> 00:08:26,430
They're not an event. And better five and better six, they are not resolved because G2, G3 they are zero.

66
00:08:26,440 --> 00:08:33,849
So therefore group what is Laplace beta one plus beta four times W and that's the that's for group one.

67
00:08:33,850 --> 00:08:40,810
That's that's exactly how we wrote the separate line for group one and similar for group two.

68
00:08:41,230 --> 00:08:49,660
Now g1 D three equal to zero. So for group two, then that's the line becomes beta two plus beta five times ten statement.

69
00:08:50,410 --> 00:08:56,649
And so in other words, I mean, this, this model from this model, we can get back.

70
00:08:56,650 --> 00:09:00,280
Well, we can we can get back to the the separated model.

71
00:09:00,280 --> 00:09:01,569
And from a separate in model,

72
00:09:01,570 --> 00:09:12,010
we can also write down into this drawing model now noting that one thing I want to point out is that this model does not equal intercept.

73
00:09:12,490 --> 00:09:20,560
So we have talk about this, right? So for this model and we have a categorical variable here, we have three categories.

74
00:09:21,190 --> 00:09:25,389
You can define three down at intervals and that will include variables.

75
00:09:25,390 --> 00:09:36,820
You could either include all three double variables in the model, but then without including interact intercept or you could include intercept,

76
00:09:37,240 --> 00:09:43,060
but it does include two number variable and leave the other one out as the reference category.

77
00:09:43,720 --> 00:09:53,890
So here this this way, by doing this, we are actually including the freedom variables without the intercept.

78
00:09:56,750 --> 00:10:02,360
Okay. So this is how we write those in matrix notation.

79
00:10:02,750 --> 00:10:08,360
And now I want to point out a thing that is subtle, but that is quite important.

80
00:10:08,720 --> 00:10:18,320
I didn't exclusive mention this last time. So here, based on this, it seems that this drawn a model and a three separate models.

81
00:10:18,920 --> 00:10:23,030
They are. Equivalent write the article.

82
00:10:23,030 --> 00:10:26,840
If you look at the three drawn, three separate models and the drawing model is wrong.

83
00:10:27,140 --> 00:10:35,590
This is another equivalent because because we just you know, we just went over the algebra and showed that we are able to rewrite this stuff.

84
00:10:35,780 --> 00:10:43,280
This has a role as a drawing model. But I do want to point out that there is a subtle difference.

85
00:10:44,400 --> 00:10:53,570
Subtle, but important. So the subtle differences that here when you write three separate models, you actually allow three separate error terms.

86
00:10:54,980 --> 00:10:58,910
And this now.

87
00:10:59,030 --> 00:11:04,070
But what part of the drawing model for the twin model here, you'll only have one error return.

88
00:11:05,000 --> 00:11:07,280
One error. That is where the whole dataset.

89
00:11:08,630 --> 00:11:16,400
Now here this is important in the sense that now if you because you are given a hope of just a single drawing,

90
00:11:16,400 --> 00:11:27,830
a better set of rules, your revision of philosophy. So the the simplest way for you to build a model is to consider this drawing a model

91
00:11:28,910 --> 00:11:34,010
because you have a single dataset and just a few other drawing model of numbers.

92
00:11:34,340 --> 00:11:36,200
And from this I was wrong.

93
00:11:36,230 --> 00:11:45,440
Well, remember that if we assume Aboriginal default normal and assume a square root in such description for ABS and this Sigma Square.

94
00:11:45,770 --> 00:11:51,910
This is the variance that is for every single individual dataset we assume out of a

95
00:11:52,070 --> 00:11:56,479
for each individual and a large number of errors is equal to see my square is equal,

96
00:11:56,480 --> 00:12:00,230
so that's less the input areas assumptions.

97
00:12:02,030 --> 00:12:07,220
However, when you fit the model to three here, if you fit this model,

98
00:12:07,670 --> 00:12:14,110
the mathematically you are separating the whole dataset and you have the single dataset you have into three datasets.

99
00:12:15,050 --> 00:12:18,890
The group one has the dataset, a group two has a device and group three has a decent.

100
00:12:20,130 --> 00:12:25,430
And then you're you're feeding a line for one, a lot of people to a logical three.

101
00:12:26,340 --> 00:12:29,760
And here you are, absolutely love this.

102
00:12:31,770 --> 00:12:40,440
If you directly feed a three line separately, then are wouldn't know that you're still treating them that are single data that are what

103
00:12:40,440 --> 00:12:45,389
a thing that of your you have three different data that and then you're feeding three

104
00:12:45,390 --> 00:12:51,150
separate separate linear regression models of are would probably just consider this for

105
00:12:51,150 --> 00:13:00,120
each suffered model are able to treat this as a having could have their own very various.

106
00:13:00,690 --> 00:13:08,999
So of course within this group within this group no higher so assume equal various on and within the cycle group are

107
00:13:09,000 --> 00:13:14,100
still assuming that you will be so within the third world are still some people gatherers convert if you do not

108
00:13:14,100 --> 00:13:24,920
tell are exposed until they are might actually allow the various reasons for differ across the three through because

109
00:13:25,080 --> 00:13:31,710
I mean again while mathematically you are separating the three datasets out of that stage three model separately.

110
00:13:32,460 --> 00:13:34,620
So what about doing that?

111
00:13:34,680 --> 00:13:44,760
I mean, if you allow this seamless word to be from the three datasets that the results become, but the results will not be equivalent.

112
00:13:46,980 --> 00:13:50,060
There will be a slight difference in this results.

113
00:13:50,070 --> 00:14:00,270
We can last time we need pull this out of this in this module we can quickly move back to.

114
00:14:04,580 --> 00:14:11,050
That is module one. You are there is an output.

115
00:14:11,060 --> 00:14:15,380
Let me see if I can very quickly find that.

116
00:14:18,440 --> 00:14:35,809
She? Yeah.

117
00:14:35,810 --> 00:14:41,540
I think it's. She's. Yes.

118
00:14:41,540 --> 00:14:45,920
Here. So okay. So for example, I mean, here we are feeding seven models.

119
00:14:46,910 --> 00:14:50,719
We are feeding the model is in two ways. One way is we feed on the drawing model.

120
00:14:50,720 --> 00:15:08,520
Recall that this is. Recall that this is an example where we have age and age and the birth weight, but not birth weight, dummy of categorical.

121
00:15:08,520 --> 00:15:14,400
So it's high versus low. So it is continuous high birth weight.

122
00:15:14,410 --> 00:15:23,820
This is a categorical. Now, this model here, the first model first set about this is based on a drawing model for you have well as written here.

123
00:15:24,420 --> 00:15:28,710
So you include both age and high birth weight and interaction.

124
00:15:29,460 --> 00:15:34,170
And the second about these two out of these are based on feeding Sabry models.

125
00:15:34,180 --> 00:15:41,490
You use, you feed a model to high, high birth weight group and then you feed the model to a to low birth weight group.

126
00:15:43,890 --> 00:15:47,510
And we can see that although the parameter estimates are all the same.

127
00:15:47,520 --> 00:15:55,560
So for example, if you look at the intercept in age, if you look at these two these two cars pass through.

128
00:15:56,040 --> 00:16:03,360
Now, this is part of the Android model. So this corresponds to the reference category that is high birth weight equal to zero.

129
00:16:03,750 --> 00:16:09,600
So that that is for low birth weight group for low birth weight group, that's actually this output here.

130
00:16:09,600 --> 00:16:16,739
So if we compare those intercept age to these two values, we can see that as their values.

131
00:16:16,740 --> 00:16:26,970
They are exactly the same. Yeah, exactly. However, if you look at the center error, this is the center net of welfare of the model separately.

132
00:16:27,360 --> 00:16:30,550
But these are places that are traveling fit of model quality.

133
00:16:31,110 --> 00:16:35,910
You can see that there is a difference. They're not exactly the same. They're not exactly the same.

134
00:16:36,370 --> 00:16:53,790
So so the other worse now although so in other words, when you fit this model separately or as we thought about you were allowed it was two absolutes.

135
00:16:54,650 --> 00:17:00,510
If have different variants, they could have different variants because if you think about this way of eating the model,

136
00:17:00,510 --> 00:17:08,160
it's just like you have two separate datasets. You have a data set for high birth weight, you have it bears out for low birth weight.

137
00:17:08,790 --> 00:17:11,999
And you didn't tell R that if they were from the same, you know.

138
00:17:12,000 --> 00:17:14,190
That's right. So that are in the same manner. Okay.

139
00:17:14,190 --> 00:17:19,680
So you have two years as how food assembly line for this and then you ask me to fit assembly line for for this,

140
00:17:19,890 --> 00:17:23,130
then I will assume equal variance for this one. Equal variance for this one.

141
00:17:24,120 --> 00:17:30,150
But I have no idea that they were from the same business. So I'm not assuming these two they have equal parents.

142
00:17:30,480 --> 00:17:36,030
So that's the reason why you'll see the difference in terms of the the center errors.

143
00:17:36,810 --> 00:17:42,600
So this is this is not the difference if you directly said, however, I mean,

144
00:17:42,600 --> 00:17:47,639
if you for this power that if you allow this there is to you assume this

145
00:17:47,640 --> 00:17:53,500
variance 2 to 4 equal between the two groups then these center errors would be.

146
00:17:53,730 --> 00:17:57,480
I think so.

147
00:17:57,630 --> 00:18:04,290
Yeah. That's that's the the the difference between fitting the Moto 350 model separately.

148
00:18:13,560 --> 00:18:19,560
But what this differs looking at is subtle so that many people, even with the difference,

149
00:18:19,560 --> 00:18:24,690
many people impacted there, will still go ahead and fill the three models and then you have only testing.

150
00:18:25,110 --> 00:18:30,390
So people still do that. But I just want to point out this difference.

151
00:18:33,710 --> 00:18:41,450
So I mean, in practice then if you go ahead and see the three supermodels and they make and then make a conclusion based on that.

152
00:18:42,680 --> 00:18:44,780
I mean, I mean, it's not it's not wrong.

153
00:18:44,900 --> 00:18:54,140
It's but you need to keep in mind that it's not based on the assumption it may not be exactly the same as fitting a drawn model.

154
00:18:56,090 --> 00:19:02,450
Okay. So now let's go back to the model date.

155
00:19:02,780 --> 00:19:08,270
So here we have written this as a draw to model in this way.

156
00:19:08,930 --> 00:19:14,930
This is our drawing model. And then here are the first three as we measure.

157
00:19:14,960 --> 00:19:18,210
These are the three intercepts corresponding to three groups.

158
00:19:18,230 --> 00:19:28,040
If we write them together and then the next three, these are the one on the slopes for for weight.

159
00:19:28,100 --> 00:19:42,010
So if we read them together. And then what we are interested in testing is whether this three groups have the same slope.

160
00:19:43,420 --> 00:19:46,620
So in other words, whether whether it was.

161
00:19:49,170 --> 00:19:52,920
So let's say this is HDL, and this is this is weight.

162
00:19:55,080 --> 00:20:02,969
So if you consider the three groups separately or if you allow while generally speaking you got beat up for a bit of

163
00:20:02,970 --> 00:20:11,130
five beta six the three slots then they will do correspond to three lines that they may look like may look like these.

164
00:20:12,120 --> 00:20:19,230
So they have different intercepts and they have different locks. So that's that's what that's the most general case.

165
00:20:19,620 --> 00:20:23,160
But now we are interested in testing whether the three slopes are the same.

166
00:20:23,450 --> 00:20:29,670
We are the worst, but we are interested in seeing whether the three lines look like these.

167
00:20:30,000 --> 00:20:36,659
They're paramount. So they still have differ in that they may still have different intercepts, but where it has them,

168
00:20:36,660 --> 00:20:41,220
whether they are parallel to each other so that if they have the same slopes.

169
00:20:41,940 --> 00:20:50,100
So this is the novel. This is where testing the three slopes are equal and the three slopes, well, again,

170
00:20:50,400 --> 00:20:58,020
from here now we need to translate this not but this is into the t times beta

171
00:20:58,020 --> 00:21:03,630
equal to a C and that's how we are based on which we can try the F statistic.

172
00:21:04,320 --> 00:21:08,940
So how to translate the lot of those as into a T time, speed up people, lose them.

173
00:21:08,960 --> 00:21:15,210
See, I mean, this requires some experience and some some practice.

174
00:21:15,990 --> 00:21:23,850
So here for a while here, you could actually you could do that by I mean by by writing it.

175
00:21:23,850 --> 00:21:32,700
But while by by actually addressing it. So how by our experience writing it into this to extract two equations.

176
00:21:32,910 --> 00:21:35,100
However, you could rather go, go,

177
00:21:35,100 --> 00:21:44,100
go from the translated analysis into t comes being able to see depends on how familiar you are with the with the matrix notation.

178
00:21:44,370 --> 00:21:49,350
But let's take a look at the first. Write it into two separate equations.

179
00:21:50,010 --> 00:21:58,180
So this number, this is for equal to five. Equal to six. That is beta for minus beta five, equal to zero and a play to five zero.

180
00:22:00,270 --> 00:22:06,800
Of course. I mean, then it would also imply beta for minus me to say is equal to zero.

181
00:22:06,810 --> 00:22:11,070
But that's redundant. Once you have this true equation then becomes redundant.

182
00:22:12,060 --> 00:22:17,100
So then based on these two equations, then you do like this.

183
00:22:17,100 --> 00:22:26,490
T So for example, this one modify the beta one beta 2334, five,

184
00:22:26,880 --> 00:22:37,070
six does t minus this beta that it will keep you beta four minus beta five and beta five might have been a six an.

185
00:22:37,100 --> 00:22:42,410
And then of course this Nobelist says that both are equal to zero then.

186
00:22:42,900 --> 00:22:50,460
But our listeners can be rather than this t times beta equal to zero right by using this particular P matrix.

187
00:22:51,090 --> 00:22:55,120
And it has two rules. So two equations.

188
00:23:01,050 --> 00:23:07,230
Yeah. So that's how we translate this. But this is into a forum a few times based on equal value.

189
00:23:07,950 --> 00:23:15,240
So once we have this novel novelist, then we are able to construct the F statistic.

190
00:23:16,890 --> 00:23:23,380
So absolute, as we call it, that is ten times better, had a minus C, but our C in this case is equal to zero.

191
00:23:23,400 --> 00:23:27,450
So we have three times better hash. And here are a few times better hat.

192
00:23:27,900 --> 00:23:31,350
And in the matrix in middle. Well, this is inverse. There's a inverse.

193
00:23:32,400 --> 00:23:43,110
Be careful. An inverse of this matrix. This is actually the balanced matrix of t times beta hat and divided by its bigger freedom, the do freedom two.

194
00:23:43,110 --> 00:23:47,880
In this case, this is the number of rule.

195
00:23:49,680 --> 00:23:57,210
Indeed right in the matrix or in either. Essentially, it's a number of constraints you impose on the data.

196
00:23:57,630 --> 00:24:07,820
If you look at here, this is not. Well, this is. So in the original model you have speed up for a rate of five better six three free crashes.

197
00:24:08,120 --> 00:24:11,810
You allow them to differ so they could be totally different.

198
00:24:12,620 --> 00:24:15,920
But on another, who says that they are all equal.

199
00:24:17,620 --> 00:24:21,640
This means essentially it to me is that you are imposing two constraints.

200
00:24:22,120 --> 00:24:26,730
Now, when they are all equal, then you say so you only have one for a parameter,

201
00:24:27,340 --> 00:24:32,140
because once you know what a beta four is, you know what beta five, beta six, what they are.

202
00:24:32,570 --> 00:24:38,590
So, so there's only one free practice. So that's why the why you are imposing a constraint.

203
00:24:38,980 --> 00:24:44,440
So that explains why. TS to the management to the road you measures to.

204
00:24:44,620 --> 00:24:48,510
And also that explains why appear you are divided by two.

205
00:24:49,360 --> 00:24:52,360
So this is where this to come from.

206
00:24:54,550 --> 00:24:59,590
Hmm. And then this whole thing, this effort led us to follow this evidence fusion.

207
00:25:01,620 --> 00:25:08,579
This is where the food your freedom and pointed this t this is equal to N minus P 20 is

208
00:25:08,580 --> 00:25:16,400
equal to minus p is equal is 26 in our case and a P is six because we have six meters.

209
00:25:16,560 --> 00:25:20,140
You know what? Okay.

210
00:25:20,410 --> 00:25:26,440
So this is the F on F statistic.

211
00:25:29,320 --> 00:25:34,620
Now from we are calling outward. Now let's let's go back to our goal for the example.

212
00:25:34,630 --> 00:25:39,610
So then from the table we are able to redo those numbers. So this is the dataset.

213
00:25:39,790 --> 00:25:43,300
So we have 26 individuals.

214
00:25:45,670 --> 00:25:48,990
And then here we define, you know, three variables.

215
00:25:49,000 --> 00:25:56,590
G one, two, three. I three dummy narratives and then we feed to this model.

216
00:25:56,650 --> 00:26:04,310
If you look at this model, this is precisely the model that we want to model that we wrote down here in this nugget.

217
00:26:04,330 --> 00:26:06,820
One is that we are excluding.

218
00:26:08,110 --> 00:26:18,190
But it turns out they're not including in out in the model so that the model that has the three with three intercepts and a three slopes.

219
00:26:20,430 --> 00:26:26,100
So this is the output from the model. The model with three intercept, three slopes.

220
00:26:26,370 --> 00:26:31,770
There's a three year corresponding to the three groups and the.

221
00:26:39,960 --> 00:26:43,890
And a month song was written or signed for three Category four three groups.

222
00:26:45,760 --> 00:26:52,930
Okay. And then this is how you construct the contrast matrix.

223
00:26:52,930 --> 00:27:09,970
T nine So this is the T that we, we found, we found this is, this is the t t matrix naturales And so by modifying this matrix matrix T by beta,

224
00:27:10,150 --> 00:27:14,130
then you will get a beta for minus beta five and beta five minus beta six.

225
00:27:14,650 --> 00:27:18,190
Right. So then essentially what you get if you, if you.

226
00:27:19,570 --> 00:27:26,860
So here this might actually is used to the R code that you can use to test the null

227
00:27:26,860 --> 00:27:34,360
hypothesis that it gives you the absolute has six and the p value and then the R output,

228
00:27:35,050 --> 00:27:39,310
our first output, the null hypothesis you are testing.

229
00:27:39,880 --> 00:27:49,600
So of course you might want to check us to make sure that indeed of what R is testing and what are you what are forecast are indeed there the same.

230
00:27:49,960 --> 00:27:58,330
So if we look at here the first last test that you're testing the the coalition for group of four for

231
00:27:58,330 --> 00:28:04,900
this interaction is actually is the slope for group one minus the slope for group two is equal to zero.

232
00:28:05,410 --> 00:28:13,060
And also you're testing the slope, y'know, scored two. Those are the slope of virtual minus slope group 3010.

233
00:28:13,240 --> 00:28:23,140
So that's what we are trying to test. And then essentially lacazette's by comparing two models.

234
00:28:24,160 --> 00:28:27,640
The city wants for jobless housing, for absolute uptick.

235
00:28:28,150 --> 00:28:31,120
It's always based on comparative models.

236
00:28:31,660 --> 00:28:40,240
So the model which is here, our code and receiving model, this is the model under the novel policies and this is the model.

237
00:28:44,250 --> 00:28:54,330
Under analysis or in other words, this is a model that is HDL equal to beta one,

238
00:28:55,080 --> 00:29:03,840
g one plus beta two g true phosphate a3g3 plus beta four times w plus and.

239
00:29:06,620 --> 00:29:08,059
So this is the reduced model.

240
00:29:08,060 --> 00:29:15,590
So the other words, the reduced model has only one slope because you are assuming all the three groups, they have the same slope.

241
00:29:15,980 --> 00:29:19,389
So you only need one slope. Verses.

242
00:29:19,390 --> 00:29:27,190
The motto to the motto to is that the full motto that allowed different intercepts and different slopes for all the three groups.

243
00:29:28,560 --> 00:29:31,950
So the answer to this is based on comparing this to models.

244
00:29:33,800 --> 00:29:38,030
You know, for the first model, for the reduced model, for the restricted model,

245
00:29:38,930 --> 00:29:45,840
the regression sample square if given by this and your freedom is equal 20 to 22.

246
00:29:46,430 --> 00:29:53,210
But the reason that we have three or four important two is, well, again, this is a minus P and it's 26.

247
00:29:53,840 --> 00:29:59,380
And for the rest of the model, we only have four values. So there is 26 minus four.

248
00:29:59,390 --> 00:30:11,480
Ms. 22 right now for a full model, the regression sum of square is this number and that with you are freedom 20 now 20 is 20 minus,

249
00:30:11,930 --> 00:30:17,300
26 minus six because we have six parameters in the full order.

250
00:30:18,770 --> 00:30:24,350
And then this degree of freedom to this is actually you can think of this in different ways.

251
00:30:24,620 --> 00:30:30,020
You can think of the first. You can think of this as the difference between history, 22 and 20.

252
00:30:30,440 --> 00:30:41,870
So that means in true or you can think of this two as as the number of constraints you imposed on patents and you're imposing two constraints.

253
00:30:42,710 --> 00:30:49,460
These these two they're they're actually they're they're they're the same is just a whether you are you're more

254
00:30:49,460 --> 00:30:56,210
comfortable thinking directly as to the difference between these two or the number of constraints you impose on us.

255
00:30:56,840 --> 00:31:00,620
But in the end, it will get to the same number, the same number of the freedom.

256
00:31:01,070 --> 00:31:10,550
And if this number here is 505 oh five, this is the difference between the regression sample squared.

257
00:31:11,240 --> 00:31:19,720
This is actually the so-called extra sample squared. And this this five 505.

258
00:31:20,170 --> 00:31:24,670
This is the actual sample square five. You know, by adding.

259
00:31:26,460 --> 00:31:36,630
And by adding the. A slope for group two and by adding another slope for a Group three into a load of

260
00:31:36,990 --> 00:31:41,730
March into the restrictive model that has only one slope for all three groups.

261
00:31:42,730 --> 00:31:46,200
This is a C for someone square. And then.

262
00:31:51,250 --> 00:32:10,030
This of statistic. Okay.

263
00:32:10,030 --> 00:32:17,930
So so this a statistic then is. Calculated my best on the river here.

264
00:32:18,290 --> 00:32:24,920
This is what to assess. So our automotive capital is the value for this absolute stock.

265
00:32:27,360 --> 00:32:30,890
Yeah. Then corresponding people on some of our.

266
00:32:32,480 --> 00:32:44,450
And the P-value is actually you know, this is so from our output we have the F value equal to 2.95 and the P value of that is the probability that.

267
00:32:49,540 --> 00:33:02,290
So this is this is the F vision we have does these with sort of freedom and let's say two, four, nine five is here right now.

268
00:33:02,290 --> 00:33:10,630
The pivot is this the area here? That's that's the pivot and that's this p this P that this is the pivot.

269
00:33:11,320 --> 00:33:15,490
And we see now this p value is not a very small is over five.

270
00:33:15,490 --> 00:33:20,710
So if we use four or five as the cutoff is larger than larger than 45,

271
00:33:21,730 --> 00:33:30,190
which means that the data does not provide sufficient evidence against the vulnerabilities.

272
00:33:30,790 --> 00:33:31,509
So in other words,

273
00:33:31,510 --> 00:33:45,460
we fail to reject the abilities and data this data does not support and is not that the data does not provide evidence against this abilities.

274
00:33:45,760 --> 00:33:55,350
So we feel very jagged that it means, well, it seems okay to assume that all three slopes are the same and it seems okay that to assume,

275
00:33:55,660 --> 00:33:58,660
to consider that, you know, the three lines they are parallel.

276
00:33:58,960 --> 00:34:03,700
So the three groups we have the same slopes. The other one is the wedge.

277
00:34:04,150 --> 00:34:09,010
In fact, odds are the three groups are the they are the same.

278
00:34:09,040 --> 00:34:16,320
They do not differ across three different groups. Okay.

279
00:34:16,800 --> 00:34:24,420
So that's the example on any questions so far.

280
00:34:32,710 --> 00:34:37,210
Okay. So that's the ecology of the slope.

281
00:34:38,290 --> 00:34:41,980
Now, similarly, we have passive inequality of intercept.

282
00:34:42,620 --> 00:34:47,429
So here again, this is the draw model. Let's draw a model.

283
00:34:47,430 --> 00:34:51,600
We look at a three intersect so we are able to pass a novel.

284
00:34:51,600 --> 00:34:54,270
This is that three intercepts are the same.

285
00:34:56,610 --> 00:35:04,830
And similar to them we can write but not with us is into a matrix four that is ten times better able to see.

286
00:35:06,120 --> 00:35:12,150
In our case, we found time to be this matrix and it seemed to be a zero vector.

287
00:35:13,690 --> 00:35:19,150
And a similarity we are able to construct. The F yeah.

288
00:35:19,390 --> 00:35:27,130
The calculator, the absolute. And here we do have this another set of our output.

289
00:35:27,400 --> 00:35:32,740
So this matrix, this is the, the T for testing the, the quality of the intercept.

290
00:35:33,460 --> 00:35:39,590
And then indeed, while we're able to check that the R is testing this.

291
00:35:39,610 --> 00:35:45,490
So this is the intercept before the first group minus intercept of the second group in zero.

292
00:35:45,910 --> 00:35:51,420
Then intercept for the second group minus intercept for the third group equal to zero.

293
00:35:53,680 --> 00:35:59,320
So that, again, we are actually essentially we're comparing a model under novelists.

294
00:36:00,960 --> 00:36:09,130
That's equal intercept to a model to the full model that allows leverage intercepts.

295
00:36:10,220 --> 00:36:16,700
And then in the end, we are able to read the absolute SD and the P value from the table.

296
00:36:16,790 --> 00:36:25,280
Right. So we have we have our value equal to 3.12.

297
00:36:28,200 --> 00:36:35,310
Africa 2.3 1a3 .12 and this p value is equal 2.66.

298
00:36:36,570 --> 00:36:47,640
So again, this is larger than .05. So it it means that the data does not provide significant evidence against this liabilities.

299
00:36:49,240 --> 00:36:56,860
So yeah, we feel we were failed to reach that analysis so that the conclusion is that we felt

300
00:36:56,860 --> 00:37:02,230
that the de la Bello's is and it seems okay that to assume that these are three,

301
00:37:02,950 --> 00:37:12,680
three groups, they have the same intersect. And that's not asking for equality of the intersex.

302
00:37:16,260 --> 00:37:20,910
Okay. So the last two slides of this module, these two slides, they are.

303
00:37:23,330 --> 00:37:30,860
As a the proof while two slides a proof we try to prove why the does do follow and after diffusion.

304
00:37:31,730 --> 00:37:40,340
So we are let's not I mean, spend too much time on these on these two slides but we can go over this very

305
00:37:40,340 --> 00:37:45,880
quickly and just point out a major step and then you guys can take place,

306
00:37:45,920 --> 00:37:49,550
take some time after lecture to go over the detailed calculation.

307
00:37:51,530 --> 00:37:57,920
So we recall out of the better hat, that is the least the square escalator.

308
00:37:58,130 --> 00:38:05,810
You may know how to follow. Gama this fusion we have shown shown this from previous modules so better how to follow normal

309
00:38:05,810 --> 00:38:14,240
distribution with this mean is a bus escalator and is various is given by this then to times speed hat.

310
00:38:15,170 --> 00:38:21,590
This is just a linear transformation are better had a better how to follow normal 14 times better how to will also

311
00:38:21,590 --> 00:38:30,200
follow normal decision and we can easily find veins foundation of tube to hand out of the various of TB the hat.

312
00:38:31,980 --> 00:38:39,330
So. So they are given by these foundations either by embargoes on or not.

313
00:38:39,340 --> 00:38:43,290
But this is equal to seeing the various is given by this.

314
00:38:44,970 --> 00:38:57,110
So then under not of others it's. The worse vulnerabilities we are able to find the diffusion of t beta how to bind the c so it follows normal.

315
00:38:57,110 --> 00:39:02,720
Those fusions will mean equal to zero in a virus less code cigaret.

316
00:39:04,550 --> 00:39:14,570
So we are able to fund this. This is after some algebra you are able to find this, this, fusion, this, and then this.

317
00:39:16,310 --> 00:39:21,350
This guy is the numerator of the statistic.

318
00:39:21,410 --> 00:39:27,110
Without that, people are afraid that if they go back to the statistic.

319
00:39:35,490 --> 00:39:39,540
Yes. Here. So if we look at the gap statistic.

320
00:39:42,260 --> 00:39:49,710
The statistic. Now, if we look at the numerator this, we look at this part of the numerator without the risk.

321
00:39:49,730 --> 00:39:53,150
Let's forget about rock for for a minute. Let's just look at the first part.

322
00:39:53,720 --> 00:40:00,710
The first part is actually what we have here.

323
00:40:02,760 --> 00:40:07,140
This is the first part. Oh, by the way, I mean the first part multiplied by C must.

324
00:40:08,430 --> 00:40:13,740
So if you look at this guy, then this guy will follow Chi Square distribution.

325
00:40:14,460 --> 00:40:18,660
Here we are using the result of that. If y follow normal will be zero.

326
00:40:18,660 --> 00:40:23,340
And I'm going to see like the one transposed time sigma inverse Times Square.

327
00:40:24,270 --> 00:40:27,040
So that is exactly the result of what we are using here.

328
00:40:27,060 --> 00:40:34,770
So this is our Y and this is our Y protocols and y and then the middle here, this is the virus.

329
00:40:35,160 --> 00:40:40,560
The virus of Y. Now we have numerous. So by applying that result, this whole thing followed.

330
00:40:40,920 --> 00:40:44,220
Chi Square distribution. Okay.

331
00:40:44,820 --> 00:40:52,650
And then the SC divided by C must were this followed cos where we have shown this from previous model

332
00:40:53,010 --> 00:41:00,990
this is a norm is out and also beta had independent of as I see we have shown this in previous module.

333
00:41:02,670 --> 00:41:04,980
Then if we look at an F statistic.

334
00:41:07,140 --> 00:41:16,530
So we have short we have just formatted this power to follow Chi Square to this fusion and this part photo chi squared is usually.

335
00:41:19,030 --> 00:41:23,220
And the top and the bottom they are numerator denominator.

336
00:41:23,230 --> 00:41:27,070
They are independent because when do have an s? I see they are independent.

337
00:41:28,390 --> 00:41:34,000
And then if we divide the top and bottom by their corresponding to our freedom.

338
00:41:36,180 --> 00:41:47,340
That is what else the district is. But by definition, this guy should have followed an EF decision and there's no river of hope.

339
00:41:48,060 --> 00:41:52,950
However, the spirit is constructed out of this building is to redevelop the Chi Square.

340
00:41:55,080 --> 00:42:00,090
Variables, the top and bottom result divided by their corresponding record freedom.

341
00:42:00,720 --> 00:42:03,770
Then the ratio follows after fusion.

342
00:42:05,830 --> 00:42:09,610
So this asked him to show us that the apps that his take on are not have list.

343
00:42:09,820 --> 00:42:19,150
It follows an EF decision. Okay.

344
00:42:19,180 --> 00:42:25,880
So this is the. Modern day.

345
00:42:27,380 --> 00:42:41,550
Any questions before we move on to the next one? Thanks.

346
00:43:02,130 --> 00:43:17,850
Notre Dame since we.

347
00:43:22,930 --> 00:43:26,980
Okay. Logic Module case about moral diagnosis.

348
00:43:28,270 --> 00:43:35,080
This is a very important one. So almost all the targets we cover are important.

349
00:43:36,830 --> 00:43:40,330
But this is this is another very important public moral diagnosis.

350
00:43:41,710 --> 00:43:44,050
So one more diagnosis means is that.

351
00:43:45,640 --> 00:43:52,570
Now, recall that four linear regression models and this is true for whatever model you assume for the year regression model,

352
00:43:52,570 --> 00:44:03,850
we have made assumptions and we have from the so-called online assumption of linearity in abundance and normality and equal variance assumption.

353
00:44:04,030 --> 00:44:05,080
So those are assumptions.

354
00:44:06,880 --> 00:44:17,950
Now, of course, intuitively those assumptions are violated, but a regression model is not a good model to use anymore, or the results are.

355
00:44:18,280 --> 00:44:23,050
I mean, we cannot completely trust the results anymore because if the assumptions are valid,

356
00:44:23,980 --> 00:44:28,570
but how do we so far, for now, we haven't really talk about how to check those assumptions.

357
00:44:28,570 --> 00:44:35,160
We just say, well, if we assume those assumptions that we have this way of assumption that this will make a difference,

358
00:44:35,170 --> 00:44:38,920
this way of finding center error in this way of constructing absolute testing.

359
00:44:39,580 --> 00:44:44,319
We have never talked about how to check those assumptions. Our model for diagnosis.

360
00:44:44,320 --> 00:44:48,900
This is about checking those assumptions, how to channels assumptions.

361
00:44:49,300 --> 00:44:53,709
I will make sure that you know, Alice, those assumptions, of course, we never believed those assumptions.

362
00:44:53,710 --> 00:45:00,220
Exactly. Oh, but how to track that, at least today, there is no dramatic violation of those assumptions.

363
00:45:04,090 --> 00:45:12,399
And also how to check Moto's action in order to check if there are some influential observations,

364
00:45:12,400 --> 00:45:17,790
outliers, and also how to check if there is medical inherited.

365
00:45:19,420 --> 00:45:23,530
So this is what we are focusing on for a model of diagnostics.

366
00:45:28,730 --> 00:45:36,889
Now let's just refresh your memory of the model assumptions we made behind linear regression.

367
00:45:36,890 --> 00:45:39,410
All not so familiar Russian.

368
00:45:39,470 --> 00:45:48,440
This is our linear regression model and our assumption has in a thought we have and have made a for assumptions linearity.

369
00:45:49,220 --> 00:45:52,220
It means that we assume the mean of y.

370
00:45:52,670 --> 00:45:58,910
The meaning of a response is linear. The linearity here is linear where the value beat us.

371
00:45:59,210 --> 00:46:01,790
So it's leading our value of this business.

372
00:46:03,080 --> 00:46:16,070
However, it is also vitally important to be clear or to to to to find out what the best functional form for X is.

373
00:46:16,790 --> 00:46:22,049
For example, let's say you saying that SBP may depend on h low blood pressure.

374
00:46:22,050 --> 00:46:30,760
We depend on age. But the question is, does it depend on age linear in a linear way,

375
00:46:30,880 --> 00:46:37,840
or there is some curvature so that dependance becomes quadratic, for example, or even other forms?

376
00:46:38,240 --> 00:46:42,700
Now how do we know that if you bend as is linear or other other way?

377
00:46:42,940 --> 00:46:46,720
Because if you simply include X. Here.

378
00:46:46,760 --> 00:46:54,500
If you simply include an X themselves, you're simply assuming that independence is really on this index as well.

379
00:46:55,430 --> 00:47:02,960
Right. But the dependance may not be linear in x, like, for example, H in May there may be some curvature.

380
00:47:03,770 --> 00:47:12,550
So we need to find the proper functional form of X to be important in order to better models

381
00:47:13,370 --> 00:47:20,210
response Y or better explain the variation of life and finding a functional form of X.

382
00:47:20,540 --> 00:47:25,370
This is often the focus of diagnostics of 40 energy,

383
00:47:25,880 --> 00:47:36,200
so we want to find out what functional form of this X should be included so that in the end we have a linear model, linear in terms of patterns.

384
00:47:38,770 --> 00:47:42,580
So while simply putting it simply so here we are.

385
00:47:43,060 --> 00:47:51,070
They're searching for the best form of X, whether it should be actually dissolved or whether it should be X squared or whether it should be some,

386
00:47:51,100 --> 00:47:56,590
you know, some other functional form of X. So that in the end, this is a good linear regression model.

387
00:47:58,690 --> 00:48:02,710
So that's linearity assumption. Another assumption is independence.

388
00:48:04,250 --> 00:48:08,470
So assumed that different individuals in the data says they are invalid.

389
00:48:09,830 --> 00:48:20,060
And also we assume the error term also follows normal distribution with the mean zero and Sigma Square virus.

390
00:48:20,330 --> 00:48:26,030
And also we assume Sigma Square does equal for all individuals.

391
00:48:26,360 --> 00:48:33,560
So equal variance or this is also called a homogeneity assumption.

392
00:48:35,750 --> 00:48:43,190
If you look at all this, all these assumptions, these assumptions are made about from the absolute error error.

393
00:48:44,270 --> 00:48:53,480
So if you look at this assumption, other than the origins, we look at all of these three these three assumptions, there are four absolute.

394
00:48:55,880 --> 00:49:02,620
So they have to check these assumptions. We need to look at the absence of the astral.

395
00:49:02,630 --> 00:49:06,530
This is the so-called the residual. If you look at what absolutely is.

396
00:49:06,530 --> 00:49:13,640
Well, absolutely as is the residual is the error term is the noise that we assume it's the noise in the data.

397
00:49:15,290 --> 00:49:24,200
So to check the assumption assumptions, I'll absolutely now we need to look at the residuals from the dataset and we can estimate this absent.

398
00:49:25,400 --> 00:49:34,550
So that's absent that. So we look at this absolute gas and we use this absolute has to check these assumptions on Amazon.

399
00:49:37,490 --> 00:49:44,990
Okay. Now here, when these assumptions are violated.

400
00:49:47,240 --> 00:49:56,400
Now, if we look at all these things, essentially putting in just one word, if the assumptions are violated, that we shouldn't trust results.

401
00:49:57,290 --> 00:50:01,940
Okay. So if you are estimating better than better Jane hat may be biased.

402
00:50:02,660 --> 00:50:10,760
If you are looking at standard errors in error or maybe either either larger than the true center out or lower or smaller,

403
00:50:10,760 --> 00:50:15,979
that was in error, but it may not be equal, which was vendor error and confidence intervals.

404
00:50:15,980 --> 00:50:19,160
It may not be accurate. And how about this test?

405
00:50:19,190 --> 00:50:26,580
The result may be invalid. So in other words, if the assumptions are violated, then basically we shouldn't trust the results anymore.

406
00:50:28,820 --> 00:50:39,590
And then the the given the degree of impact depends on which assumptions are violated and then to what extent there are body condition.

407
00:50:40,850 --> 00:50:42,260
And if our diagnostics,

408
00:50:42,530 --> 00:50:52,040
our objective is to be able to identify the violation of model assumptions to see if there are some violations and if there are.

409
00:50:52,100 --> 00:50:56,930
So the violation of which assumption? So and then.

410
00:50:58,910 --> 00:51:05,530
And also I don't by biological assumptions and also I don't the observations that may have too much influence on the results.

411
00:51:05,540 --> 00:51:14,870
Let's look at so-called influential observations who are outliers and also understand the impact of violating the assumptions.

412
00:51:18,740 --> 00:51:29,200
So for these assumptions, like one thing we like to point out is that the assumptions they are never will never hold.

413
00:51:29,210 --> 00:51:40,870
Perfect. So. So if we do model diagnostics, we will see that almost in all cases, these assumptions are violative in some degree.

414
00:51:41,260 --> 00:51:44,470
So we don't we never believe that these assumptions hold firm.

415
00:51:47,500 --> 00:51:49,450
So we will see that there are some violations.

416
00:51:50,290 --> 00:51:58,720
But it is just that whether, you know, we think the violations are so dramatic that it's the results are complete.

417
00:52:01,330 --> 00:52:09,940
I mean, we cannot we cannot trust the results anymore or we think, oh, there were also there are some violations, but they seem okay, this is mild.

418
00:52:10,210 --> 00:52:16,860
We can still go ahead. And the problem is not do with the recall of action.

419
00:52:17,080 --> 00:52:20,830
Any models we are assuming they are just approximation to a truth.

420
00:52:21,080 --> 00:52:25,630
We we don't know what the truth is, but we are using our model to approximate the truth.

421
00:52:27,130 --> 00:52:35,550
So in that sense, everything is, is, is not a perfect legal right to build a linear regression model.

422
00:52:35,560 --> 00:52:40,930
We never believe it is. It is the tomorrow but a year early.

423
00:52:40,930 --> 00:52:45,490
I mean, it should have good approximation to the truth.

424
00:52:45,760 --> 00:52:50,710
So for the assumptions as well. So, you know, the violations are mild.

425
00:52:51,230 --> 00:53:05,530
So so sometimes we think this is okay. And also our model diagnosis also include suggesting some potential extraction.

426
00:53:05,680 --> 00:53:14,050
So, for example, if linearity assumption is violated, can we make some transformation of the variables so that it linearity hope?

427
00:53:16,480 --> 00:53:25,480
Okay. And this diagnosis diagnostics usually involve iterating process in the sense that we do some diagnostic.

428
00:53:26,440 --> 00:53:35,229
And we identify with a problem or some potential problem, and then we find some solution, for example,

429
00:53:35,230 --> 00:53:41,080
with a transformational variable and we review the model and then we go back to diagnostic reason.

430
00:53:41,080 --> 00:53:46,030
We are not do standards analysis for that for the updated model.

431
00:53:46,300 --> 00:53:50,920
And then we identify some potential problem and we try to find some solution.

432
00:53:51,190 --> 00:53:55,180
We build a new model and then we go back to that. So this is an iterative process.

433
00:53:56,680 --> 00:53:59,910
If you about it, you're saying, oh, my final model seems to be okay.

434
00:53:59,950 --> 00:54:04,600
There does seem to be some dramatic violation of the current assumption.

435
00:54:07,600 --> 00:54:12,080
Okay. So let's take 5 minutes Premier.

436
00:54:12,730 --> 00:54:16,288
When thing to. Yeah.

437
00:54:22,013 --> 00:54:32,603
The midterm. It's not Thursday. And again, we are going to take the midterms together in the auditorium at.

438
00:54:37,163 --> 00:54:40,333
I do not know if you. Yes.

439
00:54:41,903 --> 00:54:48,473
Okay. And the coverage is actually after the material is after a midterm one.

440
00:54:48,763 --> 00:54:52,733
So it's then this module for midterm.

441
00:54:52,733 --> 00:54:55,833
Why? I think we covered up to module.

442
00:54:55,853 --> 00:55:05,333
We write one after the other to review from module and in few of the materials that is covered today.

443
00:55:09,053 --> 00:55:17,632
So but of course, I mean, we cannot completely separate the material covered and before I before of the major one from the attorney client.

444
00:55:17,633 --> 00:55:22,252
So of course other things that we learned after midterm what rely on the

445
00:55:22,253 --> 00:55:27,813
materials midterm too but focus will be on the materials after midterm one year.

446
00:55:28,433 --> 00:55:37,013
So that's the the coverage and the later today, we are going to post some projects except just like last time so you guys can have some practice.

447
00:55:39,513 --> 00:55:49,683
With social. And then on Tuesday, next Tuesday, we are going to have wall in the wall because we do have a lot of election materials to cover.

448
00:55:49,683 --> 00:55:58,813
So we cannot use the whole lecture Tuesday to do a review, but we will use half of the lecture as we have to make sure to have, right.

449
00:55:58,833 --> 00:56:02,462
So we can maybe used either a first half or the second half.

450
00:56:02,463 --> 00:56:05,553
So I know how to you about that,

451
00:56:06,063 --> 00:56:14,222
but we will use half it as a review and we will go over some problems in the in the practice

452
00:56:14,223 --> 00:56:21,083
exam and then we will use the other half still going over the lecture in Jerusalem.

453
00:56:22,623 --> 00:56:29,353
So that's the plan for next Tuesday. Any questions about the midterm?

454
00:56:31,173 --> 00:56:37,743
So that the class before the exam, are we going to have it in the other room or not?

455
00:56:37,763 --> 00:56:40,113
I mean, we're going to have to have the have their separate.

456
00:56:40,113 --> 00:56:45,783
So on Tuesday, it's going to be we are still going to be in this room because we will do some covers on lecture materials.

457
00:56:47,533 --> 00:56:50,613
We get to bring our own formula yet again. Oh, yes.

458
00:56:51,003 --> 00:56:55,053
So you guys still can have the same [INAUDIBLE] as last time.

459
00:56:55,443 --> 00:57:01,653
Same? The same, the same or the same format.

460
00:57:02,613 --> 00:57:09,873
So I think I still think that, you know, if one piece of paper are double sided, I think.

461
00:57:15,753 --> 00:57:23,013
Okay, any other questions? Okay.

462
00:57:25,443 --> 00:57:33,842
Now, as we just mentioned before the break, so for module diagnosis,

463
00:57:33,843 --> 00:57:41,493
it is because three assumptions are based on are for the residual so or for for absence.

464
00:57:41,493 --> 00:57:47,223
So it's very important to then we use the absolute we're asking the classroom to do model analysis.

465
00:57:48,063 --> 00:57:52,803
So now let's take a look at the absolute. What?

466
00:57:56,643 --> 00:58:01,173
So our assumption is that absent followed, normal distribution would mean zero.

467
00:58:01,353 --> 00:58:06,603
And, you know, if we look out of the back of the Badger absence from all individuals,

468
00:58:06,603 --> 00:58:14,553
then with these have shown this factor, follow this dispersion with this virus and it means that they have equal variance.

469
00:58:14,583 --> 00:58:21,933
All the members of this group will see must wear and they are environment group and everyone from different individuals their environment.

470
00:58:24,543 --> 00:58:33,003
So so this is the error that it's y minus variation what the residual is actually an absolute match.

471
00:58:33,033 --> 00:58:33,692
Oh by the way.

472
00:58:33,693 --> 00:58:45,843
So when we say residual we mean, you know, the estimated absolute so absolute the absolute that this is the residual the residual is actually y,

473
00:58:46,803 --> 00:58:53,703
the observer y minus the predicted y y hat we're estimating y y hat.

474
00:58:55,363 --> 00:59:00,313
And recall that y had it exactly equal to the had a matrix kind of like.

475
00:59:02,843 --> 00:59:06,163
So then absolutely have. Is that all right?

476
00:59:06,173 --> 00:59:10,653
Midas had a matrix h, times y. So it's a linear combination of white.

477
00:59:12,593 --> 00:59:19,013
Now, with this in mind, if we calculate the examination of absolute by minus H.

478
00:59:19,103 --> 00:59:24,353
This is just because the matrix does not depend on. So we can move it out of the exaggeration.

479
00:59:24,773 --> 00:59:31,373
It becomes a foundation. What? These values know why it's based on a linear model assumption.

480
00:59:31,373 --> 00:59:37,433
That's X time speed, so that if we separate this product, we have.

481
00:59:38,513 --> 00:59:46,643
All right. Expedia, bus, Expedia minus eight times, Expedia and then H.

482
00:59:47,313 --> 00:59:55,673
Well, I had a matrix by definition. This is H and this is how have matrices defined in times x beta.

483
00:59:56,603 --> 01:00:03,893
But realizing that if we put this two together and this will cancel with this inverse.

484
01:00:04,163 --> 01:00:10,543
So what is left is just go at some speed so that we have zero and expand as you move zero.

485
01:00:10,553 --> 01:00:14,813
So in other words, the foundation of the residual is equal to zero.

486
01:00:19,703 --> 01:00:26,063
Okay. So that's the is managing the residual.

487
01:00:27,053 --> 01:00:30,623
Now, what about the bearers of the residual, the virus?

488
01:00:30,983 --> 01:00:34,102
Now we can also very easy to on the virus. The residual,

489
01:00:34,103 --> 01:00:41,873
again is equal to this linear combination of what and the bearers of a matrix cos

490
01:00:41,873 --> 01:00:49,313
the matrix times relative interval what that is this may face times the virus.

491
01:00:52,363 --> 01:00:57,723
And. The theories of why this is in the middle.

492
01:00:57,723 --> 01:01:01,883
This is the variance of why. There is a lot.

493
01:01:01,943 --> 01:01:08,783
There is always equal to regardless of alcohol, because that's part that is constant.

494
01:01:08,793 --> 01:01:17,423
And that is that does not cover the events and the times, the course, the measures, the principles of that, plus the metrics.

495
01:01:19,463 --> 01:01:24,083
So then the middle part, let's see, y squared times on other metrics,

496
01:01:24,893 --> 01:01:29,603
you can move the sea much where you look around and other metrics really does not

497
01:01:30,233 --> 01:01:34,762
have any fact in metrics multiplication so that we have the square of A-minus,

498
01:01:34,763 --> 01:01:39,503
h h we know that is quite important.

499
01:01:39,983 --> 01:01:46,642
So the square is just equal to itself. And so Yahoo was bearers of C.

500
01:01:46,643 --> 01:01:50,593
My hat is absolute hat. And that's equal to C must win.

501
01:01:51,083 --> 01:01:58,993
I'm. And of course, absolute hack follows normal distribution.

502
01:01:59,013 --> 01:02:04,563
The reason is that the reason is that Amazon had it's a linear combination of one.

503
01:02:05,973 --> 01:02:09,273
And we know that what followed was a smooth universe.

504
01:02:09,363 --> 01:02:14,823
Amazon had also this vision. So we know that now.

505
01:02:14,823 --> 01:02:18,212
We know that Amazon had a normal distribution. We just found that.

506
01:02:18,213 --> 01:02:27,723
Found out what is meaning is what is variances. So that absolute patch followed normal distribution with this mean and this variance.

507
01:02:31,173 --> 01:02:35,763
So this is the distribution of the residual the residual abstract.

508
01:02:40,803 --> 01:02:48,003
So in other words, now the absolute count should follow normal distribution and leave out some volleyball.

509
01:02:49,783 --> 01:02:53,863
So if absolutely if this is part of our assumption,

510
01:02:54,163 --> 01:03:02,923
we assume Kosovo normal as usual if out of so-called normal diffusion that absolute that we will follow normal as usual.

511
01:03:06,163 --> 01:03:10,483
And another assumption of absolutely is that is, instead of absolutely zero zero.

512
01:03:11,593 --> 01:03:17,753
Now, if in fact, they don't have an equal to zero, then expectation absolute hat is equal to zero.

513
01:03:22,613 --> 01:03:30,083
So. So then, of course, the implication of these results is that while we could check whether absolutely have to

514
01:03:30,083 --> 01:03:35,602
follow along with the solution because we can calculate on that and we can compare well,

515
01:03:35,603 --> 01:03:39,143
we can we can measure his graph and to see whether he roughly following the.

516
01:03:40,823 --> 01:03:43,853
And also we could check whether he's managed an absolute improvement.

517
01:03:44,963 --> 01:03:51,173
So I try to do so as I kind of have a way of checking whether the assumption that absolute followed,

518
01:03:51,353 --> 01:03:56,783
normal and absolute has New Zealand when those assumptions are appropriate assumptions.

519
01:03:58,453 --> 01:04:12,782
Okay. So let's. A tour results in another result is that now based on this out of here the absolute

520
01:04:12,783 --> 01:04:17,793
follow this normally we realize that we had a whole number of just we realized that.

521
01:04:19,283 --> 01:04:22,783
Because this matrix is not adding to the matrix.

522
01:04:22,793 --> 01:04:30,613
This is A-minus minus H. There's not a lot of matrix. So it's all diagonal elements are non-zero.

523
01:04:31,823 --> 01:04:38,593
So the implication is that the components of ABS will have they are not involved.

524
01:04:40,133 --> 01:04:40,633
There's nothing.

525
01:04:41,603 --> 01:04:51,563
So the other words, if you look at the different components of abs on that, these are abso have for different individuals, for different subjects.

526
01:04:52,553 --> 01:05:04,223
They are not independent. Even even though we assume that the original was absolute, we assume that diverse individuals, they have had errors.

527
01:05:05,273 --> 01:05:10,163
That's the independence assumption. Even we are so involved in this assumption.

528
01:05:10,553 --> 01:05:14,133
But this absolute has for different individuals.

529
01:05:14,153 --> 01:05:22,403
They are no longer independent. And as for the comparisons between AB-SOUL had I and AB-SOUL have J.

530
01:05:22,853 --> 01:05:27,203
That is equal to the negative h. I. J times c square.

531
01:05:28,433 --> 01:05:32,683
The reason is because the various covariance matrix is in control.

532
01:05:33,533 --> 01:05:42,723
C must recognize I minus H from this matrix. So this is one another observation.

533
01:05:43,263 --> 01:05:52,293
And then we have assumed that equal variance in various of absolute equal to similar square.

534
01:05:54,513 --> 01:05:59,553
But under that ensemble you honor that assumption. The Bureau is of abstract.

535
01:05:59,553 --> 01:06:03,633
I hat is no longer equal to see square.

536
01:06:05,503 --> 01:06:16,353
The reason again is that the virus, if you look at various the matrix that is this guy.

537
01:06:16,363 --> 01:06:25,843
So when the Verizon fish out of the white hat, it's actually the corresponding valuable element that one element of this matrix.

538
01:06:27,433 --> 01:06:35,413
So the afterglow element, that's one minus H II from this one minus H I in Times Square.

539
01:06:35,833 --> 01:06:45,373
So in other words, that the virus of absolute R hat is slightly smaller than sigma squared,

540
01:06:46,573 --> 01:06:54,253
so small as you must make rather slight smaller mass because I is always positive one and is non-active

541
01:06:54,673 --> 01:07:00,553
because h is symmetric and positive about the matrix and this is a conclusion from the algebra.

542
01:07:01,513 --> 01:07:05,113
Let's not worry too much about it. But. But this is a fact.

543
01:07:05,533 --> 01:07:10,903
And because of this fact. So this y minus h is actually less than one.

544
01:07:11,743 --> 01:07:19,423
Less than one. That means the variance of absolute hat is always smaller than the variance of absolute back.

545
01:07:21,343 --> 01:07:26,673
So these are some observations that we can make from.

546
01:07:29,113 --> 01:07:33,583
From the fact that Mr. Abbott will have to follow this normal decision.

547
01:07:40,643 --> 01:07:48,983
So that is the absolute and the absolute that is the so-called is the so-called convention congressional residuals.

548
01:07:50,843 --> 01:07:55,943
Sometimes people call it the raw residuals on residuals.

549
01:08:00,063 --> 01:08:11,313
And that is simply why a lot of us want to have the observer Y minus the estimating Y or the observer wrong minus the predicted one.

550
01:08:11,613 --> 01:08:13,263
That's not conventional decision.

551
01:08:17,323 --> 01:08:25,243
Now for a the residual there, if we directly use the conventional residual, then there are some problems, some potential issues.

552
01:08:26,113 --> 01:08:36,163
One issue, one particular issue is that now because residual is like absent I had is why I minus why I had.

553
01:08:38,273 --> 01:08:44,483
So everyone has the same unit as y as same unit as mine.

554
01:08:45,623 --> 01:08:50,393
So that if you measure Y in different units, you get a D for values, for absolute.

555
01:08:50,423 --> 01:08:55,493
I have. For example, if your wife said, why is.

556
01:09:04,403 --> 01:09:09,483
Okay. Son. I just can't think of a very good example.

557
01:09:09,633 --> 01:09:13,443
So let's say why is weight right? So is people's weight.

558
01:09:14,373 --> 01:09:19,413
Now you can measure wind impact or you can measure weight in kilograms.

559
01:09:21,393 --> 01:09:28,173
Now if you use LB and if you use KG, you are definitely guaranteed for numerical values for this residuals.

560
01:09:29,763 --> 01:09:39,483
And by measuring why you need four units, you could imagine arbitrarily, you know, like, you know, make this an arbitrary large or arbitrarily small.

561
01:09:39,993 --> 01:09:45,243
And so that's really undesirable because everything I have depends on your unit.

562
01:09:46,353 --> 01:09:51,303
So one way to get rid of this problem is to do a centralization.

563
01:09:51,993 --> 01:09:58,283
This actually leads to the standardizing residuals. How we standardize this absolute.

564
01:09:58,403 --> 01:10:07,502
I have by saying that that the rationale behind this standardized residual is naturally okay.

565
01:10:07,503 --> 01:10:14,193
Because my you know, my original assumption, if you recall the original assumption,

566
01:10:14,463 --> 01:10:20,523
the romantic assumption we are assuming and so I follow normal with various C, my C, my square.

567
01:10:21,543 --> 01:10:27,873
Right? And then now I have a s maybe just as a y less epsilon a hat.

568
01:10:28,713 --> 01:10:39,893
Now, if I am also well, if I also have estimated Sigma Square or estimate of Sigma, if I go to see my hat, then I can divide by epsilon.

569
01:10:39,993 --> 01:10:48,103
How do I see my hat? And that sort of give me, you know, kind of a standard, normal, standard, normal distribution.

570
01:10:48,483 --> 01:10:56,612
And so that's the rationale behind this, the so-called standardized residuals.

571
01:10:56,613 --> 01:11:06,783
So we divide. And so I had a by an estimate of sigma and we call this using the well I mean,

572
01:11:06,783 --> 01:11:12,533
different people may use different notation by the notation is really not that important.

573
01:11:12,543 --> 01:11:17,013
So in this course we will use Z. I have to denote the generalized residuals.

574
01:11:17,763 --> 01:11:24,393
Just keep in mind that some other people may use other notation, but the centralized residual this.

575
01:11:25,313 --> 01:11:39,203
Is unit free. Because absent whatever unit Axl has, I mean, the CMO will have the same unit and then the unit counsel from the comment and the bottom.

576
01:11:39,503 --> 01:11:43,763
So then what? So then on a standardized residual does not have unit anymore.

577
01:11:46,473 --> 01:11:52,463
Of. Okay.

578
01:11:52,673 --> 01:12:00,263
So, yeah. So another thing I want to point out is that oftentimes later we're going to talk about that.

579
01:12:01,133 --> 01:12:06,262
One thing we look for is actually to the residual plot.

580
01:12:06,263 --> 01:12:11,003
We plot for the residuals, the calculated residuals, and to see if there is any clear pattern.

581
01:12:11,333 --> 01:12:15,413
This is a very good way, very effective way of tracking linearity of assumption.

582
01:12:15,713 --> 01:12:21,323
So we're looking for any particular pattern, any pattern pattern for the residual plots.

583
01:12:21,623 --> 01:12:28,883
Once we plot this residuals so that whatever pattern the episode had passed,

584
01:12:29,483 --> 01:12:38,763
the pattern will be preserved by this Z and the reason is that is it's yeah and a simply absent

585
01:12:38,793 --> 01:12:45,173
I have divided by a constant I see my hat is c my hat to have hand does not varied with arc.

586
01:12:45,383 --> 01:12:50,693
So for any different individuals it's all the same. So so that is just a numbers.

587
01:12:50,963 --> 01:12:58,763
So that is just a to risk ill epsilon hat so that would have whatever pattern absolutely I had has

588
01:12:59,183 --> 01:13:04,013
if you make a scatterplot for this individuals then this D I have will have the same pattern.

589
01:13:04,433 --> 01:13:11,333
So this is another thing that's worthwhile to point out and also it's easier to judge the

590
01:13:11,333 --> 01:13:20,542
magnitude here is from Z might not have some idea because I had what is easier to judge

591
01:13:20,543 --> 01:13:26,783
the magnitude of z I had and the reason is that the absolute I had it can be arbitrary

592
01:13:26,793 --> 01:13:32,313
large can be I refer to small depending on the unit that you use in matter mattering what.

593
01:13:32,663 --> 01:13:37,043
But once you standardize it becomes unit lists, unit free.

594
01:13:37,463 --> 01:13:40,823
Then the magnitude of z i had it makes more sense.

595
01:13:43,403 --> 01:13:48,963
So because is centralized. Okay.

596
01:13:49,323 --> 01:14:00,933
So because of all the all these reasons you already read Mosul, that we do not work directly on the imam.

597
01:14:00,993 --> 01:14:05,373
So you're we look at the standardized residuals.

598
01:14:07,653 --> 01:14:13,383
Okay. So that's one version of standardized residuals. Oh, sorry.

599
01:14:13,773 --> 01:14:21,432
There's another slide about that. Okay. So as summarization, if you look at a zillion hats because of the standardization,

600
01:14:21,433 --> 01:14:30,213
this the hat will roughly have we zero and rather have various equal to one not exactly so here on.

601
01:14:36,103 --> 01:14:45,793
On Thursday. So you have the exact to be exact the.

602
01:14:46,333 --> 01:14:54,792
See I had they they are although we are able to derive the but really there's there's not much point to

603
01:14:54,793 --> 01:15:02,563
derive the exact mean and variance because modeling analysis usually we don't have to be like super,

604
01:15:02,563 --> 01:15:06,012
super accurate, we're super, super precise.

605
01:15:06,013 --> 01:15:11,623
So we just look at, you know, overall pattern, whether there is dramatic mileage or not.

606
01:15:12,103 --> 01:15:20,173
So here it would be good enough to to consider this z I have total mean a roughly equal to zero zero is roughly equal to one.

607
01:15:20,293 --> 01:15:25,813
So in other words, this z, I haven't approximately to follow normal this year.

608
01:15:25,853 --> 01:15:30,883
This approximation is especially good. It's very good. But the sample size is not a small.

609
01:15:31,903 --> 01:15:40,523
If you have, let's say for example, 100 200 subjects in a dataset, then you are the Z I have, they are there.

610
01:15:40,843 --> 01:15:45,103
They should be quite close to the standard normal distribution.

611
01:15:45,853 --> 01:15:55,623
Okay. So because this I had the approach of a fellow center girl the solution then we can

612
01:15:55,623 --> 01:16:01,563
interpret the balance of or the magnitude of the guy that becomes president or more.

613
01:16:02,253 --> 01:16:06,363
This is the figure of more. And then we know that.

614
01:16:08,343 --> 01:16:13,733
Percent or normal say this is.

615
01:16:14,853 --> 01:16:23,523
This is why this is an active one. And then the area behind this very one and what this area is, is a problem.

616
01:16:23,613 --> 01:16:25,353
This policy is not right.

617
01:16:25,363 --> 01:16:36,393
And then if we look at it negative two and two and the area between negative two and two, this whole area that's that's that's about 4295.

618
01:16:38,243 --> 01:16:47,933
For the five right and then the ever be that two out of five in of 10.5, that's about 11%.

619
01:16:49,493 --> 01:16:53,063
So you have the worse we know roughly.

620
01:16:54,323 --> 01:17:03,533
The distribution of the I. So if we observe that too many of these guys had a fall, let's say, outside of narrative, to add positive to this range,

621
01:17:04,763 --> 01:17:10,943
because roughly 95% of the of the residual Z I had should have followed, which would now be entry.

622
01:17:11,483 --> 01:17:16,283
So if we see too many residual LRH, that means something is wrong.

623
01:17:17,723 --> 01:17:21,143
That is something violates some assumptions violated.

624
01:17:22,743 --> 01:17:31,112
So. So the advantage then that we're not looking at, we're working.

625
01:17:31,113 --> 01:17:36,423
On the other hand, that is we know roughly what the disputed follows if all of a sudden enrollment is built in.

626
01:17:36,813 --> 01:17:43,623
So we get compared to standard normal. If the decision deviates to art from said Roman, then we know something is wrong.

627
01:17:45,853 --> 01:17:53,593
And this can be kept in the room and can be checked by using his rant, by using Google plot, by using body fat, for example.

628
01:17:53,893 --> 01:18:00,463
Later on, tell about these. So graphical graphical tools that a talented student hacked.

629
01:18:03,143 --> 01:18:06,803
Okay. So that's one type of centralized residual.

630
01:18:07,193 --> 01:18:11,843
Another one is the so-called internal student biased residual.

631
01:18:12,893 --> 01:18:16,433
Again, this is another version of standardized residual.

632
01:18:18,953 --> 01:18:31,173
So if we recall that the parents of Armstrong far ahead, we derived the various of our head rather memories of absolute I had.

633
01:18:31,203 --> 01:18:37,103
That's equal to this guy while minus eight times sigma squared.

634
01:18:39,323 --> 01:18:49,552
So this makes us think well we standardized this absolute I had instead of dividing it by you know see my hat if I have asthma,

635
01:18:49,553 --> 01:18:52,913
you see, my hat is instead with dividing it by sigma hat.

636
01:18:53,603 --> 01:18:59,363
I should divide it by what minus h comes to my has square the square root of that.

637
01:18:59,903 --> 01:19:06,792
So in other words. No.

638
01:19:06,793 --> 01:19:12,523
Because this. Is and I'm biased as a matter of this parents.

639
01:19:12,853 --> 01:19:15,853
So when I standardized. Absolutely.

640
01:19:16,003 --> 01:19:19,123
I had. So instead of, you know,

641
01:19:19,123 --> 01:19:25,782
the C I had a we I saw it simply while this is the easy part and that's the centralization

642
01:19:25,783 --> 01:19:30,703
we don't see used for conventional knowledge working for a centralized residual,

643
01:19:32,023 --> 01:19:40,633
but realizing that the sigma had along itself while seem to have square is not an unbiased as major of the variance of the abstract.

644
01:19:41,173 --> 01:19:47,653
But this one is. This one is. So when we standardize absolute I, we use the square root of it.

645
01:19:47,683 --> 01:19:51,943
So we divide it by this whole thing instead of just see my hat.

646
01:19:56,723 --> 01:20:03,512
You know where this angel I hear this is the highest diagonal element of this.

647
01:20:03,513 --> 01:20:10,163
Had a matrix. And this is the so-called billionaire Russian billionaire Russian literature.

648
01:20:10,193 --> 01:20:14,813
People call him the maverick where the individual that had leverage.

649
01:20:15,503 --> 01:20:21,683
So that is, again, that that is just simply the ice elements of the hat matrix.

650
01:20:25,143 --> 01:20:30,213
And H I is actually equal to is equal to this guy.

651
01:20:32,493 --> 01:20:38,733
This is again, this is by simply taking the price tag of the element of the had a matrix.

652
01:20:40,563 --> 01:20:49,112
Now simple linear rational this I see is given by this is probably this probably gives a better

653
01:20:49,113 --> 01:21:02,853
intuitive understanding what is so h why it matters the degree of how far away it is from the center.

654
01:21:02,973 --> 01:21:11,403
So if you look at the the expression over here, it matters how far away this exi is from the center of x.

655
01:21:14,343 --> 01:21:20,943
So in other words, it matters whether this small in terms of x, two x,

656
01:21:21,543 --> 01:21:31,173
whether this individual eye is an outlier or whether I mean this particular value exciting is really far away from from a center.

657
01:21:33,453 --> 01:21:38,373
So that's intuitively what I stands for, what it represents.

658
01:21:40,173 --> 01:21:50,853
So that this internal is still a nice residual and standardized this absolute I had by this whole thing instead of just see my hand.

659
01:21:54,073 --> 01:21:59,043
Okay. So this internal is still seen as residual.

660
01:21:59,643 --> 01:22:05,283
It has means. So we are not going to prove this. But it has musical and hysteresis also.

661
01:22:05,733 --> 01:22:09,383
While roughly equal to one claw very close to one.

662
01:22:10,083 --> 01:22:18,662
And it follows a t news fusion with a degree of freedom equals equal to the error sum of square.

663
01:22:18,663 --> 01:22:23,883
That is. And. This is. This is in ministry d more freedom.

664
01:22:25,723 --> 01:22:33,133
So what did you practice and what you acquire this residual theory while you're at it?

665
01:22:33,163 --> 01:22:36,793
They don't compare it to what with this particular transfusion.

666
01:22:37,033 --> 01:22:38,713
They just compare it to normal this year.

667
01:22:38,713 --> 01:22:45,252
And the reason is that when this you do more freedom or the sample size is large, which is typical in the case,

668
01:22:45,253 --> 01:22:50,833
I mean, less in some extreme cases, you only have a, you know, ten, 15 subjects.

669
01:22:51,853 --> 01:22:59,593
But otherwise, I mean, if a sample size and not a very small, then this team is very close to a standard, normal decision.

670
01:23:00,343 --> 01:23:07,033
So you can simply then compare this residual internal still as residual again to a standard luminous

671
01:23:07,033 --> 01:23:13,093
urine to see whether it follows or to see whether it how close it is to standard missionary.

672
01:23:16,363 --> 01:23:20,593
Okay. This is the so-called internally standardized residual.

673
01:23:22,533 --> 01:23:27,723
Okay. And there is another version of residual that is the so-called.

674
01:23:28,293 --> 01:23:36,153
As terminal is still last residual. But the eternal external is still nice.

675
01:23:36,153 --> 01:23:40,833
Residual is calculated very similar to the internal as soon as racial.

676
01:23:41,163 --> 01:23:44,463
The difference here is that you use this.

677
01:23:44,823 --> 01:23:51,103
See my hat? This notation means that this is an escalator.

678
01:23:52,483 --> 01:23:59,413
Well, here I mean, this this guy, this the square of this is as major of that seamless square.

679
01:23:59,923 --> 01:24:06,373
So this is an escalator of seamless square with the eyes individual deleted.

680
01:24:07,213 --> 01:24:11,473
So in other words, this single up, you know,

681
01:24:11,473 --> 01:24:19,962
minus I this how to square this is you have a data set you have so you have a data set

682
01:24:19,963 --> 01:24:23,713
and value from their data so you can feel the linear regression model in the calculator.

683
01:24:23,713 --> 01:24:32,943
See my square to this sigma minus i squared that is from adidas dataset you remove subnet i.

684
01:24:34,373 --> 01:24:37,503
First Rule 79 ever removing.

685
01:24:37,883 --> 01:24:46,703
Now you have a new dataset set and then for the newly does that you fit in your right model or do whatever and add that based on what you calculate.

686
01:24:46,763 --> 01:24:52,073
Sigma maximize with sigma how to square that is what this single minus.

687
01:24:52,073 --> 01:25:00,923
I had a sigma for it. That's that's the estimate of Sigma Square where you remove or delete individual.

688
01:25:01,913 --> 01:25:12,423
Okay. So the rationale behind doing this is that we call that of.

689
01:25:15,013 --> 01:25:18,463
Absolutely. I had. This is the residual.

690
01:25:19,393 --> 01:25:28,422
This was all I had is sort of a matter of how far this particular individual I is from the rest

691
01:25:28,423 --> 01:25:35,623
of the individual or how far this this this predated wife or this particular individual is from.

692
01:25:35,623 --> 01:25:39,643
It's actually observed that way.

693
01:25:40,183 --> 01:25:56,883
Now, if this is large. So maybe I can just let me give you a look at what the results.

694
01:26:07,183 --> 01:26:10,793
Let's say these are most of the individuals, right?

695
01:26:11,353 --> 01:26:17,263
But let's say there is a guy with high value over here.

696
01:26:17,953 --> 01:26:26,903
This is X. This is what I saying. So there is one individual with Y values very, very large.

697
01:26:28,343 --> 01:26:41,033
Now, this episode I had for this particular individual Y that it should be fairly large because because if you have a linear regression model,

698
01:26:41,033 --> 01:26:43,223
well, this maybe this is the linear regression model.

699
01:26:44,993 --> 01:26:52,303
But again, the, the residual for this guy, the, the the Y had before it was gone that that is the value on this line.

700
01:26:52,403 --> 01:26:57,473
This Y have actually observed about events over here the way above.

701
01:26:58,013 --> 01:27:05,363
So the residual and this this is absent. I had this is very large for this individual.

702
01:27:09,163 --> 01:27:19,523
However, if you you if you calculate the Sigma Square hat based on all the individuals now Sigma Square hat is also very large,

703
01:27:19,753 --> 01:27:23,323
you know, because of this large epsilon.

704
01:27:24,883 --> 01:27:35,803
So C must work out. It will also be fairly large. So in that case, if you divide and if you look if we look at the internal Sudanese residual now,

705
01:27:35,803 --> 01:27:44,953
because this is very large for this individual and also this is fairly large overall is very large because of that individual.

706
01:27:45,343 --> 01:27:49,093
Now, when you have the ratio of the ratio,

707
01:27:49,093 --> 01:28:00,763
we cannot tell whether the residual will crack in the flat matter or a distance of this one corresponding to the other wise.

708
01:28:01,663 --> 01:28:15,613
So however, if we. Synchronize this Epsilon hat using the Sigma hat from all the other individuals, not including this individual.

709
01:28:15,913 --> 01:28:22,933
If you look at all the other individuals there are ys are fairly concentrated right within this region.

710
01:28:23,863 --> 01:28:29,653
So so now the corresponding you see my hat. Without this I are the individual.

711
01:28:29,923 --> 01:28:33,343
This should be, I mean, relatively normal, right?

712
01:28:33,393 --> 01:28:37,603
Not not not very not not particularly affected by this particular individual.

713
01:28:38,083 --> 01:28:42,583
So then by dividing this absolute and by this.

714
01:28:42,823 --> 01:28:45,673
Absolutely. By see my hat from all the other individuals.

715
01:28:45,793 --> 01:28:54,583
After removing the individual, it's easier to detect whether this particular y is very far away from the rest of the subjects.

716
01:28:55,633 --> 01:29:01,153
That's the rationale of of looking at this so-called action.

717
01:29:01,163 --> 01:29:08,982
As soon as residuals divided you to buy an estimate of Sigma hat after removing this particular individual.

718
01:29:08,983 --> 01:29:13,303
So it makes the assessment of each particular individual easier.

719
01:29:13,693 --> 01:29:19,633
Well, a lot easier just to kind of find out which ones are right like you.

720
01:29:20,713 --> 01:29:23,833
So let's look at the rationale behind that.

721
01:29:24,743 --> 01:29:32,903
So now was it kind of remove influence of this absolute ahead of estimating sigma squared?

722
01:29:33,673 --> 01:29:35,293
So by removing that data point,

723
01:29:36,373 --> 01:29:44,113
so make it makes the assessment of the whether that particular or something that is outlier making that assessment easier.

724
01:29:45,673 --> 01:29:46,033
Okay.

725
01:29:46,033 --> 01:30:00,433
So this individual was are this residual and also follows it has a mean zero and also there is even a positive one and it follows a tedious usual.

726
01:30:00,463 --> 01:30:04,963
Now, with this in mind, two minus one were freedom.

727
01:30:08,203 --> 01:30:12,313
And this is the most accessible residual for deep hacking outliers.

728
01:30:12,763 --> 01:30:19,933
So if you have an outlier and this one is the most sensitive, so actually sometimes residual,

729
01:30:19,933 --> 01:30:27,133
this is most sensible residual to detect such an outlier or precisely for the reason we just we just talked about.

730
01:30:27,133 --> 01:30:36,793
Right. So because it looks at the ratio of epsilon hash to see my had estimated that by removing that particular individual.

731
01:30:37,063 --> 01:30:41,953
So this makes me because it's quite a sensitive tool to detect detect requires.

732
01:30:44,693 --> 01:30:48,983
And also this is known as Leave one Out or Jackknife Residuals.

733
01:30:51,153 --> 01:30:59,223
So now we have Ford have over residuals and so we have absolute hat.

734
01:30:59,373 --> 01:31:09,633
This is the conversion residuals where this is the raw residuals and we have the I had this is not centralized residual we have

735
01:31:09,633 --> 01:31:17,373
our I had this is the internal is still a nice residual and we have our minus I have this is as from a student is residual.

736
01:31:17,733 --> 01:31:22,292
So the latter tool they are called a student biased residual now student nice

737
01:31:22,293 --> 01:31:27,993
residual these the reason is that because they follow they follow to student

738
01:31:28,003 --> 01:31:31,743
he's also referred to as the student disclosure I do school results referred

739
01:31:31,743 --> 01:31:39,722
to us as a student and so so that's why it's called a student as a residuals.

740
01:31:39,723 --> 01:31:48,753
But oftentimes people simply just refer to all these all these latter three as, you know,

741
01:31:48,783 --> 01:31:54,573
as centralized residuals as they're just based on different generalizations of,

742
01:31:55,683 --> 01:32:05,913
okay, so these residuals now for a lower resembles if your sample is not very small for our samples, they tend to provide a similar information.

743
01:32:06,483 --> 01:32:12,422
So for example, if you're looking for some patterns, then the pattern is should be preserved.

744
01:32:12,423 --> 01:32:18,033
I mean, doesn't matter which residual you are looking at, if one residual has a particular pattern,

745
01:32:18,033 --> 01:32:23,643
very clear pattern of using the other residuals, they should have the same pattern.

746
01:32:25,813 --> 01:32:31,093
Now for for lager for large samples, the latter three residuals.

747
01:32:31,633 --> 01:32:36,283
And in these three these residuals they follow standard normal distribution.

748
01:32:37,663 --> 01:32:46,543
This is this is the reason that makes the latter three more desirable or more widely applied than the first.

749
01:32:46,633 --> 01:32:53,623
Even the commercial amount for the raw residuals because again, the raw residual, it depends on the unit of line.

750
01:32:54,313 --> 01:33:00,913
So you can make it the values of which are too small or large by changing a unit of one so that the

751
01:33:00,913 --> 01:33:07,063
magnitude of absolute itself is almost meaningless because you can always change value chain magnitude.

752
01:33:07,603 --> 01:33:17,563
But the latter three are residuals because they are union this, they do not have unit, so their magnitude make more sense.

753
01:33:17,953 --> 01:33:23,743
So you can compare them to normal distribution and see whether they roughly follow standard normal.

754
01:33:24,283 --> 01:33:29,293
And if they do not, then there is something wrong in the assumptions.

755
01:33:31,243 --> 01:33:34,813
So and here this word used as a measure. These are.

756
01:33:37,083 --> 01:33:41,883
From top to bottom. This is in decreasing order of sensitivity.

757
01:33:43,583 --> 01:33:49,883
So in other words, you know this. ABDUL-AHAD As we mentioned, this is the least sensitive in detecting outliers.

758
01:33:49,913 --> 01:33:52,433
The reason, again, is because it has a unit.

759
01:33:52,973 --> 01:34:04,613
So it's you can make the values arbitrarily and arbitrarily small by changing the unit of why of an event this R had minus I.

760
01:34:04,623 --> 01:34:15,413
This is most sensitive what it did having outliers, which again because when we calculate when we standardize it the the centralization,

761
01:34:16,193 --> 01:34:20,213
the C my hat is actually after removing that about a regular individual.

762
01:34:22,553 --> 01:34:27,713
So yeah, this, this is these four type of individuals.

763
01:34:30,783 --> 01:34:35,253
Okay. Any questions about this in terms of residuals?

764
01:34:43,383 --> 01:34:48,953
Okay. And then let's look at how to check these assumptions one by one.

765
01:34:48,963 --> 01:34:52,953
So first we're going to look at linearity. So how do check linearity?

766
01:34:53,943 --> 01:34:56,043
When we say we check your narrative menu,

767
01:34:56,043 --> 01:35:04,862
what we mean is to check the functional form of X and whether you should include an X in a self for entry to self,

768
01:35:04,863 --> 01:35:10,263
or you should also include N Square, or you should include other functional form of age.

769
01:35:10,533 --> 01:35:18,333
For example, let's say for example, for if we look at SBP and h dependance on SBP of H.

770
01:35:25,143 --> 01:35:39,852
Now. If most of the dependencies look like such a dependance and this is not definite nonlinear.

771
01:35:39,853 --> 01:35:46,123
So as you know for for young kids. For for for kids.

772
01:35:46,273 --> 01:35:51,523
So as they as they grow, probably their SBP increase, as they as they grow,

773
01:35:53,473 --> 01:36:00,853
but then as they become adults so that the SBP stayed roughly flat for for a number of years.

774
01:36:01,243 --> 01:36:09,433
And then as people age, you know, as they grow older, so then probably their SBP start to increase again as they grow older.

775
01:36:09,733 --> 01:36:16,872
So overall, I mean here until you things are a little bit dramatic so that the true association may not be this dramatic,

776
01:36:16,873 --> 01:36:23,983
but but it would seem that, you know, there may be the association may not be strictly linear and may not be exactly linear.

777
01:36:25,423 --> 01:36:36,433
But how do we detect this? How do we how do we find about the crowd, first of all, for age to be included in the model?

778
01:36:37,813 --> 01:36:43,113
And this can be can be. Well, we have different approaches here.

779
01:36:43,183 --> 01:36:47,143
We have we will talk about three approaches. One is part of regression plots.

780
01:36:47,653 --> 01:36:51,493
The other ones categorize and predictor. Another one is testing the smoothing.

781
01:36:51,733 --> 01:36:57,882
Let's look at this one by one. So well.

782
01:36:57,883 --> 01:37:03,033
Our approach is we can make a residual versus covariance plot.

783
01:37:03,343 --> 01:37:06,603
Mm hmm. So this is the y use of the residuals.

784
01:37:06,933 --> 01:37:10,923
So after we get the residuals, we can make a plot of the residuals.

785
01:37:12,603 --> 01:37:19,193
So we have here we have residuals versus X.

786
01:37:19,503 --> 01:37:23,283
We may start to plot. This is one type of residual plot.

787
01:37:24,063 --> 01:37:34,443
And this sort of residual plot unit is very quite effective in revealing what what function before X should be in the model.

788
01:37:35,703 --> 01:37:48,263
So in simple linear regression, in similar linear regression, we only have one x, then we can simply make a plot of ad head versus x or versus y hat.

789
01:37:48,633 --> 01:37:55,923
But typically, I mean, we can plot and versus X to assess linearity of between X and y relationship.

790
01:37:56,673 --> 01:38:01,443
The reason that we can do this is imagine that we have two models.

791
01:38:17,203 --> 01:38:20,203
Right to order, let's say, in need of a true model.

792
01:38:20,383 --> 01:38:25,243
Well, I think the second is that the model. That's a literal model indeed.

793
01:38:25,273 --> 01:38:30,763
While the Y depends on X squared. But let's say somehow.

794
01:38:32,113 --> 01:38:36,943
You build a single linear regression model without including the square term.

795
01:38:37,723 --> 01:38:49,423
And in this case, essentially what this means is that you observe, you let this beta two square to be observed by this absolute term.

796
01:38:50,963 --> 01:38:54,673
You know, first of all, because you didn't include an x squared.

797
01:38:54,683 --> 01:38:59,793
So this x squared term entry got observed by this abstract.

798
01:39:01,423 --> 01:39:12,793
And then when you calculate absolute hat, now the absolute hat actually has a contracted function of X has a has that part in absolute.

799
01:39:13,363 --> 01:39:18,283
So that when you make a plot abstract versus X.

800
01:39:21,043 --> 01:39:24,643
Then you will see a quadratic pattern.

801
01:39:26,513 --> 01:39:36,913
You will see a pattern. That's because this quadratic term in the true model, which was not included in Erdemir,

802
01:39:36,953 --> 01:39:41,753
a similar linear regression model observed by your absolute term,

803
01:39:42,653 --> 01:39:53,153
and we're gonna encounter that additional had we make a plot versus X, now you will see that while there there is a fanatic climatic pattern.

804
01:39:53,753 --> 01:40:02,033
So Y as you see that this is a clear indication that you know, this linear, a single linear regression model is not good enough.

805
01:40:02,033 --> 01:40:08,303
We should also include a quadratic function already, a function of X in the model.

806
01:40:10,493 --> 01:40:13,763
So this is similar here. This is fairly easy.

807
01:40:14,363 --> 01:40:18,143
So again, for single linear regression of your rates,

808
01:40:18,143 --> 01:40:30,863
it's good enough if you calculate an absolute hat and to make a plot that was Y versus X and and if you see very clear pattern,

809
01:40:31,463 --> 01:40:39,793
very clear dependance of absolute hat on, that's that's a clear indication that you should consider other functional form of X in the models.

810
01:40:40,583 --> 01:40:45,033
Now if. The moral assumption is very good.

811
01:40:46,593 --> 01:40:49,983
Then the absent hedge should have no clear pattern.

812
01:40:50,463 --> 01:40:54,453
So it should be sort of randomly disputed and there shouldn't be any clear pattern.

813
01:40:54,463 --> 01:41:06,843
So for example, if there is far a good model, if you make a plot of absolute hat versus X, then you will see that this abstract should.

814
01:41:11,573 --> 01:41:19,453
You know, should be randomly should be randomly disputed distributed and because was had has has been zero reprisal has been zero.

815
01:41:19,463 --> 01:41:23,783
So it should be randomly distributed around, you know, absolute zero.

816
01:41:25,343 --> 01:41:26,803
So there shouldn't be any clear pattern.

817
01:41:26,813 --> 01:41:34,883
So if you see if you do not see any clear pattern, if you absolutely have a better than a disputed that that's an indication.

818
01:41:36,653 --> 01:41:40,253
That's a good indication. Indicating that in your model is is a good model.

819
01:41:42,433 --> 01:41:48,012
Okay. So that is for linear, simple linear regression, not for multiple linear regression.

820
01:41:48,013 --> 01:41:56,833
Things become more complex. Now, let's first of all, let's just look at the complexity, what a complexity is, and then we will start.

821
01:41:58,273 --> 01:42:01,672
So for Margot, linear rational is more complicated.

822
01:42:01,673 --> 01:42:12,343
And the reason is that if you simply make a plot of absolute bad versus opposite at scale, then the plot,

823
01:42:13,053 --> 01:42:21,102
the the pattern may be impacted by other covariates because for multiple regression you have a different covariate,

824
01:42:21,103 --> 01:42:28,813
you have to have multiple covariance and this multiple covariance URI And there is correlation among there are correlated.

825
01:42:30,133 --> 01:42:35,373
Now, for example, if you include both height and weight, Ramona, you're losing height in a weight.

826
01:42:35,383 --> 01:42:41,933
They are correlated. And then let's say education is correlated with at stake.

827
01:42:43,013 --> 01:42:48,943
And let's say that SGA has nonlinear association with white with white.

828
01:42:49,523 --> 01:42:55,823
And then when you make a plot of actual hand versus x k.

829
01:42:57,993 --> 01:43:01,263
Not likely that you will see a nonlinear pattern.

830
01:43:01,593 --> 01:43:11,793
But this nonlinear pattern is actually partly because of the because of the coordinator to it, to a clear narrative between,

831
01:43:12,093 --> 01:43:19,233
you know, between extreme and cutting and also because y depends on acting in a nonlinear fashion.

832
01:43:20,553 --> 01:43:29,583
So this solidarity among this multiple covariance really complicates things for model for multiple linear regression.

833
01:43:30,723 --> 01:43:33,243
So in other words, for multiple linear regression,

834
01:43:33,243 --> 01:43:43,413
usually a simple plot of absolute hand versus x cake may not be so effective in revealing what are the true pattern is.

835
01:43:43,893 --> 01:43:50,583
So we need to rely on a slightly more complex solution to reveal the majority owners.

836
01:43:51,123 --> 01:43:55,713
So but we will talk about that in our next slide of some.

837
01:43:55,803 --> 01:44:02,733
Q By the way.

838
01:44:02,733 --> 01:44:09,723
So again, the coverage for next week's midterm is going to have to be sound because now.

