1
00:00:02,150 --> 00:00:07,010
So unfortunately we're still on the high zone, so I'm looking forward to when that changes.

2
00:00:07,010 --> 00:00:12,680
But for now, we're still in this high CDC community risk level, so I've got my mask on.

3
00:00:16,770 --> 00:00:20,010
Quick check in on where we are in the course. So we have.

4
00:00:24,090 --> 00:00:28,260
We're here. So we're. We're going to be finishing up hand out five today.

5
00:00:29,830 --> 00:00:34,270
And we also have a homework to do, but it's still a while away.

6
00:00:34,270 --> 00:00:40,960
So if we go over to the assignments to have homework to is due October the second.

7
00:00:41,650 --> 00:00:50,860
And if you follow the announcements of the canvas site, you saw that I changed the second homework problem just a bit.

8
00:00:52,240 --> 00:01:02,880
As I was writing up the solutions, I noticed that it was a bit tricky and so I decided to write it so that you would be guided through it.

9
00:01:02,920 --> 00:01:08,379
The problem a little bit better. So if you've already started the problem, you're fine.

10
00:01:08,380 --> 00:01:13,750
All the things you've done are still part of the problem, so there's no wasted work.

11
00:01:14,620 --> 00:01:21,940
But I wanted you to think very carefully about, you know, what is the RC curve showing you?

12
00:01:22,630 --> 00:01:31,300
And in particular, what definition of the threshold are they basing their sensitivity and specificity on?

13
00:01:31,300 --> 00:01:42,910
And so in problem one, I use the threshold where the home drop variable is less than or equal to ten and defining home FEV1 progression.

14
00:01:43,540 --> 00:01:48,429
But I also want you to consider the other direction of the threshold.

15
00:01:48,430 --> 00:01:53,680
So you could also use a threshold where home drop is greater than minus ten.

16
00:01:53,680 --> 00:02:00,430
So kind of the flip A part C and fill in this table and think about sensitivity and specificity.

17
00:02:00,430 --> 00:02:05,160
And so I want you to think which I want you to be able to tell, you know,

18
00:02:05,230 --> 00:02:11,350
which one of those directions of classification is happening in this problem.

19
00:02:12,840 --> 00:02:15,600
And so this extra step is to help you go through that.

20
00:02:16,140 --> 00:02:25,970
I will say that as I was writing up solutions, I found it to be my favorite package for this problem because our has these nice threshold plots,

21
00:02:25,980 --> 00:02:32,310
you don't have to do algebra to figure out the thresholds and the corresponding sensitivity specificity.

22
00:02:32,790 --> 00:02:42,300
And for me, figuring out. Party was especially useful after looking at a threshold block.

23
00:02:43,410 --> 00:02:47,550
You can do it and says it's just a little bit more work to figure things out.

24
00:02:47,730 --> 00:02:56,160
In my opinion. So that's my if you are kind of on the fence about brushing up on or learning or for the first time,

25
00:02:56,310 --> 00:03:03,600
this might be a pitch that at least for this problem, I found it easier to understand what was going on through the ah.

26
00:03:05,210 --> 00:03:13,750
All right. Next week's lab, just as a preview, is going to help you with the third homework problem.

27
00:03:15,110 --> 00:03:19,519
By the end of today's class, you will know how to do it based on my lecture.

28
00:03:19,520 --> 00:03:23,780
But next week's lab is also going to cover material related to problem three.

29
00:03:25,240 --> 00:03:30,320
And so I think this week's lab is kind of relevant for problems one and two.

30
00:03:31,510 --> 00:03:37,050
Next week's lab is going to be relevant to problem three. Okay.

31
00:03:40,300 --> 00:03:44,800
So. Let's get to work.

32
00:03:45,700 --> 00:03:51,190
Actually, are there any more administrative kinds of questions before I start the handout back up?

33
00:03:55,210 --> 00:04:01,180
I think that I'm going to go ahead and start. So we were around 556.

34
00:04:05,850 --> 00:04:15,880
And if you recall, we were looking at an example where the outcome was yes, no for coronary heart disease or cardio, it was coronary heart disease.

35
00:04:16,560 --> 00:04:19,110
And the predictor that we were looking at was age.

36
00:04:19,110 --> 00:04:27,479
And we kind of worked up from very, very simple models to continue with age and where we left off with know stretching beyond that.

37
00:04:27,480 --> 00:04:34,050
So what other choices are there for modeling a predictor that's continuous like age?

38
00:04:34,800 --> 00:04:41,670
And so we're going to review things you've seen before, but I bet you haven't worked too much with linear spline terms.

39
00:04:41,670 --> 00:04:43,590
And so we're going to work that in now.

40
00:04:44,160 --> 00:04:51,060
Well, we're in a familiar territory with logistic regression so that when we need it later in the course you'll have this skill.

41
00:04:51,960 --> 00:05:00,150
And so this is the model that we looked at earlier where we modeled age as a continuous variable.

42
00:05:03,070 --> 00:05:09,300
Oh, this should be loud. Did anybody catch that typo? This should be logit or the.

43
00:05:09,310 --> 00:05:12,520
Oh, no, this is fine. It says odds in the middle. Log out. That's fine.

44
00:05:15,570 --> 00:05:20,280
And so alternative forms of the predictor should also be explored to see if the model fit can be improved.

45
00:05:20,280 --> 00:05:24,450
We don't want to miss an important nonlinear relationship, you know,

46
00:05:24,450 --> 00:05:32,370
that we that this assumes and we don't want to misrepresent patterns that behave differently near extremes.

47
00:05:33,600 --> 00:05:39,690
So finding an appropriate relationship between the law guides in the future can improve the research quality and applicability.

48
00:05:39,690 --> 00:05:49,559
So we always want to get an appropriate model here. So here are some things that one should consider when you have a continuous variable categorical

49
00:05:49,560 --> 00:05:58,680
versions of the predictor quadratic or even higher order terms like cubed and so on as squared,

50
00:05:58,680 --> 00:06:04,200
cubed and so on. And one of my favorites is linear spline versions of the Predictor.

51
00:06:04,200 --> 00:06:08,370
So I'm going to show you what those are, how to how to create them and how to interpret them.

52
00:06:12,630 --> 00:06:17,580
And it's useful to know when comparisons between models can rely on good ratio tests.

53
00:06:18,390 --> 00:06:24,480
So that's you can really compare nested predictors only and sometimes it's obvious and sometimes not.

54
00:06:24,480 --> 00:06:34,340
So I'm going to try to help you with that link. Or if you need to use a kick is information criteria the AIC says are output.

55
00:06:35,230 --> 00:06:40,120
And for that you can pretty much apply it as long as you're fitting it to the same

56
00:06:40,120 --> 00:06:44,070
data set with the same model non-listed predictors or nested that they're both okay.

57
00:06:44,080 --> 00:06:45,550
You can always look at AIC.

58
00:06:46,390 --> 00:06:53,110
The main advantage of elected ratio tests, of course, is that you get a p value for your papers when you're making decisions.

59
00:06:53,860 --> 00:06:59,680
But if you're choosing models based on, you know, a smaller AIC being better, you don't get a p value.

60
00:06:59,980 --> 00:07:03,550
That's the main advantage of choosing between them.

61
00:07:05,920 --> 00:07:08,440
So this part is review. I bet you've seen this before.

62
00:07:08,450 --> 00:07:16,209
So categorical versions of a predictor allow the law grad estimates to vary for each category level at a non-linear way,

63
00:07:16,210 --> 00:07:19,510
but changes within each category are lost.

64
00:07:19,720 --> 00:07:24,910
So for our coronary heart disease example, here's our log odds again.

65
00:07:26,560 --> 00:07:29,980
And what I've done here is model age.

66
00:07:31,500 --> 00:07:35,979
I think we saw this already, actually. We talked about these indicator variables.

67
00:07:35,980 --> 00:07:42,550
So this is the variable that is going to be a one whenever the inside of this parentheses is true.

68
00:07:43,270 --> 00:07:47,440
So this big guy stands for indicator. In my mind it's an indicator variable.

69
00:07:47,590 --> 00:07:52,840
It'll be zero or one one when the condition in the inside of the parentheses is true.

70
00:07:52,870 --> 00:07:57,150
So this variable will be one when the person is in their thirties zero.

71
00:07:57,160 --> 00:08:01,899
Otherwise this variable will be one when they're in their forties zero.

72
00:08:01,900 --> 00:08:05,770
Otherwise this variable will be one when they're in their fifties zero.

73
00:08:05,770 --> 00:08:11,350
Otherwise. And this one will be one when they're older than 60.

74
00:08:12,540 --> 00:08:18,059
Okay. So here is the categorical version of this model.

75
00:08:18,060 --> 00:08:27,090
But you've lost information about any changes in the odds of heart disease for someone in this age range of 30,

76
00:08:27,450 --> 00:08:32,939
they'll all be modeled the same or they'll have assumed had the same log odds of

77
00:08:32,940 --> 00:08:36,509
coronary heart disease similarly for someone in their forties and fifties and so on.

78
00:08:36,510 --> 00:08:41,160
And so if you had this creeping up cardiovascular risk, that really changes every year.

79
00:08:41,730 --> 00:08:47,490
You're losing that with this model. So you're kind of throwing away some statistical money to tell this story.

80
00:08:50,430 --> 00:08:53,840
The log odds of the coronary heart disease.

81
00:08:53,850 --> 00:08:57,870
It does change every time you hit a new decade with this model.

82
00:08:58,770 --> 00:09:04,530
But the longer odds of coronary heart disease associated with age 35 is the same as for age 30.

83
00:09:05,160 --> 00:09:07,710
You can't differentiate between the two using this model.

84
00:09:09,210 --> 00:09:17,990
And so here is kind of a plot of the fitted log odds of coronary heart disease for continuous versus categorical age.

85
00:09:18,000 --> 00:09:21,180
So the continuous part is the line.

86
00:09:21,810 --> 00:09:29,680
So let's just look at the axes here. So this is age in years is the horizontal axis and the log odds is the vertical axis.

87
00:09:29,680 --> 00:09:33,270
So the when you put age, it is a continuous variable.

88
00:09:33,270 --> 00:09:39,870
Your log odds is increasing and it's changing, getting higher and higher for every year.

89
00:09:39,870 --> 00:09:46,709
Increase of age and for the categorical version, you have a lot of ties depending on what decade they're in,

90
00:09:46,710 --> 00:09:52,650
and then there's a jump to the next level for the next decade. So there's a lot of overlap here.

91
00:09:53,730 --> 00:10:02,129
I'm not sure how well we're fitting this highest age group, but most everybody else is right, you know, on the on the continuous regression line.

92
00:10:02,130 --> 00:10:07,160
So if I was looking at this data, I would think probably the linear assumption is okay.

93
00:10:07,170 --> 00:10:14,070
My only question is, you know, at the extremes is there could we do a bit better?

94
00:10:16,560 --> 00:10:29,290
And. We already looked at this using AIC values and we did find that continuous edge did have a better fit than in categorical representation.

95
00:10:30,040 --> 00:10:36,939
But can we fit the highest age groups in the lowest a bit better by maybe allowing the

96
00:10:36,940 --> 00:10:43,839
log odds to have a bend in the middle so you can sort of instead of a straight line,

97
00:10:43,840 --> 00:10:47,230
can you bend it a little bit? So we still fit the data in the middle. Okay.

98
00:10:47,950 --> 00:10:57,379
But we get a little closer at either end. Sorry.

99
00:10:57,380 --> 00:11:03,030
I forgot to set my phone on. Do not disturb.

100
00:11:03,600 --> 00:11:09,770
Just turn the volume down. See if that works. Okay.

101
00:11:09,770 --> 00:11:14,090
Good. Let's do it. Do not disturb. All right.

102
00:11:14,100 --> 00:11:22,739
So just to reiterate, we're going to look at now linear spline forms of age that allow a bend in that otherwise linear relationship.

103
00:11:22,740 --> 00:11:28,470
So we had that already. The linear relationship here was that straight line on the plot.

104
00:11:29,630 --> 00:11:36,870
And so what we're going to do is create a new covariate. That will allow a bend in that line.

105
00:11:36,870 --> 00:11:40,859
And I'm just going to arbitrarily say I want the bend to happen at age 50.

106
00:11:40,860 --> 00:11:45,600
So I did no model building to fit like this. I just am giving this as an example.

107
00:11:47,070 --> 00:11:51,899
And so to allow a bend in that line at 50, I create this variable.

108
00:11:51,900 --> 00:11:59,100
So what is this variable? I'm going to call it age 59, and it's multiplying two things here.

109
00:11:59,160 --> 00:12:06,570
So the first is just whatever the person's age is, -50, this is always the place where you want the bend to happen.

110
00:12:07,410 --> 00:12:10,160
And then you multiply that by the zero one variable.

111
00:12:10,170 --> 00:12:18,120
See the I mean, this is going to be a zero or one and it's going to be a one when age is greater than or equal to 50.

112
00:12:19,740 --> 00:12:24,000
So I mean, just thinking, giving ourself another minute to unpack that.

113
00:12:24,870 --> 00:12:34,740
For someone who is less than 50 years old, this cover will end up being a zero because this part will be a zero.

114
00:12:36,690 --> 00:12:39,960
For someone who is greater than or equal to 50 years old.

115
00:12:40,970 --> 00:12:44,900
This variable will be that age -50 times of one.

116
00:12:47,290 --> 00:12:54,700
Okay. So even though I've got a variable name here, it helps to keep track of where it came from with the algebra,

117
00:12:55,180 --> 00:12:57,570
because they're going to be times when we want to know,

118
00:12:58,830 --> 00:13:07,959
you know, how to calculate, say, odds ratios that involve this algebra here and we don't want to mask it accidentally.

119
00:13:07,960 --> 00:13:10,990
We are just thinking of the variable name and the variable name.

120
00:13:12,980 --> 00:13:21,379
Okay. So we'll see examples of that. But I what you kind of need to remember this algebra when you see a term that's a slime so that you can like,

121
00:13:21,380 --> 00:13:24,530
for instance, see what changes with a year increase in age.

122
00:13:24,530 --> 00:13:31,650
It affects this term and it affects this term. And then the resulting change in age changes this.

123
00:13:35,380 --> 00:13:42,520
All right. So here's how you fit the model with that X determinant that allows the the bend in the line.

124
00:13:43,950 --> 00:13:50,069
And so it gives different slopes before and after age 50.

125
00:13:50,070 --> 00:13:53,340
Essentially, that's the band. So up to age 50.

126
00:13:54,880 --> 00:14:04,630
You know, this indicator is a zero. And so we still this whole term here is a zero, this age 50 split in terms of zero up to age 50.

127
00:14:04,840 --> 00:14:11,050
And so you still have this data not plus two, one age that's describing what's happening up to 50.

128
00:14:13,310 --> 00:14:20,480
And and so it'll estimate the best speed of one that does that up to age 50.

129
00:14:20,480 --> 00:14:24,020
And then after age 50, we have to look at this again.

130
00:14:25,130 --> 00:14:30,850
And after age 50, we have whatever the age is, -50 times a one.

131
00:14:30,860 --> 00:14:38,030
This is just a one when they're older than 50. So the model is better, not plus better one age plus beta two.

132
00:14:38,030 --> 00:14:41,690
And then just times, age -50, the same age that's here.

133
00:14:42,670 --> 00:14:48,310
And then I've just kind of there's a multiple times one for this indicator, but I've just sort of submerged that.

134
00:14:50,360 --> 00:14:57,829
So you really still do have a single line. You just have to kind of figure out what's the slope of the new line and what's the intercept.

135
00:14:57,830 --> 00:15:05,809
So everything that's multiplied by age is going to be the the trajectory after 50, the slope after 50.

136
00:15:05,810 --> 00:15:09,049
So that's going to be beta one plus beta two.

137
00:15:09,050 --> 00:15:12,230
There's a beta two times H here. So this is the new slope.

138
00:15:13,620 --> 00:15:22,739
In that line from the graph after you hit age 50, and then everything that's not multiplied by age ends up being a new kind of intercept.

139
00:15:22,740 --> 00:15:29,580
So we had better not over here. And then we have this beta two -50 from over here.

140
00:15:29,940 --> 00:15:41,700
And so. Then beta one plus the two is the slope you the change per year increase once you are after age 50.

141
00:15:43,320 --> 00:15:49,649
And this is the intercept of the whatever the line is that goes back all the way to zero.

142
00:15:49,650 --> 00:15:55,410
But we never interpret this. This is always with age after 50.

143
00:15:55,410 --> 00:16:01,590
We only interpret this part for age after 50. And so in age, being zero isn't really a story.

144
00:16:02,800 --> 00:16:07,870
For that line. It's just sort of every line has a has a, you know, mythical place.

145
00:16:07,870 --> 00:16:11,830
It would hit zero if you stretched it back to zero. And that's what this is.

146
00:16:15,150 --> 00:16:22,110
All right. And of course, we always know you never interpret models outside the range of the observed data.

147
00:16:22,680 --> 00:16:24,629
So arguably,

148
00:16:24,630 --> 00:16:33,060
this data not doesn't mean anything either because we don't have anybody that's age zero in our data set that would have a legacy odds data not.

149
00:16:37,280 --> 00:16:43,440
Okay. So let's go ahead and see how this works software wise.

150
00:16:44,130 --> 00:16:48,720
So here's source code for putting in the data set.

151
00:16:48,720 --> 00:16:52,320
Again, this data sits on canvas. If you want to play with it, here are variables.

152
00:16:53,410 --> 00:17:00,640
And then here is how to create the variable in SAS. So we need to create this indicator variable.

153
00:17:00,650 --> 00:17:06,309
So this variable is going to be a one whenever age is greater than or equal to 50.

154
00:17:06,310 --> 00:17:10,120
That's what the T stands for, greater than or equal to 50 and zero otherwise.

155
00:17:12,060 --> 00:17:18,870
And then the linear spline term that allows the bend at 50 is going to take that same indicator variable.

156
00:17:21,100 --> 00:17:27,170
And multiply it times the age -50.

157
00:17:27,190 --> 00:17:34,500
So this is going to be an actual number here. All right.

158
00:17:35,400 --> 00:17:38,760
So I can get you know, if you're kind of if you're not perked up,

159
00:17:38,760 --> 00:17:43,710
it's kind of you just need to pay attention inside of parentheses when it's a logical expression like greater.

160
00:17:43,800 --> 00:17:48,540
Is this greater than that? Yes. No, that's going to turn into a binary.

161
00:17:49,200 --> 00:17:53,909
But here I'm just using order of operations for parentheses with no logical term in the middle.

162
00:17:53,910 --> 00:18:01,260
This is actually going to be a number in the middle. So just, you know, make sure you're not confusing the two.

163
00:18:03,510 --> 00:18:06,820
All right. So this is the variable now that will put in the model.

164
00:18:06,870 --> 00:18:11,250
So here's the logistic model with age and that spline term.

165
00:18:13,000 --> 00:18:17,860
And. Here is the output. For that from Seth.

166
00:18:17,860 --> 00:18:21,100
We'll see it in a minute. It'll be the same output, just different coke.

167
00:18:22,060 --> 00:18:26,080
And so, you know, you get odds ratio estimates for these.

168
00:18:27,940 --> 00:18:33,999
But you're going to need to be careful because remember when we looked at the log odds for someone less than 50,

169
00:18:34,000 --> 00:18:44,080
it really only involved this parameter here. So the odds ratio for a one year increase in age for someone less than 50 is going to be this 1.109.

170
00:18:45,910 --> 00:18:54,490
SAS will give you E to the .0239 and put it here, but we don't have a meaningful interpretation of that on its own.

171
00:18:55,090 --> 00:19:01,260
The law guides for someone greater than 50 had a slope that involved both the beta one and the beta two.

172
00:19:01,330 --> 00:19:07,120
Right. So we're going to have to figure out how to estimate that, what we can do.

173
00:19:07,120 --> 00:19:11,620
But the point estimates is if we can do that by hand using the formula I just gave you,

174
00:19:11,620 --> 00:19:18,310
but we're going to have to figure out and remember how to do contrast statements to get confidence intervals for this,

175
00:19:18,880 --> 00:19:23,950
you know, odds ratio for someone greater than 50 when they increase by a year.

176
00:19:25,370 --> 00:19:30,930
So don't when you've got these blind terms, don't use that second line about it.

177
00:19:30,980 --> 00:19:35,630
It doesn't mean anything that's clinically meaningful.

178
00:19:37,330 --> 00:19:44,890
So for less than 50, it's still the way it always was because it only involved that data for age.

179
00:19:45,730 --> 00:19:52,299
So in those age, 50 years or less, the odds ratio for CSG corresponding to one year increase in age is just 1.19.

180
00:19:52,300 --> 00:19:54,940
So you can use this in the confidence limits in the first row,

181
00:19:56,500 --> 00:20:06,910
but the slope for the age greater than 50 involved both the beta for age and the beta for this line term.

182
00:20:07,690 --> 00:20:13,719
And so this is going to be the odds ratio for that age range.

183
00:20:13,720 --> 00:20:15,459
So in those older than 50,

184
00:20:15,460 --> 00:20:24,340
an odds ratio for coronary heart disease corresponding to a one year increase in age is E to the beta one plus beta two or 1.136.

185
00:20:26,190 --> 00:20:29,510
All right. And actually these are pretty close.

186
00:20:29,520 --> 00:20:34,400
So I already have a hint that maybe this blind term won't be that helpful.

187
00:20:34,430 --> 00:20:36,920
Oh, and here's a p value to help with that story, too.

188
00:20:37,940 --> 00:20:46,730
So this is a bit of an academic exercise, seeing how to do this from here, because this blind the band didn't really help fit the data better.

189
00:20:47,630 --> 00:20:54,500
Based on this p value for this blind turn now, let's go ahead and go on with this to see how we get confidence intervals and so on.

190
00:20:55,100 --> 00:20:57,710
So just from the previous page, for those older than 50,

191
00:20:58,430 --> 00:21:05,149
the CAC odds ratio corresponding to one year increase in age was either the one plus beta two with the estimates put here,

192
00:21:05,150 --> 00:21:07,220
and then this is what it ended up being.

193
00:21:08,090 --> 00:21:14,510
And so we can always check this by hand to make sure that the sass output we get for the confidence intervals.

194
00:21:14,510 --> 00:21:19,879
Correct? So that can obtain such estimates and more importantly,

195
00:21:19,880 --> 00:21:26,900
the corresponding confidence intervals for you that we can't do by hand as long as you know this formula for the correct odds ratio.

196
00:21:29,430 --> 00:21:42,239
And so just going through some of this code, again, this is the same code we had a moment ago, but now I'm going to add in a contrast statement.

197
00:21:42,240 --> 00:21:48,420
So every regression procedure in SAS tends to have some ability to do this.

198
00:21:48,420 --> 00:21:54,480
Sometimes they call it a contrast statement, sometimes they call it an estimate statement.

199
00:21:55,140 --> 00:22:00,870
So for logistic, it's a contrast statement, but it's, you know, for whatever reason,

200
00:22:00,870 --> 00:22:04,859
different packages you have to oh, I always have to look up which version it is.

201
00:22:04,860 --> 00:22:10,530
Is it called contrast or estimate? But the syntax is identical or nearly identical.

202
00:22:11,100 --> 00:22:15,660
So I'm putting a label here and single quotes that I want to see.

203
00:22:15,810 --> 00:22:19,530
The odds ratio knows H greater than 50. That's just for my own benefit.

204
00:22:20,430 --> 00:22:26,160
And then here is where you're putting in the formula. So what is what does this mean?

205
00:22:26,190 --> 00:22:31,829
What I'm saying is for each covariate in my model statement,

206
00:22:31,830 --> 00:22:38,010
so I have implicitly the intercept is there and I have age and I have age greater than 50 spline.

207
00:22:38,010 --> 00:22:41,040
So the I have these labels for the cover.

208
00:22:41,040 --> 00:22:44,790
It's in the model in even including this implicit intercept term.

209
00:22:46,190 --> 00:22:50,590
And then the formula that I made involves beta one plus beta two.

210
00:22:50,600 --> 00:22:52,950
It doesn't actually involve beta zero at all.

211
00:22:52,970 --> 00:23:03,680
So for The Intercept, I don't need any beta zero's, but I need one beta one in my formula and I need one beta two in my formula.

212
00:23:04,340 --> 00:23:08,780
And so it'll basically just assume that you want something to do with.

213
00:23:10,040 --> 00:23:14,780
Zero times beta zero plus one times beta one plus one times beta two.

214
00:23:15,290 --> 00:23:19,820
So you can change these numbers if you need different versions of this formula.

215
00:23:24,020 --> 00:23:31,930
And then. There's usually some kind of a slash and some options and ah,

216
00:23:32,340 --> 00:23:40,320
I tend to estimate equals both almost all the time because some packages will automatically give you the formula,

217
00:23:41,280 --> 00:23:44,639
you know, with, with an exponential action done for you without asking.

218
00:23:44,640 --> 00:23:49,750
And some have to explicitly ask for it and I can never remember which is which packaged as it was.

219
00:23:49,750 --> 00:23:55,380
So I just ask for both. So sometimes I might get duplicate output, but at least I know what I have.

220
00:23:56,190 --> 00:24:03,319
So. This is going to make sure I have not just what Beethoven was made it to look like,

221
00:24:03,320 --> 00:24:06,800
but e to the Beethoven perspective too, and what it looks like with confidence intervals.

222
00:24:08,490 --> 00:24:15,790
And so you get something that looks like this. This is the same odds ratio for those greater than 50 worked out by hand.

223
00:24:15,800 --> 00:24:19,780
So this is a double check that we got these numbers right in our contrast statement.

224
00:24:20,890 --> 00:24:25,810
It's always a good idea to double check that. And now I have I have confidence limits as well.

225
00:24:31,530 --> 00:24:35,120
And I kind of already alluded to this earlier.

226
00:24:35,130 --> 00:24:41,810
So how do we assess statistical significance of this boring term? Well, you know, the same way we test for any data.

227
00:24:41,820 --> 00:24:45,870
So the world test is a one line answer.

228
00:24:45,870 --> 00:24:54,269
That's very quick. That's fine. So I would just look at this automated output for that one line and use that especially with a p value this big.

229
00:24:54,270 --> 00:24:57,389
I don't feel the need to test likelihood ratio or anything else.

230
00:24:57,390 --> 00:24:58,800
I just will look at this number.

231
00:24:59,490 --> 00:25:08,910
And so that supply term does not statistically improve the model compared to continuous age, modeled as a linear term by itself.

232
00:25:15,360 --> 00:25:20,550
So I mean, you can still look at the likelihood ratio test. So if you were to do the liquid ratio test.

233
00:25:21,580 --> 00:25:25,330
You would look at incest, you would look at the model fit statistics,

234
00:25:26,050 --> 00:25:32,890
and you're saving this value over here for intercept with covariates for the full model that has this line in it.

235
00:25:34,150 --> 00:25:38,170
And then for the reduced model that doesn't have this blind term in it.

236
00:25:38,170 --> 00:25:41,590
So you're comparing these two numbers and they're almost identical.

237
00:25:42,490 --> 00:25:48,380
So the difference between minus two log L for those two numbers is almost zero.

238
00:25:48,390 --> 00:25:55,810
So that's where the big P value comes from. And so how do you know that this was nested within the full model?

239
00:25:55,840 --> 00:25:58,750
How do you know that a good ratio test was possible?

240
00:25:59,800 --> 00:26:07,750
Well, you know, the idea is, is that for the you should always be able to get to the reduced model from the full model.

241
00:26:08,230 --> 00:26:14,950
And so the reduced models actually a subset of the full models.

242
00:26:14,980 --> 00:26:20,200
So if I knew all the covariates in the full model, I can still figure out all the covariance and the reduced model.

243
00:26:20,200 --> 00:26:28,530
That's a quick way to figure it out. And you can also use the AIC if you want to.

244
00:26:28,830 --> 00:26:34,050
It doesn't give a p value for the manuscript, but you would look at the same model fit statistics.

245
00:26:34,800 --> 00:26:39,480
And when you're looking at AIC, you're always looking for the model that has the smaller AIC.

246
00:26:39,480 --> 00:26:49,230
You choose that one based on that criteria. And so the model, which is linear and alone, again, it has a lower AIC, so you go with that.

247
00:26:57,630 --> 00:27:07,670
Okay. So here's our code. And so, you know, to create this blind term, it's very similar logic to what we did.

248
00:27:07,670 --> 00:27:11,629
And. So inside the parentheses, they have a logical cause.

249
00:27:11,630 --> 00:27:19,400
There's this greater than equal thing. So the data set is CD8, C T age, it's a data frame.

250
00:27:19,400 --> 00:27:23,300
And if I put a dollar sign, I say I want the variable age from that data frame.

251
00:27:24,930 --> 00:27:30,270
And so if that variable is greater than or equal to 50, this variable will get a one.

252
00:27:30,300 --> 00:27:34,740
So I'm actually saving this variable right back into the data frame stage.

253
00:27:36,090 --> 00:27:39,380
And otherwise it'll be a zero. And here's the spline term.

254
00:27:39,390 --> 00:27:45,330
So I'm using this same variable I just put in this CD HD data frame.

255
00:27:46,720 --> 00:27:51,700
Here. And then I'm multiplying it by the actual age.

256
00:27:51,700 --> 00:27:55,210
-50. So this will be a number. How many?

257
00:27:55,660 --> 00:27:58,300
Whatever the age. You know, how many years greater than 50 they were.

258
00:27:59,680 --> 00:28:09,820
So for modeling, I like to save my formula first, just so that I can find very quickly where things and and make my code look a little cleaner.

259
00:28:10,390 --> 00:28:14,410
So here's my model where I'm including the age supply.

260
00:28:15,280 --> 00:28:23,650
So age plus age greater than 50 supply. And then I put it into the GLM where I say, well here's, here's the formula,

261
00:28:24,310 --> 00:28:33,070
here's the data frame where I have all these variables in the formula and I want to have a binomial family with a logit link.

262
00:28:34,900 --> 00:28:42,420
And just like in SAS, if you just want to look at the model fit, you can look at the model fit.

263
00:28:42,430 --> 00:28:47,649
This is going to put out the coefficients and confidence intervals for the parameter estimates.

264
00:28:47,650 --> 00:28:54,160
But we, we had the same problem not being able to interpret this blind parameter on its own really.

265
00:28:54,970 --> 00:28:59,980
So we need to have a contrast statement in our as well. So I use the package mult count.

266
00:29:01,450 --> 00:29:06,950
And here is the kind of the key where you're putting in the formula for your contrast.

267
00:29:06,970 --> 00:29:11,230
So this first part or the C is in this matrix.

268
00:29:11,230 --> 00:29:18,450
That's where you're putting. The numbers for how many beta nights do you have?

269
00:29:18,460 --> 00:29:21,070
So we will have zero times theta,

270
00:29:21,070 --> 00:29:31,720
not because we don't use the intercept plus one times the beta one four age plus one times the beta two for this long term.

271
00:29:33,400 --> 00:29:38,690
Now it says whenever you have a zero, you can actually just get away with not putting that part at all.

272
00:29:38,710 --> 00:29:47,290
So I had written Intercept Zero in my contrast statement and SAS, but if I just left that part out, it would have given me the same answer.

273
00:29:48,010 --> 00:29:58,360
But in ah, it really is paying attention to every row of your parameter table and how many times they want you want that parameter in your formula.

274
00:29:58,870 --> 00:30:02,740
And so you have to have a place holder for each one of these.

275
00:30:03,490 --> 00:30:12,520
So even though intercept is really not being used in the formula, R needs me to have a spot there for it with a zero or you won't get the right thing.

276
00:30:14,460 --> 00:30:17,900
So three three coefficients are coming out of your table.

277
00:30:17,910 --> 00:30:22,590
Think of it that way and you need three numbers for how many times that coefficients

278
00:30:22,590 --> 00:30:26,670
being used when you're adding the coefficients together for your formula.

279
00:30:27,900 --> 00:30:31,709
So that's the hard part. And then everything else is fairly straightforward.

280
00:30:31,710 --> 00:30:45,060
You're just using this G.L. H t function and you've saved the model fit into this variable and you've saved the contrast you wanted in this variable.

281
00:30:45,150 --> 00:30:48,540
So that will give you the contrast result.

282
00:30:48,540 --> 00:30:58,080
Then I've just got some code here that will show you the, you know, the odds, the odds ratio in the confidence interval for the contrast statement.

283
00:30:59,750 --> 00:31:02,930
And then R is really nice about like the ratio tests.

284
00:31:02,990 --> 00:31:08,840
You don't have to really pay attention to where in the output the likelihood is.

285
00:31:09,320 --> 00:31:15,200
You just have to use the variables where all that information is saved and use this test function.

286
00:31:15,560 --> 00:31:20,690
And it will do. It'll figure out how many degrees of freedom you have, what your p value is.

287
00:31:21,440 --> 00:31:25,220
So R is actually a little bit, uh, idiot proof.

288
00:31:26,240 --> 00:31:32,969
No one in here is an idiot, but r does the thinking for you where it says you're finding the numbers in the tables,

289
00:31:32,970 --> 00:31:41,690
then you're subtracting and you have to remember degrees of freedom and it's more work and sass in r for your future.

290
00:31:41,690 --> 00:31:45,290
Maybe you don't want to remember all those details. You just want to have it done for you.

291
00:31:45,290 --> 00:31:47,090
So there's another advantage to R here.

292
00:31:48,370 --> 00:31:54,460
You can also get the AC results by putting in where you saved all the model fit information for these two and compare them pretty quickly.

293
00:31:57,870 --> 00:32:01,000
So here's what the hour output looks like. So here's the coefficient table.

294
00:32:02,550 --> 00:32:10,520
This is the quick calculation of exponential in the parameters to get the odds ratio exponential

295
00:32:10,520 --> 00:32:15,780
hitting the confidence limits to get the corresponding confidence limits for the odds ratio.

296
00:32:16,620 --> 00:32:24,150
Again, you can't interpret the second row on its own. So we're going to be looking at the contrast results that we programed.

297
00:32:25,080 --> 00:32:33,900
And so here is the same odds ratio we got. And Seth, for odds ratio for DHT, when you are older by here,

298
00:32:33,930 --> 00:32:38,490
if you're 50 and above and the confidence limit so that matches are with Seth

299
00:32:39,120 --> 00:32:44,070
and here the lucky to raise your test is calculated for you with the p value.

300
00:32:44,300 --> 00:32:51,330
Now most of us you can look at the ACS and again the AIC that goes with the model that just had continuous age is smaller.

301
00:32:51,330 --> 00:32:54,330
So we pick that one. All the outputs kind of the same.

302
00:32:57,900 --> 00:33:00,990
All right. So are.

303
00:33:01,990 --> 00:33:05,010
So that's the spline term. So we're going to be using that a lot.

304
00:33:05,020 --> 00:33:11,950
I know I'm going to write about spline terms in homework three when we learn count models and I'm just going to try to

305
00:33:11,950 --> 00:33:19,600
exercise that muscle of your brains that it's automatic by the time we get to longitudinal measure kind of regression models.

306
00:33:21,780 --> 00:33:30,500
Okay. So. It's also a good idea just to review if you're trying to pick between a categorical variable and an ordinal variable.

307
00:33:31,040 --> 00:33:33,800
I just want to refresh your memory about how this is done as well.

308
00:33:34,370 --> 00:33:40,540
So if you're predictors categorical when then you typically need to consider whether model it with indicator variables.

309
00:33:40,550 --> 00:33:47,000
That's like the class statement in R or as an ordinal variable where you just use the numbers one,

310
00:33:47,000 --> 00:33:51,230
two, three, four or five, whatever are as a single variable.

311
00:33:53,620 --> 00:33:59,589
And I'm going to use that data from a committee committees.

312
00:33:59,590 --> 00:34:09,430
The Tamia example, I don't work in this disease area, so if I'm bludgeoning that committee's Artemio pronunciation, I apologize.

313
00:34:10,240 --> 00:34:16,910
But that is the data set that is focused on here in this table below.

314
00:34:16,930 --> 00:34:27,490
So this is a. A, you know, a parasitic kind of diagnosis.

315
00:34:28,440 --> 00:34:36,090
And here in the columns is yes and no if it was present in the study participants.

316
00:34:36,750 --> 00:34:45,459
And on the rose, we have something called the f r mutation status.

317
00:34:45,460 --> 00:34:54,660
So Plasmodium falciparum die harder folate reductase, which I'm just going to call data far from here on in.

318
00:34:55,530 --> 00:35:04,260
And we have we have rows saying no mutation, one mutation with its label or two mutations with the two labels.

319
00:35:05,820 --> 00:35:11,660
And so this kind of looks like an order, potentially an ordinal variable,

320
00:35:11,670 --> 00:35:22,110
but it's hard to tell for sure what's going to be better coating this a012 based on the number of mutations or using categorical versions of these.

321
00:35:24,790 --> 00:35:31,400
So. We've done this before by hand, you know, in your in your homework.

322
00:35:32,090 --> 00:35:39,770
You know, we've got we've had to do things like deciding do we use Pearson's Chi Square test or do we use a trend test?

323
00:35:40,490 --> 00:35:47,420
So we've done this kind of thing before. Now we're thinking about it in a logistic regression context that we're tying this loop closed.

324
00:35:48,590 --> 00:35:53,390
For how we do this in logistic regression. All right.

325
00:35:55,170 --> 00:36:01,920
And so you can sort of see that there might be some pattern here that we're all could make sense potentially, you know,

326
00:36:01,920 --> 00:36:10,860
because it's increasing with the number of mutations, but it's not entirely clear whether or not categorical will be better.

327
00:36:11,960 --> 00:36:19,610
So what logistic regression model should we be considering? So one of them is this ordinal variable.

328
00:36:19,610 --> 00:36:23,360
So it's a single member that is either a01 or two.

329
00:36:23,360 --> 00:36:29,330
And so when you interpret this variable, you're looking for a one unit increase in the number of mutations.

330
00:36:29,660 --> 00:36:39,850
And what that does to the log on to the odds ratio. And the other version is, Sir, my mouth is a little weird.

331
00:36:40,150 --> 00:36:47,170
The other version is a categorical representation where you use this indicator verbal and says,

332
00:36:47,170 --> 00:36:50,020
Does this automatically for you if you just use the class statement,

333
00:36:50,020 --> 00:36:57,520
but just writing it out explicitly, this is the indicator that you've got one mutation, so it's zero or one one.

334
00:36:57,520 --> 00:37:03,460
If you've got one mutation and this is another zero one variable, that's a one if you've got two mutations.

335
00:37:06,940 --> 00:37:10,180
And so this is how we write out the norm, the ordinal model.

336
00:37:11,700 --> 00:37:18,750
And so my claim is that this ordinal model of nested in the categorical model.

337
00:37:20,540 --> 00:37:28,960
But it's you know, this is usually something where it's a perk up moment right here because this is going to be a general pattern.

338
00:37:29,000 --> 00:37:32,720
I'm trying to point out to you that maybe you've seen, maybe you haven't seen,

339
00:37:33,650 --> 00:37:39,620
but it's sometimes hard to tell that these are nested because this these variables,

340
00:37:40,490 --> 00:37:47,540
you know, in the larger model with all the categories, they don't explicitly have this variable in the model.

341
00:37:48,950 --> 00:37:56,569
And usually we can tell if it's nested, if there's extra variables on top of this other one that we have in the model, then we know it's nested.

342
00:37:56,570 --> 00:38:06,920
So my claim to you is that it is nested because I can rewrite the categorical model to look like this ordinal model plus something extra.

343
00:38:07,910 --> 00:38:13,580
There's an algebraic relationship that allows me to do that every single time you encounter this situation.

344
00:38:15,450 --> 00:38:21,900
And so I'm going to claim that the lucky ratio tests with one degree of freedom can be used to compare the models.

345
00:38:22,140 --> 00:38:26,250
So the one degree of freedom is, you know, the number of parameters difference.

346
00:38:26,260 --> 00:38:30,450
I had three here estimated in a categorical two here, estimated in the ordinal.

347
00:38:32,150 --> 00:38:35,540
So the difference one is the degree of freedom for this test.

348
00:38:35,550 --> 00:38:43,110
So what is this work? So in the privacy of your own room, you can verify that this algebra works out.

349
00:38:43,110 --> 00:38:49,860
But I am going to walk you through it a little bit because it might not be immediately clear.

350
00:38:51,430 --> 00:38:58,360
So this is the model for the categorical model where I've got the ordinal model up

351
00:38:58,360 --> 00:39:03,850
front and then there's something extra that makes it equal to the categorical model.

352
00:39:05,310 --> 00:39:12,240
Okay. So let's let's go through this. So if my number of mutations is zero in the categorical model.

353
00:39:13,370 --> 00:39:18,380
That I'm modeling the lower gods is equal to beat or not. Okay, so what happens down here?

354
00:39:19,280 --> 00:39:24,200
If my number of mutations is zero, this will be zero.

355
00:39:24,200 --> 00:39:30,020
So there won't be a beta one and this will be zero.

356
00:39:30,620 --> 00:39:33,829
So there won't be this this weird coefficient in front either.

357
00:39:33,830 --> 00:39:42,850
So it really still will be beta. Not that I'm assuming. So checking if you have one mutation.

358
00:39:42,850 --> 00:39:47,590
I should have beta not plus beta one being modeled for the log odds.

359
00:39:48,720 --> 00:39:50,070
If you have one mutation.

360
00:39:50,070 --> 00:40:00,510
So if you have one mutation, we definitely have the beta naught and we'll have a one times beta one and for one mutation this part is zero.

361
00:40:00,510 --> 00:40:05,880
So we'll still have the same thing. Beta not plus beta one, right.

362
00:40:08,350 --> 00:40:12,950
So the last thing a check is. Four, the number of mutations is equal to two.

363
00:40:12,960 --> 00:40:18,210
So if the number of mutations is equal to two, I have data not this is a zero.

364
00:40:18,330 --> 00:40:22,409
So I don't have beta one, but I do have a one here.

365
00:40:22,410 --> 00:40:28,500
So I do have beta two. So the categorical model for two mutations should be beta, not plus beta two.

366
00:40:29,600 --> 00:40:36,049
And so down here, let's just check that. So for this model, I eventually want to get paid in Apple's beta two, right?

367
00:40:36,050 --> 00:40:41,630
So I've got beta not I have a beta one here times two.

368
00:40:43,650 --> 00:40:47,250
And the categorical model. I have no beta one term.

369
00:40:48,870 --> 00:40:54,330
So I need to fix that. So I have to remember there's something extra here need to get rid of.

370
00:40:54,750 --> 00:40:58,030
So when there's two mutations, this is going to be a one.

371
00:40:58,830 --> 00:41:02,280
And I get beta two minus two times beta one.

372
00:41:02,400 --> 00:41:03,900
So look what we've got here.

373
00:41:03,900 --> 00:41:15,150
So beta not plus two mutations times beta one, which I immediately subtract out here and I'll have better not plus beta two again.

374
00:41:17,150 --> 00:41:20,000
So this model is algebraically the same as this model.

375
00:41:21,610 --> 00:41:28,960
But it's easier when you look at this model to see that this is a that the ordinal models, the subset here of the predictors.

376
00:41:29,830 --> 00:41:37,930
All right. I'm never again going to show you this algebra or make you prove to me this algebra.

377
00:41:38,350 --> 00:41:43,690
This is meant to prove it to you once so that you will forever know this without

378
00:41:43,690 --> 00:41:48,639
having to do the algebra that whatever crush regression setting you're in,

379
00:41:48,640 --> 00:41:52,450
linear, logistic, any of the regression models of our future.

380
00:41:53,320 --> 00:41:57,520
If you have an ordinal model, zero one, two or one, two, three, or whatever,

381
00:41:58,360 --> 00:42:05,769
and you have a categorical model that instead uses the class statement itself or fits it as an or,

382
00:42:05,770 --> 00:42:15,159
you know, a categorical variable and are those are nested and you can use like a two ratio test without having to prove it with the algebra.

383
00:42:15,160 --> 00:42:21,730
You can just trust me from this point forward that no matter how complicated, no matter how many categories,

384
00:42:22,660 --> 00:42:31,450
there's always a way to go to write the categorical model using the automatic ordinal model plus some extra stuff.

385
00:42:36,030 --> 00:42:41,460
All right. Actually, before I jump ahead is are there any questions about that?

386
00:42:46,050 --> 00:42:49,300
What could I? Is there something that I can help?

387
00:42:50,390 --> 00:42:54,250
Make more clear. Okay.

388
00:42:56,860 --> 00:43:01,840
So if algebra isn't your thing, you know, if you don't want to spend time thinking about that, just.

389
00:43:02,810 --> 00:43:05,480
Take it for granted that this will always be the case.

390
00:43:10,100 --> 00:43:19,009
So in our example with the computers, the team here, I'm putting in the data here and I, you know,

391
00:43:19,010 --> 00:43:28,160
we've got the number of mutations, 012 we've got whether it can be just the team is present or not.

392
00:43:28,550 --> 00:43:30,830
And then we've got the cell counts from our table.

393
00:43:33,100 --> 00:43:40,290
And present meaning one for present zero otherwise, and before he would have used freq to analyze this data.

394
00:43:40,300 --> 00:43:43,420
So here I'm using proc logistic to do a logistic regression.

395
00:43:44,260 --> 00:43:51,160
I'm still going to have this freq count statement because I've got rows for every cell of a table with counts.

396
00:43:52,090 --> 00:43:56,319
And to do the categorical version and SAS, you can just use the class statement.

397
00:43:56,320 --> 00:43:59,230
It will create those zero one variables for you.

398
00:44:00,220 --> 00:44:10,959
I personally like to force a choice of the reference group, so whenever I use the class statement I really prefer to choose.

399
00:44:10,960 --> 00:44:16,330
Here I'm choosing zero mutations, my reference group and you just use this syntax to do that.

400
00:44:16,990 --> 00:44:23,950
And then when the output comes out, I'll have the odds ratios I'm most interested in because I forced the choice of the reference group.

401
00:44:24,970 --> 00:44:28,780
Here's the model statement. I do the same thing with the outcome.

402
00:44:28,780 --> 00:44:33,090
So this is the yes no variable for the team here.

403
00:44:33,670 --> 00:44:44,020
And I'm explicitly saying I'm modeling the log odds of that condition with the one and then there's no confusion.

404
00:44:44,500 --> 00:44:50,709
SAS likes to make assumptions for you if you don't force these on staff and they may not choose what you like.

405
00:44:50,710 --> 00:45:01,000
So I just force that. So here's the categorical model basically from the class statements making the categories and the ordinal version.

406
00:45:01,000 --> 00:45:06,550
I just remove the class statement. It will assume that it's an ordinal variable if you don't have the class statement.

407
00:45:10,150 --> 00:45:18,400
So here is the estimated categorical model with the parameter estimates, which we'll look at in a little bit further.

408
00:45:20,140 --> 00:45:24,910
Um, this is much more informative, by the way, than a contingency table analysis alone.

409
00:45:24,910 --> 00:45:29,920
So from the logistic regression framework, because of the output that's available there,

410
00:45:30,490 --> 00:45:37,780
it's easy to get p hats for the different risk profiles, the covariates and so on.

411
00:45:38,110 --> 00:45:45,429
It's easy to get odds ratios and everything. So rather than just a p value from, say, a Pearson Chi square test of association,

412
00:45:45,430 --> 00:45:49,990
you're getting a lot more information in a very compact set of code and output.

413
00:45:52,060 --> 00:46:05,590
And so here is some of the output where, you know, I've got what the odds ratio is comparing one mutation versus zero mutations.

414
00:46:06,460 --> 00:46:12,400
You know, that was this e to this parameter estimate here and the confidence limits.

415
00:46:12,790 --> 00:46:17,650
And I've also got the odds ratio comparing two mutations to zero.

416
00:46:17,800 --> 00:46:23,100
So that's basically each of this parameter estimate here. All right.

417
00:46:23,100 --> 00:46:25,920
So all of it comes out very quickly and easily.

418
00:46:27,870 --> 00:46:37,230
And we also get as part of the self output, all three tests you might be interested in like would raise your test score test in the world test.

419
00:46:38,190 --> 00:46:41,700
You know, just remember last time I went through a graphic kind of saying how those tests

420
00:46:41,700 --> 00:46:45,390
are all trying to get statistically at the same idea in slightly different ways.

421
00:46:46,230 --> 00:46:54,990
The score test turns out to be algebraically the same as the Pearson Chi Square test of association between the rows and columns.

422
00:46:54,990 --> 00:47:00,690
So we learn to do it through the OR. We reviewed how to do it with contingency tables before,

423
00:47:00,690 --> 00:47:07,440
but this is the exact same p value you get, so you can always use logistic regression to replicate.

424
00:47:07,890 --> 00:47:12,129
You don't have to do two different analyzes, you'll get the same p value, whichever method you use.

425
00:47:12,130 --> 00:47:15,720
So logistic regression is kind of a nice, easy way to do that.

426
00:47:20,210 --> 00:47:27,280
And the model fit statistics make it easy to choose between different models that you might be consistent considering.

427
00:47:27,290 --> 00:47:32,990
So these particular statistics are for the full model.

428
00:47:34,480 --> 00:47:40,990
Which is categorical, but we have to get model fit statistics for the ordinal model to choose between those two.

429
00:47:41,680 --> 00:47:49,479
So we're going to probably be holding on to this one 37.954 for the AIC comparison,

430
00:47:49,480 --> 00:47:55,060
and we'll be holding on to this 131.954 for the likelihood ratio test comparison.

431
00:47:57,870 --> 00:48:03,060
So here is the output from the ordinal model. So this is now the 012 variable.

432
00:48:04,160 --> 00:48:13,970
And this is how to ride out the logistic regression model. And so the odds ratio is saying for any increase in the number of mutations.

433
00:48:15,780 --> 00:48:17,380
Actually, I think I have a question here.

434
00:48:17,430 --> 00:48:25,169
What is this thing that was just midway in telling you, I'm just going to continue this interpretation of 2.02.

435
00:48:25,170 --> 00:48:32,310
NULL is for every increase in the number of mutations from 0 to 1 or equivalently, from 1 to 2.

436
00:48:33,430 --> 00:48:37,210
Estimated odds ratios 2.0 to non here the conference limits.

437
00:48:38,110 --> 00:48:47,050
So of course the question is is this is it really fair to say that the increase from 0 to 1 mutations is the same?

438
00:48:48,140 --> 00:48:51,620
Kind of odds ratio as the increase from 1 to 2.

439
00:48:52,010 --> 00:48:55,700
So the categorical gives us two separate odds ratio for that.

440
00:48:55,700 --> 00:49:06,559
Let's just look back real quick. So when we model them separately, they increase or I guess we don't have the increase from 1 to 2.

441
00:49:06,560 --> 00:49:10,380
Exactly. Here we had the increase from 0 to 1 and we had to increase from 0 to 2.

442
00:49:10,500 --> 00:49:16,610
So we don't have the middle one exactly here.

443
00:49:21,190 --> 00:49:24,750
So I mean, I might get I'm getting a little distracted here.

444
00:49:24,760 --> 00:49:30,970
Like, do I teach him how to do that now or not? I don't think I'm you know, I'm probably to say something just because I raised it.

445
00:49:31,480 --> 00:49:38,170
So if you wanted to know the odds ratio, comparing it to mutations to one mutation.

446
00:49:40,240 --> 00:49:44,350
Then you would you would it would involve these two parameters here.

447
00:49:45,040 --> 00:49:51,250
Right. So the odds log out on the top would involve the beta,

448
00:49:51,260 --> 00:49:58,950
not plus this beta two and the log odds on the bottom would involve the beta, not plus the beta one.

449
00:49:58,960 --> 00:50:10,720
And so the difference between these betas, this the beta two minus beta one would be the contrast we would need to get at that.

450
00:50:11,840 --> 00:50:14,959
And so it would be elevated to minus one would be what we need.

451
00:50:14,960 --> 00:50:20,120
So you could actually work it out from here or you could be real clever and just

452
00:50:20,120 --> 00:50:25,069
change your reference group and quickly get the answer if you needed to do that.

453
00:50:25,070 --> 00:50:32,330
If you just changed your reference group to that one mutation, you would automatically get a comparison between the beta two and beta one.

454
00:50:33,350 --> 00:50:38,240
So sorry for that digression. I was thinking for a moment that I could just quickly look at that.

455
00:50:38,900 --> 00:50:47,000
The ordinal model is assuming that each of those changes between zero one and between one and two is the same.

456
00:50:48,470 --> 00:50:56,390
And so, you know, that's that's an assumption. So we need to choose between those two versions of the model.

457
00:50:56,870 --> 00:51:03,229
Here's the output for the the fit statistics for the ordinal model and the score

458
00:51:03,230 --> 00:51:07,540
test here also has a p value that agrees with the test we've learned before.

459
00:51:07,550 --> 00:51:14,240
So this one degree of freedom test p value is equivalent to the Cochrane Armitage trend test.

460
00:51:15,170 --> 00:51:21,020
So if you were trying to do that, that Cochrane Armitage trend test from the previous handout, we had,

461
00:51:22,460 --> 00:51:29,240
you know, testing to see whether this ordinal predictor is associated with given to us the team here.

462
00:51:30,970 --> 00:51:35,530
Then you'd get the same answer whether you have this or no version of the number of mutations,

463
00:51:35,530 --> 00:51:40,030
or if you go ahead to freaking do that Cochran Armitage trend test, you get the same answer.

464
00:51:43,450 --> 00:51:48,849
And so here's the model fit statistics for the the auto models.

465
00:51:48,850 --> 00:51:55,750
So we want to pull this AIC value. And here I've just kind of summarized everything from the two models.

466
00:51:55,760 --> 00:52:02,200
So this AIC value for the intercept and covariance is here.

467
00:52:02,350 --> 00:52:07,810
So that was the AIC for the auto model. And here's the AIC we saw earlier for the categorical model.

468
00:52:07,840 --> 00:52:12,890
So based on AIC criteria, lower is always better, right?

469
00:52:12,910 --> 00:52:16,060
So the ordinal model is what's recommended based on AIC.

470
00:52:18,050 --> 00:52:24,410
You don't get a p value from that comparison. So the likelihood ratio test, we can use it because we argued these are nested.

471
00:52:25,430 --> 00:52:33,650
And so the likelihood ratio test is pulling this number over here for the ordinal and then the comparable number for the categorical model.

472
00:52:34,190 --> 00:52:36,829
And really they're very close together.

473
00:52:36,830 --> 00:52:44,300
So the difference between those two is so small that you're lucky the duration of test fails to detect the difference between them.

474
00:52:45,500 --> 00:52:52,080
And so what does that mean? Whenever we fail to detect the difference between the model we choose, the smaller,

475
00:52:52,100 --> 00:52:57,170
reduced model and the smaller reduced model is the one that only uses a single predictor.

476
00:52:57,200 --> 00:53:04,560
The ordinal predictor. So the likelihood ratio test is also saying the ordinal model is the preferred version,

477
00:53:05,250 --> 00:53:08,250
but the categories don't give you any extra help in estimating things.

478
00:53:14,240 --> 00:53:17,930
All right. So our code of promos right after this.

479
00:53:17,930 --> 00:53:26,629
We'll take a break. So the our code reading in the data and labeling it up, making data frames,

480
00:53:26,630 --> 00:53:33,930
this is stuff that you would have done earlier when you were learning about the, you know, the contingency tables and things.

481
00:53:33,930 --> 00:53:36,409
So here here's where the modeling part starts.

482
00:53:36,410 --> 00:53:46,280
So I'm saving into my formula, the outcome for one or zero for having Clementine's the team here present or not,

483
00:53:47,120 --> 00:53:55,760
this is like the equals sign of the model. And then in R when I want to use a class variable, they have this thing as dot factor that they use.

484
00:53:56,570 --> 00:54:06,110
So as dot factor will create those zero one variables for one mutation zero one variable for two mutations.

485
00:54:07,010 --> 00:54:15,200
So that's like the class statement. And so here is the model fit where I'm.

486
00:54:16,190 --> 00:54:19,409
Fitting the number of mutations that have categorical variables.

487
00:54:19,410 --> 00:54:23,930
So I put kind of cat in the the name of the output with my formula,

488
00:54:23,930 --> 00:54:30,290
the data set and the binomial family with a logic link summarizing the output there.

489
00:54:32,330 --> 00:54:36,530
And so that'll be the categorical model here.

490
00:54:36,530 --> 00:54:38,690
I'm looking at odds ratios and confidence intervals.

491
00:54:39,590 --> 00:54:47,989
And then for the ordinal version of the model, the main difference is that in the formula, I don't have this as doc factor anymore.

492
00:54:47,990 --> 00:54:51,860
So this is going to assume it's a single variable 012.

493
00:54:52,940 --> 00:54:58,760
All the other code is similar. I've saved the model fit into something where the or d for ordinal.

494
00:55:00,040 --> 00:55:08,410
So this is summarizing everything from that model fit. Looking at the ACS for the different models, categorical versus ordinal,

495
00:55:08,800 --> 00:55:13,660
and then using our test to compare them and decide which one is the preferred model.

496
00:55:14,650 --> 00:55:16,959
And here's kind of a hodgepodge of all the outputs.

497
00:55:16,960 --> 00:55:27,010
So this is output from the categorical information, the odds ratios and confidence intervals very similar to SAS, everything similar SAS here.

498
00:55:27,760 --> 00:55:30,790
Here is the model fit when we assume ordinal.

499
00:55:31,860 --> 00:55:43,080
Information with odds ratios, confidence intervals, etc. of AIC values and the P value for the likelihood ratio test.

500
00:55:43,620 --> 00:55:47,880
So it's pretty much similar to what you get in SAS here.

501
00:55:49,700 --> 00:55:58,609
And. So I'm going to take a break here and then we're going to talk about made analysis and how to do it within the logistic regression framework.

502
00:55:58,610 --> 00:56:07,480
So I want you to take a break before we jump in there. So let's get back here about eight after nine.

503
00:56:11,300 --> 00:56:35,480
10 minutes. The.

504
00:56:52,370 --> 00:57:04,150
Yeah. I.

505
00:57:10,330 --> 00:57:34,110
We're talking. In.

506
00:58:05,980 --> 00:58:31,320
It's easier to. So I.

507
00:59:28,610 --> 01:00:03,100
I know. It's not.

508
01:00:10,120 --> 01:00:43,340
Yeah. That's where.

509
01:01:43,250 --> 01:01:47,850
It's not. Yeah. I think I need to.

510
01:02:01,250 --> 01:02:29,470
I know. Okay.

511
01:02:32,780 --> 01:02:54,190
Know. It's not.

512
01:03:02,920 --> 01:03:51,660
This is. Right.

513
01:05:28,480 --> 01:05:32,280
To coincide with the sentencing.

514
01:05:50,370 --> 01:05:58,360
Okay. Let's get back to work. So.

515
01:06:00,850 --> 01:06:02,680
Hopefully I'm getting across that.

516
01:06:03,220 --> 01:06:11,830
Any analysis that you can do with contingency tables is a special case of a logistic regression model, and made analysis is no exception.

517
01:06:13,150 --> 01:06:21,730
So I'm going to go back to the same studies of intravenous magnesium that we looked at earlier for suspected acute M.I.

518
01:06:22,720 --> 01:06:26,710
And now we're going to talk about how to model this with logistic regression.

519
01:06:26,740 --> 01:06:30,490
So here are our data sets right from the various studies.

520
01:06:32,100 --> 01:06:36,930
And so how shall we do this? So what are the variables that we need to identify for the model?

521
01:06:36,990 --> 01:06:40,950
We need to identify the yes or no outcome. Well, here.

522
01:06:42,320 --> 01:06:46,970
The yes, no outcome is, you know, was there a death or was there not a death?

523
01:06:47,910 --> 01:06:53,100
So you can model the death as a one know, death as a zero for each participant.

524
01:06:55,210 --> 01:07:01,660
So then you need a predictor. So we know the treatment group is going to be a predictor because that's the main interest of each of these studies.

525
01:07:02,680 --> 01:07:07,420
So we can have a yes or no variable for magnesium versus control.

526
01:07:07,780 --> 01:07:13,750
So maybe magnesium as a one, control as a zero and have that as a predictor.

527
01:07:14,560 --> 01:07:19,390
So what else do we need? So in made analysis.

528
01:07:20,730 --> 01:07:24,130
Remember, we need to adjust for the study.

529
01:07:24,150 --> 01:07:30,840
That's the big point of made analysis is adjusting for study and making sure that all of the.

530
01:07:32,030 --> 01:07:36,200
Various odds ratios that you're combining are telling the same story.

531
01:07:36,200 --> 01:07:40,010
So we did something called a test for homogeneity of the odds ratios.

532
01:07:41,340 --> 01:07:45,719
So we need to figure out how to do that and when and if that test is okay.

533
01:07:45,720 --> 01:07:48,840
We estimate an overall odds ratio that is just for study.

534
01:07:49,590 --> 01:07:58,410
So how do you do that? So in logistic regression, and this is going to be asking you to participate if you if you don't mind.

535
01:07:59,370 --> 01:08:02,670
So in logistic regression, when you want to adjust for confounder.

536
01:08:03,820 --> 01:08:13,380
How do you do that? So the model for yes, no for the outcome, death versus no death will have a predictor for magnesium versus control.

537
01:08:14,610 --> 01:08:22,410
How do you adjust for study? When you're how do you adjust for a confounder in general for logistic regression?

538
01:08:26,810 --> 01:08:29,840
Yeah. Yeah.

539
01:08:29,840 --> 01:08:32,930
So it's probably hard on the recording to hear.

540
01:08:32,930 --> 01:08:44,479
But yeah, you add a covariate for a study and should that study cover it be a categorical variable or an ordinal variable,

541
01:08:44,480 --> 01:08:55,610
or what's the form of the covariate that we want to put in? Kendra Yeah, so we want to have a categorical variable for study.

542
01:08:55,610 --> 01:09:00,200
So we're going to have, we have seven studies, so we need six parameters.

543
01:09:01,690 --> 01:09:04,780
For indicating which study the data is coming from.

544
01:09:04,780 --> 01:09:10,830
And then the one you leave out is the reference group. So that's how we're going to adjust for study.

545
01:09:12,640 --> 01:09:18,790
And we're also going to need to figure out how to test for homogeneity.

546
01:09:18,790 --> 01:09:24,129
But I think we're going to hold off on that question for a moment. But in the back of your mind, think about how would I test for homogeneity?

547
01:09:24,130 --> 01:09:30,310
Because that's going to come up soon. So here's the model where we adjust for studies.

548
01:09:30,320 --> 01:09:33,380
We've got the log of the outcome for death versus no death.

549
01:09:36,110 --> 01:09:40,280
They do not plus the treatment group. So I'm putting this indicator variable.

550
01:09:40,280 --> 01:09:45,470
Remember whenever I hear I'm thinking of the zero one variable where it's one when the inside is true.

551
01:09:45,500 --> 01:09:48,890
So this could be one if they're on magnesium, zero otherwise.

552
01:09:49,850 --> 01:09:57,200
And then I've got just arbitrarily chosen study one to be my reference group for the categorical variable for study.

553
01:09:58,010 --> 01:10:02,960
And I have all of my little indicator variables here for which study we're talking about.

554
01:10:03,860 --> 01:10:14,330
And so from this model, if I want to know the overall odds ratio for magnesium versus control adjusted for study,

555
01:10:15,140 --> 01:10:35,210
what is that based on these coefficients? Or I'm going to ask it in a slightly different way to hopefully jog a mental memory from an earlier class.

556
01:10:35,220 --> 01:10:41,740
Right. So what is the odds ratio for taking magnesium versus control?

557
01:10:41,740 --> 01:10:46,040
Adjusting for study? Oh, it's the same way.

558
01:10:46,040 --> 01:10:50,780
I said the same thing twice. Okay. What was the odds ratio for this predictor?

559
01:10:51,620 --> 01:10:54,470
Holding all the other predictors constant. How about that?

560
01:10:55,340 --> 01:11:00,560
That's a different way of saying the same thing, but maybe with a mental memory jog involved.

561
01:11:11,610 --> 01:11:15,479
Some of you are politely letting someone else shine, which is really lovely.

562
01:11:15,480 --> 01:11:24,480
But one of you. Please be brave now. Yes.

563
01:11:26,760 --> 01:11:29,880
It's e to the. Yes, thank you. It's E to the beta one.

564
01:11:30,180 --> 01:11:39,389
So we're looking at the odds associated with death when I increase this predictor by one level.

565
01:11:39,390 --> 01:11:44,250
So that's comparing magnesium equals 1 to 0. I did hear you correctly.

566
01:11:44,250 --> 01:11:47,700
Right. You're laughing as if I totally switched your answer.

567
01:11:48,120 --> 01:11:53,310
Okay, my hearing's bad, but I thought I got that right.

568
01:11:53,550 --> 01:11:57,340
Yeah. So it's either the beta one. All right.

569
01:11:57,340 --> 01:12:01,420
And that is adjusted for study because we had study in the model.

570
01:12:01,510 --> 01:12:06,190
So holding all of their values, it doesn't matter which study you're thinking of,

571
01:12:06,190 --> 01:12:11,260
we're assuming they have the same odds ratio e either the beta one in this model.

572
01:12:11,950 --> 01:12:16,020
So this model's actually assumed. Homogeneity.

573
01:12:16,140 --> 01:12:19,380
It doesn't matter which study. If I think about study two.

574
01:12:20,550 --> 01:12:24,540
And I'm looking at the odds ratio within study two. It's still going to be the 81.

575
01:12:24,540 --> 01:12:32,250
If I think about study three, I'm thinking about the odds ratio within the study three, I'm still going to end up getting into the beta one.

576
01:12:32,400 --> 01:12:38,280
So this model assumes homogeneity of the odds ratios and that the overall odds ratio is either the beta one.

577
01:12:40,080 --> 01:12:49,200
All right. And so in a sense, we kind of input this data before.

578
01:12:49,210 --> 01:12:53,020
So this is still putting in the data from all the different studies.

579
01:12:53,050 --> 01:13:01,600
Each data line is a study table that's saying which study it was were the on magnesium one versus zero.

580
01:13:01,600 --> 01:13:06,999
Otherwise, did they die one if they did zero otherwise?

581
01:13:07,000 --> 01:13:13,120
And then the count. Of how many people had that coverage profile.

582
01:13:14,140 --> 01:13:18,100
And so the seven studies I've put in here and I've used this trick with the at at

583
01:13:18,130 --> 01:13:21,850
so that I can put more than one real data on a line just to compress the slide.

584
01:13:23,400 --> 01:13:30,510
And then a logistic regression. I'm using fruit count because I do have counts here of people with similar corporate profile.

585
01:13:31,440 --> 01:13:39,060
I'm treating study as a categorical variable with the class variable, and I've mindfully chosen one study one to be the reference group.

586
01:13:41,100 --> 01:13:48,690
And so the moral dead is the outcome with event equals one magnesium variables study variable is a class variable.

587
01:13:48,960 --> 01:13:59,130
So on. And then just for comparison, you know, reminding us how it went when we did the Cochrane Mental Handful method of meta analysis from before.

588
01:13:59,130 --> 01:14:02,520
So we can compare and see that they're very similar.

589
01:14:06,020 --> 01:14:14,810
And so this is the significance for magnesium and its effect on mortality.

590
01:14:15,470 --> 01:14:18,770
And so we'll need to look at odds ratios in a moment.

591
01:14:18,770 --> 01:14:25,639
But we the overall meta analysis is going to come up with a significant effect of magnesium,

592
01:14:25,640 --> 01:14:30,650
and it looks like the magnesium group is lowering your odds of death.

593
01:14:30,650 --> 01:14:34,280
So that's a good thing, right? So.

594
01:14:35,670 --> 01:14:47,010
Here are the coefficients, just the model written out. And here's the odds ratio for death adjusting for study ratio for death.

595
01:14:47,010 --> 01:14:52,710
If you're on magnesium versus other with confidence limits, that looks good.

596
01:14:53,100 --> 01:14:57,540
And then here's what we got with the mental handful method.

597
01:14:57,540 --> 01:15:01,560
So it's not exactly perfectly the same. It's pretty close.

598
01:15:02,460 --> 01:15:11,450
It's definitely getting at the same story. So within a little wiggle room in the estimates, you can do a made analysis in logistic regression.

599
01:15:12,230 --> 01:15:16,780
And so you just want to pause for a moment.

600
01:15:16,790 --> 01:15:22,069
When we did made analysis, we said we can combine anything, not just odds ratios.

601
01:15:22,070 --> 01:15:28,310
As long as we had estimates and confidence intervals and so on, we could combine any kind of statistic from papers.

602
01:15:28,850 --> 01:15:36,140
And what this is saying is that you can do the same in logistic regression, but if you have any regression,

603
01:15:36,650 --> 01:15:41,479
if you have any data set from different studies with individual level data,

604
01:15:41,480 --> 01:15:47,960
you can use a regression model to perform a meta analysis to you just have to adjust for the study as a variable.

605
01:15:50,090 --> 01:15:55,640
And just anecdotally, I'm working on that now for a Cox regression model for survival data.

606
01:15:56,210 --> 01:15:59,870
And it's it's the challenges in getting the individual level data.

607
01:15:59,870 --> 01:16:04,339
And there's quite a lot of maneuvering to get the different pharmaceutical

608
01:16:04,340 --> 01:16:09,860
companies involved to supply their individual data for that kind of an analysis.

609
01:16:09,860 --> 01:16:14,690
But with the right personalities, persuading it can be done, it's just difficult.

610
01:16:20,440 --> 01:16:25,540
All right. And then an hour. This is the getting all of the variables in an hour.

611
01:16:26,320 --> 01:16:29,559
I probably could have tried to get them all in long rows, too,

612
01:16:29,560 --> 01:16:35,020
but then it would have been very hard to tell where one entry stopped and the other started since, you know.

613
01:16:35,890 --> 01:16:51,219
So here's that read in with the variable names and in R I've saved all the variable names in this made it one long and done the formula

614
01:16:51,220 --> 01:16:58,600
here with the A's factor for studies that'll be a categorical variable and then the rest is very similar syntax to what we've done before.

615
01:16:59,740 --> 01:17:04,870
This is a bit of a trick that I've had in the slides before, I believe,

616
01:17:04,870 --> 01:17:09,909
but it's just so nice to get the odds ratio on the on the correct scale with the

617
01:17:09,910 --> 01:17:13,780
confidence interval and then the p value you have to kind of hunt how to get it.

618
01:17:13,780 --> 01:17:17,980
But this is how you'll get the p value from the logistic regression output.

619
01:17:21,370 --> 01:17:26,019
And so here's the coefficient table and here's kind of the Frankenstein together

620
01:17:26,020 --> 01:17:30,430
output on the regular odds ratio scale with the confidence interval and P value.

621
01:17:30,700 --> 01:17:35,859
And so this is the same as you get with SAS but with many more significant digits.

622
01:17:35,860 --> 01:17:45,040
So I think in various handouts I sometimes stick around a rounding function in here to reduce the amount of output.

623
01:17:45,040 --> 01:17:50,410
So you might see it the word round sneak in sometimes if the output is a little simpler looking.

624
01:17:53,740 --> 01:17:57,420
All right. So. We've already gotten the adjusted odds ratio.

625
01:17:57,630 --> 01:18:04,310
So now the question is how do we confirm that homogeneity of the odds ratios across studies is appropriate?

626
01:18:04,320 --> 01:18:10,250
So how do we do this test? That we did before in the contingency table framework.

627
01:18:10,250 --> 01:18:16,910
How do we do this test that the odds ratio for study one is similar to the odds ratio of two and so on.

628
01:18:17,450 --> 01:18:27,589
So how do we do that in the regression framework? So we've sort of already fit the model that assumes this.

629
01:18:27,590 --> 01:18:37,550
So we have to have a comparison model where we allow odds ratios for the the treatment to vary by study.

630
01:18:42,820 --> 01:18:50,530
So how can we do that in this regression framework? How can we allow the odds ratios for the treatment effect?

631
01:18:51,570 --> 01:19:02,090
To be different by studying. Are you not saying things because you're reading ahead in the slides and you don't want to just say what you read?

632
01:19:02,720 --> 01:19:07,980
What's happening here? Oh, okay.

633
01:19:08,000 --> 01:19:14,480
You don't want to be like the person in the class who just reads ahead and says, well, that's going to make class interaction more difficult.

634
01:19:16,490 --> 01:19:25,310
All right. So if you're reading ahead, then we should we should look at the interactions between study and magnesium.

635
01:19:26,120 --> 01:19:32,630
And so that's like six interaction terms that will be added to the model.

636
01:19:34,120 --> 01:19:38,560
So instead of with the class statement, you can just solve the mystery for me.

637
01:19:38,570 --> 01:19:47,170
I'm sitting here processing that. So in we just add this magnesium time study to get those six interactions that come from the class statement.

638
01:19:48,170 --> 01:19:51,020
So I'll show you how that happens and are as well.

639
01:19:52,550 --> 01:19:57,920
And so in a sense, you have to really keep track of these model statistics to work out what's going on.

640
01:19:57,920 --> 01:20:01,370
And so the model fits statistics with the interaction term.

641
01:20:01,370 --> 01:20:08,569
That's the full model or over here in this column under intercepts and covariates and

642
01:20:08,570 --> 01:20:13,879
without the interaction terms are in this column here with intercepts and covariates.

643
01:20:13,880 --> 01:20:18,770
And just I don't know about you. I always look at the AIC first because it doesn't require any math.

644
01:20:18,770 --> 01:20:24,440
And so the smaller AIC in these two models is without the interaction terms.

645
01:20:24,620 --> 01:20:31,939
So that's good news for homogeneity because the AIC prefers the model where you assume a common odds ratio,

646
01:20:31,940 --> 01:20:40,610
it assumes the model, the smaller AIC is for the model without all of the interaction terms between study and treatment.

647
01:20:42,250 --> 01:20:45,580
And for the likelihood ratio test, we have to do math here instead.

648
01:20:45,610 --> 01:20:49,959
So we were saving this number in this number, taking the difference.

649
01:20:49,960 --> 01:20:53,800
And then we have to know the degrees of freedom for the chi square test to use.

650
01:20:53,800 --> 01:20:57,120
So. We have. Here's the difference.

651
01:20:58,430 --> 01:21:02,330
Between the reduced model, the full model.

652
01:21:04,420 --> 01:21:10,389
And so we get 8.832 and we have six degrees of freedom because there were six

653
01:21:10,390 --> 01:21:15,610
interaction terms that were added to the larger model versus the smaller model.

654
01:21:16,360 --> 01:21:21,280
And so using your favorite calculator slash package,

655
01:21:21,280 --> 01:21:27,160
you can come up with the p value that goes along with this value for kind of squared distribution with six degrees of freedom.

656
01:21:28,470 --> 01:21:35,670
And so we failed to reject this null hypothesis that the simpler model is is good.

657
01:21:35,760 --> 01:21:38,850
So we picked the simpler model without the interaction terms.

658
01:21:41,510 --> 01:21:46,770
And it's close, but not identical to the homogeneity test that park freak would give you.

659
01:21:46,790 --> 01:21:53,960
So, you know, it's still kind of in the same ballpark of eight point something pretty high with six degrees of freedom.

660
01:21:54,320 --> 01:21:59,629
It's pretty close. So the logistic regression framework, again, you can do this test.

661
01:21:59,630 --> 01:22:03,110
It won't be precisely the same. It'll be pretty close to a meta analysis.

662
01:22:03,120 --> 01:22:06,860
So in your homework too.

663
01:22:07,310 --> 01:22:13,820
The final homework problem is doing a meta analysis using the logistic regression framework and kind of practicing this.

664
01:22:14,830 --> 01:22:17,950
And next week's lab will help with that as well.

665
01:22:23,250 --> 01:22:30,810
Okay. So with the class statement, there are some additional nice statistics that are produced called Type three statistics.

666
01:22:31,770 --> 01:22:37,020
And so the output looks kind of like this for the type three statistics.

667
01:22:37,680 --> 01:22:47,980
And this column labeled World Statistic and Chi Squared is doing a test to see if study here.

668
01:22:48,330 --> 01:22:53,070
This is the model which is magnesium in the and the six study indicators.

669
01:22:53,820 --> 01:22:58,710
So this is saying is study helpful in predicting mortality and it is.

670
01:23:01,870 --> 01:23:08,530
So it's not categorical variables not helpful in the model versus categorical variable is helpful.

671
01:23:09,490 --> 01:23:15,300
And so this red box thing is saying, yeah, there's mortality differences across the study.

672
01:23:15,310 --> 01:23:22,790
So some of the studies had more mortality than others. But based on our model, we were to test for the interaction.

673
01:23:22,790 --> 01:23:29,600
So regardless of how much mortality was in each study, the odds ratio comparing treatment groups was comparable.

674
01:23:34,390 --> 01:23:38,980
And so here's the Type three statistics that included the interactions.

675
01:23:39,790 --> 01:23:45,760
And so this is the world test to see if those interactions are important or not with a p value.

676
01:23:46,480 --> 01:23:53,650
So this is very nicely done for you as an alternative to the like, the ratio test for seeing whether the interactions are important.

677
01:23:54,280 --> 01:24:01,670
So it's the world version of the homogeneity test. Here's some our code.

678
01:24:02,840 --> 01:24:06,860
So I'm calling this formula related to homogeneity.

679
01:24:06,860 --> 01:24:12,349
But I after the fact, I was like, are these really the best names for my output?

680
01:24:12,350 --> 01:24:19,040
Because this is the model that assumes that homogeneity is not there, that I'm allowing different odds ratios.

681
01:24:21,310 --> 01:24:26,800
For each study because I got the interaction between the treatment group and this as dark factor study.

682
01:24:27,550 --> 01:24:33,960
So a better name for this might have been form dot no maybe.

683
01:24:33,970 --> 01:24:39,340
Or something to indicate that I was allowing for homogeneity here. But I was thinking homogeneity test in my head.

684
01:24:39,340 --> 01:24:49,980
So that's what you got. Here's the fit, sticking that formula in there, and then the homogeneity test that corresponds to that wall test we just saw.

685
01:24:50,910 --> 01:24:55,950
You can use this A0 call and Cornwall test.

686
01:24:55,950 --> 01:25:02,040
Remember I think that I mentioned earlier that we'd overwritten the odd world test and so to

687
01:25:02,490 --> 01:25:07,590
refer to the masked function while that test we have to do this echoed from the package colon,

688
01:25:07,590 --> 01:25:10,170
colon and the name of the function from that package.

689
01:25:10,920 --> 01:25:17,069
And so we'll and you have to say which terms, which are the rows that the output that you're testing are zero.

690
01:25:17,070 --> 01:25:26,160
So we actually count. You know, in our output for the parameter estimates, which were this the six rows that went with those interaction terms.

691
01:25:26,970 --> 01:25:34,950
So that's a bit of work, but otherwise you'll get similar answers to SAS to Here's the wild, the wild version of the homogeneity test.

692
01:25:35,520 --> 01:25:37,080
And then of course, we were,

693
01:25:37,230 --> 01:25:44,390
we're so pleased in our that we can just say LR test to get the liquid ratio test without even figuring out where parameters are.

694
01:25:44,490 --> 01:25:48,810
That's actually easier in R and here is that like good ratio test with the P value.

695
01:25:52,800 --> 01:26:04,650
Okay. So just to give you a sense here, we're rounding in on the end of this topic that I'm hoping that we'll be able to finish this hand out today.

696
01:26:05,850 --> 01:26:11,110
But what's left here is just a reminder about how to deal with interaction terms.

697
01:26:11,110 --> 01:26:12,900
So this is a bit of review here.

698
01:26:14,160 --> 01:26:23,820
And then at the end of the handout is some kind of nice advice about doing automated model fitting, which is nice as well.

699
01:26:23,830 --> 01:26:30,360
So I need to get you all on the page on how to deal with interaction terms because this happens in every single regression model.

700
01:26:30,810 --> 01:26:38,070
So reminding us how it works in a common logistic framework that you've already seen in another class is

701
01:26:38,070 --> 01:26:43,350
going to be helpful to really get it solid for this one familiar model before we stretch to do models.

702
01:26:44,780 --> 01:26:53,240
All right. So if this is something that you've avoided owning completely in previous courses, that is a luxury that is now going away.

703
01:26:53,270 --> 01:26:57,139
You need to own this completely for not just logistic regression,

704
01:26:57,140 --> 01:27:02,420
but all the other models that we're going to show you because it's such an important part of understanding data.

705
01:27:03,940 --> 01:27:10,660
So I have a very simple example here, but the ideas extend to any interaction.

706
01:27:10,670 --> 01:27:14,220
So this comes from a low birth weight dataset.

707
01:27:14,230 --> 01:27:17,650
So the outcome was whether the baby had a low birth weight.

708
01:27:18,250 --> 01:27:21,280
So as one, if they were low birth weight baby, hence the name low.

709
01:27:21,820 --> 01:27:24,850
And then there is an exposure.

710
01:27:26,320 --> 01:27:34,850
That is, if the mom, the very pregnant mom is weighs less than 110 or not.

711
01:27:34,900 --> 01:27:39,760
So you can imagine a pregnant woman who weighs less than 110 has a very low weight.

712
01:27:40,510 --> 01:27:44,290
On a good day, people like to be this this weight.

713
01:27:44,860 --> 01:27:50,980
But if you're pregnant, that's way too low. So that's the risk factor for low birth weight, baby.

714
01:27:51,490 --> 01:27:57,970
And then there's an effect modifier here of mother's age, and it's measured as a continuous variable.

715
01:28:00,160 --> 01:28:02,950
And we're just going to keep it continuous for the purpose of this example.

716
01:28:03,760 --> 01:28:13,710
And so here's a model for the log odds of a low birth weight baby with the low weight mom age and the interaction.

717
01:28:13,720 --> 01:28:19,420
And so we just want to review how to deal with these interaction terms and how to interpret the models and that sort of thing.

718
01:28:21,200 --> 01:28:28,849
And so the low birth weight odds ratio comparing moms with weight less than 110 versus

719
01:28:28,850 --> 01:28:33,170
greater than or equal to hundred pounds depends on age because of this interaction.

720
01:28:37,870 --> 01:28:42,370
So if you have a mom that is less than 110.

721
01:28:43,550 --> 01:28:51,340
Right. Then when you go through this model, the log odds for a low birth weight baby is bad enough.

722
01:28:51,500 --> 01:28:56,010
Plus beta one times one because it's the low weight mom variables.

723
01:28:56,010 --> 01:29:08,000
The one here. Plus payday, two times age, whatever that age is, plus beta, three times a one for being a low weight mom times age.

724
01:29:08,010 --> 01:29:15,090
So here's the log odds for for a low birth weight baby if the mom is less than 110.

725
01:29:16,310 --> 01:29:28,660
So, uh. Blog Odds for mom, who weighs more than 110 now has zeroes for the low birth weight variable.

726
01:29:29,020 --> 01:29:35,050
So what we're going to see is this beta to age.

727
01:29:35,860 --> 01:29:41,049
We're looking at an odds ratio comparing, you know, the low weight versus not low weight in the beta.

728
01:29:41,050 --> 01:29:45,220
Two age term is the same in both of these formulas. So that's going to cancel.

729
01:29:46,060 --> 01:29:49,750
And the beta naught is also in both of these formulas, that's going to cancel.

730
01:29:49,750 --> 01:29:54,010
But we have two terms that don't cancel.

731
01:29:54,010 --> 01:29:57,550
One is the one we're used to seeing left over beta one.

732
01:29:57,730 --> 01:30:01,960
So there's a one beta one here times a zero between here. So we'll have one beta one left over.

733
01:30:02,650 --> 01:30:06,990
But we also have this term. Made three times.

734
01:30:07,000 --> 01:30:10,930
Age in this term is a zero, so we have no way to cancel that term.

735
01:30:11,470 --> 01:30:20,290
So when we look at the logs log odds ratio, we've got theta one plus beta three H, you know,

736
01:30:20,290 --> 01:30:26,320
you're basically subtracting off subtracting this model from the top model to get that log odds ratio.

737
01:30:30,620 --> 01:30:34,040
So the log odds ratio depends on how old the mom is.

738
01:30:34,670 --> 01:30:42,430
And so the odds ratio is going to depend on how old the mom is. All right, because we're expending cheating, whatever that number turns out to be.

739
01:30:43,310 --> 01:30:47,180
And so every different age is going to have a different odds ratio here, right?

740
01:30:47,210 --> 01:30:57,590
You plug in different numbers for age. You get a different odds ratio. So I've arbitrarily picked I think this was the mean age.

741
01:30:57,590 --> 01:31:00,649
I can't remember what I why I picked age 22.9.

742
01:31:00,650 --> 01:31:02,690
I think it was the average age in the data set.

743
01:31:03,800 --> 01:31:13,970
And so I've I've in this year reading in the data for this low birth weight problem, I've created the indicator variable.

744
01:31:13,970 --> 01:31:17,600
This will be one when the mom's weight is less than or equal to 110.

745
01:31:18,350 --> 01:31:24,740
And I've also created for later a centered version of age where I take the moms age and subtract off the mean.

746
01:31:24,890 --> 01:31:28,820
That's going to come up later. But for now,

747
01:31:28,820 --> 01:31:38,090
here's the the model with the whether the mom is low weight or not age and the interaction and I've put in this

748
01:31:38,090 --> 01:31:47,719
contrast here where I want to know what the low birth weight odds ratio is for a low weight mom who is 22.9 years old.

749
01:31:47,720 --> 01:31:54,200
So if I want a single number, I have to pick a single age and talk about the odds ratio for single age.

750
01:31:54,200 --> 01:32:02,180
So I just pick this one as an example. And so the formula I remember was e to the beta one plus age times beta three.

751
01:32:03,470 --> 01:32:14,060
So that formula did not involve beta, not it canceled, it involved a beta for the low weight variable.

752
01:32:14,060 --> 01:32:18,940
This was the beta one. The age variable canceled.

753
01:32:20,420 --> 01:32:29,780
But the coefficient for the interaction had h times, you know that uh, parameter for the interaction.

754
01:32:30,680 --> 01:32:35,329
So usually I've so far I've always had like a zero or one for all of these,

755
01:32:35,330 --> 01:32:42,470
but I actually need my formula to be e to the theta one plus theta three times whatever the number of ages.

756
01:32:44,100 --> 01:32:49,710
So my formula here is one better one and a 22.9 times the beta three.

757
01:32:50,610 --> 01:32:56,280
So this is eight times the data three. All right.

758
01:32:56,280 --> 01:33:02,270
And so what does the output look like? Use the parameter estimates.

759
01:33:03,960 --> 01:33:12,750
And I don't want you to necessarily get bogged down by this, but, you know, just a heads up, the interaction is not statistically significant.

760
01:33:12,760 --> 01:33:17,100
So this is a learning exercise on how to interpret interaction.

761
01:33:17,100 --> 01:33:21,090
But at least for this story, there wasn't a significant interaction.

762
01:33:22,860 --> 01:33:35,490
Well, we'll look at the data a little bit more later, but I want to focus on this odds ratio for being low weight as a mom versus not.

763
01:33:36,000 --> 01:33:44,580
So what's the odds ratio for low birth weight? Comparing a low weight mom to one who's not when you're thinking about a mom who is 22.9 years old.

764
01:33:45,540 --> 01:33:50,010
So here is that odds ratio and the confidence limit.

765
01:33:50,020 --> 01:33:53,820
So for this particular age person,

766
01:33:54,720 --> 01:34:02,340
there is a significant significantly higher odds of having a low birth weight baby if you're a low birth weight mom.

767
01:34:03,480 --> 01:34:08,670
I can tell that because this doesn't overlap the null hypothesis of one.

768
01:34:08,850 --> 01:34:11,639
All right. We have that trick of comparing confidence intervals.

769
01:34:11,640 --> 01:34:17,820
And if it does if it doesn't overlap with the null hypothesis, it's going to be significant.

770
01:34:18,150 --> 01:34:23,040
I don't have a handy p value though. I mean, the contrast in here isn't giving me a handy p value.

771
01:34:25,130 --> 01:34:32,630
And if I want a p value that corresponds to this, I also don't have a handy p value from here because this p value,

772
01:34:32,960 --> 01:34:38,420
you know, this p value is just about the low birth weight coefficient.

773
01:34:38,750 --> 01:34:43,400
This p value is just about the age coefficient and this p value is just about interaction coefficient.

774
01:34:43,970 --> 01:34:50,330
I don't have a p value that goes with beta one plus 22.9 times beta three.

775
01:34:50,600 --> 01:34:55,160
It doesn't give me that pebble, it involves two coefficient so I can't kind of wing it from this table.

776
01:34:57,920 --> 01:35:02,540
So so far what I can say is that for a 22.9 year old mom,

777
01:35:03,410 --> 01:35:12,740
the odds of having a low birth weight baby are 2.58 times higher if she's less than 110 versus greater than 110.

778
01:35:13,040 --> 01:35:17,180
And I can put the confidence limits here from this output, but I don't have a p value really yet.

779
01:35:20,840 --> 01:35:28,249
So I foreshadowing I did create a centered version of age around 22.9.

780
01:35:28,250 --> 01:35:33,830
And so I'm going to use that to trick my way into getting a p value to add to this sentence.

781
01:35:40,260 --> 01:35:42,629
Oh, and then just a little footnote here.

782
01:35:42,630 --> 01:35:50,130
The interaction was not statistically significant by the world test, so in practice we wouldn't include the interaction in the model.

783
01:35:50,790 --> 01:35:56,400
But I want to continue forward with how to interpret interactions and get all the statistics you need for them.

784
01:36:00,980 --> 01:36:07,070
So the parameter for the main effect for the low rate mom is much easier to interpret if age is centered first.

785
01:36:07,850 --> 01:36:13,940
And so that's what we're going to do next. And that is also how we're going to get a p value that corresponds to this contrast.

786
01:36:16,830 --> 01:36:22,950
So I'm going to here use now the same model.

787
01:36:22,950 --> 01:36:27,660
But instead of age I'm going to put age centered around 22.9.

788
01:36:29,770 --> 01:36:37,149
And so my claim is that now when I want to get the odds ratio for low birth weight baby comparing

789
01:36:37,150 --> 01:36:41,830
low weight mom to not only white mom when we're all talking about a 22.9 year old mom,

790
01:36:42,820 --> 01:36:45,820
my claim is that this contrast statement will get me there.

791
01:36:47,290 --> 01:36:50,439
All right. So remember, mom is 22.9.

792
01:36:50,440 --> 01:36:57,420
So in this model, I'm still going to have a baby two times this low weight variable in my formula.

793
01:36:57,430 --> 01:37:05,009
That's where this one comes from. I don't have a baby that has to do with age being centered.

794
01:37:05,010 --> 01:37:12,299
That's still the case. But I do have a baby three times the centered version of age times low weight mom.

795
01:37:12,300 --> 01:37:17,340
Well, the centered version of age is the mom's age -22.9.

796
01:37:17,340 --> 01:37:23,520
So if I'm thinking about the odds ratio for a 22.9 year old, this age centered variable becomes zero.

797
01:37:25,010 --> 01:37:34,490
So suddenly the odds ratio I'm interested in has a zero here and I don't need that parameter in my my odds ratio anymore.

798
01:37:36,150 --> 01:37:40,590
So if you if you sent her age first around the age you're interested in.

799
01:37:42,130 --> 01:37:48,010
Then you've reduced the problem to only being this e to the beta for the low weight mom variable.

800
01:37:50,440 --> 01:37:58,780
And so now I have an easy p value to attribute to my contrast statement because it's all about this one parameter.

801
01:37:59,800 --> 01:38:02,830
So this row now for low weight, mom.

802
01:38:04,340 --> 01:38:11,810
It's going to be the same now as this row of the contrast statement that I wanted.

803
01:38:12,210 --> 01:38:24,670
Well, obviously, because I forced it to be that way. But notice that the odds ratio is 2.5757 in the confidence limits.

804
01:38:24,680 --> 01:38:28,280
That's the same as I got before I sent her page on the previous slide.

805
01:38:29,660 --> 01:38:33,100
So remember this. This is the odds ratio.

806
01:38:33,110 --> 01:38:37,820
When I didn't center age in the confidence limits and by centering age,

807
01:38:38,930 --> 01:38:43,970
I now only need one parameter to get at that same estimate and confidence interval.

808
01:38:44,390 --> 01:38:52,670
So the main effect of age when you exponential is the odds ratio, the same odds ratio I was interested in with the non centered version.

809
01:38:53,990 --> 01:38:59,250
So in fact, the advantage of this trick is that I have a p value that goes in my sentence.

810
01:38:59,270 --> 01:39:08,300
Now I can use this single p value for the low weight mom, because that's the only parameter in this model that was used to get that odds ratio.

811
01:39:09,440 --> 01:39:17,269
So in my previous sentence, now I can say P equals 0.0085 and have a complete manuscript $0.40 with all the elements.

812
01:39:17,270 --> 01:39:20,929
I like. Direction point estimate.

813
01:39:20,930 --> 01:39:29,150
Confidence interval p value. 41 minutes down here, a little bit more on parameters.

814
01:39:30,170 --> 01:39:35,440
The log odds ratio associate with the low mothers weight for 22.9 year old mom.

815
01:39:35,460 --> 01:39:47,150
Yep. And age the age centered parameter is the log odds ratio associated with one year increase in age for mom that is greater than 110.

816
01:39:51,140 --> 01:39:55,000
Yeah. So the interaction term actually works that way too, right?

817
01:39:55,010 --> 01:40:00,860
So, um, because I have the centered age.

818
01:40:05,050 --> 01:40:19,180
This age centered parameter ends up being the odds ratio for a one year increase when low birth weight when the mom is actually greater than 110.

819
01:40:21,490 --> 01:40:26,680
Because this age centered variable in the interaction is going away.

820
01:40:34,800 --> 01:40:39,850
Okay. So here's our code to get the same output we got with SAS.

821
01:40:39,850 --> 01:40:45,339
So here's our. The same logic is used everywhere.

822
01:40:45,340 --> 01:40:51,820
It's just the code that's different. So reading the data in, you can play with this with the data set in cannabis if you want to.

823
01:40:52,540 --> 01:40:58,870
It's being read into a data frame where I have a variable called age sent that I'm creating here.

824
01:40:59,230 --> 01:41:08,050
It's the age variable -22.9. I'm creating the variable for the low weight mom here.

825
01:41:10,020 --> 01:41:19,139
And then creating the very the model formula here is with low weight mom non centered

826
01:41:19,140 --> 01:41:25,830
age and the interaction and then later I'll have the centered age and the interaction.

827
01:41:26,670 --> 01:41:35,309
So here's the model fit coefficient summary and this is the part that's probably the newest to those of you who are our users,

828
01:41:35,310 --> 01:41:40,500
and that is installing the package that will give you the contrast you need.

829
01:41:40,500 --> 01:41:44,040
So we need we've already done that earlier in the handout.

830
01:41:44,040 --> 01:41:48,060
So if you haven't already installed that, you need to do so before you do this next bit.

831
01:41:48,780 --> 01:41:52,470
So the low birth weight orders for low weight manage 2.9.

832
01:41:54,020 --> 01:42:04,430
Comes from this contrast where we have four parameters and this is the multiple of beta not we want and we don't need.

833
01:42:04,460 --> 01:42:08,540
Better not. Here's the multiple of beta one we need.

834
01:42:08,540 --> 01:42:14,210
We needed one beta one value. That was for the low mom's birth weight.

835
01:42:15,020 --> 01:42:21,200
Right. Low weight. Mom is the first thing in this list that's going to show up. The age variable cancel that in that odds ratio.

836
01:42:21,200 --> 01:42:32,030
But we do need age times the data for the interactions that since 22.9 that's the age and then you need at times that beta for the interaction.

837
01:42:32,630 --> 01:42:36,830
All right so that's our formula and here are our contrast results.

838
01:42:36,830 --> 01:42:40,159
And I have to tell you that our wins yet again in this game,

839
01:42:40,160 --> 01:42:51,440
because I can get the p value for my contrast right here within R without having to fiddle with centering the mother's age at all.

840
01:42:52,940 --> 01:42:57,739
So this is going to give me that right away and I'll show you what the results look like with centering age as well.

841
01:42:57,740 --> 01:43:03,650
But R allows me to skip that step because I get a row that corresponds to my contrast with the p value right there.

842
01:43:06,260 --> 01:43:10,700
And so here's the model where I was just using regular age.

843
01:43:10,700 --> 01:43:15,800
Here's the contrast. I want it. And look, that p value just comes up so nicely for us in our.

844
01:43:17,920 --> 01:43:21,500
You can certainly center age just to confirm you get the same thing and stuff.

845
01:43:21,520 --> 01:43:27,100
So I've created a centered age variable earlier and I'm putting that in the model now with the interaction.

846
01:43:28,610 --> 01:43:35,000
Fitting the model and summarizing the same information so you can just compare.

847
01:43:35,030 --> 01:43:43,310
This is definitely the same as you got with fast when you centered age and this is identical to the non centered age version.

848
01:43:44,660 --> 01:43:55,220
You know, when we put in the right contrast statement, so for the center stage we only needed the coefficient for the low, the low birth weight mom.

849
01:43:55,550 --> 01:43:59,270
Everything else was not involved in the formula when she sent her age.

850
01:44:01,390 --> 01:44:04,450
So that's definitely has its advantages.

851
01:44:04,450 --> 01:44:10,480
But this is just yet another time. Ah, saves you a little bit of work if you need that p value for your sentence.

852
01:44:14,670 --> 01:44:22,020
So just a couple of notes that I find useful to really notice.

853
01:44:22,020 --> 01:44:27,870
This is a bit of a perk up moment of when you're fitting models with interactions in them.

854
01:44:28,560 --> 01:44:33,090
It's quite important that if the interaction statistically significant,

855
01:44:33,090 --> 01:44:39,330
that you must keep the main effects in the model, even if they're not statistically significant.

856
01:44:40,380 --> 01:44:47,590
So if there's a story about an interaction. You're not just trying to fit the data well.

857
01:44:47,800 --> 01:44:54,610
If it was just about fitting the data well, you might not need the main effects if they're not useful in terms of P values.

858
01:44:54,610 --> 01:45:02,410
But you're not just trying to fit the data well, you're trying to explain the story behind the data to another audience.

859
01:45:03,400 --> 01:45:13,709
So when you submit your paper. The reviewers are going to need to see that the main effects were in the model along with the interaction,

860
01:45:13,710 --> 01:45:21,990
because otherwise if you don't have the main effects in there, you might see a significant interaction term.

861
01:45:21,990 --> 01:45:29,520
But it could mean either that there was a true interaction and you appropriately decided that main effects weren't important or.

862
01:45:30,920 --> 01:45:38,270
A reviewer could also decide that it looks like an interaction that's really driven by the main effect.

863
01:45:38,270 --> 01:45:42,800
And so the question of the question, the results, they'll say.

864
01:45:43,820 --> 01:45:48,260
This interaction might be entirely driven by a main effect that you didn't include in the model.

865
01:45:48,260 --> 01:45:53,900
So I don't know if this is really a story about an interaction or you are a sloppy modeler.

866
01:45:55,960 --> 01:46:01,060
So to describe the story to another audience,

867
01:46:01,060 --> 01:46:06,940
you have to include the main effects just to convince someone that you've accounted for the main effects.

868
01:46:06,940 --> 01:46:10,720
And even after accounting for them, you observe the significant interaction.

869
01:46:13,480 --> 01:46:18,040
So those people who are assessing your results need to be able to tell the difference.

870
01:46:19,150 --> 01:46:23,760
And there's no trust in research when it comes to this stuff in a paper.

871
01:46:23,950 --> 01:46:32,080
If you don't show that you accounted for the main effects, they say they'll just assume that what you're showing them is.

872
01:46:33,180 --> 01:46:37,140
Is based on the fact and that you just incorrectly model to your data.

873
01:46:39,860 --> 01:46:43,550
So keep them in there, even if the the main effects are not significant.

874
01:46:45,390 --> 01:46:51,020
Now. I actually forgot that I was doing propensity score matching in the Senate.

875
01:46:51,020 --> 01:46:56,540
So we have two topics left, propensity score matching and model fitting.

876
01:46:56,900 --> 01:47:00,950
And so we are going to have to save that for next time because we've run out of time.

877
01:47:00,950 --> 01:47:10,099
So we'll start next time on slide one or two. The following handout Handout six is teaching how to do multiple computation.

878
01:47:10,100 --> 01:47:17,270
It's a very short handout. So I, I suspect, will be able to finish this handout and handout six next time.

879
01:47:19,220 --> 01:47:25,880
All right. So that's it for today. I'm going to turn off the recording and have a wonderful rest of your week.

