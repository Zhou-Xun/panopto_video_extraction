1
00:00:02,820 --> 00:00:12,719
All right. Hope everybody had a good weekend. So today our goal is trying to finish Hannah 70 and also to do a GM review.

2
00:00:12,720 --> 00:00:16,230
So you will finally find it handy to have.

3
00:00:18,300 --> 00:00:21,690
Hendo 70 and an open.

4
00:00:22,820 --> 00:00:23,930
But we will start with the.

5
00:00:25,820 --> 00:00:35,780
Hannah oh 730 So before I get started, any burning questions about the homework number two or, you know, midterm logistics?

6
00:00:44,600 --> 00:00:50,870
Okay. I don't see any hands here. I also want to say that for this week, I'm going to add.

7
00:00:52,290 --> 00:00:53,700
Change office hours.

8
00:00:54,090 --> 00:01:01,920
So today, from 5 to 6 p.m., I will be hosting the office hour in my office just in case you have questions about homework or other things.

9
00:01:02,220 --> 00:01:08,730
And for tomorrow, I will also have an office hour from 5 to 5:45 p.m.

10
00:01:09,630 --> 00:01:13,800
So please feel free. Feel free to drop by and ask any questions.

11
00:01:15,660 --> 00:01:20,129
So let's start with handout oh 770. And this is the second,

12
00:01:20,130 --> 00:01:30,930
the final example of dynamics models and the primary goal of this example is trying to illustrate to you how to use slides in a mixed model context.

13
00:01:36,800 --> 00:01:42,860
And also we will touch on the definition of balance of the data.

14
00:01:42,890 --> 00:01:51,500
And here we will be talking about how to align the data so that we have in meaningful ten zero for everybody.

15
00:02:01,370 --> 00:02:07,790
Okay. So this example is to study the influence of menarche on changes in body fat accretion.

16
00:02:08,480 --> 00:02:12,140
And let's take a look at a study back around the lots of words here.

17
00:02:12,800 --> 00:02:18,110
But I'm trying to go through all these detail with you. So this is a longitudinal study.

18
00:02:18,920 --> 00:02:27,260
It is a prospective study on body fat accretion in a cohort of 162 girls from an MIT growth and development study.

19
00:02:28,010 --> 00:02:34,970
And the background is that in general, the increases in the body fatness in girls begin just before around menarche.

20
00:02:35,540 --> 00:02:41,209
And although it has been presumed that increase in body fatness levels of approximately four

21
00:02:41,210 --> 00:02:47,270
years of these changes in body fat accretion had not been studied in population based samples,

22
00:02:47,270 --> 00:02:53,420
at least at the time when this data was collected. So how is this launched your design?

23
00:02:54,170 --> 00:02:56,420
What is this, your design? So at the start of the study,

24
00:02:56,420 --> 00:03:05,340
all of the girls were premenopausal and non-obese as determined by a triceps skin for the thickness less than 85 percentile.

25
00:03:05,810 --> 00:03:12,690
And all the girls were followed over time, according to a schedule annual measurements until four years up and marking the final measurements.

26
00:03:12,710 --> 00:03:16,700
What was scheduled on the fourth anniversary of the reported date of menarche?

27
00:03:17,360 --> 00:03:21,680
And what's the outcome? It is the percent body fat.

28
00:03:24,940 --> 00:03:33,130
So what is the analysis set up? Our goal is trying to study the changes in percent body fat before and after menarche

29
00:03:33,730 --> 00:03:37,630
and time here is coded time since menarche and can be positive or negative.

30
00:03:38,440 --> 00:03:44,380
And this designs on balanced when the timing of measurements is defined as time since a girl experience menarche.

31
00:03:44,800 --> 00:03:51,430
So just for the sake of clarity, I'm going to draw, you know, just two hypothetical girls.

32
00:03:51,850 --> 00:03:57,120
So these are four time points. I think they have maybe have more than four points.

33
00:03:57,490 --> 00:04:03,070
But I think this example will clarify the meaning of inherit on balanced.

34
00:04:03,490 --> 00:04:08,160
So this is girl number one. And you have another girl's data.

35
00:04:08,490 --> 00:04:12,120
Supposedly, they measure at the same calendar time.

36
00:04:16,130 --> 00:04:24,770
All right. So this is another person. However, in this study, we define time zero as a time of anarchy.

37
00:04:24,950 --> 00:04:30,830
Right? We do not know a priori, at least when we recruited girls when menarche would happen or the age of an argument.

38
00:04:30,860 --> 00:04:34,130
So if I say for this girl, mean I can have it here.

39
00:04:37,740 --> 00:04:44,340
While for the second girl when our cure occurred here. Right now, how to define type zero according to definition.

40
00:04:45,150 --> 00:04:51,480
It is the timing of menarche and also it makes a lot of biological sense where you want to line them at a landmark event.

41
00:04:51,810 --> 00:04:57,090
So what you do, as we have seen earlier, if you recall, is trying to copy.

42
00:04:57,120 --> 00:05:00,570
So it is trying to move the second girl to the left a little bit.

43
00:05:00,990 --> 00:05:07,040
How should I do that? How about use a lasso? Okay.

44
00:05:07,040 --> 00:05:11,190
So. Huh? Let me try that again.

45
00:05:12,150 --> 00:05:15,900
I don't want to lead. Just bear with me for a moment. Okay. Let's go.

46
00:05:21,110 --> 00:05:24,740
Now. So. Control C.

47
00:05:26,530 --> 00:05:31,000
Control. Oh, there you go. So we ought to do this right.

48
00:05:31,090 --> 00:05:36,370
So you can see that we have to align. Align the time zero here.

49
00:05:39,080 --> 00:05:46,160
So if you compare 1 to 1 and two, you can see that if you look at the crosses, the timings are the same and shared across people.

50
00:05:46,580 --> 00:05:51,860
While you consider one and two prime, you can see that because of the alignment,

51
00:05:52,190 --> 00:05:55,730
the timing when the measurements occurs somehow is not common anymore.

52
00:05:56,300 --> 00:05:59,870
And so this is what I meant by the design inherent in balance,

53
00:05:59,870 --> 00:06:06,050
because we did not know the timing of menarche for each girl at the time of recruitment.

54
00:06:06,980 --> 00:06:10,280
And also because we defined this time to be time zero.

55
00:06:10,310 --> 00:06:18,530
Right? And this time zero does not need to be the timing of any measurements, you know, for the girls.

56
00:06:19,010 --> 00:06:24,080
And also, you have positive timings and negative timings relative to that event.

57
00:06:27,050 --> 00:06:31,610
But still, we will call this jackals. One, two, three, four.

58
00:06:31,650 --> 00:06:41,139
Right. So the index will be the same. Now with this, just a quick summary of data.

59
00:06:41,140 --> 00:06:50,230
We have 162 girls and a total of about 10,050 body fat measurements for each person.

60
00:06:50,230 --> 00:06:58,120
On average, we had 6.4 measurements and in the sample the average age of NAKI was 12.8 years.

61
00:06:59,710 --> 00:07:04,750
So this is a plot we call this spaghetti plot.

62
00:07:06,660 --> 00:07:15,990
So we connected the dots of each person. So we have this we got a sense of how variable these body fat measurements are across time.

63
00:07:16,740 --> 00:07:22,470
And clearly we can do something smarter. It's a population level, less curve.

64
00:07:22,830 --> 00:07:29,010
It's a smooth and technique which help draw where the how the trend would change in the population.

65
00:07:29,460 --> 00:07:36,090
So the vertical dashed lines representing the time zero which is the timing of

66
00:07:36,090 --> 00:07:42,540
menarche because we sort of we have a line of data so we can be sure that that.

67
00:07:44,680 --> 00:07:49,480
You know, vertical line is the timing ability for every girl.

68
00:07:50,780 --> 00:07:52,519
Quick observation right here.

69
00:07:52,520 --> 00:08:02,360
You can see that the body fat secretion seems to be increasing with age but with different slopes before and after the landmark event in menarche.

70
00:08:04,230 --> 00:08:04,490
Right.

71
00:08:04,490 --> 00:08:15,770
So it is because of this exploratory analysis that we want to explore the new spline, which is a piecewise need your device to approximate this trend.

72
00:08:17,030 --> 00:08:25,849
And the primary reason for hypothesizing different rates is because, you know, menarche signifies a period of time when the body changes a lot.

73
00:08:25,850 --> 00:08:34,040
And clearly, it may be quite a it may be associated with a higher rate of change in the body fat.

74
00:08:34,700 --> 00:08:43,550
So here is analysis plain WhatsApp offices where we hypothesize that they have they have different slopes and the not is at the menarche time.

75
00:08:48,130 --> 00:08:52,410
Also we will be because this is an example for an genomics model.

76
00:08:52,740 --> 00:09:01,350
We will be playing with, I believe, two sets of intercepts and slopes and corresponding two sets of random intercepts.

77
00:09:01,350 --> 00:09:10,620
Random slopes. Number four.

78
00:09:11,430 --> 00:09:18,600
So here, because we have decided that ten zero is a time of menarche and that means different ages for different people.

79
00:09:19,230 --> 00:09:23,070
So we did not use knots that represents the same age for each subject.

80
00:09:23,850 --> 00:09:29,850
So the knot, although it is represented by zero, but it means different age for different subjects.

81
00:09:30,930 --> 00:09:36,210
So this is the model we are going to play with.

82
00:09:37,020 --> 00:09:43,410
So it can be seen as the fixed effect part and the random effect part.

83
00:09:46,310 --> 00:09:56,480
Okay. So if I draw a visualization.

84
00:09:57,970 --> 00:10:01,930
This is the time after time since.

85
00:10:03,400 --> 00:10:07,260
Anarchy. So we got to have zero.

86
00:10:10,550 --> 00:10:13,610
And before that, it's pre monarchy and after that, it's post monarchy.

87
00:10:15,020 --> 00:10:19,060
All right. Now. For the population.

88
00:10:19,360 --> 00:10:25,130
We got to have a line. All right. So now the question for you is, what's the intercept and what is the slope?

89
00:10:25,970 --> 00:10:33,920
Well, in the population, we often assume that. So the population will often assume the random effects have been zero.

90
00:10:33,980 --> 00:10:41,330
Right. So a typical person, a typical girl will have ab1, ib2, I'd be three, I've been zero.

91
00:10:41,360 --> 00:10:46,630
That's what I meant by typical. So what's left essentially is the population version.

92
00:10:46,640 --> 00:10:56,710
So this is beta one. This is body fat, sorry, body fat percent and slope prior to time zero.

93
00:10:58,150 --> 00:11:03,170
So that got to be. Peter, too.

94
00:11:03,500 --> 00:11:08,510
All. So for every any change in time, the change and the body fat is updated to.

95
00:11:11,670 --> 00:11:14,970
I'm not sure it's always increasing, but from the data it looks like so.

96
00:11:15,720 --> 00:11:19,230
After the monarchy. Right. It has another slope, right?

97
00:11:20,180 --> 00:11:25,930
For every unit change in time. What's this change in the mean body fat percentage?

98
00:11:27,930 --> 00:11:31,380
So the way to work this out is you have to figure out which term is better.

99
00:11:31,410 --> 00:11:39,950
Right. So does this term matter if you change, if you if a girl increases in one year of age, that matters.

100
00:11:39,960 --> 00:11:44,310
Right? How about this one? It is a plus.

101
00:11:44,310 --> 00:11:52,290
It's a positive part. Right. It will be exactly, T.J., if TJ is positive, because we're looking at post minorities all the time are positive.

102
00:11:52,800 --> 00:11:57,540
So for every unit, increasing age beta three also contributes to the effect.

103
00:11:58,540 --> 00:12:01,900
So the slope will be B to two plus B to three.

104
00:12:02,860 --> 00:12:07,120
Right. And it is a common mistake to assume that the slow speed of three only.

105
00:12:09,700 --> 00:12:18,890
All right. So you can see here by introducing Beta three, we have, you know, made the model flexible to accommodate two different slopes.

106
00:12:20,520 --> 00:12:24,630
So that finishes discussion about the you know.

107
00:12:27,370 --> 00:12:33,330
Population level. So if you're going to draw a line extending the previous slope.

108
00:12:33,340 --> 00:12:44,130
Right. So this red line corresponding to the null hypothesis that beta three equals zero, which is to say that some.

109
00:12:47,310 --> 00:12:56,430
Drop my mic. All right. So this red line corresponds to the null hypothesis that the rate of change in the percent

110
00:12:56,430 --> 00:13:00,690
body fat per unit increase in age is going to be the same before and after menarche.

111
00:13:01,500 --> 00:13:05,190
This likely is biologically implausible,

112
00:13:06,090 --> 00:13:14,610
but we our goal here is to quantitatively use the data to reject it or consider it adequate enough to describe data, as you will see later.

113
00:13:14,940 --> 00:13:18,720
This will be rejected, but this is null hypothesis and we will be formulating.

114
00:13:21,340 --> 00:13:24,880
All right. Now, if you go back to the random if that part.

115
00:13:24,940 --> 00:13:31,870
Clearly, every person will have some deviations, not only in terms of the intercept here.

116
00:13:34,960 --> 00:13:41,650
Right. But also in terms of the slope. Okay.

117
00:13:42,610 --> 00:13:47,790
So here. So what is this slope? For this person, I.

118
00:13:52,250 --> 00:13:55,650
So it should be beta two plus B two II.

119
00:13:55,700 --> 00:14:02,220
Right. So beta two is a population level rate of change in the percent body fat and B to II

120
00:14:02,270 --> 00:14:07,900
is this person's particular deviation from the population rate of change currently,

121
00:14:07,910 --> 00:14:17,160
because the first segment of the read trajectory is steeper than the first green line.

122
00:14:17,180 --> 00:14:23,060
So the beta b2i showed here is positive right now if you look at second one.

123
00:14:24,220 --> 00:14:30,340
So this is per unit change in time post when Aki right.

124
00:14:30,370 --> 00:14:34,420
My my question for you is what is this quantity.

125
00:14:36,380 --> 00:14:41,959
So it it represents for this person only what's the rate of change in the percent body fat post menarche.

126
00:14:41,960 --> 00:14:55,610
Right. So clearly it is going to deviate from the population level, which has been a two plus better three plus one plus B two plus B three, I hear.

127
00:14:57,110 --> 00:15:04,550
All right. So again, it is a common mistake to ignore the ignore the B two component.

128
00:15:07,650 --> 00:15:10,830
So in this case because the second red segment.

129
00:15:12,020 --> 00:15:18,860
Is shallower than the second green segment. You can see the two plus three is going to be negative.

130
00:15:20,660 --> 00:15:26,840
I'm going to pause you for 30 seconds just to make sure that everybody is on the same page with the visualization here.

131
00:15:30,560 --> 00:15:36,230
And by the way, all the points I made is on the is shown in the bullets so we can review them later.

132
00:15:44,840 --> 00:15:49,550
All right. Before we look at the model results, I want to ask you two questions.

133
00:15:53,630 --> 00:15:59,140
First. What's the variance?

134
00:16:01,520 --> 00:16:08,130
Of slow. Slopes before t equals zero.

135
00:16:09,120 --> 00:16:17,970
And what's the variance? Of slopes after t equals zero.

136
00:16:18,550 --> 00:16:25,740
I'll give you one minute to think about it and discuss maybe with your neighbors, and we will be using these quantities a bit later.

137
00:16:25,740 --> 00:16:32,020
But I think this is a good place to. Discuss this concept of variance of slopes.

138
00:17:14,830 --> 00:17:20,860
So if we look at the first question it is asking, the variance of slopes for two times equals zero.

139
00:17:21,340 --> 00:17:28,370
Scientifically, that just means that. What's the variability in the rate of change in body fat before girl experience when Aki.

140
00:17:29,200 --> 00:17:32,590
So that variability characterizes what's the pre menarche.

141
00:17:33,640 --> 00:17:38,050
Heterogeneity is in the population. Right so that will be.

142
00:17:39,720 --> 00:17:47,370
Variance of just be too high, right? How about the variance of slope after t equals zero?

143
00:17:49,860 --> 00:17:54,269
In English just means that what's the variability across people in terms of the

144
00:17:54,270 --> 00:18:00,030
rate of change of the body fat percentage after a girl has experienced menarche?

145
00:18:00,120 --> 00:18:07,700
Right. And that variability is. Me too, I.

146
00:18:09,060 --> 00:18:14,040
Is this one right? There's a second one has to be greater than the first one.

147
00:18:15,920 --> 00:18:20,330
Not necessarily. Right. Because it can be partitioned into a few terms.

148
00:18:33,650 --> 00:18:37,430
Right. So clearly I'm going to use a color.

149
00:18:37,460 --> 00:18:44,580
Clearly, you can see that these two things are the same. So what determines the relative magnitude of these two variances will be?

150
00:18:45,950 --> 00:18:51,260
The term that I'm going to draw in, in red circle and in red.

151
00:18:51,290 --> 00:18:55,520
So this part. So if this red part is positive,

152
00:18:56,060 --> 00:19:04,550
then we will have a higher variability in the rate of change of the body fat percentage after the monarchy relative to the treatment period.

153
00:19:04,710 --> 00:19:07,950
Right. Can it be smaller?

154
00:19:08,520 --> 00:19:12,060
Can it be smaller than that one? Then the first term.

155
00:19:15,040 --> 00:19:18,280
So. I.e. the term in red. Circle.

156
00:19:19,120 --> 00:19:24,150
Circle. Red. When the shape is that, can this term be negative?

157
00:19:33,470 --> 00:19:39,400
Of course it can be negative. So I'll give you an extreme example.

158
00:19:39,400 --> 00:19:45,250
What if B three is minus B two? I right. So that variance is zero and it's definitely smaller than the parents would be two.

159
00:19:45,700 --> 00:19:54,080
Anyway, that's a mathematical fact. Anyway, so these two variances can be one can be bigger than the other and the other way around.

160
00:19:54,100 --> 00:20:02,750
So the primary reason for this is because we a priori don't know what's the sign of this sign of magnitude, of this covariance.

161
00:20:03,070 --> 00:20:07,060
Remember, this is covariance between the two random effects.

162
00:20:10,700 --> 00:20:14,530
So in the fitted model we will be calculating this quantities,

163
00:20:14,540 --> 00:20:22,880
hopefully this set up so that we set that concept up so you can relate back to this discussion.

164
00:20:26,810 --> 00:20:30,890
I'm happy to answer any questions if you feel you need some additional clarification.

165
00:20:33,130 --> 00:20:37,110
Yes. Yeah. Why don't we consider.

166
00:20:39,100 --> 00:20:42,940
The parents of the Beatles. So the question is, why don't we consider various debaters?

167
00:20:43,630 --> 00:20:48,400
So this has a lot to do with the frequentist perspective we're taking here.

168
00:20:48,850 --> 00:20:51,940
So betas are what we call population level parameters.

169
00:20:52,480 --> 00:20:56,020
They are fixed. They are considered fixed.

170
00:20:57,460 --> 00:21:00,460
So it is fixed, but it does not mean we know it.

171
00:21:00,560 --> 00:21:06,670
Right. So it's a fixed but unknown quantity. One population has one number.

172
00:21:07,670 --> 00:21:13,910
You follow that you will, if you consider it in that population, your dynamics will follow that particular data.

173
00:21:14,930 --> 00:21:21,950
While for B, you can see they are different in the beta because while it's a Roman letter,

174
00:21:22,070 --> 00:21:27,410
it's a English letter, it's usually represent some kind of a random quantity.

175
00:21:27,770 --> 00:21:35,719
And most importantly, it's indexed by I right. You have a B, I have a B and that may be different, but for the population level,

176
00:21:35,720 --> 00:21:42,590
you and I share the same beta, so there's no variability in that sense for Bayesian in the audience.

177
00:21:42,590 --> 00:21:48,890
I know some of you are. Then clearly there are lots of nothing is fixed.

178
00:21:48,980 --> 00:21:52,310
That's what you can say in a Bayesian paradigm.

179
00:21:52,520 --> 00:21:56,780
Everything can be assigned with a distribution, be it a prior or hyper priority.

180
00:21:56,780 --> 00:22:02,990
Well, but I'm not going there because I don't want to confuse more people if we don't need to.

181
00:22:03,200 --> 00:22:11,690
So for the for the purpose of this lecture, let's consider betas as population quantities so they're fixed but unknown.

182
00:22:12,770 --> 00:22:21,590
This is the most familiar context you have encountered in 650 and 650 1b2 ip 3ib why these are indexed by I.

183
00:22:22,160 --> 00:22:28,940
So inherently we are assuming it is possible that b two eyes say can be different across people.

184
00:22:30,470 --> 00:22:37,000
So we have barons and their barons as it is across population.

185
00:22:37,010 --> 00:22:43,370
So just to to follow follow your lead. I'm going to say that it is conditional all the betas.

186
00:22:43,970 --> 00:22:49,730
Hopefully it is more satisfying for for some of you here.

187
00:22:52,540 --> 00:23:01,450
Okay. Question. Good question. So we fit in model and we can see that these are the estimates.

188
00:23:01,900 --> 00:23:06,820
And if you focus on the this term, it basically is beta three, right?

189
00:23:08,230 --> 00:23:10,220
So if you are going to if we're going to use a wall,

190
00:23:10,240 --> 00:23:18,520
the statistic which is to divide the estimate by the center and we have a Z estimate of Z value of 8.98,

191
00:23:18,820 --> 00:23:24,280
it is pretty large value and you can claim this is significant term.

192
00:23:25,300 --> 00:23:33,070
So this rejects null that a single line would characterize both the pre and post menarche trends.

193
00:23:33,190 --> 00:23:40,270
So we do need beta three next. We do have some of the model estimate in the mixed model.

194
00:23:42,150 --> 00:23:45,450
These are the estimate of the matrix called G.

195
00:23:45,630 --> 00:23:52,650
And if you recall, G essentially is the variance covariance matrix of the random effects.

196
00:23:52,650 --> 00:24:01,880
In our context, we have three, right? I'm just writing down the definition.

197
00:24:15,420 --> 00:24:22,770
There we go. Now, what this table shows essentially are these quantities.

198
00:24:24,800 --> 00:24:32,390
Okay. We will return to these when we're doing calculations, especially about the variabilities of the rate of change before and after menarche.

199
00:24:32,750 --> 00:24:40,140
Let's look at the final one. Can you guys recall what that's what that term is? This one.

200
00:24:48,030 --> 00:24:52,980
Yeah. The answer is simple. It's basically the measurement error burns.

201
00:24:54,480 --> 00:24:59,040
So he produced that estimate as well. And if you look at title here, it says Ramo.

202
00:24:59,490 --> 00:25:05,560
Okay. So. My question is what does it mean to do a remo in the mixed models?

203
00:25:06,670 --> 00:25:11,150
Well, you have done. General.

204
00:25:11,360 --> 00:25:14,660
Sorry, general. Any model with the correlate errors.

205
00:25:15,920 --> 00:25:20,610
You have learned grammar in that context. But what does it mean to do Rambo here?

206
00:25:24,030 --> 00:25:34,380
So I'm going to describe in general a high level way, but it should not be surprising that it basically is subsumed in what you've learned.

207
00:25:35,350 --> 00:25:38,590
Okay. So in genomics models, right. You have y i.

208
00:25:39,650 --> 00:25:43,670
Equals XY beta plus z i.

209
00:25:43,820 --> 00:25:47,300
B plus epsilon i.

210
00:25:48,830 --> 00:25:53,360
This is what we have. We have also expected value of VI.

211
00:25:56,050 --> 00:26:01,780
Equals zero. We have expected value addition y equals zero.

212
00:26:02,560 --> 00:26:14,470
We also have the independent of oh sorry B by independent Z and by independent of the errors and errors independent of the.

213
00:26:15,340 --> 00:26:19,000
Exercise. Right. So you have all these distributional assumptions.

214
00:26:24,990 --> 00:26:28,170
The reason why I can write it now so quickly is because they are so standard.

215
00:26:28,200 --> 00:26:35,520
So when you are when you have to do longitudinal research or these kind of research, you sort of it will come naturally to you.

216
00:26:35,730 --> 00:26:39,030
But I think my point is not about all these assumptions.

217
00:26:39,030 --> 00:26:44,669
These are important. But my point is that these models were conditioning upon by.

218
00:26:44,670 --> 00:26:51,670
Right. For example, we can ask what's the model for the.

219
00:26:54,210 --> 00:27:00,520
So this one is. We know it is this one, right?

220
00:27:06,090 --> 00:27:09,830
If you condition and by then you average out the errors.

221
00:27:09,840 --> 00:27:18,260
Errors have zero mean. So you got that term. Now return to the context where you learned the Remo.

222
00:27:21,300 --> 00:27:26,360
Did you learn random effects when you learn, Remo? I don't think so.

223
00:27:27,230 --> 00:27:34,430
Right, because that was pretty early. I was still discussing multiyear goals and density, so I don't think you have learn random effects at that time.

224
00:27:34,910 --> 00:27:38,990
So we couldn't have talked about condition by right by was not in use.

225
00:27:39,530 --> 00:27:45,140
So we were only talking about this thing. Right. So how do you capture this thing then?

226
00:27:45,770 --> 00:27:50,309
Well, this time basically is using well, based on the iterated expectation.

227
00:27:50,310 --> 00:27:58,710
Yeah. I don't need to repeat this many times, but I think this is on by.

228
00:27:58,730 --> 00:28:05,670
This is on the Y. Okay. So this term, this literally expectation will give you outside beta.

229
00:28:06,110 --> 00:28:13,370
Oh, sorry. Let me make sure that I scroll to the right place so I don't hit the scroll bar.

230
00:28:14,210 --> 00:28:22,220
Okay. All right. So this is the result and this should not be surprising again to you because exit beta is added

231
00:28:22,220 --> 00:28:28,310
to the FBI and by has a zero and z multiply mean zero quantity is going to have a mean zero.

232
00:28:28,850 --> 00:28:35,390
So this is a mean structure, right? So when you look at this, hey, this essentially is what you learn when you do the general and in your model.

233
00:28:36,470 --> 00:28:40,230
Not generalized, but general. General, you know, model.

234
00:28:40,250 --> 00:28:43,580
Okay. But what's the veterans cover and structure again?

235
00:28:44,060 --> 00:28:49,310
At that time when you were learning the memo, you couldn't have learned, at least from me, the random effects.

236
00:28:49,700 --> 00:28:56,600
So we would have would need to specify the variance, cover and structure, but only condition upon exi.

237
00:29:02,770 --> 00:29:08,470
Okay. So how do you calculate this? We have done this in handouts oh seven.

238
00:29:08,800 --> 00:29:12,160
If you forget, this is a perfect indication that you need to review this.

239
00:29:12,640 --> 00:29:16,060
Okay. For the sake of time, I'm just going to write down what it is.

240
00:29:16,060 --> 00:29:40,700
I think you'll be familiar. So this is the formula.

241
00:29:40,700 --> 00:29:45,500
When you assume when you assume R equals Sigma Square and I here.

242
00:29:46,820 --> 00:29:52,090
Okay. So.

243
00:29:53,670 --> 00:30:04,230
In summary, if you look at term one and two, they should have provided you with a specification of general model when you started this class.

244
00:30:04,950 --> 00:30:13,640
No random effects. Just how the mean. She look like the covers house cover related album that's turn number one.

245
00:30:15,170 --> 00:30:22,710
And how the variance would be. You know, involves it involves no random effects.

246
00:30:24,080 --> 00:30:30,050
So if you look at here, you know the term a zig zag transpose plus sigma squared.

247
00:30:30,980 --> 00:30:34,850
What are they? Well, here you have the parameters about BS, right?

248
00:30:35,570 --> 00:30:44,480
Sorry. G's. Do you want 1g2 to g33g, one to G one, 3 to 2, three what have you.

249
00:30:44,520 --> 00:30:47,890
Yeah. And here it's another unknown.

250
00:30:48,050 --> 00:30:51,250
It's the error of the measurement error. Right.

251
00:30:51,710 --> 00:30:58,850
So you can see here, although this term looks very specialized because we started from the Intermix model, but after all,

252
00:30:58,880 --> 00:31:07,070
conceptually it is a variance covariance matrix only condition on the covariates you observed and they are parameterized by one,

253
00:31:07,070 --> 00:31:11,900
two, three, four, five, six, seven parameters. These, these are these matrix.

254
00:31:12,560 --> 00:31:16,160
You should know exactly what they are, so they are considered as fixed.

255
00:31:17,030 --> 00:31:22,730
All right. So long story short, because Remo can give you.

256
00:31:24,330 --> 00:31:32,410
A less biased estimate of the variance parameters. It is going to give you less biased estimate of all these GS and sigma squares.

257
00:31:33,440 --> 00:31:41,060
Right. This is basically how Remo is conducted in the context of an economics model.

258
00:31:43,000 --> 00:31:49,420
But there are differences. I believe when we were talking about Remo, the function we use, what goals?

259
00:31:49,460 --> 00:31:56,430
Write goals. But can you specify a various command structure like this easily enjoys?

260
00:31:58,360 --> 00:32:01,270
Maybe you can, but. I don't think I use that ever.

261
00:32:02,350 --> 00:32:14,070
So in general, the LME function in mixed model packages will be able to fit these models because these various governance structures are specialized.

262
00:32:14,800 --> 00:32:18,370
So. That's all I want to talk about.

263
00:32:18,460 --> 00:32:25,840
Just trying to say that to what you have learned earlier in terms of Remo subsumes this as a special case.

264
00:32:25,850 --> 00:32:47,190
So the algorithm applies to the setting as well. Now let's look at the results.

265
00:32:47,200 --> 00:32:52,450
Well, these are the estimates this week have their interpretations. Well, first, we have discussed this, right.

266
00:32:52,470 --> 00:32:59,620
This basically is whether you reject or fail to reject the null beta three equals zero.

267
00:32:59,900 --> 00:33:10,810
It turns out that we reject outright. Second, the variability from girl to girl in the rates of fat accretion.

268
00:33:10,840 --> 00:33:17,260
Many girls are losing body fat, while others are gaining body fat during the primary menarche period.

269
00:33:18,160 --> 00:33:22,510
So let's do this exercise. Let us calculate.

270
00:33:24,030 --> 00:33:28,680
OC. What's the variance of B to II?

271
00:33:29,700 --> 00:33:33,990
By calcul I mean in this case, just get get numbers from the slides.

272
00:33:39,220 --> 00:33:43,240
So I'm going to do this with you. But for the second one, you're going to try this.

273
00:33:43,990 --> 00:33:56,760
So for the. Variability of the slopes before when it will be the viability of various variants would be too high.

274
00:33:56,790 --> 00:34:01,210
So it is the term here. So the estimate is this.

275
00:34:01,990 --> 00:34:13,510
All right. But now. If I want you to give me a 95% interval for the rate of change of the body fat percentage.

276
00:34:14,540 --> 00:34:20,190
Before Menarche for all the girls. How would you do that? Well first.

277
00:34:21,390 --> 00:34:27,420
You have to remember that Be2 represents a person's deviation from the population rate of change.

278
00:34:27,630 --> 00:34:36,680
Right? So if your goal is trying to calculate a 95% interval for the rate of change across people in the population,

279
00:34:37,130 --> 00:34:42,410
you've got to centered around the population rate of change which is beta two out, right.

280
00:34:45,720 --> 00:34:53,990
And. The random effects estimate says around this term, we have a variability of what?

281
00:34:55,040 --> 00:35:00,090
Of. Well, how about sanitation?

282
00:35:01,490 --> 00:35:04,660
Square root of 1.63119.

283
00:35:12,610 --> 00:35:16,450
Standard deviation of b2i.

284
00:35:17,590 --> 00:35:21,879
It is estimated to be this one. So now in the population,

285
00:35:21,880 --> 00:35:31,810
you want to sort of figure out what's this range that characterizes a plausible 95% interval for the rate of change in the population?

286
00:35:31,840 --> 00:35:37,790
Three Menarche. What you would need to do is just to figure out what are the lower and upper bounds.

287
00:35:37,990 --> 00:35:46,370
Right. So the lower bound will be two had -1.9, six times 1.9311.

288
00:35:47,480 --> 00:36:01,020
The upper one will be. Later to the population estimated rate of change plus 1.6 times the 1.6311.

289
00:36:02,430 --> 00:36:12,700
Right. So from the prior slide, you know, this number is estimated at two is estimated to be .42.

290
00:36:12,720 --> 00:36:16,720
How about that point for two? You plug that in?

291
00:36:18,740 --> 00:36:29,180
And then what you get will be. Minus point oh 9% to 2.92%.

292
00:36:31,460 --> 00:36:41,900
Okay. So essentially you are just investigating what's the distribution of what's the estimated distribution of this thing?

293
00:36:43,950 --> 00:36:48,069
Right. It centers at Baylor, too. And it is.

294
00:36:48,070 --> 00:36:54,030
It's. Stand deviation is characterized by the deviation of B to II.

295
00:36:54,920 --> 00:37:00,020
So based on this number, you can see that some girls experience decrease in body fat free menarche.

296
00:37:01,370 --> 00:37:07,660
Some girls experience increase in body fat from menarche. And you can see it's pretty symmetric.

297
00:37:07,870 --> 00:37:13,749
So that's why we say there is a lot of variability from girl to girl in the rates

298
00:37:13,750 --> 00:37:18,330
of fat accretion and many girls are losing body fat while others are gaining.

299
00:37:19,360 --> 00:37:27,010
So in the exam or in the questions I may be asking, I will not give you the exact term you will need to calculate.

300
00:37:27,310 --> 00:37:38,770
But I would ask, Hey, please provide me a 95% of all of the rate of change prime a.k.a.

301
00:37:38,890 --> 00:37:42,700
So you should look at the model and figure out what are the terms that are relevant.

302
00:37:42,940 --> 00:37:50,150
So we'll give you all the numbers that the model outputs provided and you need to find the right number and do this calculation.

303
00:37:50,170 --> 00:37:54,040
So I think this will be important note to remember.

304
00:37:55,770 --> 00:38:00,180
Now. Let's do what? But I mean, you do the.

305
00:38:03,070 --> 00:38:08,930
Yeah. Well, this has had the variability of. The slope of anarchy.

306
00:38:08,930 --> 00:38:18,830
And your goal is trying to provide your goals, trying to provide the 95% interval.

307
00:38:21,500 --> 00:38:26,900
For the rate of change.

308
00:38:28,010 --> 00:38:31,480
Sorry for the rates of change. Among.

309
00:38:34,200 --> 00:38:38,650
The girls. Post.

310
00:38:39,960 --> 00:38:43,640
Minority. Yeah.

311
00:38:43,650 --> 00:38:47,420
What's that interval? What's the lower bound? What's the upper bound?

312
00:38:48,080 --> 00:38:52,790
And you can see. You can see whether they cross a zero.

313
00:38:52,790 --> 00:38:59,930
And so now your task. I'll give you 2 minutes to do this.

314
00:39:00,410 --> 00:39:06,140
Your task is trying to find these two numbers that I have marked by question mark.

315
00:39:06,830 --> 00:39:10,550
The hand is not exactly answer. Okay, but it will be relevant.

316
00:39:11,950 --> 00:39:16,210
So what should I do? Should I scroll to numbers or should I just keep it here?

317
00:39:17,670 --> 00:39:21,040
Maybe I should. Zoom at.

318
00:39:22,690 --> 00:39:27,700
This is the part where it's a bit difficult. Is it okay?

319
00:39:27,700 --> 00:39:31,720
I just do this so that at least you can see the numbers.

320
00:39:32,170 --> 00:39:38,770
But the question should be, what's the 95% interval for the rate of change post menarche across the girls,

321
00:39:40,240 --> 00:39:43,240
you need to provide the lower and upper bound of that interval.

322
00:39:57,510 --> 00:40:01,890
Yeah. Feel free to discuss if you want. It's supposed to be an exercise.

323
00:42:08,450 --> 00:42:11,790
All right. So. Do you guys need?

324
00:42:11,790 --> 00:42:15,530
One more minute. Let's do one moment.

325
00:43:18,070 --> 00:43:22,000
All right. Let's try to work all this together.

326
00:43:22,900 --> 00:43:26,260
So. First.

327
00:43:26,260 --> 00:43:36,809
What's the quantity we're going to study? Well, it is paid to two plus beta, three plus B two, II plus B three.

328
00:43:36,810 --> 00:43:40,990
I right. We have discussed this when we were visualizing the curves.

329
00:43:41,590 --> 00:43:47,520
So for a subject I this is her. Rate of change post menarche.

330
00:43:48,430 --> 00:43:53,320
And because the final two terms B to IP three are different across people.

331
00:43:53,680 --> 00:44:02,010
So there will be variabilities, right? So you need to figure this figure out that this term is shared across all the people.

332
00:44:02,980 --> 00:44:09,400
So all the variation is around this particular term. When you are using the model for the results, you got to plug in the estimate.

333
00:44:14,480 --> 00:44:18,110
Plus or minus the variability around this estimate.

334
00:44:18,500 --> 00:44:22,520
Right. Because we were asking for 95% interval.

335
00:44:23,420 --> 00:44:28,220
So you got to figure out what's the variance of the second.

336
00:44:29,890 --> 00:44:33,420
Set of. Some B two IP, three II.

337
00:44:33,640 --> 00:44:40,750
So it should be 1.96 times the square root of the variance of b2i.

338
00:44:40,750 --> 00:44:45,050
Plus b3i. So this ought to be the answer.

339
00:44:45,560 --> 00:44:53,300
Okay. Now, let me say it again. This is the estimated population level rate of change.

340
00:44:53,600 --> 00:45:03,660
Individual people are going to vary around that. And because we assumed that it felt the B's be to I plus e three I's.

341
00:45:03,680 --> 00:45:10,140
These are Gaussian distributed. We're going to use a classical number 1.96.

342
00:45:10,160 --> 00:45:18,310
What does 1.6 mean? Anybody. 97.5% control over girls and standard girls in distribution.

343
00:45:18,970 --> 00:45:30,190
So that will give you a central mass of 95%. And this basically is the estimated standing ovation of B to II plus B three II.

344
00:45:30,610 --> 00:45:33,520
Now, your job is trying to figure out what are these numbers.

345
00:45:34,540 --> 00:45:40,840
So B two, I had A and B three, how hard you can just get from the table number one, which is on page 25.

346
00:45:41,740 --> 00:45:51,490
And for the same time, I'm going to assume you can do that. So this is 2.46 plus one time one and six times this time.

347
00:45:52,450 --> 00:45:57,430
So what is in this thing? Well, it is three times.

348
00:45:57,700 --> 00:46:04,660
Okay. It is the variance estimate of b2i plus the variance estimate of B three.

349
00:46:05,620 --> 00:46:12,370
And one final little [INAUDIBLE] plus two times the covariance estimate of b two.

350
00:46:12,370 --> 00:46:17,080
ib3. I don't forget this term ever. Meaning X models.

351
00:46:18,490 --> 00:46:22,640
This can cause a lot of problems. Ignore that. So what are these terms?

352
00:46:22,660 --> 00:46:25,690
Well, this is what this is. Basically.

353
00:46:27,900 --> 00:46:33,380
1.63119. How about this term?

354
00:46:34,040 --> 00:46:38,330
This basically is this one way. What is this?

355
00:46:39,110 --> 00:46:42,650
2.75. Let's say. How about here?

356
00:46:45,970 --> 00:46:51,130
They say there's a two times the covered investment, which is this one right between the two and B three.

357
00:46:56,340 --> 00:46:59,850
So if you sum up all these numbers together, what you will get is.

358
00:47:04,510 --> 00:47:17,460
How should I do this? So, uh. Okay.

359
00:47:17,480 --> 00:47:20,720
So this is what you would get if you follow this calculation.

360
00:47:21,650 --> 00:47:30,050
And if you expand this, this will be an interval of 0.62% to 4.3%.

361
00:47:31,280 --> 00:47:35,570
So this interval is will be the answer. If I asked you this in midterm.

362
00:47:35,780 --> 00:47:41,250
These are the two numbers you need to choose. And what does this mean?

363
00:47:41,430 --> 00:47:45,420
Well, it just means that we do not see, at least with height.

364
00:47:46,140 --> 00:47:50,460
We see for most of the people, the rate of change seems to be positive,

365
00:47:50,670 --> 00:47:56,250
indicating that the fat percentage seems to be increasing over time after menarche.

366
00:47:57,660 --> 00:48:03,960
Okay. So this is how we interpret the results from a mixed model.

367
00:48:05,460 --> 00:48:12,780
Uh. I'm going to pause again for 30 seconds just to see if you have any questions.

368
00:48:23,060 --> 00:48:27,470
And we can also talk about correlation, but this one essentially is the.

369
00:48:39,180 --> 00:48:43,850
Z. G. G. T. Plus Sigma Square.

370
00:48:43,860 --> 00:48:51,560
I. One, two, three, four, five, 6789959.

371
00:48:52,360 --> 00:48:56,420
And Z here is basically. Um.

372
00:49:00,800 --> 00:49:04,930
1111. One, two, three.

373
00:49:07,380 --> 00:49:11,230
Four. One, two, three, four, five, six, seven, eight.

374
00:49:11,980 --> 00:49:16,390
Okay. I'm very bad at structuring this. Let me do this again. So.

375
00:49:19,550 --> 00:49:23,150
111111. Okay.

376
00:49:27,440 --> 00:49:30,470
One, two, three, four, five, six, seven, eight, nine. Okay.

377
00:49:30,560 --> 00:49:35,270
And then it is minus four. One, three, minus two, minus one.

378
00:49:36,520 --> 00:49:42,880
Zero. 1234000001234.

379
00:49:44,660 --> 00:49:52,640
And you plugging all the housing here. So that's how you got this estimate of marginal covariance and clearly you can normalize it to get correlation.

380
00:49:53,430 --> 00:50:03,320
Um, so actually this answers one question I raised maybe in the previous lecture or two lectures before.

381
00:50:03,980 --> 00:50:07,309
Nobody has measurements exactly occurred at time.

382
00:50:07,310 --> 00:50:10,440
Minus four, minus t minus two, minus one zero.

383
00:50:11,300 --> 00:50:22,980
But we still can. You plug in an artificial design matrix and calculate the covariance, right?

384
00:50:24,630 --> 00:50:31,860
So that is that is what I meant by ILO sample calculation of our marginal covariance.

385
00:50:32,400 --> 00:50:43,360
Or if you normalize it, there will be a marginal correlation. Now, again, in this example, let's look at the shrinkage estimate.

386
00:50:43,450 --> 00:50:51,010
I'm not going to ask the calculator. All your focus is directed towards how similar individual blobs are to the population curve.

387
00:50:51,550 --> 00:50:59,470
So the solid satellite in the center is the estimated population trend and we show two random people.

388
00:51:00,100 --> 00:51:06,310
One person's raw data is showing triangles. The other person's data is shown in the empty circles.

389
00:51:06,850 --> 00:51:11,530
As you can see, that's the person represented by all the empty circles.

390
00:51:12,490 --> 00:51:16,750
Her blob seems to be more similar to the population trend.

391
00:51:16,780 --> 00:51:22,980
Why is that though? First, the person at the bottom has fewer data points.

392
00:51:24,020 --> 00:51:32,120
Number two, the second person seems to have a lot of variability in the measurement, right, when the measurement is lousy for that person.

393
00:51:32,510 --> 00:51:37,670
You've got to trust the population because your own data is not going to be so precise.

394
00:51:38,670 --> 00:51:44,280
In contrast, if you look at the person at the top, you can see that this person first has more data points.

395
00:51:44,610 --> 00:51:52,080
So she can be more confident in saying, I don't care about population, I'm just going to use my own data to predict my own trajectory, you know?

396
00:51:53,520 --> 00:52:02,720
Second, as you can see, the variability of the triangles around the top, broken lines to be a bit smaller.

397
00:52:02,910 --> 00:52:07,170
So the measurement error seems to be less of the problem for the person at the top.

398
00:52:07,890 --> 00:52:15,240
So clearly that means we do not need to worry a lot about the person that the data for the person at the top.

399
00:52:15,510 --> 00:52:20,550
So we can use more we can we can use less information from the population to do the prediction.

400
00:52:24,390 --> 00:52:35,850
Here it is showing you the in the very vague line, the ordinarily square estimate of each person's data.

401
00:52:35,940 --> 00:52:39,900
So this vague line, it's just using the triangles to fit the line.

402
00:52:40,080 --> 00:52:45,090
You can do that because you can fit a ordinary squares using nine data points.

403
00:52:45,240 --> 00:52:50,880
No problem. You can do another separate fit using the empty circles.

404
00:52:51,030 --> 00:52:59,520
Right. And again, this shows that if you contrast the difference between the vague line and.

405
00:53:00,750 --> 00:53:04,350
The shrunken line you can see for the person at the bottom.

406
00:53:05,920 --> 00:53:10,330
There is a much larger difference between the line obtained by this person along.

407
00:53:11,640 --> 00:53:15,030
Versus the lying obtained by the shrinkage.

408
00:53:15,510 --> 00:53:25,230
So this again is saying there are some, you know, borrowing of information happening for the person at the bottom.

409
00:53:26,960 --> 00:53:32,570
Causing the two lines, the big lie and the shrunken line to be quite different.

410
00:53:35,150 --> 00:53:38,540
Finally, some quick remarks to conclude this slide.

411
00:53:39,380 --> 00:53:40,390
Sorry, this handout.

412
00:53:42,350 --> 00:53:50,299
First, in this example, we have shown that mixed effects models can be used to obtain estimates of each girl's growth trajectory over time,

413
00:53:50,300 --> 00:53:54,260
because we can estimate their B's that the random effects.

414
00:53:56,510 --> 00:54:01,940
Number two, you can see from the previous example, if two people differ in number of measurements,

415
00:54:02,480 --> 00:54:08,180
if two people defers in their measurement errors, there could be different degrees of shrinkage happening for different people.

416
00:54:09,470 --> 00:54:17,180
And this is a, again, characteristic feature of the shrinkage method.

417
00:54:18,080 --> 00:54:25,490
And basically it is a borrowing of information happening for a person who has accurate more data points,

418
00:54:25,850 --> 00:54:27,950
less borrow information from other people as needed.

419
00:54:28,370 --> 00:54:36,230
But for a person who has few data points and very noisy measurements, and that person's trajectory prediction needs to rely on other people's data.

420
00:54:37,340 --> 00:54:41,420
So the final point is that there is a paper called The Blob is a good thing.

421
00:54:42,230 --> 00:54:45,950
I think I mentioned this sometime. This is a good paper to read.

422
00:54:46,400 --> 00:54:56,000
If you are curious about this line of research and if you are going to do a longitudinal data analysis, I sort of feel this is a must.

423
00:54:56,240 --> 00:54:59,510
So there is no escape from this. I'm sorry to say that.

424
00:55:00,830 --> 00:55:06,690
Anyway, that concludes the handout also, and we will return after 5 minutes to review the generalized,

425
00:55:07,480 --> 00:55:10,580
generalized meaning models, which should be something you are familiar with.

426
00:55:11,750 --> 00:55:16,520
We will see you. And at 402.

427
00:55:55,800 --> 00:55:59,920
Yeah. Yep.

428
00:56:00,230 --> 00:56:03,920
Oh, yeah. No. Well, I.

429
00:56:06,470 --> 00:56:10,250
Okay. So you compare the blue line and the red line, which one is shallower?

430
00:56:14,670 --> 00:56:18,030
Yeah. Sorry. You just I.

431
00:56:18,390 --> 00:56:23,160
Yeah. Yeah, but how. How do you draw it this way?

432
00:56:24,120 --> 00:56:31,900
Yeah, I'm just. It's a random person. Yeah. So, like, so you just randomly draw.

433
00:56:32,980 --> 00:56:37,420
Well, this is just a random person, so I just go random line. Clearly, you can draw a steeper line here.

434
00:56:37,720 --> 00:56:41,500
Okay, so I can draw. Yeah, of course. For another person.

435
00:56:41,730 --> 00:56:45,639
Something positive? Yeah. Okay. Yeah. I just feel like.

436
00:56:45,640 --> 00:56:48,910
Oh, how do you know? It's like this person's like.

437
00:56:49,360 --> 01:00:35,630
It could be. Yeah. Sorry. I should be more clear. Okay, everybody.

438
01:00:35,750 --> 01:00:39,010
So. Okay, everybody, can you.

439
01:00:39,070 --> 01:00:42,980
Can you guys hear me? Sure. Okay, now. All right.

440
01:00:45,150 --> 01:00:52,560
Let's get back to work. And we have about 80 minutes to cover as much as we can for the generalized models.

441
01:00:52,920 --> 01:00:56,400
Again, this is just a summary of what you've learned in 651,

442
01:00:57,540 --> 01:01:08,220
because I assume that there are lots of technicalities you probably have forgotten or so I'm just going to review the key concepts.

443
01:01:09,240 --> 01:01:14,160
We will not talk about like estimation techniques, but primarily just about model formulation.

444
01:01:16,890 --> 01:01:21,480
So why was generalized model necessary?

445
01:01:21,750 --> 01:01:27,020
First, it is because obviously we have responses that not a continuous right.

446
01:01:27,030 --> 01:01:33,350
So far we have been talking about near makes models, medium models and those are treating the outcomes as continuous.

447
01:01:33,360 --> 01:01:38,550
We have many non continuous outcomes and longitudinal studies is not an exception.

448
01:01:38,800 --> 01:01:47,820
We have binary data and for example you may represent the presence, absence of certain respiratory illness and it may be nominal.

449
01:01:48,240 --> 01:01:53,030
What does it mean? But nominal? Just categorical and not ordered, but category.

450
01:01:53,040 --> 01:02:04,559
Right? So you may say that's the type of transportation for commuting is a nominal or categorical, if you will, car, bus, cycling, walking.

451
01:02:04,560 --> 01:02:09,060
You can not really, you know, put an order to it.

452
01:02:09,720 --> 01:02:11,760
Three counts, for example,

453
01:02:11,760 --> 01:02:20,040
the number of epileptic seizures in the four week interval or the number of fish that your family car during the summer trip.

454
01:02:20,040 --> 01:02:26,040
And you can relate that to whatever other coverage you want to have or you know, for example,

455
01:02:26,040 --> 01:02:33,330
in survey responses like you have those like herd scale responses strongly agree, agree, not neutral disagree and strongly disagree.

456
01:02:34,830 --> 01:02:44,399
So the framework of generalized Nina model is trying to provide a basis for dealing with these long, continuous responses.

457
01:02:44,400 --> 01:02:48,660
And our extensions will build upon the generalized New Year model.

458
01:02:49,440 --> 01:02:56,459
I must say that it is a little bit unfortunate that the naming of many models are going to produce the same acronyms.

459
01:02:56,460 --> 01:03:03,330
Right, Joanne? Well, gender role models is also called GLA, but they are totally different thing.

460
01:03:06,940 --> 01:03:12,129
So generalized is trying to generalize from continuous to non.

461
01:03:12,130 --> 01:03:20,970
Continuous. General role is trying to. Generalize from independent observations to depend on observations.

462
01:03:21,360 --> 01:03:47,740
So you got to make that distinction. Disclaimer This lecture assumes that you have taken a biostatistics 51 equivalent.

463
01:03:48,720 --> 01:03:53,070
Graduate level class on Johns for independent observations.

464
01:03:53,280 --> 01:04:00,360
It's, of course, prerequisite. If you are adventurous and have not taken this class, you probably will find it a struggling.

465
01:04:02,150 --> 01:04:03,380
In the second part of this class.

466
01:04:04,250 --> 01:04:14,240
It is intended for this panel to be brief but rigorous in self complete review of the key components of alarms under independent observations.

467
01:04:14,990 --> 01:04:21,680
Independent observations and the graphs and materials will be handy for learning longitudinal extensions.

468
01:04:22,250 --> 01:04:28,190
So after this particular handout, we will assume that we are on the same page for generalized models.

469
01:04:28,520 --> 01:04:37,220
Clearly you have the 651 materials to help you review the necessary technical components, but I will not do that too much.

470
01:04:37,310 --> 01:04:43,340
I will assume that you have got a good grade from that class and be able to carry out the review.

471
01:04:45,970 --> 01:04:50,800
But I'm confident that these slides will help you in even reviewing those 651 slides.

472
01:04:51,610 --> 01:04:58,239
Three Learning Objectives. Number one, we got to describe the three parts of the specification.

473
01:04:58,240 --> 01:05:05,380
For generalized models. There are random component, systematic component.

474
01:05:06,550 --> 01:05:11,200
A link function, by the way, should I say, component or component, which is the right way?

475
01:05:13,120 --> 01:05:17,710
I am looking at native speakers here. I think the first component.

476
01:05:18,390 --> 01:05:21,480
Component, yeah, but I was watching Steve Jobs video.

477
01:05:21,490 --> 01:05:25,270
He was saying components anyway. Okay.

478
01:05:26,560 --> 01:05:29,560
All right. So. Component.

479
01:05:29,990 --> 01:05:40,720
All right. Number two, we're going to describe the in detail the generalized models for binary and account responses under canonical link functions.

480
01:05:41,230 --> 01:05:49,480
And I am sure some of you have forgotten what canonical link function is and you're forgiven

481
01:05:50,230 --> 01:05:56,890
because that's a totally technical term and we will not be sort of dwelling on that too much.

482
01:05:57,550 --> 01:06:04,330
But you at least need to know why people have decided to work with canonical link functions.

483
01:06:05,310 --> 01:06:12,600
One benefit is that they produce the most commonly used interpretations of regression coefficients in Japan.

484
01:06:13,680 --> 01:06:24,120
So that's objective to object. Number three, after you have finished this slide, combined with your review of your 651 slides,

485
01:06:24,540 --> 01:06:27,840
you ought to be able to explain what over dispersion is.

486
01:06:28,170 --> 01:06:34,590
Okay, and what are the potential causes, what are the consequences, and what are the approaches to address them?

487
01:06:35,910 --> 01:06:40,930
So worst case, you forget everything here. Best case, you know, everything.

488
01:06:41,680 --> 01:06:46,780
But I think to me it is really important that even if have forgot some of it,

489
01:06:47,110 --> 01:06:50,679
we will try to build it up when we're introducing launching an extension.

490
01:06:50,680 --> 01:06:58,930
So you still have plenty of time to review those terms, but whenever you have questions, please do not hesitate to post mpesa or ask us.

491
01:06:59,380 --> 01:07:04,000
I think that both myself and the GSEs are very happy to, um,

492
01:07:04,390 --> 01:07:10,450
to get you back to track or to sort of reactivate the part of your brain that stores that knowledge.

493
01:07:11,710 --> 01:07:21,830
So three objectives and we'll go through them one by one. Part one conceptual preview of the longitudinal extension to Julian.

494
01:07:22,400 --> 01:07:31,760
So this part is not exactly a review of La Jolla, but rather I need to give you a reason to focus on the review of generalized models.

495
01:07:32,060 --> 01:07:39,740
What is? Why bother? We do this. So it turns out that it is a good framework for us to build extensions.

496
01:07:39,740 --> 01:07:54,080
And there are a few extensions. First, the reason why two alarms are important is because it unifies as many different causes of regression models.

497
01:07:54,100 --> 01:08:03,490
It subsumes the largest regression plus regression. Cumulative logit regression, many different regressions into the same umbrella.

498
01:08:03,900 --> 01:08:09,530
You know, that's what's beautiful. You know, you unify everything to the same theory.

499
01:08:12,050 --> 01:08:21,960
Number two. It needs extension because all those things you learn in 651 are based on independent observation and assumption.

500
01:08:22,020 --> 01:08:30,810
As you have seen, we are singularly focused on the situation where observations from the same person might be correlated.

501
01:08:32,190 --> 01:08:41,020
So we have a good foundation that people have worked out in 1970s, Angela.

502
01:08:41,250 --> 01:08:47,520
So we need to use that. And second, we do see a gap that they did not deal with quality data.

503
01:08:50,480 --> 01:08:53,420
So in this spirit, what are the possible approaches?

504
01:08:53,960 --> 01:09:00,590
So in this clause, pretty much extending towards the end of this class, we will be talking about them in sequence.

505
01:09:01,400 --> 01:09:05,570
The first approach is what we call approach one, the marginal models.

506
01:09:05,780 --> 01:09:09,590
It has a neat name called G Generalized Estimating Equations.

507
01:09:10,530 --> 01:09:16,840
Excuse me. The second approach is called generalized genomics models.

508
01:09:17,380 --> 01:09:19,750
Okay, let's talk about the second approach.

509
01:09:19,930 --> 01:09:26,140
The second approach is what you just learn, you know, seven just with an additional term called generalized.

510
01:09:26,430 --> 01:09:32,250
Right. So as you can see, it will be pretty much analogous to what you've learned in a mixed model.

511
01:09:32,260 --> 01:09:38,820
But we just have to pay attention to the fact that we are now trying to deal with continuous outcomes there.

512
01:09:38,830 --> 01:09:44,350
We would need to deal with length functions and all sorts of nonlinearity, which will cause a lot of troubles.

513
01:09:45,250 --> 01:09:50,650
Okay. So it will have a similar flavor as indeed it makes model approach number one.

514
01:09:51,460 --> 01:10:04,330
That is the margin model. That is the model that specifies the expectation of why I give an EXI equals excited beta.

515
01:10:05,380 --> 01:10:09,010
And it will also specify the covariance of y given z.

516
01:10:10,240 --> 01:10:18,340
So actually I need to put a G here because you are now dealing with a possibly non continuous outcome.

517
01:10:19,120 --> 01:10:26,890
So if you have these two terms, you can conduct this technique called generalized estimating equations.

518
01:10:27,670 --> 01:10:30,070
Why? It's called estimating equations that will be clearer.

519
01:10:30,460 --> 01:10:38,590
But for now, only the only thing you need to know is that as long as you specify the variance and the main structure,

520
01:10:39,190 --> 01:10:44,410
you are set to perform this model. It is called marginal.

521
01:10:45,160 --> 01:10:49,750
Why do we call that marginal? Because you don't see a conditional on.

522
01:10:51,380 --> 01:10:54,920
You don't see a conditioning on by Ray. I am just crossing that out.

523
01:10:56,290 --> 01:10:59,470
You have average over the bye. So that's why it's called marginal.

524
01:11:02,220 --> 01:11:08,520
And another beautiful thing is that you don't have to make the assumption that ball bearings provides model.

525
01:11:08,520 --> 01:11:11,880
Correct. To be able to perform valid inference.

526
01:11:12,570 --> 01:11:20,010
So pretty much this is what people call robustness against specification of the variance governance structure.

527
01:11:22,450 --> 01:11:34,510
Approach one and two basically comprise the two major extensions of Sri Lanka, from independent observations to longitudinal observations.

528
01:11:38,090 --> 01:11:40,700
Both are built on jobs, so we need to review that.

529
01:11:46,080 --> 01:11:54,390
I think what I'm going to do is just skip these next two slides, eight and nine, because they are a little bit too nuanced.

530
01:11:54,990 --> 01:11:59,970
So let's go directly into the Jalan G lens itself.

531
01:12:00,780 --> 01:12:03,240
It is for independent observations,

532
01:12:04,290 --> 01:12:13,560
and you can consider that every person has a single univariate response and this response could be continuous or non continuous.

533
01:12:15,180 --> 01:12:21,880
It is widely applicable and it works with different data types.

534
01:12:21,900 --> 01:12:29,070
I will not repeat that. So notational wise we can see it is just following the convention.

535
01:12:29,070 --> 01:12:38,400
In 651 class you have a design matrix, you have outcomes and you need three parts of specification to deal with the two to specify jalan.

536
01:12:40,950 --> 01:12:47,760
Okay. So the first component, the distributional assumption, it is also called the random component.

537
01:12:47,820 --> 01:12:49,380
All right. It is called random component.

538
01:12:49,710 --> 01:12:58,270
For example, when you're assuming a distribution is Gaussian or Bernoulli or person, that's a random component because it involves a stochastic city.

539
01:13:01,510 --> 01:13:06,940
And it basically specifies a probable probable this a mechanism by which responses seem to be generated.

540
01:13:07,180 --> 01:13:16,630
Right. And, you know, Gaussians, Bernoulli's or Poisson, they are all in the family called exponential family.

541
01:13:17,450 --> 01:13:25,280
Um, exponential family is an extremely fundamental and popular costs of probability distributions,

542
01:13:25,940 --> 01:13:34,309
and it basically includes normal binary binomial special cases in that specification.

543
01:13:34,310 --> 01:13:41,450
Often we will have an additional term called PHI.

544
01:13:42,400 --> 01:13:48,820
Which is to incorporate the possibility of over dispersion.

545
01:13:54,280 --> 01:13:58,720
So if you drive in diagram. So this is all distribution.

546
01:14:09,690 --> 01:14:12,900
Oh. Distribution. Okay. And this is the.

547
01:14:14,020 --> 01:14:20,080
Exponential family. The inner circle.

548
01:14:20,620 --> 01:14:25,300
Okay. So there are panel bernoulli binomial on.

549
01:14:26,570 --> 01:14:35,120
And, uh, normal, whatever. And definitely there are distributions not exponential, not in exponential family right now.

550
01:14:35,330 --> 01:14:38,900
Anybody give me an example. That's a distribution, but not exponential.

551
01:14:39,140 --> 01:14:46,379
A distribution. At least No.

552
01:14:46,380 --> 01:15:02,450
Two, I think. A tea distribution.

553
01:15:03,790 --> 01:15:08,290
Or our coffee. Right. Coffee. These are not exponential family.

554
01:15:14,060 --> 01:15:15,890
I want to expand a little bit more on the variant.

555
01:15:15,900 --> 01:15:23,630
So here the variants of why essentially it's to say that how does this probability distribution family specify the variants of the outcome?

556
01:15:24,470 --> 01:15:29,840
It is comprised of two parts, the scale parameter and the variants function.

557
01:15:30,320 --> 01:15:33,590
So it might be a little bit confusing. Various functions, not variants.

558
01:15:33,920 --> 01:15:41,100
It is just how the terminology is have stuck. So you've got to realize that the entire thing is a variance, but the first thing is called scale.

559
01:15:41,130 --> 01:15:44,690
The second stage is called variance function. When the two things are multiplied together,

560
01:15:45,050 --> 01:15:51,590
you've got the variants and in the most commonly encountered set of distributions

561
01:15:52,100 --> 01:15:56,450
you can see for normal you have the variant function being one and the five,

562
01:15:56,450 --> 01:16:00,470
basically equal Sigma Square. Finally, you have five equals one.

563
01:16:00,800 --> 01:16:08,170
The person you have five equals one again. And the canonical link is at the end, but I'm not going to talk about that right now.

564
01:16:09,220 --> 01:16:18,010
So that's why I say that for certain distributions, FI is not a parameter that requires estimation like Bernoulli and for sunsetting.

565
01:16:18,340 --> 01:16:24,750
But for other distributions fies are known like in a Gaussian situation where phi is actually the sigma squared.

566
01:16:24,760 --> 01:16:29,470
And as you know, we do need to to estimate the sigma screening in a regression.

567
01:16:34,360 --> 01:16:42,640
And another prominent feature is that when you move from continuous responses to non continuous responses,

568
01:16:43,060 --> 01:16:46,570
the variance of the response often depends on the mean.

569
01:16:46,900 --> 01:16:50,290
For example, if you look at Bernoulli, right, if you look at this formula,

570
01:16:51,280 --> 01:16:56,380
the variance is basically mu times one minus mu and mu is a probability you have a success.

571
01:16:57,730 --> 01:17:05,710
Or if you call the heads of a coin to be success, that new is a number between zero and one success probability.

572
01:17:05,980 --> 01:17:10,090
So the variance is dependent upon the success probability and same thing for

573
01:17:10,090 --> 01:17:14,950
person which is famous for its property that the mean and the variance are equal,

574
01:17:15,190 --> 01:17:23,560
right? So the variance with depends on the mean. And we do not do this in Gaussian based in any regression, right?

575
01:17:23,590 --> 01:17:25,900
We do not assume the variance are dependent upon me.

576
01:17:30,520 --> 01:17:37,480
So that's why in the models, often the first set of models you encounter are models with the same variances.

577
01:17:38,350 --> 01:17:42,130
What's that name? What's that? What's the name of those kind of models?

578
01:17:45,440 --> 01:17:54,360
Homo statistics 80. And if you assume the variance is different, it's called a hero status to see, which is a crazy word.

579
01:17:54,370 --> 01:17:59,670
But you do know that there are assumptions to relax the same variances.

580
01:18:04,150 --> 01:18:10,480
So that's the first component, the random component, or equivalently, the distribution assumption.

581
01:18:10,900 --> 01:18:14,800
And that's where the normal Bernoulli binomial person comes in.

582
01:18:15,520 --> 01:18:19,210
Now, the second component, it's the call it what's called systematic component.

583
01:18:19,810 --> 01:18:24,070
After all, when you're doing a regression analysis, you want to be smart.

584
01:18:24,130 --> 01:18:32,710
You want to put in every explanatory variables into the model so that any changes in explanatory variables can explain the change in the response.

585
01:18:33,100 --> 01:18:37,870
We need a component for that, and that is called systematic component.

586
01:18:38,650 --> 01:18:43,810
Or you can call that a regression component, right? So here we in general denote this.

587
01:18:45,700 --> 01:18:50,080
So it I it's again a Greek number. It's individual specific.

588
01:18:50,560 --> 01:18:52,380
Why it's a Greek letter.

589
01:18:52,390 --> 01:19:01,600
Well it it is a combination of all the predictors and the weights of those predictors are betas, as you know, in the linear regression coefficient.

590
01:19:02,410 --> 01:19:04,630
Second, it is subject specific.

591
01:19:04,720 --> 01:19:13,720
ADA has index of I write your ETA may be different from mine y well betas are the same, but the covered values may be different.

592
01:19:14,850 --> 01:19:21,510
Age, gender, what have you. So Ada is what we call the systematic component.

593
01:19:24,330 --> 01:19:29,670
However, we're still just talking about constructing in a predictor.

594
01:19:29,670 --> 01:19:34,180
We have not related this to the distributional parameters of the outcome.

595
01:19:34,470 --> 01:19:39,960
So this is where the link function will come into play.

596
01:19:40,620 --> 01:19:46,170
So link function essentially is trying to connect the meaning of the outcome here.

597
01:19:46,470 --> 01:19:52,020
Well, at least in this class meal is often reserved for the mean of a random variable.

598
01:19:52,800 --> 01:19:56,880
So this equation says, okay, my response.

599
01:19:57,270 --> 01:20:01,320
The meaning of my response got to be explained by a set of keywords.

600
01:20:02,220 --> 01:20:11,190
But how do we do that? We say after you transform the mean by the function G That's fully explained by the linear combination here, right?

601
01:20:11,760 --> 01:20:18,290
So that's why we have this thing. And G here is called the drink function.

602
01:20:18,740 --> 01:20:23,000
Or rather, you can call this a transforming response.

603
01:20:25,050 --> 01:20:38,140
Okay. I want to return back to this previous slide just to talk about the medium need here.

604
01:20:39,490 --> 01:20:43,990
Whenever we talk about need here, it is never or in the covariance.

605
01:20:44,460 --> 01:20:48,010
Okay. It's in the betas, for example.

606
01:20:49,480 --> 01:20:58,930
These two these three are here, but these two are nonlinear because for the bottom two equations it depends on linearly.

607
01:21:00,520 --> 01:21:04,990
Again, I think they are lots of confusion regarding this term.

608
01:21:05,410 --> 01:21:09,030
For example. Now.

609
01:21:09,120 --> 01:21:11,310
Meteor, how to say this?

610
01:21:13,120 --> 01:21:21,520
Some people call, say polynomial regression to be nonlinear regression, but really that's linear regression in terms of the coefficient.

611
01:21:21,820 --> 01:21:31,120
It is nonlinear in terms of covariates. Okay. So I think by default, when you talk about need here, if I hear that, I hear that.

612
01:21:31,480 --> 01:21:37,270
I assume you're talking about Nina in beta unless you tell me, hey, Jim, I know you're going to pick on me.

613
01:21:37,810 --> 01:21:45,220
I'm going to say the nonlinear or Nina is in EPS, then we are on the same page again.

614
01:21:45,520 --> 01:21:48,850
To me, this is a problem with terminologies.

615
01:21:49,450 --> 01:21:57,340
And now, as you have felt that I always talk about these terminologies not because I want to show off how much things about terminology are wrong.

616
01:21:59,380 --> 01:22:02,650
It is just because you will encounter them and you will get frustrated.

617
01:22:02,740 --> 01:22:08,500
And until one person tells you that, you'll probably keep frustrated.

618
01:22:08,500 --> 01:22:18,250
So I hope this can be or some some approach to remove that mitigate that frustration.

619
01:22:19,720 --> 01:22:29,590
Okay. I am out of time. So let's continue on Wednesday with a little bit of this review and the actual course review.

620
01:22:30,250 --> 01:22:30,790
Thanks, everybody.

