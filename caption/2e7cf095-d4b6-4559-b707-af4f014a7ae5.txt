1
00:00:01,900 --> 00:00:05,760
All right. Good afternoon, everybody. So today we're going to start from Tempe.

2
00:00:06,130 --> 00:00:08,650
As I was looking at the handout ten,

3
00:00:08,780 --> 00:00:16,540
I think because Handout Tempe has a review part and pretty much I will just be repeating myself if I'm going to cover then Tempe.

4
00:00:16,870 --> 00:00:24,670
So I think it's good idea to start from Tempe and this is basically the second of a total of four handouts.

5
00:00:24,910 --> 00:00:34,420
It covers the generalized unit mix models. And after that, we'll have the example and some additional technical details.

6
00:00:34,690 --> 00:00:38,410
So why don't we get started? Oh, sorry.

7
00:00:38,440 --> 00:00:49,690
Just some logistics. For those of you who are curious about the course progress, it doesn't hurt to remind everybody where we are.

8
00:00:50,290 --> 00:00:58,300
We are here. So it is about interpretation of the coefficients and also the inference more generalized need to mix models.

9
00:00:59,230 --> 00:01:08,410
And after that, we will have on November 30th to guest lectures from two of my students.

10
00:01:08,920 --> 00:01:13,660
And I have also posted a handout and now 11 on this data.

11
00:01:13,900 --> 00:01:20,680
So essentially you can I will not be able to teach this lecture handouts.

12
00:01:20,800 --> 00:01:27,610
So I am putting that up here for you to just take whatever you want from that handout.

13
00:01:27,640 --> 00:01:35,049
I know Ms. Data itself is a beast, and I cannot hope to cover everything in that particular handout.

14
00:01:35,050 --> 00:01:40,900
So just to use that slide as a guide of the terminologies and what techniques to look for.

15
00:01:41,680 --> 00:01:45,670
And finally, there will be two team presentations.

16
00:01:46,030 --> 00:01:49,120
So one on December the fifth. The other on December 7th.

17
00:01:49,820 --> 00:01:56,510
Okay. So regarding that. What I'm going to do is to go through go to here.

18
00:01:57,740 --> 00:02:03,170
So you can go to this Google sheet. Currently, I am looking at 12 teams.

19
00:02:03,170 --> 00:02:06,380
So these six are on December 5th.

20
00:02:07,250 --> 00:02:17,569
I will of them actually I will use a on this day I will try to randomize the order presentation using some particular seed,

21
00:02:17,570 --> 00:02:24,800
actually seed of that date. So hopefully that will be just a pre specific protocol.

22
00:02:26,390 --> 00:02:29,180
And then for those of you who have not decided team name,

23
00:02:29,720 --> 00:02:36,320
hopefully these team members have met and then just put down the team name to indicate the team has met.

24
00:02:36,440 --> 00:02:43,400
Okay. All right. I know that's you guys have met, but just trying to get us some interesting team names.

25
00:02:46,420 --> 00:02:50,060
Going back to the. Of course, structure.

26
00:02:53,640 --> 00:03:00,110
So after that there will be I have received many proposals, so I'm still in the process of getting some feedback to you.

27
00:03:00,120 --> 00:03:06,720
Hopefully by tomorrow and the tomorrow and after the team presentation,

28
00:03:07,200 --> 00:03:17,970
essentially you will need to submit the final project report by December 20th and we will have a few days to grade the final report.

29
00:03:19,530 --> 00:03:24,450
So homework for will be assigned, but homework will be optional.

30
00:03:25,980 --> 00:03:31,110
The way it works is that homework for itself is worth eight points out of 100.

31
00:03:31,680 --> 00:03:37,200
So if you submit homework for first off, then you're homophobic graded.

32
00:03:37,260 --> 00:03:42,540
So if you get A's, then you get eight solid points and then your final project will be worth or 40 points.

33
00:03:43,560 --> 00:03:50,310
Okay. But if you do not submit, homework for your final project will be graded 48 with a total of 48 points.

34
00:03:51,030 --> 00:03:56,580
Okay. So if you think if you think you have a better chance of getting higher score on homework for it, just do homework for.

35
00:03:56,910 --> 00:04:00,510
If you think you don't care, then just do the final project.

36
00:04:02,040 --> 00:04:06,269
So. That's the overall plan.

37
00:04:06,270 --> 00:04:12,170
And we will try to be very clear on the rule when we are giving out homework for it.

38
00:04:12,180 --> 00:04:16,650
But that's just for you to preview this plan. Um.

39
00:04:17,950 --> 00:04:23,260
Oh. And the final project, I believe I have said it to be after the examination period.

40
00:04:23,350 --> 00:04:28,210
So hopefully this can mitigate some of the stress you are having with the final exam.

41
00:04:29,140 --> 00:04:36,910
Any burning questions at this time for the people who are here? Okay.

42
00:04:39,510 --> 00:04:43,420
All right. So let's go back to the handout.

43
00:04:43,600 --> 00:04:47,590
So this is handout ten B, the focus is on interpretation and inference.

44
00:04:47,980 --> 00:04:51,490
And I think many points I have touched on, many points that I will cover today.

45
00:04:51,790 --> 00:04:55,390
But I think this slide should make these points pretty concrete.

46
00:04:56,350 --> 00:04:57,850
There are two learning objectives.

47
00:04:57,880 --> 00:05:06,430
The first one is to contrast the interpretations of regression coefficients in generalized mixed model with those from marginal models.

48
00:05:07,120 --> 00:05:12,969
And there will be you will need to understand that starting from generalized dynamics model,

49
00:05:12,970 --> 00:05:17,050
how to integrate the random effects to arrive at a marginal model.

50
00:05:17,620 --> 00:05:27,820
And we will use a relatively simple example of logistic mixed model to demonstrate the difference

51
00:05:29,110 --> 00:05:34,780
between the beta in the generalized mixed model and the beta in the derived marginal model.

52
00:05:35,080 --> 00:05:44,980
And there will be some pretty important differences. Number two, it is the general understanding of the estimation in generalizing new mixed model.

53
00:05:46,690 --> 00:05:53,459
So. You know, if you want to learn the what's under the hood for estimating,

54
00:05:53,460 --> 00:05:58,460
generalize and unit mix model, I think that will need to take additional one or two lectures on.

55
00:05:58,680 --> 00:06:04,210
They will involve numerical integration. So I am not going to talk about those details.

56
00:06:04,230 --> 00:06:13,170
I'm going to just give you the R function name and also some options that can decide how accurate you can calculate the likelihood.

57
00:06:13,980 --> 00:06:19,350
So those are the two goals, the invitation contrasting your amendment model models.

58
00:06:19,650 --> 00:06:26,040
And number two, the general estimation scale. So a quick, quick review of generalizing in mixed model.

59
00:06:26,760 --> 00:06:30,570
It is like general linear model, generalized model.

60
00:06:30,600 --> 00:06:40,460
It has three parts specification. The first part is that you need to decide what is a distribution of J given the biz.

61
00:06:40,800 --> 00:06:43,470
So it is this distribution.

62
00:06:43,920 --> 00:06:56,550
And I just want to remind you guys that when I'm writing A given B, it is the conditional conditional distribution of quantity A given b.

63
00:07:04,840 --> 00:07:09,550
Given the OC. So we need to specify what's the distribution.

64
00:07:10,840 --> 00:07:13,840
Right. Usually it's an exponential family.

65
00:07:19,830 --> 00:07:24,760
But when you're specifying. Exponential form of distribution.

66
00:07:24,760 --> 00:07:33,250
You still have a few other components to specify, which includes the conditional mean and conditional variance.

67
00:07:34,450 --> 00:07:36,040
So let's talk about the variance first.

68
00:07:36,520 --> 00:07:47,950
Essentially, we often use a scale parameter if we multiply that by a variance function called a v v as a function of the mean.

69
00:07:49,800 --> 00:07:56,460
Okay. So the second component is the unit predictor.

70
00:07:57,300 --> 00:08:00,930
So it connects the predictors to.

71
00:08:02,440 --> 00:08:11,980
The meaning of the conditional distribution. So first we have these terms and the first first term is the fixed effects.

72
00:08:12,790 --> 00:08:20,290
The second term is the random effects. And there are some together into a meta predictor called ADG.

73
00:08:20,920 --> 00:08:27,880
And AJ is then linked to the mean of this conditional distribution through a link function.

74
00:08:28,690 --> 00:08:32,110
So this is pretty similar to generalized NINA model.

75
00:08:32,380 --> 00:08:39,160
The only difference is that when you are talking about the mean, it is the conditional meaning of y j given by.

76
00:08:40,300 --> 00:08:47,010
Okay. Number three.

77
00:08:47,520 --> 00:08:52,920
Number three, it is the random effects. So in general, random effects.

78
00:08:55,810 --> 00:09:01,780
Is like Q dimension and it general generally follows a distribution called F.

79
00:09:03,190 --> 00:09:12,759
This F can be arbitrary, but in most of the models we are going to fit F it actually is a multi very Gaussian with Q dimension.

80
00:09:12,760 --> 00:09:17,600
We mean zero and variance coherence g. It's not a vector.

81
00:09:17,600 --> 00:09:23,760
It's a matrix. So I'm going to move this. So this is the common assumption.

82
00:09:24,240 --> 00:09:28,620
And further we would assume that by is independent the website here.

83
00:09:30,010 --> 00:09:35,020
What does that mean? It basically says that for.

84
00:09:36,800 --> 00:09:40,130
The random effective subject. I it is not.

85
00:09:40,310 --> 00:09:47,000
Its distribution is not dependent upon exercise. So when is one possible violation of this condition?

86
00:09:48,380 --> 00:09:52,490
It can be that, you know, say the variance of B.

87
00:09:52,820 --> 00:09:57,980
Well, here we just say it is a single. It is characterized by a single.

88
00:09:59,140 --> 00:10:05,980
Variance covariance matrix g. Right. What if the covert is sex male versus female?

89
00:10:06,010 --> 00:10:12,360
What if the g is different between the two groups? Then you will have you will not have the bi independent z.

90
00:10:13,090 --> 00:10:23,020
So those situations are worse studied in the literature and if not careful can cause bias in the estimation of the betas.

91
00:10:23,530 --> 00:10:30,280
But here, for discussing this general scheme, we will just assume BI is independent.

92
00:10:31,150 --> 00:10:37,510
And we will briefly talk about some fixes by referring to a paper.

93
00:10:38,140 --> 00:10:48,330
But I think this is an assumption we're going to work with. So these three parts comprise the specification of a generalized genomics model.

94
00:10:48,720 --> 00:10:58,290
Another thing I want to say before we move on is that you do know that Y is a vector of responses y one up to y,

95
00:10:58,290 --> 00:11:10,710
i and I and in general we made make the following assumption, which is to say that y i j is independent of y j prime given by.

96
00:11:11,250 --> 00:11:18,750
Okay. So it is to say suppose we are given the random effects and actually converts to.

97
00:11:20,470 --> 00:11:23,380
Then the responses are going to be independent.

98
00:11:23,830 --> 00:11:30,310
And the reason why this will be helpful is that it helps with writing down the full likelihood, as we'll see.

99
00:11:30,550 --> 00:11:39,250
So this is also a assumption that we commonly assume and in text is saying that given the random effects, why JS Independent each other.

100
00:11:40,090 --> 00:11:48,880
So this is called the conditional independence assumption. Now this, this, this is a quick review of generalized new mixed model.

101
00:11:49,030 --> 00:11:55,570
Then we are going to contrast to different approaches to modeling.

102
00:11:55,840 --> 00:11:59,740
So the first one clearly is development. The other one is marginal model.

103
00:12:01,160 --> 00:12:03,440
They are different in the target. What do I mean by that?

104
00:12:03,920 --> 00:12:10,190
So if you look at the generalized mixed model specification in this mean it is conditional on the by.

105
00:12:11,470 --> 00:12:20,350
Right. And when you're doing negotiation, you're basically trying to say, hey, what's the difference between x jake or supposed x NGL scaler?

106
00:12:21,100 --> 00:12:24,160
Right. And you're going to do this difference.

107
00:12:29,830 --> 00:12:34,000
So for every unit change in the covered x.

108
00:12:34,510 --> 00:12:37,600
J. What's the change in the conditional mean?

109
00:12:38,680 --> 00:12:43,250
Right. So the key thing here is that we've got a condition on the unobserved random effects.

110
00:12:45,880 --> 00:12:52,540
So by. Saying We are fixing the random effects.

111
00:12:52,550 --> 00:12:57,020
We are essentially shifting our target of inference to the individual right because.

112
00:12:58,370 --> 00:13:03,230
When you condition by you're talking to one kind of person who has a particular value by.

113
00:13:04,770 --> 00:13:12,720
However, if you look at the marginal model, so this is all we need to specify the main structure.

114
00:13:13,960 --> 00:13:18,370
Often we would write x i j t beta.

115
00:13:19,450 --> 00:13:27,130
So here I'm going to add an additional superscript called M to indicate that this beta is from a marginal model.

116
00:13:27,970 --> 00:13:30,070
So how do we do the interpretation?

117
00:13:30,100 --> 00:13:41,980
Well, same thing, which is to say that, hey, what if we said x j to x plus x plus one minus y j to outside j equals x.

118
00:13:43,240 --> 00:13:49,090
Okay. Now, this contrast does not have to be between the same person.

119
00:13:49,840 --> 00:13:56,650
What if X's is like sex and it is indicator?

120
00:13:57,430 --> 00:14:02,170
So if we said X equals zero here, then it's a contrast between two sexes.

121
00:14:02,200 --> 00:14:06,580
Nobody can, you know, alternate between the sexes, like, so frequently and.

122
00:14:07,550 --> 00:14:09,710
You are comparing two sub populations.

123
00:14:10,900 --> 00:14:22,150
So this is basically saying that, hey, we are investigating the question about contrasts between subpopulations, not necessarily within each subject.

124
00:14:22,540 --> 00:14:28,120
So how does the average response vary across different subsets of the study population defined by the covered?

125
00:14:30,190 --> 00:14:38,740
So this difference is all we care about. You know, she cares about the effect of the cover upon.

126
00:14:39,640 --> 00:14:45,370
The main response within the subject, while margin model cares about between sub population comparisons.

127
00:14:45,910 --> 00:14:53,500
Fortunately, there is a link between the two kind of models, so the link pretty much is summarized like this.

128
00:14:54,460 --> 00:15:05,170
We know that by integrating over the by and using the light level of iterated expectations, we can receive or derive the marginal mean right.

129
00:15:06,070 --> 00:15:12,370
So all we will do next or in the in the in many slides to follow,

130
00:15:12,640 --> 00:15:20,049
we will first try to at least derive what this is and then integrate over by and we will see what's the consequence.

131
00:15:20,050 --> 00:15:26,020
And the first thing you will see is that it might not be that straightforward if you have a nonlinear link function.

132
00:15:28,540 --> 00:15:36,840
So let's start with that. Me. Here, I'm writing down this conditional me and I'm putting a superscript c there to

133
00:15:36,850 --> 00:15:42,490
emphasize that the speeder was specified under a model that conditional on by.

134
00:15:43,630 --> 00:15:50,950
Now if you are going to do this derivation, you have to have a starting point.

135
00:15:50,980 --> 00:16:00,760
Our starting point is to add them and here we are assuming the link function is identity identity link.

136
00:16:04,480 --> 00:16:15,760
Now, as we have seen before, if we average over by essentially because of linearity of expectation and because expectation of B is zero.

137
00:16:17,010 --> 00:16:26,970
Then you basically will receive this thing, right? So if I started with a beta C here, all the beta C will percolate through until the end.

138
00:16:27,060 --> 00:16:32,370
Right. So what you see here, hey, at the end of the bullet to it is that, hey,

139
00:16:32,370 --> 00:16:40,530
the marginal mean of y j is j transpose times beta c look, it is a beta with a C, not beta with them.

140
00:16:41,270 --> 00:16:49,110
Unfortunately, you know, in this case, if you simply rewrite this as excited j transpose beta, they're the same thing.

141
00:16:49,260 --> 00:16:56,999
I just like how you would have written them. So in the first bullet, our starting point is generally in a mixed model.

142
00:16:57,000 --> 00:16:59,990
We integrate out. We arrive at the marginal model.

143
00:17:00,000 --> 00:17:05,640
It turns out that it has the exact same specification as you would if you start by specifying the margin model.

144
00:17:07,080 --> 00:17:16,950
So this is sweet if you have the identical link, but what if we have a generalized mixed model that truly is has a non-linear link function?

145
00:17:17,250 --> 00:17:25,940
So here G is nonlinear. And say we start with a generalized mixed model.

146
00:17:25,940 --> 00:17:34,640
So I put BTC there to indicate that that's our starting point and see how that BTC would manifest after we have integrated all the BI.

147
00:17:35,390 --> 00:17:42,860
So what do we do? Well, we just do something that's pretty straightforward or tedious if you want to see that.

148
00:17:43,490 --> 00:17:47,000
So this is the expectation. And we just do.

149
00:17:48,110 --> 00:17:58,000
Expectation is by. This one, right.

150
00:17:59,020 --> 00:18:03,270
Okay, then you have this one. What is this?

151
00:18:04,280 --> 00:18:10,640
We simply just put G into here and then you have the expectation, right?

152
00:18:10,820 --> 00:18:15,210
So it is this one. I'm going to put a C here.

153
00:18:18,520 --> 00:18:26,650
Hope everybody's with me here. Now, we are at a critical technical point, which is the operation of these two things.

154
00:18:27,100 --> 00:18:30,920
So. Let's assume. If.

155
00:18:32,290 --> 00:18:36,580
E RBI and g inverse can interchange order.

156
00:18:39,140 --> 00:18:50,670
Should I say exchange. Order. Right.

157
00:18:51,030 --> 00:19:08,000
What do we have? Then we would have. We would have this right.

158
00:19:08,010 --> 00:19:15,090
And inside the parentheses you basically will have g inverse oxide, g t btc, right?

159
00:19:16,340 --> 00:19:20,170
Wow. Look at this. BTC appeared.

160
00:19:21,730 --> 00:19:25,180
In the end, as if you can just imagine a model.

161
00:19:25,840 --> 00:19:32,030
So that's why. People. Can just start from here.

162
00:19:35,480 --> 00:19:41,360
So they just say, I don't care about generalized name X model, I'm just going to care about the model model.

163
00:19:41,360 --> 00:19:45,010
I'm going to specify this form. And it's going to be equivalent.

164
00:19:45,490 --> 00:19:50,350
And the equivalence is only guaranteed if you have this exchange.

165
00:19:50,770 --> 00:19:57,040
You have this exchange of order. Right. But what if you cannot change the order?

166
00:20:04,660 --> 00:20:19,330
How about I just right now? Sort of. Otherwise you will be stuck.

167
00:20:19,870 --> 00:20:23,560
You will just have an extra. I have to copy it down.

168
00:20:23,680 --> 00:20:27,910
So. In general, this cannot be further simplified.

169
00:20:32,500 --> 00:20:38,320
In general, it cannot be simplified. So there is no way you can just.

170
00:20:39,490 --> 00:20:46,840
Start from the general staff on the Marshall model and get at this integration.

171
00:20:50,250 --> 00:20:54,710
So returning to this particular bullet here, that's what I'm saying here.

172
00:20:57,220 --> 00:21:03,400
So this is see. In general, this in this inequality will be true.

173
00:21:04,430 --> 00:21:10,250
Which says that if you start with a generalized mixed model and for the B2C there,

174
00:21:10,730 --> 00:21:14,570
if you try to integrate out by, you know, in general will not be equal to this.

175
00:21:16,160 --> 00:21:23,340
Okay. So essentially this exercise is showing you that.

176
00:21:25,290 --> 00:21:28,500
Depending on how you start the model.

177
00:21:30,150 --> 00:21:39,990
And depending on the link function, you will not always have the same indentation for the for the beta you will get.

178
00:21:40,320 --> 00:21:48,380
So. It is only when EBI and Gene Vers can exchange order that you can interpret data in both ways.

179
00:21:48,680 --> 00:21:52,670
Either within a subject issue and a change in the covered.

180
00:21:53,000 --> 00:21:58,170
We have a slight change in the average response rate.

181
00:21:58,310 --> 00:22:04,670
The other way to do this is pay for two subpopulations with different in one units of the cohort.

182
00:22:05,060 --> 00:22:10,120
What's the difference in their mean response? So this can only be true if you can exchange order.

183
00:22:10,130 --> 00:22:23,010
Otherwise you can not do that. So this highlights the importance to be very clear about the interpretation when you are interpreting, Peter.

184
00:22:23,980 --> 00:22:33,160
So to summarize for near link functions, the regression coefficients in the generalized unit models have quite distinct invitations.

185
00:22:33,550 --> 00:22:40,750
So these two classes of regression models have distinct targets of inference for the margin models.

186
00:22:40,960 --> 00:22:45,610
The scientific questions are concerned with the change in the mean response over time,

187
00:22:45,610 --> 00:22:49,690
in study population and the impact of coverage upon these changes.

188
00:22:50,230 --> 00:22:52,540
While for generalized dynamics models,

189
00:22:53,110 --> 00:23:03,760
it is for it is concerned with a change in the response for any particular individual and the impact of covers on these so on these changes.

190
00:23:04,180 --> 00:23:09,340
So this in general have population averaged interpretation.

191
00:23:16,990 --> 00:23:20,290
And this one has a subject, specific interpretation.

192
00:23:29,230 --> 00:23:36,580
So let's look at one example to make this clearer. Here you are looking at a hypothetical data.

193
00:23:37,210 --> 00:23:42,790
These are the are representing the true propensity for disease or disease risk.

194
00:23:43,240 --> 00:23:46,750
And we have three types of individuals, A and B and C.

195
00:23:47,020 --> 00:23:53,210
Suppose the population has only these three kind of individuals of equal weights, of equal numbers.

196
00:23:53,230 --> 00:24:00,310
Okay. And every person have two time to two occasions, one at baseline, the other post baseline.

197
00:24:00,640 --> 00:24:06,220
So for subjects like the baseline, risk of the disease is point eight.

198
00:24:08,070 --> 00:24:13,140
Post baseline is 0.67. So the difference is like -0.13, right?

199
00:24:13,710 --> 00:24:19,800
And if you go down the rows and B and C clearly has different baseline propensities of development disease.

200
00:24:20,160 --> 00:24:23,360
So in general, C seems to be more robust and more healthier. Right.

201
00:24:23,760 --> 00:24:31,100
And as like more frail. And for each of these contrasts, we can just calculate difference, right?

202
00:24:32,060 --> 00:24:36,000
So for all three kinds of people, the risk of disease decrease.

203
00:24:36,470 --> 00:24:39,680
And what's the average of those differences?

204
00:24:40,310 --> 00:24:47,550
It is basically. It is basically -0.13.

205
00:24:48,240 --> 00:24:54,600
Okay. So notice how we got minus one one three. We first take the difference, which is within subject changes.

206
00:24:55,150 --> 00:24:59,100
Okay. And then average over people. Now we can do the other way.

207
00:24:59,340 --> 00:25:03,090
We can say, what's the population? Average baseline disease risk.

208
00:25:03,780 --> 00:25:08,280
We just average over .8.5, four, two, three.

209
00:25:08,700 --> 00:25:13,980
That is ..5. And we do the same thing for the disease risk post baseline.

210
00:25:14,310 --> 00:25:21,300
And that's .37. Now, after you have done those averages first and then you take the difference.

211
00:25:22,170 --> 00:25:29,470
Well, plus baseline minus the. Three minus the baseline, the number is again, -0.13.

212
00:25:29,950 --> 00:25:40,090
Right. So, look, essentially this means that regardless of what operations you take first, you will get the same effect estimates.

213
00:25:40,690 --> 00:25:45,070
The first one is the average of differences as we have done a third column.

214
00:25:46,430 --> 00:25:54,140
The second way of calculating is we take the differences of the averages, which go to the same answer.

215
00:25:54,200 --> 00:25:59,299
Right. So essentially the reason why we are right, the same answer is because we are.

216
00:25:59,300 --> 00:26:03,950
You say we're using the identity link because we're talking about rich difference.

217
00:26:04,670 --> 00:26:10,800
Now let's look at the. How we have represented it.

218
00:26:10,830 --> 00:26:19,830
So basically BTC is like -1.3 liter and was also -1.3 for the scale for the risk difference.

219
00:26:25,550 --> 00:26:34,970
Remember Peter is has the within subject difference comparison and premise for the population average comparison.

220
00:26:36,060 --> 00:26:39,820
Now if we change and the effect of.

221
00:26:41,370 --> 00:26:46,050
What would change the effect scale from the risk difference to log odds ratio?

222
00:26:46,080 --> 00:26:54,960
Let's see whether that same calculation holds up. So for the first person, what we can do is to calculate log odds at the baseline.

223
00:26:57,100 --> 00:27:00,400
And log ons at the post baseline.

224
00:27:01,830 --> 00:27:04,830
That number if you take the difference. Actually, this one, right.

225
00:27:05,100 --> 00:27:08,130
If you take the difference, it will be minus .68.

226
00:27:08,670 --> 00:27:15,480
Well, if you have a negative log odds ratio, it means that it's kind of the odds seems to be decreasing.

227
00:27:16,740 --> 00:27:20,460
And if you do the same calculation, but within each of the subjects.

228
00:27:20,640 --> 00:27:27,180
Right, you got similar log odds ratios, minus .71 minus point seven.

229
00:27:27,210 --> 00:27:36,270
Right. So one thing you can do is that to summarize all those three log odds ratios for within subject comparison, you can average them.

230
00:27:36,420 --> 00:27:40,650
Right. What's the average? -0.6, nine, seven.

231
00:27:43,850 --> 00:27:52,720
So I'm going to ask you, is this better still be down? Siemens conditional interpretations better means marginal population average interpretations.

232
00:27:54,760 --> 00:28:01,140
I would say BTC is the is this number why?

233
00:28:01,150 --> 00:28:06,700
Well, we first did within subject comparisons in terms of long odds ratios and then we average them over.

234
00:28:06,850 --> 00:28:10,840
So it's kind of a summary quantity for within subject comparisons.

235
00:28:11,560 --> 00:28:16,600
Now to get the beta em, which is the margin comparison or population average comparison, what do we do?

236
00:28:16,990 --> 00:28:24,760
Well, we get this number point five, which represents the average baseline population disease risk.

237
00:28:24,880 --> 00:28:31,900
We ask, what's the log odds with just 2.5 divided by one minus point five for the.

238
00:28:33,240 --> 00:28:40,860
Average at the post I post baseline period we get the probability at points three seven then we do the.

239
00:28:42,460 --> 00:28:50,580
Log, log, all calculation again. And then we take the difference in this direction, the post baseline minus the baseline.

240
00:28:50,940 --> 00:28:58,410
So if you calculate the number, actually the number is going to be. So this is too.

241
00:28:58,440 --> 00:29:04,620
This is a one. So two minus one equals one -0.5, three, two.

242
00:29:06,360 --> 00:29:11,790
And relatively, the magnitude has been shrunk about 25%.

243
00:29:14,170 --> 00:29:16,870
Right. So this is the toy example.

244
00:29:16,960 --> 00:29:22,630
And I want to pause for a moment because I think that for those of you who are looking at this for the first time could be extremely confusing.

245
00:29:22,990 --> 00:29:27,430
These are the numbers never greater than one. Somehow there is a story to be told.

246
00:29:29,390 --> 00:29:36,950
And by the way, I think that as you progress in your career, you probably never need to deal with a number greater than one of.

247
00:29:37,190 --> 00:29:39,350
Mostly because you have to deal with the probabilities.

248
00:29:39,860 --> 00:29:48,370
Now for Peter C, with the long odds ratio, you're always starting from the within subject comparison and then the average over it, right.

249
00:29:49,800 --> 00:29:55,020
Peter erm which is to have comparison between two.

250
00:29:55,900 --> 00:30:00,490
Sub populations in this case which to some populations subpopulations of time

251
00:30:00,970 --> 00:30:04,450
at the baseline period and the subpopulation that the post based on period.

252
00:30:04,960 --> 00:30:08,440
So to do that comparison you have the average over you know,

253
00:30:09,910 --> 00:30:16,450
you have to get a kind of log odds for all the people ABC in the at the base time period

254
00:30:16,990 --> 00:30:20,080
and then do that again for the post based on period and that difference is this.

255
00:30:20,560 --> 00:30:31,110
So. So this means that if you have a link function that's not identity, you will have some discrepancy in such kind of comparison.

256
00:30:31,690 --> 00:30:35,020
And this highlights the importance of what?

257
00:30:37,490 --> 00:30:44,250
Of making sure you are interpreting. You know, the model coefficients in the right way.

258
00:30:54,110 --> 00:31:03,320
So for those of you who are wondering what steps to model. So I'm going to write down here for the top it is a probability of YJ equals one given.

259
00:31:04,310 --> 00:31:13,520
Ex i. J. Um b i equals one equals zero plus beta one times i.

260
00:31:14,770 --> 00:31:19,170
Time J equals post baseline.

261
00:31:20,920 --> 00:31:28,160
Plus. Bye. Okay. So this is the. This is the first one.

262
00:31:28,250 --> 00:31:33,070
While for the second one you are dealing with closure.

263
00:31:39,090 --> 00:31:52,020
Probability of y i j equals one given x j and by this equals theta zero c plus beta 1ci time.

264
00:31:53,370 --> 00:31:57,410
J equals post. Plus by.

265
00:31:58,560 --> 00:32:07,330
Right. And for the marginal model, essentially his logit probability of y j equals one.

266
00:32:08,080 --> 00:32:16,450
So j by equals one equals theta zero plus beta one times i.

267
00:32:17,540 --> 00:32:26,640
Time J equals post. So what I am saying is that these two numbers are going to be estimated with different values.

268
00:32:58,240 --> 00:33:01,480
Oh. Any questions before we proceed? Because I think I will not.

269
00:33:01,990 --> 00:33:06,819
I have basically told the story over the next few, five or six slides.

270
00:33:06,820 --> 00:33:22,210
So I will not go through the slides again. Okay.

271
00:33:22,330 --> 00:33:28,990
Let's go to Slide 19. So it's like 19 provides a kind of discussion point.

272
00:33:29,320 --> 00:33:34,910
So what's your first subject specific? Or population averaged.

273
00:33:35,880 --> 00:33:39,300
Which if that makes more sense. Actually, both makes sense.

274
00:33:39,300 --> 00:33:50,220
Depends on your scientific question. So let's imagine if you are a person who is very how to say you want to know your health,

275
00:33:50,220 --> 00:33:55,650
you want to know how your health would progress and or if this if you are a physician to a patient.

276
00:33:55,650 --> 00:34:01,220
Right. I think the immediate responsibility is that you want to know your health.

277
00:34:01,230 --> 00:34:07,709
You want to take good care of your patient. So you probably want to have subject specific effect.

278
00:34:07,710 --> 00:34:15,960
Right. For example, you can tell this person, if you're going to be one year older, know your average changing the response will be this much.

279
00:34:16,080 --> 00:34:22,500
Right? So you're essentially interpreting with a subject in mind, you're just conditioning on this person.

280
00:34:23,100 --> 00:34:26,520
So if you want to do this kind of communication or interpretation,

281
00:34:26,790 --> 00:34:32,670
you'd better do a generalized genomics model because it has such a specific interpretation,

282
00:34:33,360 --> 00:34:36,870
say, for every one year increase in age, what's the effect upon the outcome?

283
00:34:38,280 --> 00:34:44,290
Now. If you're not a patient, if you're not a physician for a patient.

284
00:34:44,290 --> 00:34:50,080
But you'd rather if you are a policymaker, if you're a senator, if you are House representative,

285
00:34:50,320 --> 00:34:56,280
you want to know whether a drug should be introduced into the population, right?

286
00:34:59,750 --> 00:35:05,150
So you want to care about the difference between, say, two subpopulations.

287
00:35:05,600 --> 00:35:08,660
One has a drug, the other don't have doesn't have the drug.

288
00:35:09,020 --> 00:35:11,330
And what's the difference in the mean response?

289
00:35:11,660 --> 00:35:20,930
So in those cases in general, you would want to proceed with a model that's marginal, that has the the has a margin model specification.

290
00:35:20,960 --> 00:35:24,420
The beta aims. Right.

291
00:35:24,930 --> 00:35:32,370
Because after all, you know, save your public health research or your policy maker you care about for the general population.

292
00:35:32,760 --> 00:35:41,760
What's the average change in the response that you can how to, say, produce by introducing this new and promising drug into the population?

293
00:35:42,330 --> 00:35:45,930
Does that person care about individual? Yes, this person may care about any individual,

294
00:35:45,930 --> 00:35:52,080
but his or her responsibility is to take care of the people in the district or take people in the, you know, in the.

295
00:35:53,820 --> 00:35:56,460
In regions where this person is responsible for.

296
00:35:56,700 --> 00:36:09,990
So my point is that both models, the generalized near mix model and the marginal model have it have their places in scientific research.

297
00:36:10,380 --> 00:36:17,130
So it's never true that one model is superior to another. It's rather about the scientific question.

298
00:36:19,170 --> 00:36:48,090
So any questions on this point? Okay.

299
00:36:50,290 --> 00:36:53,620
So this means that when you're doing project, you probably want to also consider,

300
00:36:53,710 --> 00:36:57,970
you know, which model is more suitable for the scientific question you're investigating.

301
00:36:58,520 --> 00:37:10,280
And you know. A braindead application of two models to one data sets showing the technical superiority of the group will not work.

302
00:37:10,570 --> 00:37:15,230
Okay, so you have to be clear that the model is going to solve the problem.

303
00:37:15,950 --> 00:37:20,130
Now. Part two estimation inference.

304
00:37:20,520 --> 00:37:27,389
So in general, this has been the most difficult part of the last lecture,

305
00:37:27,390 --> 00:37:31,590
but since two or three years ago, I decided to cut down on the technical difficulty.

306
00:37:33,990 --> 00:37:41,100
So I will not talk about expectation maximization or our likelihood of paralyzed selective estimation.

307
00:37:42,060 --> 00:37:46,910
So I think my goal here today is rather to introduce how it is done.

308
00:37:46,920 --> 00:37:49,920
I think that should be pretty easy to understand.

309
00:37:50,130 --> 00:37:55,530
Is it understood? But if you are interested in these techniques, you will have to read something else.

310
00:37:56,580 --> 00:38:00,959
I think in the department, I believe based on how it is, I don't know whether he will do this again,

311
00:38:00,960 --> 00:38:04,890
but I think he used to offer advanced longitudinal data analysis.

312
00:38:05,190 --> 00:38:10,440
I think perhaps he will talk about these things, but I'm not exactly sure about his syllabus.

313
00:38:10,440 --> 00:38:13,310
But clearly there will be more resources you can use in department.

314
00:38:15,240 --> 00:38:21,420
So for the inference, the primary tool we're going to use is the likelihood inference based inference.

315
00:38:22,170 --> 00:38:27,360
Now, as a reminder for marginal models, what do we do to do estimation?

316
00:38:28,230 --> 00:38:34,200
Well, for marginal models, you do not need to specify the full distribution of all the responses.

317
00:38:34,290 --> 00:38:38,280
Right? You just specify the marginal means, the variance of pairwise associations.

318
00:38:40,080 --> 00:38:42,870
If you have done homework three, I think you probably are familiar with that.

319
00:38:43,410 --> 00:38:48,030
And so there you don't have a joint distribution of the vector responses.

320
00:38:48,420 --> 00:38:53,100
However, when you are dealing with generalized Vitamix models, as I alluded to earlier,

321
00:38:53,340 --> 00:38:58,980
you often need to decide what is the exponential family distribution you want to specify in the first place,

322
00:38:59,400 --> 00:39:04,590
and then you move on to specify the ME in that particular distribution family and the variance, right.

323
00:39:05,430 --> 00:39:11,700
So the good thing is that now with generalized gimmicks model, we have a for distribution to work with.

324
00:39:11,790 --> 00:39:17,580
So that naturally leads to likelihood based inference and what are some.

325
00:39:18,870 --> 00:39:22,770
And once you have the likelihood, you pretty much can have everything.

326
00:39:23,280 --> 00:39:27,959
But that's a that's a very how to say likelihood.

327
00:39:27,960 --> 00:39:36,660
Inference is very different is very complicated. I used to I was visiting Lancaster University with this professor, Peter Diggle.

328
00:39:36,690 --> 00:39:40,580
This guy is a great guy and very smart guy and is basically saying that, hey, Jim,

329
00:39:40,680 --> 00:39:48,540
I know you're doing Bayesian friends and you can write in the model and everything else then becomes how do you deal with the likelihood function?

330
00:39:48,930 --> 00:39:52,979
So that turns out to be extremely true in many of the problems.

331
00:39:52,980 --> 00:39:56,640
So whenever you're working on a problem, when you write down likelihood.

332
00:39:58,140 --> 00:40:03,960
You have to ask, how can I do inference? If you can just use a medicine like estimation, probably that's not going to be methods paper.

333
00:40:04,500 --> 00:40:11,520
But if you encounter some difficulty of using the method, using the lack of a function, then you can invent your method to deal with it.

334
00:40:11,550 --> 00:40:16,410
There are like pseudo likelihood, pairwise likelihood, many different things.

335
00:40:17,310 --> 00:40:20,520
So going back here is that we're blessed with with functions.

336
00:40:20,730 --> 00:40:25,500
All right. Now, when you have, like, functions, what are the things you can buy?

337
00:40:25,770 --> 00:40:31,400
Well, you can deal with missing data. Especially missing under Adam.

338
00:40:31,610 --> 00:40:40,460
Now, in contrast, in gear without any modification is going to be where it's going to be valid for missing completely at random.

339
00:40:42,020 --> 00:40:49,669
But there are extensions to do way to g to deal with missing or missing out random for

340
00:40:49,670 --> 00:40:54,710
generalized mimics model based on likelihood it can deal with and they are pretty easily.

341
00:40:55,790 --> 00:40:59,450
So let's see, how do we deal with the likelihood?

342
00:40:59,570 --> 00:41:04,670
Well, let's clarify the goal. The goal is primarily about the fixed effects.

343
00:41:05,120 --> 00:41:11,270
And I think I want to emphasize this is beta C and also the random effect covariance parameter G.

344
00:41:11,590 --> 00:41:15,750
Right. So that's the those are the parameters we often care about.

345
00:41:15,900 --> 00:41:25,380
And number two, generalized genomics model has been extremely powerful in making predictions because you can predict the random effects.

346
00:41:25,950 --> 00:41:33,420
Right. And then if you plug that in, it will be applicable to future data points you have not collected data on.

347
00:41:35,680 --> 00:41:45,130
Number three. So for these for the library inference and generalized dynamics model, it is often much more complicated.

348
00:41:45,400 --> 00:41:51,340
And we will talk about that primarily because integration now is less than straightforward.

349
00:41:52,060 --> 00:41:57,000
So actually, I want to ask so for second, are you a master student or second user?

350
00:41:57,130 --> 00:42:02,990
What do you have a numerical. Statistics.

351
00:42:05,670 --> 00:42:10,920
I do have a designated class to teach expectation maximization and CMC.

352
00:42:11,730 --> 00:42:15,110
Like girls in courses or whatever now. Okay.

353
00:42:15,330 --> 00:42:18,719
So I think it's probably a good idea that for me to not talk about that today.

354
00:42:18,720 --> 00:42:23,760
But I think in general all those techniques will have its place in these kind of models.

355
00:42:24,090 --> 00:42:34,079
And I think that I do think there country or gen college should say and hiring com in our

356
00:42:34,080 --> 00:42:38,930
department I think they have these statistics computing classes introducing these algorithms.

357
00:42:38,940 --> 00:42:44,640
I think if you a person who is an engineer at heart, you want to build things, want to solve things.

358
00:42:45,180 --> 00:42:48,300
I highly encourage you to take computation courses.

359
00:42:49,260 --> 00:42:54,060
Why? It will help you get a job. Okay, well, that's.

360
00:42:54,300 --> 00:43:04,320
That's true, actually. That's why it's funny. But number two, you know, when you when you are actually, you know, looking at the algorithm,

361
00:43:04,320 --> 00:43:11,010
you know, how the information from the data got extracted to produce the effect estimates.

362
00:43:11,250 --> 00:43:21,570
So I think in some if you have those courses on the on the list, you probably if you have time, if your advisor allows it, you know, just take it.

363
00:43:22,460 --> 00:43:26,000
It's gentle. It's good investment in time. And don't worry about programing.

364
00:43:26,000 --> 00:43:29,750
Language is all like Python. You know, Julia doesn't matter.

365
00:43:29,750 --> 00:43:35,510
As long as you learn some technique, that's okay. So how do we do, Emily?

366
00:43:35,510 --> 00:43:40,700
Internalize Intermix model. So we just want to write down the joint probability distribution.

367
00:43:40,700 --> 00:43:44,720
It can be factories into this form. It should be very simple for you guys.

368
00:43:45,230 --> 00:43:54,440
And now this part is using some assumptions we've made, which is to say that the Y, I, j and the y j prime is independent,

369
00:43:54,480 --> 00:44:00,559
given the by and exi j often the x is not written down here because it's often assumed given.

370
00:44:00,560 --> 00:44:05,030
So we're not writing that down here. But on the right I'm writing that down just to remind you.

371
00:44:05,450 --> 00:44:11,150
So when you have this condition independence, you can fact arising entire vector of outcome into pieces.

372
00:44:11,290 --> 00:44:17,640
Right. Now. This is the joint distribution.

373
00:44:17,830 --> 00:44:23,399
Right. And in each piece, we will have one friend like G.

374
00:44:23,400 --> 00:44:26,920
Pravda, like Peter. Right.

375
00:44:26,940 --> 00:44:29,670
And perhaps other parameters like fi in here.

376
00:44:31,020 --> 00:44:39,750
So if you do not integrate by you still have like how many like and people's random effects is unknown, right?

377
00:44:40,260 --> 00:44:43,960
But you do not necessarily, at least when you estimating better care about them.

378
00:44:43,980 --> 00:44:47,610
So what you do is to integrate. I'll be busy after all of them.

379
00:44:49,470 --> 00:44:52,980
So how do we do that? Well, essentially it is evolving.

380
00:44:54,750 --> 00:44:59,370
Numerical Quadrature. What does culture mean?

381
00:45:06,390 --> 00:45:14,880
But it's essentially. Just. You know, when you were learning calculus, it just drawing these rectangles.

382
00:45:25,650 --> 00:45:29,160
You're just calculating the area in these rectangles.

383
00:45:29,430 --> 00:45:35,159
And as the number of rectangles increases, you will have a good approximation of the actual area under the curve.

384
00:45:35,160 --> 00:45:39,270
And errors are always there. You know, you have these errors.

385
00:45:41,920 --> 00:45:45,310
Anyway. So this is the Riemann integral.

386
00:45:53,720 --> 00:45:56,720
In Chinese actually just called a just forgive me. Here.

387
00:45:56,720 --> 00:46:01,080
I just want to. All right.

388
00:46:01,290 --> 00:46:10,110
So in the center of this slide, essentially, you can write down the approximation like this for capital K number of.

389
00:46:11,280 --> 00:46:20,380
This is what we call number of causes your points. So v1v2 up to VK.

390
00:46:21,010 --> 00:46:25,930
So the more quality points you have, the better approximation you will have.

391
00:46:26,830 --> 00:46:32,710
So this w k is representing the weight.

392
00:46:32,980 --> 00:46:37,090
But what do we mean by the weight here? Clearly by has its own distribution.

393
00:46:37,300 --> 00:46:45,970
So if by following Gaussian write those larger bits is going to receive less weight than bits that are in the middle.

394
00:46:46,600 --> 00:46:52,660
So in general, this is the form where you are or other packages are going to use.

395
00:46:53,290 --> 00:46:59,350
And this parameter K can be chosen. So I will have those that slide for you.

396
00:46:59,800 --> 00:47:07,840
So in general K, if you specify okay to be 1000, that's going to be very accurate that you will pay a lot of price in terms of computational time.

397
00:47:08,290 --> 00:47:13,030
So in practice, I think by default it's using 50, I believe.

398
00:47:13,040 --> 00:47:20,470
But if you want to change the number of quadrature points, you can change that to see whether the results are different.

399
00:47:20,500 --> 00:47:25,810
If they're roughly similar, you probably say that the number of closure points is okay.

400
00:47:28,640 --> 00:47:35,330
So once you have the estimates of. Oh, sorry. So once you calculate these, you've got to maximize with respect to beta and g.

401
00:47:35,870 --> 00:47:39,260
How do we do that? Well, are there is this optimum function?

402
00:47:39,780 --> 00:47:43,220
Um, but I am just going to be very hand-waving here.

403
00:47:43,790 --> 00:47:47,060
I suppose you can do that, maximization. You can get the beta out.

404
00:47:47,390 --> 00:47:50,000
If I had g. Huh? What can we do next?

405
00:47:50,030 --> 00:48:01,070
Well, you can use this conditional expectation of VI, given why I plugging all these estimates to produce an estimate of this person's random effects.

406
00:48:01,370 --> 00:48:07,840
You may be wondering, hey, how do I get this? Well, remember, you know, you have this thing as a joint distribution.

407
00:48:07,850 --> 00:48:19,760
Yeah. And now you can write down the joint, the distribution of by given y, which is one which is F VII divided by.

408
00:48:20,630 --> 00:48:27,690
Oh. Sorry I did it wrong. The joint distribution.

409
00:48:29,720 --> 00:48:38,580
Divided by the marginal distribution. And the margin on distribution can be calculated by what?

410
00:48:49,270 --> 00:48:57,280
So basically baseball, right? So something like this.

411
00:49:00,670 --> 00:49:06,940
It is problem specific. So I do not provide a general formula, but essentially you can use the condition expectation to produce the.

412
00:49:09,320 --> 00:49:17,210
I produced the prediction. So once you got this, then according to the generalizing a mixed model, you know.

413
00:49:19,630 --> 00:49:29,070
Essentially as what is Exide transposed on the speed of hands plus z transpose times by right pat.

414
00:49:29,590 --> 00:49:35,830
And then you can just say I j is the g inverse of whatever it's here, right?

415
00:49:36,190 --> 00:49:51,500
So the prediction can be done. So this is basically how we play with the model outputs.

416
00:49:52,520 --> 00:49:55,580
You specify the model, you input the data.

417
00:49:55,610 --> 00:50:02,660
You've got a few things the beat estimates, the estimates, the estimates and the random effect predictions which can be used to predict trajectory.

418
00:50:03,890 --> 00:50:08,840
What a few remarks. So far we have assumed that the distribution random effects are multiple.

419
00:50:08,840 --> 00:50:12,350
Is multivariate normal? Does it have to be multiple or normal?

420
00:50:13,040 --> 00:50:19,510
It doesn't have to be. There is even fancier things you can do.

421
00:50:20,320 --> 00:50:35,350
For example, there is a process. I think if you work with anybody in the department.

422
00:50:36,780 --> 00:50:41,490
Probably you will encounter this. So it basically says that.

423
00:50:43,210 --> 00:50:47,550
By follows distribution. But we do not know what's that distribution.

424
00:50:47,560 --> 00:50:51,850
Let's put onto it. Actually, let me make it more make it more precise.

425
00:50:52,870 --> 00:50:59,949
It's a fun, fun thing to explore if you have time. It follows an F, but we do not know what F is.

426
00:50:59,950 --> 00:51:06,250
So we put a prior flash and it's a three year process and it's much more flexible.

427
00:51:09,870 --> 00:51:14,430
And but what's the consequence of potentially missing specifying the random effects?

428
00:51:14,760 --> 00:51:24,060
It turns out that, especially for discrete response variables, misses specifying random effects can have some big impacts.

429
00:51:24,780 --> 00:51:29,250
Um, for, I would say for prediction or random effects.

430
00:51:35,990 --> 00:51:43,270
And. However, I think it is again a hand-waving statement that in general the estimates of fixed

431
00:51:43,270 --> 00:51:47,980
effects are less sensitive to this specific specification of random effect distributions.

432
00:51:48,460 --> 00:51:58,030
So if you care about beta in general, those dependance of beta estimates upon the PS distribution specification is going to be less sensitive.

433
00:52:00,380 --> 00:52:10,700
Um, so it is interesting that in 1990s that people just published top general papers on these topics and now it's like in the textbook.

434
00:52:11,420 --> 00:52:14,670
So I think in general. You know what?

435
00:52:14,670 --> 00:52:18,210
I was going to ask you if it's quite exciting, but now it's kind of, hey, we know this already.

436
00:52:23,560 --> 00:52:28,330
Here it is about the assumption of buy independent of excise.

437
00:52:28,330 --> 00:52:32,950
So this means that the random effects distribution does not depend on.

438
00:52:33,760 --> 00:52:42,790
But what if this is not true? So one example is that one exposure group is more heterogeneous than another.

439
00:52:42,800 --> 00:52:54,160
So it is to say the distribution for by can depend on a cover like by if x i j equals one.

440
00:52:54,430 --> 00:52:59,290
It follows F1 distribution by given x aj equals zero.

441
00:52:59,290 --> 00:53:04,630
It follows f zero. Say this is very spread and this is very concentrated.

442
00:53:05,140 --> 00:53:10,150
Then it may have a big impact on the beat estimates.

443
00:53:10,510 --> 00:53:21,970
It turns out that my adviser and his student, my brother, actually wrote a paper in 2000 talk about talking about this issue.

444
00:53:33,310 --> 00:53:38,740
Basically their point of view is that if you are concerned about us, just use marginal models.

445
00:53:39,610 --> 00:53:44,889
And they have made some good points there because as to advance, I'm not going to talk about just talk about that.

446
00:53:44,890 --> 00:53:51,490
But just to give you a sense, there are some, you know, technical reasons why margin models sometimes could be preferred.

447
00:53:51,910 --> 00:54:07,000
So I'm going to just leave that reference here. If you want to do any independent reading with me on this, we can talk.

448
00:54:07,020 --> 00:54:14,160
But I think in general, all these final remarks I have said are a little bit subtle.

449
00:54:14,490 --> 00:54:23,910
I think for your projects, you know, I think if you care about data, then clearly you can just do the multivariate normal for the random effects.

450
00:54:24,210 --> 00:54:31,200
And don't worry too much about it unless you have a program that can run by distribution, that random effect distribution, that's not normal.

451
00:54:31,410 --> 00:54:39,590
But otherwise, I myself would not be too concerned. And second is that you have to be a bit you have to explore a little bit.

452
00:54:39,600 --> 00:54:41,790
For example, if you're modeling the random effects.

453
00:54:42,180 --> 00:54:48,950
I do see hugely different variabilities in the, you know, tightness of, say, of those trajectory between two groups.

454
00:54:48,960 --> 00:54:53,640
If so, then clearly you are violating this assumption of being independent of that site.

455
00:54:53,880 --> 00:54:57,000
Then you have to consider what to do.

456
00:55:00,030 --> 00:55:03,599
But within this within the scope of this class, I think if you say, hey,

457
00:55:03,600 --> 00:55:11,050
we said this multivariate Gaussian random effect model with the by independent of X, I would do that.

458
00:55:11,070 --> 00:55:18,510
So this is a result in a discussion we say that, hey, Jim said in the class he referred to this paper that maybe some issues and then you can talk

459
00:55:18,510 --> 00:55:23,460
about these issues that I know that you have watched this video or you have attend this lecture,

460
00:55:24,120 --> 00:55:30,780
at least you know it is it is kind of trying to push you a little bit towards more realistic data analysis.

461
00:55:31,050 --> 00:55:38,459
But I know that this class is not supposed to be a have to say a dissertation research course.

462
00:55:38,460 --> 00:55:44,950
So I have to provide some scope. So as long as you do the standard models, I'm going to be okay.

463
00:55:45,030 --> 00:55:48,540
Just. You have to be aware of the limitations. All right.

464
00:55:48,780 --> 00:55:52,020
Let's take 5 minutes break and then we can spend some time on the example.

465
00:55:52,830 --> 00:58:21,350
And that will be 10.10 psi. Sorry.

466
01:02:30,350 --> 01:02:36,670
All right. So let's get back to work and we're going to have a relatively light hand out.

467
01:02:36,680 --> 01:02:46,530
It's a hand out Tennessee. So I think it's just trying to demonstrate, see what you have learned regarding the inference and interpretation in Java.

468
01:02:46,580 --> 01:02:51,110
MM How is that going to be applied in this date example?

469
01:02:51,650 --> 01:03:00,470
So the learning objective here is to illustrate a formulation fitting and interpretation using the case study involving repeated binary outcomes.

470
01:03:00,860 --> 01:03:09,410
So data is amenorrhea data, it's a trial contraceptive women and the model will be a logistic regression with random effects.

471
01:03:09,890 --> 01:03:15,050
So what's the study background? There are lots of words here, but I think it's pretty straightforward.

472
01:03:15,410 --> 01:03:25,610
So two treatments, a 100 milligram or 150 milligram of this drug called Dmpa on the day of randomization.

473
01:03:26,540 --> 01:03:31,549
And we have follow ups for a few occasions.

474
01:03:31,550 --> 01:03:35,840
So the timeline looks like this. So it's zero. It's called baseline.

475
01:03:37,320 --> 01:03:47,490
And we have 100 milligram this drug or 150 milligram, the drug is coded as one of zero as treatment variable.

476
01:03:47,700 --> 01:03:51,440
It does not change over time and it has a few.

477
01:03:51,450 --> 01:03:57,840
Every person has a few follow ups. So this is zero, three months, six months, nine months.

478
01:03:58,650 --> 01:04:04,650
And then you have this is 1 to 3, four.

479
01:04:05,520 --> 01:04:13,170
You have a 12 month follow up. So everybody will have a baseline for measurements after the follow up.

480
01:04:14,840 --> 01:04:20,450
So throughout the study, each woman completed the menstrual diary that recorded any vaginal bleeding pattern.

481
01:04:20,450 --> 01:04:25,190
The spirit terminus is the diary data were used to determine whether women experience

482
01:04:25,560 --> 01:04:30,590
amenorrhea the absence of menstrual bleeding for a specific specified number of days.

483
01:04:31,010 --> 01:04:35,180
So this is representing the fact that the outcome will be a binary outcome.

484
01:04:35,690 --> 01:04:47,750
And there is a total of 1251 women who completed the menstrual diaries, and we use them to generate the binary sequence, the exact definition.

485
01:04:48,680 --> 01:04:56,090
I believe somewhere in the original paper, but I think for the simplicity, trust us that the data itself has been processed and clean.

486
01:04:58,640 --> 01:05:02,510
So for this study, there is there was substantial dropout.

487
01:05:03,500 --> 01:05:11,150
So in this particular analysis, we will set this issue aside, but we can discuss the missing data issue in the later lecture.

488
01:05:11,540 --> 01:05:17,210
So if we're going to finish this slide, I think we do have one extra lecture so I can talk about missing data, I guess.

489
01:05:20,160 --> 01:05:27,440
Know the outcome, again, is a binary response indicating whether women experienced amenorrhea in the in the four successive three month intervals.

490
01:05:27,830 --> 01:05:36,139
What's the goal of analysis to determine the subject's specific changes in the risk of amenorrhea over over the

491
01:05:36,140 --> 01:05:44,150
course of the study 12 months and the influence of dosage of dmpa on the changes in the woman's risk of amenorrhea.

492
01:05:44,390 --> 01:05:48,260
So essentially the first covered is the time, right.

493
01:05:48,920 --> 01:05:53,120
As you move along in the study. What's the what? How does it impact the risk?

494
01:05:53,150 --> 01:05:56,060
So it's a within subject change of the cohort.

495
01:05:56,420 --> 01:06:04,610
However, for the drug dmpa, you know, once a person got assigned to a drug sorry, a dose, that dose is constant.

496
01:06:04,850 --> 01:06:08,000
It is time invariant across across the period of study.

497
01:06:08,240 --> 01:06:11,510
So it is actually has to be between subject comparison.

498
01:06:11,900 --> 01:06:17,870
And I think the message there is that we've got to be careful what that means when you do not have a.

499
01:06:19,570 --> 01:06:25,690
The treatment does not change over time. So what's the model?

500
01:06:25,720 --> 01:06:29,380
Well, here is a model we just coated up with.

501
01:06:30,290 --> 01:06:43,870
Y j representing the binary outcome. And we are putting down the main step of time the quadratic term for time interaction between the dose.

502
01:06:43,880 --> 01:06:47,870
Basically, it's a binary indicator, one for the higher dose, zero for a lower dose.

503
01:06:48,260 --> 01:06:56,180
And we also interrupt dose indicator with a quadratic term and we have the random if random intercept by here.

504
01:06:56,570 --> 01:07:02,270
So this VI basically is representing that, you know, even before the study started,

505
01:07:02,780 --> 01:07:07,180
different women have different risks of of having this binary outcome.

506
01:07:07,190 --> 01:07:11,930
So we also say that different people have different baseline propensity of developing the outcome.

507
01:07:12,500 --> 01:07:17,690
And because we are dealing with the study that collected data with based randomization.

508
01:07:17,690 --> 01:07:24,730
So again, you don't see the. You don't see the dose manufacturing there.

509
01:07:24,960 --> 01:07:28,910
Okay. So it is basically protected by the randomization.

510
01:07:32,010 --> 01:07:41,250
And here we basically is placing faith on the randomization to hope that it has balanced out the differences between the two treatment groups.

511
01:07:41,640 --> 01:07:46,190
What anybody in the seminary today. Now.

512
01:07:47,100 --> 01:07:53,500
You guys, when you have time, go to seminars. So yesterday there was a job candidate talking about re randomization.

513
01:07:53,520 --> 01:07:59,690
If you randomize people into two groups, what if the two groups have imbalances on observed coworkers?

514
01:07:59,700 --> 01:08:03,239
What do you do? You proceed with a study or do re randomize.

515
01:08:03,240 --> 01:08:11,010
So that guy is saying hey, let's re randomize until the observed cohorts seems to have balanced between the two groups.

516
01:08:11,940 --> 01:08:21,690
Go to seminars if you can. So that's so I suppose we can say hey now we have done this randomization but and and again the main reason we do not

517
01:08:21,690 --> 01:08:27,150
include the main effect for dose is because we think the randomization has protected against imbalance between two groups,

518
01:08:27,780 --> 01:08:34,560
although chance imbalance can occur. What is the what is the.

519
01:08:37,600 --> 01:08:40,580
Distributional model we are going to assume for a while.

520
01:08:40,640 --> 01:08:49,270
J It's a binary verbal, so we're going to assume it's a Bernoulli distribution and without any over dispersion.

521
01:08:49,720 --> 01:08:56,350
So if you look at bullet for it is saying the variance takes a classical Bernoulli variance form and the scale parameters one.

522
01:08:56,470 --> 01:09:01,180
So just Bernoulli no over dispersion here. Can we do something better?

523
01:09:01,180 --> 01:09:06,820
Yes, maybe we can. But here for illustration, I'm just going to set five as one for the random effect.

524
01:09:07,690 --> 01:09:13,570
We're going to specify it's a Gaussian with zero and a single variance of sigma B squared.

525
01:09:15,100 --> 01:09:26,790
As I have alluded to, the fact that we include by here is that we think different women have different risks of baseline, you know, based on risks.

526
01:09:26,900 --> 01:09:35,690
Okay now by this thing is not in the is not indexed by J OC so regardless of what

527
01:09:35,710 --> 01:09:42,040
occasion by will be applied to that occasions chance of developing the outcome.

528
01:09:42,400 --> 01:09:46,060
So it is reflecting the overall propensity of developing outcome.

529
01:09:48,720 --> 01:09:55,380
Now this is the code. You don't have to read them on the screen, but I'm going to tell you what they are.

530
01:09:56,340 --> 01:10:00,720
You can have the slide in front of you on a computer so you can refer to this later on in here.

531
01:10:00,870 --> 01:10:05,670
This is a library. Let me. Four. And the function is g l m e r.

532
01:10:06,090 --> 01:10:09,830
I don't. I don't think you have used this function before. At least not a product.

533
01:10:09,840 --> 01:10:16,530
Probably not in the homework so far, but this can be used to fit generalize and to mixed models.

534
01:10:16,920 --> 01:10:22,440
So here I am specifying the families binomial it will give us the specification for the variants

535
01:10:24,120 --> 01:10:31,410
and here in a g q is representing the number of adaptive Gaussian cause your points so

536
01:10:31,650 --> 01:10:36,719
specifying that to 50 you know you can specify to 100 you will find that it is going to be a

537
01:10:36,720 --> 01:10:42,900
little slower for that missing data and the dot action here for now I'm just going to omit.

538
01:10:45,470 --> 01:10:54,840
It roads that have missing data. For this particular main specification, you can see that it is including the time,

539
01:10:55,350 --> 01:11:02,190
quadratic term of time and the interaction between the treatment group and time interaction between the treatment group and quadratic time.

540
01:11:02,610 --> 01:11:12,540
And finally, this little thing, this is representing that for observations from the same person defined by the same ID,

541
01:11:13,740 --> 01:11:16,800
there is going to be one added random intercept term there.

542
01:11:20,570 --> 01:11:26,510
So when you look at the results, you can find that this is the estimate, basically the.

543
01:11:29,770 --> 01:11:34,750
The random, if at variance is estimated as 5.065.

544
01:11:34,990 --> 01:11:40,900
So this is saying, hey, you know, there's a huge variability in the baseline risk for the outcome.

545
01:11:42,250 --> 01:11:47,240
And these are the beta keys. And we will return to these later.

546
01:11:47,260 --> 01:11:50,260
We don't have to look at them at this time.

547
01:11:50,530 --> 01:11:55,569
But in general, when you are trying to study whether treatment worked or not,

548
01:11:55,570 --> 01:11:59,080
you're focused on the interaction term between the treatment and the time.

549
01:11:59,500 --> 01:12:05,920
And if they are significant, it is indicating that the treatment somehow has altered the trajectories of the MI responses.

550
01:12:05,950 --> 01:12:12,820
Right. And both terms seems to be significant. In the next slide, I'm showing you the.

551
01:12:14,050 --> 01:12:20,200
I just copy it out to our help file indicating what is in HQ.

552
01:12:20,230 --> 01:12:28,420
You don't have rhythm, but essentially it is the number of points for the adaptive gauss hermitt approximation.

553
01:12:30,460 --> 01:12:33,880
So you can always check. For me, 50 works well.

554
01:12:41,270 --> 01:12:45,860
In this slide, I'm going to skip because it is just trying to interpret the betas.

555
01:12:46,220 --> 01:12:53,790
I want to sort of spend some time on the slide, which is how which is to interpret the between subject covariates.

556
01:12:53,810 --> 01:13:01,550
Remember, every subject either got a high dose or a low dose assignment, and that assignment does not change over time.

557
01:13:02,450 --> 01:13:06,200
Given that we are fitting a generalized mix model. What does that mean?

558
01:13:06,410 --> 01:13:14,960
Right. Because in generalizing a mixed model, we care about beta C for every unit increase in the covered that can change within that person.

559
01:13:15,290 --> 01:13:19,850
We have blah blah change in the mean response. But what about this dose?

560
01:13:20,240 --> 01:13:24,650
So it turns out that is actually not that straightforward to interpret.

561
01:13:25,880 --> 01:13:29,270
We can try, but it is just not that straightforward.

562
01:13:31,860 --> 01:13:37,420
So here is the truck. Okay. For two different women.

563
01:13:39,160 --> 01:13:45,670
OC who happened to have the same underlying risk of experiencing amenorrhea prior to randomization.

564
01:13:46,120 --> 01:13:49,690
Basically, this is to say the same by.

565
01:13:51,320 --> 01:13:54,760
For two women who happens to be having the same random effects.

566
01:13:54,770 --> 01:13:58,310
I know that's kind of a little bit hypothetical.

567
01:13:59,860 --> 01:14:03,900
But who differ in the terms of dosage. One is in the treated high dose group.

568
01:14:03,910 --> 01:14:09,209
The other is a low dose group. And we can make this contrast.

569
01:14:09,210 --> 01:14:14,040
And that contrast is basically the coefficient in front of the treatment indicator.

570
01:14:14,370 --> 01:14:23,430
So this is less than satisfying because how do you know there is a person who is comparable and by.

571
01:14:24,470 --> 01:14:29,540
They're in the to two groups. It's hard to know, although you can say that.

572
01:14:29,540 --> 01:14:34,460
Hey, so we're randomization. You probably can guarantee that, but you do not know who is who.

573
01:14:34,490 --> 01:14:39,920
I hope which two have the same bits, so it can be a little bit tricky to interpret.

574
01:14:39,920 --> 01:14:44,050
But this is our best try to interpret that between subject cover.

575
01:14:45,910 --> 01:14:49,660
So using that perspective, it is to say that.

576
01:14:52,470 --> 01:14:58,260
The ratio of the increased odds of amenorrhea to 12 months for women assigned to the high dose versus another woman

577
01:14:58,260 --> 01:15:05,610
with the same risk of amenorrhea prior to randomization was assigned a low dose as this one essentially is the.

578
01:15:08,610 --> 01:15:11,910
It's basically trying to interpret the coefficient. You find the treatment.

579
01:15:16,480 --> 01:15:23,080
Finally we have this random effect variance estimate of 5.065.

580
01:15:23,410 --> 01:15:32,500
So whenever you have this number, you can use this number to calculate the baseline, how to say what's the range of baseline risk.

581
01:15:32,920 --> 01:15:42,100
So essentially you are need. You're going to look at the model with these terms.

582
01:15:47,890 --> 01:15:51,460
Right. That's it. Because we're only looking at the baseline.

583
01:15:51,470 --> 01:15:58,370
So everything that's not baseline is going away. Basically time equals zero.

584
01:16:04,610 --> 01:16:07,670
Then you can have the speed at zero from the model estimate.

585
01:16:08,360 --> 01:16:15,990
It is this one. And then you can have VI, which has a estimated variance of 5.065.

586
01:16:16,440 --> 01:16:27,510
So what you do is that you just do x bit of theta zero hat -1.9, six times the square root of the Sigma Square.

587
01:16:28,160 --> 01:16:34,450
The estimate. And then do the other one. So it should be pretty straightforward.

588
01:16:39,850 --> 01:16:44,410
And this is what you got. So the base interest seems to be very, a lot.

589
01:16:46,590 --> 01:16:52,250
Representing the heterogeneity across people. So.

590
01:16:53,760 --> 01:16:57,300
Do we want to fit a model that has a more random effects?

591
01:16:57,450 --> 01:17:01,980
Look, we only fit in model with random intercept. Can we play with a random model?

592
01:17:02,700 --> 01:17:06,780
Yes, you can. But sometimes the information can be quite limited in the data.

593
01:17:07,230 --> 01:17:11,040
Remember, we are only dealing with four occasions.

594
01:17:11,310 --> 01:17:15,630
Right. And. And the data are binary.

595
01:17:17,430 --> 01:17:24,330
And it's actually quite hard to study more than one random effects in these kind of settings.

596
01:17:27,070 --> 01:17:35,649
And how is how does it manifest? Usually when you have say, you know, if you put in, say, three random effects, random randomness at random.

597
01:17:35,650 --> 01:17:40,690
So for the time. Random stuff of times squared. In general, the algorithm will fail to converge.

598
01:17:40,690 --> 01:17:41,740
You will throw errors.

599
01:17:42,160 --> 01:17:49,660
So those are indications that actually, although the model itself is legitimate, but the data is struggling to give you estimates.

600
01:17:56,180 --> 01:17:59,690
The final three slides are pretty simple.

601
01:18:00,260 --> 01:18:08,180
Essentially, it is trying to say that the choice of the number of positive points actually matters in terms of the estimation.

602
01:18:08,210 --> 01:18:15,410
Remember that when we are doing the optimization, we basically need to calculate this likely to function.

603
01:18:15,680 --> 01:18:22,260
After integrating all the bits and the the better we can approximate this function that the,

604
01:18:22,570 --> 01:18:28,130
the better we can get the estimates close to to the optimal estimates.

605
01:18:29,180 --> 01:18:36,009
So. This is a kind of example where you're playing with different numbers.

606
01:18:36,010 --> 01:18:43,510
Of course, your points are either in HQ. If you have one quadrant of points, it is going to be extremely fast.

607
01:18:44,890 --> 01:18:57,490
But. You know, if you increase more, say, to 50, the log likelihood clearly increased more meaning that if when you were using 100 points,

608
01:18:57,490 --> 01:19:01,600
the local has now been maximized and then the commuting time definitely increased.

609
01:19:01,990 --> 01:19:06,550
And also looking at looking at overall trend, the log likely seems to be stabilizing.

610
01:19:07,060 --> 01:19:14,980
And, you know, the estimates of the parameter sigma B squared seems to be stabilizing as well.

611
01:19:15,340 --> 01:19:18,880
So pretty much for here, I guess at 20 quadrant points is good enough.

612
01:19:19,180 --> 01:19:25,780
So it strikes a good balance between the approximation for the target function and also the computing time.

613
01:19:26,290 --> 01:19:32,100
Again, I don't think there is a general recipe of how to choose the causation points.

614
01:19:32,110 --> 01:19:35,620
I guess you just have to try to the numbers and produce this table to get a sense that,

615
01:19:35,620 --> 01:19:39,880
hey, you know, beyond a certain point, the result seems to be stabilizing and satisfying.

616
01:19:44,470 --> 01:19:48,160
This is the final slide of this particular panel.

617
01:19:48,970 --> 01:19:52,180
What's the main point of this handout? What if we fit a model?

618
01:19:52,660 --> 01:20:03,320
Imagine a model. What do we get? Logit of.

619
01:20:11,710 --> 01:20:16,090
Equals excise duty data. So no random effects.

620
01:20:16,870 --> 01:20:26,300
Right. What do we get? So these are the estimates here. I'm going to write down the all the other estimates from glamor.

621
01:20:27,400 --> 01:20:35,570
These are the -3.8 or six. 1.130.04.

622
01:20:36,790 --> 01:20:42,160
Two minus and 0.5, six and minus.

623
01:20:43,150 --> 01:20:52,810
0.1096. So if you contrast each pair, you can see that, hey, using the marginal model.

624
01:20:54,780 --> 01:21:00,250
Almost everything is shrunk. Towards zero.

625
01:21:02,340 --> 01:21:05,790
So this phenomenon actually is theoretically predictable.

626
01:21:06,720 --> 01:21:10,350
It is what we call attenuated coefficient estimates relative to July.

627
01:21:10,350 --> 01:21:19,200
Mm. And the reason is very simple in that when you start, when you start with this model.

628
01:21:33,240 --> 01:21:38,000
So just with one by och. And then you can derive right.

629
01:21:38,070 --> 01:21:55,630
You can derive the marginal form. Is approximately.

630
01:21:56,710 --> 01:22:04,740
Expert of. Ixi j t beta c divided by one over one plus.

631
01:22:06,000 --> 01:22:09,810
M squared. So the m is a number. I will not show what that is.

632
01:22:11,400 --> 01:22:19,500
But the idea here is that if you start with a generalized near mix model and then you forcefully integrate LBI,

633
01:22:19,920 --> 01:22:24,390
it is not going to be exactly of a logit form, but it can be rewritten into this form.

634
01:22:24,730 --> 01:22:29,910
However, you will estimate what your estimates, the entire thing.

635
01:22:30,000 --> 01:22:33,960
So when you forcefully fit a marginal model, you will get here.

636
01:22:34,680 --> 01:22:41,700
And because it has always been a C divided by one plus something, so it is always going to be shrinking towards zero.

637
01:22:42,210 --> 01:22:45,960
I have not proven this, but this is some fact that caused this issue.

638
01:22:47,070 --> 01:22:53,040
So the final point is that when you said two different models, the same data, the betas can be quite different.

639
01:22:53,430 --> 01:22:56,610
And in logit case, it is always changing towards zero.

640
01:22:58,140 --> 01:23:02,940
All right, thanks, everybody, for your patience. And I think that covers the handout, Tennessee.

641
01:23:03,840 --> 01:23:09,780
Thanks, everybody. See you next week. Oh, by the way, next week we don't have the Wednesday class, so.

642
01:23:10,900 --> 01:23:12,340
Right. Is that right? Okay.

