1
00:00:02,430 --> 00:00:07,720
And let's turn now to a thing you want to bring out your guests.

2
00:00:07,740 --> 00:00:17,730
Put this in the guest bedroom. You want to save someone who is going to be stressed.

3
00:00:17,760 --> 00:00:26,999
And it's a pretty good chance he was going to lie and say, I had a guest lecture because I couldn't make it and it was going to be him.

4
00:00:27,000 --> 00:00:37,200
But I figured out that three people were going to come to class. So what I release homework for is done.

5
00:00:39,600 --> 00:00:43,080
Two more to go. At least one more to go. We'll see if we get to the last one.

6
00:00:45,740 --> 00:00:52,920
Cervical Cancer Exam 208.

7
00:00:53,520 --> 00:01:03,570
So Exam two, I have written the exam and I just want to give you some details.

8
00:01:03,810 --> 00:01:07,470
Try and be as clear as possible. All right.

9
00:01:08,290 --> 00:01:12,480
Last time the exam was 14 questions over and a half.

10
00:01:14,020 --> 00:01:17,860
So this time it's going to be 13 questions.

11
00:01:20,600 --> 00:01:27,420
It's going to be 2 hours only because I noticed most people took 90 minutes.

12
00:01:27,460 --> 00:01:33,680
Again, maybe you like to just stay on the computer for 90 minutes, but I'm going to add half an hour if you want it.

13
00:01:33,920 --> 00:01:40,880
If you don't want, it's there. 13 questions each equally weighted.

14
00:01:44,120 --> 00:01:48,860
So you know how my questions worked on the first sound.

15
00:01:57,890 --> 00:02:07,220
Little choice. So there are going to be seven questions that are multiple answers.

16
00:02:07,250 --> 00:02:10,600
So one of these tell me which of the following do you think are true?

17
00:02:11,350 --> 00:02:14,920
Could be one of them. Could be all of them. It's never none of them.

18
00:02:17,260 --> 00:02:20,680
So always pick one. You want to do a little bit of optimizing here.

19
00:02:21,310 --> 00:02:27,040
Multiple choice means there is only one correct choice. And I will say choose one of the following that you think is right.

20
00:02:28,630 --> 00:02:32,920
There's one matching. And it makes things up here a little bit.

21
00:02:32,930 --> 00:02:37,820
So there's one matching question and then numeric calculation.

22
00:02:44,400 --> 00:03:01,400
Three. So this is not data analysis that says, I give you information.

23
00:03:01,910 --> 00:03:06,470
You can punch the numbers into a calculator or our or whatever computer you want to use.

24
00:03:08,060 --> 00:03:12,890
But all the information is given and the problem there is no using our or anything like that.

25
00:03:13,310 --> 00:03:17,660
So given some information, you're going to computer number for me and tell me what that number is.

26
00:03:20,060 --> 00:03:26,060
So I think last time there was one. Now there are three. So we're talking about linear, mixed models.

27
00:03:26,390 --> 00:03:29,690
Oh, that's the other thing I want to specify.

28
00:03:30,260 --> 00:03:33,870
So, yeah, linear mix models and G.

29
00:03:35,180 --> 00:03:39,140
So no deal on M&Ms. We're going to cover that before the exam, but it's not on the exam.

30
00:03:41,150 --> 00:03:46,040
There are going to be five, six questions. So six questions on linear mixed models.

31
00:03:46,040 --> 00:03:52,910
There's four easy ones.

32
00:03:54,920 --> 00:03:58,100
And that leaves three questions. Right. Those.

33
00:04:01,120 --> 00:04:09,150
It's going to be three questions that put both together. Because again, there are times when G.E. and Eric's models are doing the same thing.

34
00:04:09,180 --> 00:04:13,829
We talked about this in class. So do you know when you can use the lyrics model?

35
00:04:13,830 --> 00:04:17,940
One can use G one. Could you do either one? Things like that.

36
00:04:18,000 --> 00:04:23,130
All right. So maybe skewed a little bit more towards linear mixed models, but not very much.

37
00:04:29,090 --> 00:04:38,780
So for linear, mixed models, what do you need to know? So you need to know the concept of a random intercept.

38
00:04:39,830 --> 00:04:44,210
Of course. And it slopes.

39
00:04:47,590 --> 00:04:51,520
What are we modeling by putting a random intercept in the model in an hour, remember?

40
00:04:52,630 --> 00:05:05,000
What are we doing when we put a random slope into that model? Because of her election insurance reform.

41
00:05:06,910 --> 00:05:15,970
I try to look a little professional when I'm lecturing the relationship to variance.

42
00:05:18,390 --> 00:05:37,860
Covariance. She's.

43
00:05:40,010 --> 00:05:45,260
Tween persons within.

44
00:05:51,610 --> 00:05:55,929
Subject scanned.

45
00:05:55,930 --> 00:06:00,009
How do random effects allow us to model within and between?

46
00:06:00,010 --> 00:06:08,760
Subject variation? Fiction of random effects.

47
00:06:11,950 --> 00:06:14,890
So again, we didn't do a whole lot of this in the homework,

48
00:06:14,900 --> 00:06:21,640
but the whole point of this is everybody's getting their own deviation from the population intercept.

49
00:06:21,770 --> 00:06:28,480
Those things are called blocks as linear, unbiased predictors, and we derive those from a Bayesian perspective.

50
00:06:31,080 --> 00:06:40,360
Model selection. So how do you pick between a random intercept or a random intercept slot model?

51
00:06:41,230 --> 00:06:45,250
How do you decide whether you should have an interaction in your model, things you did in the homework?

52
00:06:47,570 --> 00:06:50,590
Oh, and then again, we're going to do a little bit more of this today.

53
00:06:52,470 --> 00:06:58,980
On this. And we could do some more of it in term station.

54
00:07:00,340 --> 00:07:03,430
Right. So you fit a model. You get this output from our.

55
00:07:04,360 --> 00:07:07,510
How do you interpret the parameters in a linear mixed model?

56
00:07:10,730 --> 00:07:14,240
And then topics for Gini and the examples open note.

57
00:07:14,450 --> 00:07:26,959
So you can have all your lecture notes out there in front of you and so forth, bring whatever you would like to bring in and in.

58
00:07:26,960 --> 00:07:31,730
G What is the idea of the sandwich variants versus a model based?

59
00:07:34,370 --> 00:07:47,540
It's a smarter. Membership relationship g.

60
00:07:48,140 --> 00:07:56,270
G to g. Tell us how is G related to generalize these squares that we looked at earlier in the semester?

61
00:07:58,650 --> 00:08:05,030
And really all that is left is, again, this model selection that if we choose between different covariate structures,

62
00:08:05,620 --> 00:08:12,710
this idea of QIC and want to go through that a little bit today, given what I emailed you about this weekend.

63
00:08:13,810 --> 00:08:15,700
And the parameter interpretation.

64
00:08:20,930 --> 00:08:30,070
So how do you interpret coefficients from a persan or a binomial g model in a way that folks can can understand what's going on?

65
00:08:30,400 --> 00:08:34,900
We don't have a best in class. Right.

66
00:08:34,940 --> 00:08:41,320
So that's a lot. Oh, that is everything in the lecture notes that I could think of asking a question of.

67
00:08:47,580 --> 00:08:56,220
Again. It's going to open. It opens on Thursday, November 18th, 12 noon.

68
00:08:58,820 --> 00:09:01,880
And it's going to close the following Tuesday.

69
00:09:04,400 --> 00:09:08,060
12:18 p.m. Friday.

70
00:09:08,510 --> 00:09:12,979
Is it today? What's one end up in the last exam?

71
00:09:12,980 --> 00:09:23,830
Was it Friday? Yeah. Much less severe.

72
00:09:24,100 --> 00:09:30,130
So Friday the 18th. So that gives you prior to Friday, Saturday, Sunday, Monday or Tuesday.

73
00:09:32,030 --> 00:09:35,090
If. 2 hours.

74
00:09:35,700 --> 00:09:38,840
I think that's doable. Some questions are longer than others.

75
00:09:38,870 --> 00:09:42,230
Some should take very little time. Some will take a little bit of that.

76
00:09:46,430 --> 00:09:47,120
I will go in.

77
00:09:47,120 --> 00:09:55,280
So when you finish the exam again, you will get a grade because I need to go in and try and resolve some personal credit as much as I can.

78
00:09:56,210 --> 00:10:05,810
So I will go in after the fact and modify things as much as I can in a positive direction, not in a negative direction.

79
00:10:07,010 --> 00:10:11,060
Other questions? Yes. Will you be in your office on Friday?

80
00:10:11,210 --> 00:10:15,410
I have that one again. I'm sure.

81
00:10:16,220 --> 00:10:22,120
I'm sure. He says he sure. Really?

82
00:10:24,370 --> 00:10:27,790
That's great.

83
00:10:28,570 --> 00:10:33,210
December 18th. November 18th. Focus.

84
00:10:33,280 --> 00:10:36,310
We only have class. Right. The idea.

85
00:10:36,790 --> 00:10:39,879
Yes. I have it on my calendar that I will be busy with class.

86
00:10:39,880 --> 00:10:51,700
So I'll be in my office during that regular again support class on Friday the 18th.

87
00:10:55,970 --> 00:11:00,260
Jim Woolsey and I experience office.

88
00:11:03,710 --> 00:11:08,260
To 330. And in fact, I'll say 2 to 4 since the exam is 2 hours long.

89
00:11:11,910 --> 00:11:15,230
To stick around for 2 hours, 2 to 4 p.m. on that day.

90
00:11:25,210 --> 00:11:29,290
That's exam number two. Oh. Final exam?

91
00:11:32,170 --> 00:11:36,310
Yeah, this is it. No, it's not my order. You know I'm not.

92
00:11:37,190 --> 00:11:44,190
But that's exam two coming up soon. Hard to believe you're in there semester.

93
00:11:44,230 --> 00:11:48,420
Ready? They're going two do.

94
00:11:48,430 --> 00:11:51,520
Oh, yes. The final exam.

95
00:11:55,910 --> 00:12:06,050
So the final exam, as I mentioned last week, is going to be an analysis of federal data that you and I are submitting every week.

96
00:12:09,080 --> 00:12:12,620
I'm going to ask a series of questions related to the data.

97
00:12:13,220 --> 00:12:25,400
So in our Studio Cloud right now, I have a folder called Plan with them and in there is an example of what the sample data will look like.

98
00:12:25,430 --> 00:12:29,870
So I pulled off the first five weeks of information just to show you what it looks like.

99
00:12:30,080 --> 00:12:33,260
These are not the data that you will analyze,

100
00:12:34,070 --> 00:12:41,420
but if you want to see what the data will look like and you can play around with near mix models and so forth,

101
00:12:41,420 --> 00:12:46,700
you're now being given a chance to do so before the test security.

102
00:12:46,730 --> 00:12:55,600
So I loaded it and here's what it looks like. So I have taken all of your IDs and I have changed them into a number from 1 to 56.

103
00:12:55,610 --> 00:13:01,249
I'm included in this data. So there's 56 of us. So randomly to you as a number 21 is 56.

104
00:13:01,250 --> 00:13:06,829
So no student IDs in here. I took the dates and put them in chronological order.

105
00:13:06,830 --> 00:13:12,200
So you just have week number first four zero is whether you told the computer,

106
00:13:12,200 --> 00:13:15,890
whether you knew circle what the game was before the semester started or not.

107
00:13:17,390 --> 00:13:23,090
This is your age. You can see there's one individual in this dataset who is 54, so that's not very a blended,

108
00:13:24,560 --> 00:13:30,860
but otherwise I don't think you're gonna be able to tell who is who hated my data pretty easily.

109
00:13:31,700 --> 00:13:36,319
My score is the score you got on Farkle computer score.

110
00:13:36,320 --> 00:13:41,420
Cup score is the score that the computer got. And then I had created a variable called winner.

111
00:13:41,430 --> 00:13:44,390
So winner was if you got a score higher than the computer.

112
00:13:45,630 --> 00:13:53,340
So again, the beauty of these data are that I could have analyzed a continuous outcome as you analyze a binary outcome.

113
00:13:55,110 --> 00:14:00,120
So that was intentional. So those are the data.

114
00:14:00,120 --> 00:14:06,390
That's what they look like again. And I'll add the rest of the semester hoping I'll be able to get the last semester on there.

115
00:14:09,510 --> 00:14:13,890
I thought about giving you the original data set as it all entered Jornada.

116
00:14:15,600 --> 00:14:18,900
There are lots of dates in there that are just wrong.

117
00:14:20,850 --> 00:14:25,710
You know, you transpose the 2022, some of you used your birth year.

118
00:14:28,500 --> 00:14:31,890
Other kinds of stuff, ideas that didn't.

119
00:14:32,490 --> 00:14:37,630
You know, you transpose a number in your ideas. And so I had to go back and figure that out.

120
00:14:37,650 --> 00:14:42,820
But anyways, it's always interesting with a real desire to be straightforward and it's not.

121
00:14:42,840 --> 00:14:47,280
Anyway, I think you're going to have complete data.

122
00:14:47,310 --> 00:14:51,270
I think I'll have the same number of observations for everybody, but I don't know.

123
00:14:51,300 --> 00:14:55,380
So the semester goes. Yeah. So the data are there.

124
00:14:55,740 --> 00:15:00,190
The plan, again, is to. I don't know how long this exam should stay open.

125
00:15:00,210 --> 00:15:08,910
I'm trying to think if I'm going to do this, I don't. Oh, yeah. Can you just send out, like, another, like, update on who is on track for those?

126
00:15:09,060 --> 00:15:12,360
Yes. Yeah.

127
00:15:12,360 --> 00:15:17,700
I did it for a couple of people who are deficient. But if everybody else was in the ballpark, I just left it alone.

128
00:15:21,700 --> 00:15:27,100
Again, if you're missing one of your Farkle scores, I'm not going to take one plane off of your final grade.

129
00:15:27,460 --> 00:15:32,150
It's just a way to get you guys to do this. But I'm not that kind of a jerk, so.

130
00:15:33,040 --> 00:15:36,310
But I would like everybody to have enough. So I wanted you. But don't worry.

131
00:15:37,390 --> 00:15:42,880
If you're off by a couple, just play a couple more times and we'll get it all straightened away.

132
00:15:44,730 --> 00:15:50,590
Yeah. So don't lose data if you want. And think about how you might analyze the GDP model or a linear mix model.

133
00:15:51,610 --> 00:15:59,830
You know, is there a trend over time as the mean score of individuals varied by whether they knew how to play Farkle or not?

134
00:16:00,130 --> 00:16:03,730
There's a group variable there. All the things we've been doing in the homework assignments.

135
00:16:05,320 --> 00:16:09,320
All right. That's the plan for the final exam.

136
00:16:18,370 --> 00:16:22,310
Yes. Yes, yes. That's right.

137
00:16:26,540 --> 00:16:31,909
So let's do this.

138
00:16:31,910 --> 00:16:41,840
Let's turn to. I want to review some parameter interpretation.

139
00:16:46,360 --> 00:16:52,240
Because this just might be on the exam, as I have said many times in this class.

140
00:16:53,170 --> 00:16:55,490
All right. I want to make sure everybody is up to speed there,

141
00:16:55,540 --> 00:17:01,780
because I'm realizing that maybe you didn't get as much time with this in your previous classes as as I hoped.

142
00:17:03,280 --> 00:17:15,940
Give an example here. Longitudinal 653 longitudinal over their longitudinal data.

143
00:17:18,730 --> 00:17:35,360
100 individuals six. Outcomes for an individual.

144
00:17:38,630 --> 00:17:46,330
I. At time.

145
00:17:46,330 --> 00:17:52,450
J J It's going to be one or two, three or four. It's going to be binary.

146
00:17:58,780 --> 00:18:04,080
Right. So again, I'm going to be generic here.

147
00:18:04,090 --> 00:18:07,669
There's some event you're measuring over time in these people and zeros.

148
00:18:07,670 --> 00:18:11,470
The indicator that nothing happens and one is indicated that something hasn't.

149
00:18:12,190 --> 00:18:15,250
And again, what are we interested in? We're interested in the probability.

150
00:18:18,180 --> 00:18:28,510
My energy is equal to a one. Right.

151
00:18:28,520 --> 00:18:41,580
So you use gaming. Right.

152
00:18:41,760 --> 00:18:47,060
And in the real world, you would tell me what working correlation structure you used as a matrix, right?

153
00:18:47,070 --> 00:18:54,720
In estimation. But we did that tell you we knew which one it was.

154
00:18:58,470 --> 00:19:07,530
So you're going to get parameter estimates, the standard errors. For this model to log.

155
00:19:09,790 --> 00:19:13,120
PJ One minus. PJ Right.

156
00:19:13,180 --> 00:19:16,300
Logit. The logit link that thing.

157
00:19:17,430 --> 00:19:22,500
It's an intercept. What's this?

158
00:19:39,990 --> 00:19:45,590
So again, two groups of individuals in these data. So we've got an indicator of whether or not they're in group two.

159
00:19:49,830 --> 00:19:59,910
J is 1234 continuous time, not categorical time, air and time is continuous.

160
00:20:07,430 --> 00:20:11,720
And here is your output. You get a coefficient, you get an estimate.

161
00:20:13,080 --> 00:20:16,960
You get a standard here and we're going to use the empirical standard errors.

162
00:20:16,980 --> 00:20:24,990
The sandwich variance estimate doesn't really matter. For the purposes of our discussion here, we're going to talk about interpreting coefficients.

163
00:20:29,420 --> 00:20:35,479
And you get a 0.15 here and a 0.52 here in the negatives.

164
00:20:35,480 --> 00:20:39,049
0.370.

165
00:20:39,050 --> 00:20:43,250
3.910.48.

166
00:20:43,790 --> 00:21:02,710
So there's your output. So, you know, go back to your investigators that you're working with.

167
00:21:03,640 --> 00:21:06,670
And how do you describe the association?

168
00:21:10,980 --> 00:21:22,050
Of group membership. The probability of having an event.

169
00:21:30,360 --> 00:21:34,080
Is there a relationship between group membership and the event occurring?

170
00:21:36,820 --> 00:21:44,970
How would you do that? Which coefficient are we talking about?

171
00:21:48,030 --> 00:21:53,040
Okay. Beta one. So 0.52.

172
00:21:54,840 --> 00:21:58,020
Does that mean anything? Yeah. It means something.

173
00:21:58,620 --> 00:22:04,920
It means something. Is it interpretable? I guess I should say for most folks, it's not.

174
00:22:05,790 --> 00:22:12,110
What is interpretable? Thanks to the exponential.

175
00:22:12,620 --> 00:22:17,210
So we might think about computing the exponential value of 0.5 to.

176
00:22:19,860 --> 00:22:28,740
It's 1.68. I did this at a time. What is 1.68 odds ratio?

177
00:22:38,200 --> 00:22:45,390
So it's the odds of event. It's in the numerator.

178
00:22:52,220 --> 00:22:57,200
So it's group one in the numerator or the denominator as crucial in the numerator or the denominator.

179
00:22:58,550 --> 00:23:02,600
The group which group two is in the numerator.

180
00:23:10,640 --> 00:23:14,720
So what's the odds ratio? Comparing Group two to Group one.

181
00:23:16,460 --> 00:23:21,590
So Group two has 68% higher odds of the event than Group one does.

182
00:23:22,640 --> 00:23:26,650
Whatever that is, right?

183
00:23:30,120 --> 00:23:36,780
And same way for time again. I should be more specific at the same visit.

184
00:23:38,020 --> 00:23:41,080
Very controlling for time. There's a tiny variable in here.

185
00:23:42,040 --> 00:23:48,550
So she has to be the same in both in the comparison between the group to group one person.

186
00:23:52,540 --> 00:23:56,080
And likewise, we get exponentially -0.37.

187
00:23:58,440 --> 00:24:06,720
And that examines the odds ratio between two different time points in the same group for the tragedy of the event occurring.

188
00:24:08,820 --> 00:24:12,680
Right. So make sure you you grasp all of that.

189
00:24:14,520 --> 00:24:24,270
Again, if this is the standard error of beta one, then we can talk about getting a standard error for the odds ratio.

190
00:24:25,110 --> 00:24:29,970
You can talk about getting a 95% confidence interval. Not going to have to do that on a test.

191
00:24:31,320 --> 00:24:35,780
You don't need to do a Delta message in five, 6 minutes.

192
00:24:36,950 --> 00:24:42,800
I'm not going to have to do any of that. But it's important to be able to interpret these things for folks much less than the inference.

193
00:24:43,180 --> 00:24:53,090
Inference is important. Is there a significant, significant association between group membership and the event occurring?

194
00:24:55,690 --> 00:25:05,280
Yes. Yes. Somebody said, how do you know? And roughly the company say that roughly the confidence interval doesn't include zero.

195
00:25:05,290 --> 00:25:17,200
And how do you know that? Just. No, I just know because the standard error is five times smaller than the estimate.

196
00:25:17,810 --> 00:25:20,320
The ratio of the estimate of the standard error is almost five.

197
00:25:21,190 --> 00:25:25,720
If it's two or higher, you're going to get significant spread compared to a normal distribution.

198
00:25:26,620 --> 00:25:30,280
So you don't need a P value here to know that in fact, there's a significant association.

199
00:25:30,980 --> 00:25:36,430
It's huge. And therefore the confidence interval, confidence interval on the log ends won't contain zero.

200
00:25:36,880 --> 00:25:40,660
The confidence interval for the odds ratio will contain one.

201
00:25:45,700 --> 00:25:54,730
Plus, I like my iPad better. Based on this model.

202
00:25:58,220 --> 00:26:16,070
It's. So again, we can only have three parameters in this model and when there is a little bit differently.

203
00:26:17,120 --> 00:26:27,130
So I'm not going to have an intercept. So the intercept times the indicator a in at times those in group zero indicator for those in group one.

204
00:26:29,110 --> 00:26:35,920
And then plus time. The same outfit.

205
00:26:38,370 --> 00:26:41,640
I'm going to bother.

206
00:26:45,540 --> 00:26:48,820
Let's assume the same output again.

207
00:26:50,730 --> 00:26:56,670
We can connect these two models to each other. Let's just say you decided to set a non intercept model so it looks like this.

208
00:26:56,680 --> 00:27:02,880
They got the same outfit output coefficients with estimates and standard errors.

209
00:27:05,010 --> 00:27:09,350
Much. One better. 2.5.

210
00:27:09,350 --> 00:27:15,980
2.5. 2.3.

211
00:27:18,120 --> 00:27:34,680
Sweet. So now what's the interpretation of it or not?

212
00:27:39,120 --> 00:27:43,530
We tend to interpret. We didn't do that assignments. What's what's the interpretation a better one now.

213
00:27:47,210 --> 00:27:55,200
They don't want to hear even. So if you exponential rate that, what are you getting?

214
00:27:59,620 --> 00:28:05,530
Hmm. You're getting odds. You're just getting the odds for one of the groups that.

215
00:28:07,180 --> 00:28:12,760
This is the ad's attempt to.

216
00:28:18,010 --> 00:28:21,730
Right. So the log adds.

217
00:28:23,260 --> 00:28:30,640
The other one measures the log odds in group. One group to GI equals one way that was encoding the.

218
00:28:33,320 --> 00:28:37,580
Embedded in that hat is the odds of the event in Group one.

219
00:28:41,650 --> 00:28:49,360
So the odds ratio is the element of needed to maintain this speed of one.

220
00:28:57,450 --> 00:29:03,750
Estimated odds ratios that so when you have an intercept, when you have an intercept,

221
00:29:04,320 --> 00:29:07,260
there is a reference group you're implicitly putting into your model.

222
00:29:07,470 --> 00:29:12,209
When you take the intercept out and you want to compare two groups, you have to do a little bit more,

223
00:29:12,210 --> 00:29:18,570
or you have to take a difference of parameters like street beta one.

224
00:29:18,570 --> 00:29:24,180
How are the odds or do you need to take the X program? I think.

225
00:29:27,440 --> 00:29:31,900
So, Peter, what is the long odds? Yep. So Peter has been on one hand.

226
00:29:31,990 --> 00:29:35,480
Is he in the red or not?

227
00:29:37,280 --> 00:29:44,780
Thank you. When I could choose anybody there, either one had a slug, odds exponentially to get to me.

228
00:29:46,760 --> 00:29:51,860
So the odds ratio is the ratio of those two exponential coefficients, which is the exponent of the difference.

229
00:29:52,610 --> 00:29:57,319
Just should be able to do all that. And again,

230
00:29:57,320 --> 00:30:04,670
the math gets a little bit harder here if you want to get a standard error because the

231
00:30:04,670 --> 00:30:10,130
standard error for the odds ratio involves the standard error of this difference.

232
00:30:11,440 --> 00:30:15,940
Well, usually in the output, all you get is the standard error of this one and the standard error of this one.

233
00:30:16,570 --> 00:30:18,400
You're missing the covariance.

234
00:30:19,820 --> 00:30:27,260
And so you have to go in and pull out the various covariance matrix and get the covariance true in order to do stuff, right?

235
00:30:27,260 --> 00:30:37,790
So all the computations get a little more complicated. But I was told you saw a lot of some of that in 650, maybe not so much at 651.

236
00:30:41,450 --> 00:30:45,350
So make sure you can interpret coefficients when we're not talking about normal data.

237
00:30:46,100 --> 00:30:51,230
So it's really important. Skilled folks don't folks don't understand odds ratios.

238
00:30:52,490 --> 00:30:56,270
Even when we try to teach it to them.

239
00:30:58,040 --> 00:31:05,480
All right. One other thing I want to share before we move on to the key elements.

240
00:31:09,490 --> 00:31:15,300
We should have The X-Files here. We don't. Project work.

241
00:31:16,520 --> 00:31:22,970
So far it's worked for the company.

242
00:31:25,990 --> 00:31:39,240
Rich. So I want to go through this as a learning exercise for you because I tell people I teach and

243
00:31:39,240 --> 00:31:43,710
work with to never use a library if you don't fully understand what that library is doing.

244
00:31:45,580 --> 00:31:52,650
And you all use a library that I didn't fully understand what it was doing, and I don't think it's doing the right thing.

245
00:31:54,010 --> 00:31:59,420
So you should always be careful with libraries in our book.

246
00:31:59,430 --> 00:32:03,890
Once in a while you're going to catch a book in there. So this has to do with the QIC.

247
00:32:03,910 --> 00:32:05,520
So let's what is QIC?

248
00:32:06,120 --> 00:32:15,060
QIC was the measure that I told you to use to compare to models to decide what correlation structure might work better in the analysis.

249
00:32:15,870 --> 00:32:19,000
And again, because we don't have a likelihood we couldn't use AIC.

250
00:32:29,600 --> 00:32:41,210
So if we had a likelihood compared to our investor models.

251
00:32:49,000 --> 00:32:57,580
Using like a key information criteria, AIC, which is negative two terms of likelihood.

252
00:33:04,280 --> 00:33:15,540
Plus two times the number of parameters. Right.

253
00:33:15,660 --> 00:33:20,720
So a more complex model is going to add on to the AIC.

254
00:33:20,730 --> 00:33:25,650
We want to lower AIC. So a complex model with a lot of parameters isn't preferred.

255
00:33:26,070 --> 00:33:30,360
Even if it improves the likelihood at a certain point the penalty term.

256
00:33:33,390 --> 00:33:48,370
And so for quasi in likelihood. Which is essentially at this point.

257
00:33:51,470 --> 00:33:56,870
So what happened in 2001? There's a paper on biometrics.

258
00:33:59,020 --> 00:34:06,460
I have now read. We lost his composure.

259
00:34:07,210 --> 00:34:13,360
So what is QIC? How does QIC generalize or look like?

260
00:34:13,360 --> 00:34:16,480
What AOC was very likely to approach.

261
00:34:20,430 --> 00:34:25,260
But before you can do that. So let's start before I just throw out the formula for you.

262
00:34:26,710 --> 00:34:36,300
So for a given working correlation. Yeah.

263
00:34:36,630 --> 00:34:41,870
This will all be in my lecture notes next year. Didn't really give you much information on QIC.

264
00:34:42,480 --> 00:34:48,420
So for a given working correlation matrix. Ah, so you've got data, you say I'm going to use exchangeable correlation.

265
00:34:57,270 --> 00:35:01,770
We're going to compute the kiosk for a model that used that correlation structure.

266
00:35:09,050 --> 00:35:37,780
You're fitting two models. It's best for working after G with independence.

267
00:35:39,010 --> 00:35:42,190
Right. Which equals a glow.

268
00:35:44,590 --> 00:35:51,800
Most of the time. What do you want to say from that model?

269
00:35:51,820 --> 00:35:59,290
You want to save what he called the Omega II, which is the model based?

270
00:36:02,770 --> 00:36:04,870
Variance of your regression parameters.

271
00:36:06,220 --> 00:36:12,790
So use some independence and you get this model based variance covariance matrix for your regression parameters.

272
00:36:12,790 --> 00:36:22,120
You're going to save that. You're going to set G working.

273
00:36:27,360 --> 00:36:34,050
Art. This is the thing you're interested in is already useful correlation structure relative to nothing especially.

274
00:36:37,410 --> 00:36:42,090
So there again, you're going to save there kind of a thing called VFR.

275
00:36:44,100 --> 00:36:51,150
So this is the. Sandwiched for the empirical again what our calls for robust.

276
00:37:00,470 --> 00:37:05,430
So again, remember, this beta hat is different from this data hat.

277
00:37:05,450 --> 00:37:14,060
This is the beta hat you got from an independence model. This is the way to do that with this correlation matrix as a weight matrix in the estimation.

278
00:37:14,900 --> 00:37:18,230
They're both consistent, but they are different.

279
00:37:21,810 --> 00:37:26,280
Huh? Let's call it better than. Because then I define better.

280
00:37:26,590 --> 00:37:40,970
Ah. Because the coefficient. Miss you.

281
00:37:50,250 --> 00:37:53,070
So we're going to send a correlation matrix from the independence model.

282
00:37:53,340 --> 00:38:00,330
We're going to say the coefficient estimates and the robust variance estimated from the assumed the model with the assumed correlation structure.

283
00:38:16,920 --> 00:38:25,200
So remember that there's a fitted mean that you're getting in this model give me your head is the linear predictor so je inverse

284
00:38:25,200 --> 00:38:32,880
of the linear predictor for your fitted that's the fitted function and our gives you the means for a given set of covariates.

285
00:38:40,090 --> 00:38:46,370
He was. That's.

286
00:38:49,430 --> 00:39:02,810
In the second model, someone took a fitted means from the model I'm interested in the one with the correlation matrix are so.

287
00:39:04,070 --> 00:39:08,540
The key risk for a model that uses correlation structure are in GDP.

288
00:39:11,000 --> 00:39:14,270
It looks like AIC, it's negative two in terms of likelihood,

289
00:39:15,140 --> 00:39:25,610
but a quasi likelihood which we have probably haven't seen and we haven't shown in this class yet, plus two times the trace of the product.

290
00:39:26,710 --> 00:39:31,570
But the inverse. Of this matrix.

291
00:39:34,410 --> 00:39:52,700
Is just a be. And this one. The thing was, as was the number of parameters in the AC computation.

292
00:39:55,770 --> 00:39:59,280
If the product of those two matrices is an identity matrix.

293
00:40:00,780 --> 00:40:05,280
And the trace, which is the diagonal. Some of the diagonals will be p.

294
00:40:07,740 --> 00:40:12,960
So he came up with an idea that tried to compare again the model with no correlation it to this idea.

295
00:40:13,710 --> 00:40:20,880
And again, if those two things lead to an identity matrix, then we're back to the idea of the P value.

296
00:40:20,880 --> 00:40:25,900
That was an AC. What the heck is quasi likelihood?

297
00:40:28,540 --> 00:40:39,400
You and you. You know, this isn't a class and cause I like weird.

298
00:40:46,920 --> 00:40:58,030
So. So for us, physical right now is equal to the law and likelihood.

299
00:41:02,460 --> 00:41:13,170
We're exponential families. So again, what am I not going to have you do this on the exam?

300
00:41:13,270 --> 00:41:19,140
This is the stuff that I'm going to test you on. It's just for your overall breadth as a statistician.

301
00:41:22,640 --> 00:41:31,310
So if we have person data right we know that the density of an observation given its mean.

302
00:41:32,710 --> 00:41:42,990
Right. That is the likelihood of the main parameter in. You want to turn?

303
00:41:56,220 --> 00:42:05,620
That's the prison density. The logo now is negative and you know.

304
00:42:18,660 --> 00:42:23,440
If you take the log of that, I get something like this. I'm only interested in the part that involves you.

305
00:42:24,280 --> 00:42:27,610
This is just a constant. So we ignore that.

306
00:42:35,240 --> 00:42:39,890
That's the quality I like to get for a day. No fear of the sun model, just the light of the likelihood.

307
00:42:43,130 --> 00:42:48,650
Again, we can come up with quasi likelihood functions that don't come from an exponential family.

308
00:42:48,830 --> 00:42:52,890
And then things get a little more complicated. If you want to explore more.

309
00:42:52,910 --> 00:42:57,950
You can read that article that I sent you. And if you do the same thing for binomial data.

310
00:43:08,150 --> 00:43:14,059
Because of the quasi likelihood for me, A.J., is with a little bit of algebra.

311
00:43:14,060 --> 00:43:18,560
I'm not sure much. It's why A.J. Hammer still gets.

312
00:43:24,480 --> 00:43:29,270
Plus was change.

313
00:43:33,740 --> 00:43:40,500
Right. So again, that's just writing off the binomial density and taking the log and getting rid of anything that doesn't involve new in it.

314
00:43:43,780 --> 00:43:48,840
So you get your fitted values from your G model. You plug them into that function, you add them all up.

315
00:43:48,990 --> 00:43:52,170
Because that's an assumption of independence here.

316
00:43:52,170 --> 00:43:58,170
And the Q I see function, the quasi function. So again, you were talking about here.

317
00:43:58,740 --> 00:44:02,040
So again, this is just taking the.

318
00:44:02,040 --> 00:44:07,500
Q the quasi good value for every observation over every person and time point adds them all up.

319
00:44:08,190 --> 00:44:12,959
That's the quasi likelihood. And then it takes two times a trace of the product of those two things.

320
00:44:12,960 --> 00:44:17,580
That's. Q I see. Now I do all this.

321
00:44:19,830 --> 00:44:24,390
I was asking myself quite a bit over the weekend and on Monday.

322
00:44:26,820 --> 00:44:32,250
This is where it's valuable to know things before you go and set libraries again.

323
00:44:32,250 --> 00:44:37,020
You're not going to try and do the algebra for every free library that you use in our.

324
00:44:39,280 --> 00:44:44,620
But what I discovered was that I tried to compute the X values myself.

325
00:44:44,650 --> 00:44:50,050
I'm like, Oh, I know how to compute A.C. now. I don't need a library. And I sort of computing them.

326
00:44:50,770 --> 00:44:55,620
And they weren't always the same as the kiosks in the G part function.

327
00:44:57,790 --> 00:45:07,750
And so that either means I don't know how to use that library around for a long, long time, or I have just discovered a mistake that I can remember.

328
00:45:07,770 --> 00:45:12,430
QIC see, I don't care about what the value of QIC is, it's how different they are between models.

329
00:45:13,090 --> 00:45:17,080
I'm assuming the pack function is making the same mistake across models.

330
00:45:17,980 --> 00:45:22,300
And so when you take the difference, it's irrelevant, right? There's just a scaling or shifting problem,

331
00:45:22,810 --> 00:45:28,630
but it's really annoying when you think you know a concept and the library doesn't give you the exact value.

332
00:45:30,070 --> 00:45:35,209
So in future iterations of this class and you're not going to be doing it anywhere this semester,

333
00:45:35,210 --> 00:45:39,190
but there is a library called Capital M, Human Capital M Little.

334
00:45:39,190 --> 00:45:43,420
You can a little and that computes QIC.

335
00:45:44,950 --> 00:45:49,180
And when I used that library to compute the C values.

336
00:45:52,420 --> 00:45:59,860
So again in my cosine data analysis file now if you want to download it and updated it so that and

337
00:46:00,790 --> 00:46:09,610
so I set the model using G library and then I get the QIC using this new library instead of G pack.

338
00:46:10,330 --> 00:46:19,690
So I don't need to reset the model either. And I have some code here for you if you ever want to compute QIC by yourself.

339
00:46:21,340 --> 00:46:26,230
So again, a computer quasi likelihood. This is what I just wrote on the board here.

340
00:46:26,950 --> 00:46:32,560
The Y times log new minus new and I add it up over across all observations.

341
00:46:33,610 --> 00:46:40,660
There's the naive variance from the independence model, the robust variance from the assumed model and accuracy value.

342
00:46:41,260 --> 00:46:45,510
And I get the same number as that library constantly.

343
00:46:45,520 --> 00:46:53,500
So anyways, so if you ever want to use QIC values, I would highly recommend this library rather than a g pack.

344
00:46:54,760 --> 00:47:01,330
And I've also got it. And so my little little learning exercise and when you want to teach something,

345
00:47:02,260 --> 00:47:06,340
you should make sure you know what's going on before you teach it to students and they're struggling.

346
00:47:08,740 --> 00:47:11,800
The other thing I had forgotten is, again, in a G model.

347
00:47:12,850 --> 00:47:19,120
If every one of your variables is a dummy variable, if you've got two groups and you've got categorical time.

348
00:47:20,950 --> 00:47:27,400
If everything is balanced, if every person has the same number of observations in their model that's measured at the same point in time,

349
00:47:28,000 --> 00:47:29,560
it's a fully balanced design.

350
00:47:30,250 --> 00:47:37,810
G will give you the same coefficient estimates and the same robust variance estimate no matter what your working correlation is.

351
00:47:39,730 --> 00:47:42,850
So there's no need to worry about what your working correlation is.

352
00:47:42,850 --> 00:47:46,060
Just use the robust editor as if you want it.

353
00:47:46,960 --> 00:47:53,920
Independence goes back to my idea. Just use independence. Don't worry about modeling the correlation structure so much.

354
00:47:53,920 --> 00:47:57,010
Just use independence and get robust.

355
00:47:57,010 --> 00:48:02,140
Engineers. Robots. No, you don't. You get sandwich servers.

356
00:48:05,630 --> 00:48:11,150
So I've been reading up a lot and gee, there are a number of people who suggest this idea for most designs.

357
00:48:12,050 --> 00:48:20,480
If you're going to use G.E., you're going to again, remember, you always gain efficiency whenever you get the model as close to the truth as possible.

358
00:48:21,890 --> 00:48:29,240
So if you have any belief that exchangeable correlation is a reality, you should probably use it to gain a little bit of efficiency.

359
00:48:29,900 --> 00:48:32,870
But there are lots of nice papers out there that did simulation studies,

360
00:48:33,350 --> 00:48:39,889
the gains in efficiency over independence, and using a robust standard error versus modeling.

361
00:48:39,890 --> 00:48:46,550
The correlate structure. The gains in efficiency for most data sets is trivial 1%.

362
00:48:46,820 --> 00:48:47,600
2%.

363
00:48:48,690 --> 00:49:01,700
So that's I think it's very suitable to assume independence and use a working sorry, a robust and empirical sandwich, Senator, and be done with it.

364
00:49:03,890 --> 00:49:11,060
G is also very useful whenever you have data that is hard for you to figure out the correlation structure.

365
00:49:12,080 --> 00:49:16,550
So a very complicated example is when I was working with the dental school.

366
00:49:17,600 --> 00:49:25,040
So hopefully very shortly this hopefully you go to the dentist and hopefully at some point they count.

367
00:49:25,430 --> 00:49:28,430
You hear them say three, three, 3 to 4, one.

368
00:49:29,300 --> 00:49:32,750
Hopefully you don't hear anything above three, but you probably do.

369
00:49:34,190 --> 00:49:41,000
Again, they're measuring the depth of the pocket. When they stick a probe in the gum, they see how far down it can go before it hits a tissue.

370
00:49:41,630 --> 00:49:46,880
And if it's too deep, they're worried that food and bacteria are going to get trapped in there and you're going to have problems.

371
00:49:48,080 --> 00:49:53,450
Anyways, I participated in studies in which there was a binary indicator.

372
00:49:53,450 --> 00:49:55,970
So when they probe, they also measure whether it bled or not.

373
00:49:57,870 --> 00:50:03,179
And I don't know any human in the world who never bleeds when the dentist prods their gums,

374
00:50:03,180 --> 00:50:08,160
but they don't want too much of that, so they get a binary indicator of bleeding on probing or not.

375
00:50:09,180 --> 00:50:12,720
So I get a zero or a one for every tooth.

376
00:50:14,170 --> 00:50:19,480
Six different times. So they go around every tooth and then they go and every mouth in your head.

377
00:50:21,040 --> 00:50:25,480
And then they do that longitudinal phase of increased use.

378
00:50:25,570 --> 00:50:35,530
Yeah. So you said you often had a concern about human beings.

379
00:50:35,740 --> 00:50:40,420
So happy Halloween, fellow creatures.

380
00:50:40,660 --> 00:50:44,740
So if you're right, if you make a tooth like a hexagon because our teeth are all hexagonal.

381
00:50:44,770 --> 00:50:54,490
Right? So they're going to probe at six different points on a tooth and see if the gum bleeds and then they do it on every one of your teeth.

382
00:50:55,600 --> 00:50:59,980
So you get six binary outcomes. Yet how many teeth does a adults have?

383
00:51:04,390 --> 00:51:07,750
Oh 20 something, you know, 28.

384
00:51:07,900 --> 00:51:14,140
It's more than 22, but it's 20 something. So you get six observations on every tooth times the number of teeth,

385
00:51:15,100 --> 00:51:19,470
and then imagine doing that at baseline three months, six months, nine months, 12 months.

386
00:51:20,350 --> 00:51:27,160
So those are the kind of data I analyzed. Trying to come up with a correlation structure is almost impossible.

387
00:51:27,910 --> 00:51:34,090
Right. Teeth and next to each other are probably correlated, but we know that teeth on top of each other are correlated.

388
00:51:34,900 --> 00:51:37,180
You know, teeth that have the same function are correlated.

389
00:51:39,100 --> 00:51:44,440
So in that instance, I'm just going to throw it all into glee and get a working correlation matrix.

390
00:51:45,040 --> 00:51:50,110
All of the weird correlation structure should be taken account for in those in the robust standard errors.

391
00:51:52,120 --> 00:51:59,080
G is extremely useful for cases where you can't really figure out when there's multiple nesting of correlation to it.

392
00:51:59,560 --> 00:52:06,440
If you have multiple kids in a school, there's a maybe a variation among schools.

393
00:52:06,460 --> 00:52:09,370
And then you might ask kids different questions and so forth.

394
00:52:09,670 --> 00:52:16,570
So whenever the correlation structure is too complex to think about it being error one or exchangeable,

395
00:52:17,740 --> 00:52:23,020
that's where a robust senator is really handy unless you want to try and figure out the correlation structure.

396
00:52:24,560 --> 00:52:27,970
So, all right.

397
00:52:28,090 --> 00:52:35,290
Where are we at 358? That's it for G as a question.

398
00:52:35,470 --> 00:52:41,650
You sure can. So if we're in a situation where the key is the same, but we yeah,

399
00:52:42,100 --> 00:52:48,100
we I think that the correlation structure through like Eddie is the same, like exchangeable then.

400
00:52:48,490 --> 00:52:48,740
Yeah.

401
00:52:48,820 --> 00:53:01,570
We still gain efficiency by using the by doing exchangeable modeling correlation is exchangeable as opposed to sandwich or GLC and sandwich s majors.

402
00:53:01,570 --> 00:53:05,860
Right. And then we can do that. Yeah. Let me think I think I understand your question.

403
00:53:07,480 --> 00:53:12,670
If you're getting the same currency values for all the models. Yeah, that means you have fully balanced data, right?

404
00:53:14,380 --> 00:53:19,180
Sure. Yeah. No, that's the wrong answer.

405
00:53:21,430 --> 00:53:26,530
I don't mean to put you on the spot. Oh, my.

406
00:53:26,860 --> 00:53:36,330
Great. It's still here. Yes, it's still here. Why did it?

407
00:53:36,380 --> 00:53:42,350
Why is it so small? All right, here's a chassis value for any model.

408
00:53:42,860 --> 00:53:49,339
It's a function of your parameter estimates. It's a function of what you get when assuming independence.

409
00:53:49,340 --> 00:53:56,030
So that's the same across all correlation structures. There's a reference here, but for any correlation structure,

410
00:53:56,420 --> 00:54:01,640
you're going to get robust standard errors and you're going to get your parameter estimates.

411
00:54:02,720 --> 00:54:10,010
If you get the same parameter estimates and the same robust variance estimate, you're going to get the same KYC.

412
00:54:10,100 --> 00:54:12,620
That's the only way you can get the same QIC across.

413
00:54:12,620 --> 00:54:19,610
Models, for the most part is if all the coefficients are the same and the robust variance estimates are the same.

414
00:54:20,060 --> 00:54:24,620
The only way that happens is when you have a fully balanced design.

415
00:54:27,560 --> 00:54:33,950
What about the case? You emailed us about the what was it like, categorical time?

416
00:54:34,250 --> 00:54:38,960
That's what I mean, right? If you have categorical time, all of your covariates are zero one.

417
00:54:40,040 --> 00:54:45,019
Right. Indicates this time point or not. Yeah. So when whenever you have that situation.

418
00:54:45,020 --> 00:54:50,450
So again, if you have, let's be even more specific if you have fully balanced data.

419
00:54:51,670 --> 00:54:55,930
And you've hit time is categorical. Then you will get the same currency values.

420
00:54:56,350 --> 00:55:04,390
If you said time is continuous, you will not get the same size because your designed matrix doesn't have zeros and ones anymore.

421
00:55:04,660 --> 00:55:10,760
It has the actual time points. Yeah. So let's keep more of what you asked me then.

422
00:55:10,880 --> 00:55:20,660
Yeah. So tell me again what you said. So in this situation, are you at the same time or approximately the same?

423
00:55:20,830 --> 00:55:25,550
I see. I assume that, like, very small differences are not really meaningful.

424
00:55:27,080 --> 00:55:31,490
Okay. So and your question was, is this is empirically you see a certain correlation structure.

425
00:55:32,150 --> 00:55:38,240
Yeah. So if currency doesn't help you make a decision, but the empirical correlation structure does, then use that information.

426
00:55:39,470 --> 00:55:44,630
By empirical you mean like from like the legs. The legs, the idea that you were doing.

427
00:55:47,420 --> 00:55:55,040
Again, remember, though, you're going to get the same standard errors when time was categorical, you're getting the same robust standard errors.

428
00:55:56,420 --> 00:56:03,230
So what I mean gain. And when I say you gain efficiency, that means across many, many of us in repeated sampling.

429
00:56:03,890 --> 00:56:09,860
If I do this process, I'm going to gain efficiency. That doesn't mean you will gain efficiency in your single analysis.

430
00:56:12,110 --> 00:56:17,540
But that's still that's all takes place. That will mean even if the kiosks the same.

431
00:56:18,850 --> 00:56:21,850
When we're doing modeling correlation versus sandwich.

432
00:56:22,660 --> 00:56:26,840
If you're going to use the sandwich variance estimate. But if you're always.

433
00:56:30,650 --> 00:56:34,280
There's a special case where you're always going to get the same answer.

434
00:56:35,610 --> 00:56:39,030
Right? Yeah. Categorical time. Fully balanced data.

435
00:56:40,760 --> 00:56:44,780
There. It doesn't matter what you do, you're always going to get the same answer for any correlation structure.

436
00:56:46,010 --> 00:56:49,640
What I'm talking about is other situations where they're working on correlation matters.

437
00:56:49,840 --> 00:56:53,390
So working correlation just just does not matter at all.

438
00:56:54,230 --> 00:56:58,500
No. Because you're going to get the same estimates. Okay.

439
00:56:59,280 --> 00:57:04,079
So in that case, you just might as well always do. You see, the cases are the same.

440
00:57:04,080 --> 00:57:07,360
You might as well. Alicia's Douglas Plus Image.

441
00:57:08,280 --> 00:57:13,020
If you see that the crises are all the same, then you need to explain to your investigator.

442
00:57:13,400 --> 00:57:16,920
Yeah. The reason they're the same is a peculiarity of this.

443
00:57:17,760 --> 00:57:21,830
The way you model the data. Okay, and say it's irrelevant.

444
00:57:21,850 --> 00:57:28,169
Doesn't matter what we do, but tell them this a unique case because then they're going to walk out and I go,

445
00:57:28,170 --> 00:57:32,070
I'll sit here, just tell me that it doesn't matter, right all the time.

446
00:57:33,000 --> 00:57:40,440
You didn't say that. But yes, sir, that you also spend a few minutes on Friday going over interpretations for person regression.

447
00:57:40,680 --> 00:57:46,080
Sure. Thanks. Of course. So ten years ago on Friday.

448
00:57:46,680 --> 00:57:54,470
But if we're going to talk about. Yes, just to build on that, could you talk about interpreting models with intention?

449
00:57:55,490 --> 00:57:59,660
Yeah, yes. Yes. Those are fun.

450
00:58:00,060 --> 00:58:05,340
So this is why I don't fit interaction models.

451
00:58:06,660 --> 00:58:20,940
That's hard. Okay, so interpretation of standards and interaction models and fy22 like anything else?

452
00:58:24,210 --> 00:58:29,580
No arbitrary systems. You're going to. Yeah.

453
00:58:29,910 --> 00:58:37,400
All right. So, again, all of that stuff we just kind of talked about for the last half hour, it's just an odd situation in, you know,

454
00:58:38,340 --> 00:58:46,240
when you have data on a certain in a certain structure, a fully balanced, categorical time, everything's a dummy variable in your model.

455
00:58:46,410 --> 00:58:48,390
That's the only time that this happens.

456
00:58:50,150 --> 00:58:56,930
And again, I can't I'm going to write a paper because I cannot find a simple damn little paper that makes that clear.

457
00:58:58,280 --> 00:59:01,420
And you think it would be useful in an applied setting to know that fact?

458
00:59:03,380 --> 00:59:07,460
All right. We're going to get to two many slides today.

459
00:59:07,610 --> 00:59:14,270
And I looked at my slides and boy, are they dense, really.

460
00:59:17,240 --> 00:59:22,130
I was in full steam in August, I guess, working on algebra.

461
00:59:22,460 --> 00:59:29,430
And I thought that was were lovely slides. Let's just start talking about where we want to go here.

462
00:59:30,180 --> 00:59:37,020
So we're going to go on to G outcomes. So a generalized, linear, mixed model.

463
00:59:38,940 --> 00:59:43,890
So it's got Glenn in there and it's got the mixed part.

464
00:59:44,580 --> 00:59:48,450
So we're going to have a GLM plus random of X.

465
00:59:53,230 --> 00:59:56,290
Let's get specific. We're going to fit out.

466
00:59:56,500 --> 01:00:01,900
We're going to switch to person model like we did at Hallmark for so long.

467
01:00:03,190 --> 01:00:07,840
And again, you AJ is the x y AJ.

468
01:00:10,520 --> 01:00:16,230
Variance of. Is going to be some function of the mean.

469
01:00:17,040 --> 01:00:23,459
We're going to use the canonical link and everything so that this is new ampersand model says that the variance is equal to the

470
01:00:23,460 --> 01:00:34,950
mean and we're going to say that's a function of group membership and since everybody is measured at the same points in time,

471
01:00:34,950 --> 01:00:44,370
so, T.J., not T.J. and by the time we get to Friday, we'll be talking about the lovely interaction model that looks like that's a glioma.

472
01:00:46,980 --> 01:00:55,030
So that is again, is a marginal model. That is the model for any one measurement at any one time point on the person.

473
01:00:55,420 --> 01:01:01,330
So we're modeling the marginal mean and then we induced correlation and g through a weight matrix,

474
01:01:02,260 --> 01:01:06,850
exchangeable air one, whatever, and just went into the estimating equation very nicely.

475
01:01:09,250 --> 01:01:16,450
So somebody came along in the I don't know what time this was nineties and said,

476
01:01:16,450 --> 01:01:21,700
well, if we can throw random effects into a linear mixed model into a linear model.

477
01:01:21,700 --> 01:01:25,570
Excuse me, why can't we put random effects in a generalized linear model?

478
01:01:26,770 --> 01:01:34,360
So why can't I say that there's a random intercept? Maybe there's a random slope.

479
01:01:39,720 --> 01:01:57,360
Let's look. We assume that these two random effects are centered at zero.

480
01:01:57,360 --> 01:02:01,100
They're normally distributed and they have some variance covariance structure.

481
01:02:04,240 --> 01:02:11,080
And the only reason errors show up in linear mixed models is to induce the distribution of normality.

482
01:02:12,160 --> 01:02:15,940
Here we don't have errors because the distribution has already implied.

483
01:02:16,570 --> 01:02:21,910
We already talked about Poisson or binomial, so there are no error terms in any of this.

484
01:02:23,410 --> 01:02:25,600
So this is a generalized, linear, mixed model.

485
01:02:26,710 --> 01:02:35,740
It is a log linear model for Poisson data, and we've incorporated a random intercept and the random slope French individual on the log scale.

486
01:02:35,740 --> 01:02:43,139
On the log scale, the means, right. And now the challenge comes in.

487
01:02:43,140 --> 01:02:49,140
How do we estimate these gains? What is the estimating equation for those things?

488
01:02:50,190 --> 01:02:53,550
Can we predict these like we did in linear, mixed models?

489
01:02:54,930 --> 01:02:58,320
Can we estimate what the variance covariance parameters are?

490
01:03:03,510 --> 01:03:06,570
And then we start asking the questions about what is beta one?

491
01:03:08,650 --> 01:03:15,300
One here. Excuse me. And even if we say, well, we know we're on the large scale, so I'm going to say exponential of that.

492
01:03:15,310 --> 01:03:18,700
We'll talk about that on Friday. Interpreting a along linear model.

493
01:03:19,640 --> 01:03:28,390
But what is be of beta one hit and I intimated this earlier in a previous lecture but.

494
01:03:30,300 --> 01:03:36,030
Data one here. It's interpretation is conditional upon everything else in the model.

495
01:03:38,280 --> 01:03:47,950
Which includes these guys now. These folks now. So data one is quantifying the association of root membership with the mean.

496
01:03:49,450 --> 01:03:57,170
Among people who have these random effects, it's conditional upon these random effects.

497
01:03:58,060 --> 01:04:03,880
And so genomes talk about this being a subject specific model versus a marginal model.

498
01:04:08,220 --> 01:04:12,450
So the parameter you get is the parameter estimate that you get from this model.

499
01:04:14,140 --> 01:04:19,750
Is not the overall population association of group membership with outcome.

500
01:04:20,920 --> 01:04:26,780
It is a subject specific interpretation. But.

501
01:04:28,540 --> 01:04:31,360
And I'm going to show you some notes that hopefully make that a little bit clearer.

502
01:04:32,690 --> 01:04:41,960
The challenge in all of this is that if I want to go from this interpretation to a population average interpretation.

503
01:04:43,910 --> 01:04:50,480
That was easy to do with linear mixed models because everything was linear and normal expected values.

504
01:04:50,850 --> 01:04:55,790
All everything just carried through because everything was linear. This thing is not linear anymore.

505
01:04:56,420 --> 01:05:06,060
Vue is E to all of that. So getting all of these things is a little more complicated than it was.

506
01:05:06,510 --> 01:05:09,690
The problem, again, if you think of if you're in the Bayesian class,

507
01:05:10,950 --> 01:05:18,420
we don't have conjugate C here anymore in the linear mixed model, I had normal and then I had random effects that were normal.

508
01:05:18,750 --> 01:05:27,450
And so the posteriors and everything were normal. Here you've got four sun and then you've got normality for the random effects.

509
01:05:29,160 --> 01:05:38,280
Things don't integrate out nicely, and thankfully people have spent their careers trying to figure all this out in programing for us.

510
01:05:39,570 --> 01:05:44,910
And so we have gone from the children functions now that we didn't when I was a student, when I was a student,

511
01:05:44,910 --> 01:05:52,800
there was no function for a job and yet there were lots of papers and I had the joy of programing, one such algorithm.

512
01:05:55,740 --> 01:05:57,660
But we don't have to do that anymore.

513
01:06:00,350 --> 01:06:07,139
The tradeoff of all of this is I'm going to have you analyze the person and binary data that you had over foreign homework.

514
01:06:07,140 --> 01:06:15,560
Five Now in the genome and as I have in one of my slides, I have said, gee, many,

515
01:06:15,560 --> 01:06:22,400
many, many times over the last 20 years I have finished a year or am maybe two times.

516
01:06:24,580 --> 01:06:34,580
They are notoriously hard to converge. And they also have a strange interpretation that philosophically I don't think I'm really interested in.

517
01:06:34,850 --> 01:06:37,430
So we're going to get into that over the next couple of lectures.

518
01:06:38,810 --> 01:06:46,310
So I'm going to be curious to see how many of you in the homework are able to get these models to converge with your datasets?

519
01:06:46,640 --> 01:06:50,330
Not because I'm mean and cruel, but because I don't know.

520
01:06:51,380 --> 01:06:59,690
Remember what we're trying to do now? We are trying to model the variability between individuals and within the same individual.

521
01:07:01,370 --> 01:07:07,219
That's fairly straightforward to do when everybody has a continuous outcome, but not one.

522
01:07:07,220 --> 01:07:16,590
Everybody has a zero. Not focus on zero one, two, three, maybe a four, maybe a five where you have those kind of numbers for everybody.

523
01:07:16,640 --> 01:07:20,750
It is hard to tease out what variation is within a person versus between people.

524
01:07:21,860 --> 01:07:25,910
And it gets even harder when everybody's outcomes are zeros and ones.

525
01:07:26,390 --> 01:07:31,670
How do you assess two components of variability when everybody can only have one of two values across time?

526
01:07:32,620 --> 01:07:38,140
It is really hard to estimate random slopes when you have binary data, for example.

527
01:07:40,190 --> 01:07:45,470
So theoretically, all of this is really cool in application.

528
01:07:46,010 --> 01:07:54,110
I think it's pretty problematic. So most of you should get things that work at least a random intercept.

529
01:07:54,530 --> 01:08:00,920
You should be able to fit a random intercept to your persona and your data can over time the random slopes.

530
01:08:01,560 --> 01:08:11,540
We're going to find out. I would highly recommend that you only have a random intercept or a random continuous time of that.

531
01:08:12,290 --> 01:08:15,139
Don't have multiple random categorical time effects.

532
01:08:15,140 --> 01:08:21,800
The more random of x you throw in the model, the closer you are to frustration because it's not going to converge.

533
01:08:22,910 --> 01:08:28,520
But with that, I want to stop because there's no reason for me to go into slides.

534
01:08:30,800 --> 01:08:41,030
So Friday, when I talk about interpretation some more and think about your title, my next question and maybe go.

535
01:08:44,580 --> 01:08:49,720
Happy Halloween. I.

