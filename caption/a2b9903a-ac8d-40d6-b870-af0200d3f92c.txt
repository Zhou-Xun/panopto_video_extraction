1
00:00:02,520 --> 00:00:05,820
I think right now it's killing a.

2
00:00:09,710 --> 00:00:26,820
You just look. People actually got it right, which is let's put it through.

3
00:00:27,910 --> 00:00:59,980
I always say that. It's like.

4
00:01:07,420 --> 00:01:57,600
You. Yeah, you're right.

5
00:01:57,630 --> 00:02:09,640
Good morning. Thank you for coming on this cold and last morning before the holidays.

6
00:02:11,720 --> 00:02:15,890
Um, so a couple of announcements before we start.

7
00:02:17,180 --> 00:02:20,540
Today is the last, uh, in-person lecture.

8
00:02:21,590 --> 00:02:33,740
Um, and, uh, the next three lectures will be on a flipped classroom modality through which you will have.

9
00:02:36,820 --> 00:02:40,030
Videos for each lecture to watch.

10
00:02:41,200 --> 00:02:47,140
They will be published this week, but then for the.

11
00:02:49,130 --> 00:02:58,120
Lecture time. Uh, the GSA will be here to answer any questions you may have about the lectures.

12
00:02:58,130 --> 00:03:06,680
I mean, to be this ring is, you know, they are so nice that they will answer pretty much any question, even if it's not related to the videos.

13
00:03:07,760 --> 00:03:14,800
Okay. Then the last so that those will be three lectures.

14
00:03:16,530 --> 00:03:22,590
One week from today, Monday, then the Wednesday and then the following Monday, and then the Wednesday.

15
00:03:23,010 --> 00:03:29,070
Following will be the policy review, which again will be a session led by De Dios.

16
00:03:30,540 --> 00:03:35,980
And then the following Monday will be the final. And you will be off the hook.

17
00:03:37,240 --> 00:03:41,190
Um. Uh, women's.

18
00:03:46,580 --> 00:03:52,220
Office hours. There is no office hours this Wednesday because of the holidays.

19
00:03:52,730 --> 00:03:57,410
The homework less homework is due tomorrow. And.

20
00:03:59,650 --> 00:04:05,260
The lab today is going to be to help you with the homework.

21
00:04:06,340 --> 00:04:10,810
Um. There are no more laughs.

22
00:04:11,200 --> 00:04:18,260
He's the last one. And then office hours will resume.

23
00:04:19,330 --> 00:04:22,420
Next Monday as usual until.

24
00:04:23,590 --> 00:04:26,719
I believe. The end of classes.

25
00:04:26,720 --> 00:04:31,040
We should be. The. My thing is that.

26
00:04:32,000 --> 00:04:37,380
Usually all the wins must be the. The Thursday before the exam.

27
00:04:39,170 --> 00:04:42,710
Any questions on those final logistics?

28
00:04:44,730 --> 00:04:52,180
Oh. Okay. Well, by the way, it was nice seeing several of you on Friday at the poster session,

29
00:04:53,920 --> 00:05:01,420
so I'm sure you enjoyed and are looking forward to see yourselves next year there.

30
00:05:01,810 --> 00:05:12,780
So. Just please make sure not to use the odds ratio in non case control studies say so a few studies like that.

31
00:05:13,790 --> 00:05:16,949
Mm hmm. Of course. Of course.

32
00:05:16,950 --> 00:05:21,110
I didn't see nothing. Huh. All right.

33
00:05:21,120 --> 00:05:30,950
So, um, research synthesis. Um, so we are done pretty much with everything I wanted to tell you.

34
00:05:30,960 --> 00:05:39,510
Next. The next lectures will be on statistical analysis. So I just wanted to say a few words about research synthesis studies.

35
00:05:39,630 --> 00:05:45,460
Um, because. You know, they are common.

36
00:05:45,480 --> 00:05:55,360
They are helpful. Some of you may have, in fact, already done this and you may be reading them.

37
00:05:55,380 --> 00:06:00,120
So it's important for you to at least know what what this is.

38
00:06:00,660 --> 00:06:05,510
Look. So we will say a few words about why.

39
00:06:07,060 --> 00:06:13,090
We should synthesize research. What are the types of research synthesis studies?

40
00:06:13,390 --> 00:06:19,750
Uh, we'll say a couple of words about narrative reviews, and then we'll say another couple of words about meta analysis.

41
00:06:20,590 --> 00:06:30,700
Um, and we'll focus possibly on publication bias because this was something that we kind of promised we will mention in.

42
00:06:31,740 --> 00:06:35,210
The randomized trial session, but we ran out of time.

43
00:06:35,220 --> 00:06:39,490
But it's important, right? So to begin with.

44
00:06:40,560 --> 00:06:43,610
What's what's out there in the epidemiologic literature?

45
00:06:43,740 --> 00:06:46,760
What types of studies can you find?

46
00:06:46,770 --> 00:06:53,790
So obviously the most common are original research papers and those are the papers that,

47
00:06:54,090 --> 00:07:02,370
you know, several of you may write even during your early or um, summer internships sometimes.

48
00:07:03,290 --> 00:07:10,830
And those are papers where basically the data and the methods are described.

49
00:07:11,400 --> 00:07:18,990
Basically you tend to use data and you have to describe the methods, um, used to collect those data.

50
00:07:22,990 --> 00:07:35,800
They are peer review, meaning you submit your paper with your research findings and the editor typically sends the paper out to review.

51
00:07:35,830 --> 00:07:43,120
I mean, unless the editor sort of heads a paper from the beginning, in which case they will just give you the kiss of death.

52
00:07:44,120 --> 00:07:49,749
And if that's not the case, then, you know, they see that there is a chance for your paper.

53
00:07:49,750 --> 00:07:54,010
They just hope they may send it out to review by experts.

54
00:07:56,470 --> 00:08:07,030
And so that's called the peer review process. And what happens is that, you know, those, uh, colleagues, uh, typically you as an author,

55
00:08:07,030 --> 00:08:11,320
you obviously don't know usually who gets to review your paper.

56
00:08:11,940 --> 00:08:20,400
Um, so they will provide comments and then send it back to the editor and the editor will decide whether, you know,

57
00:08:20,470 --> 00:08:33,250
the comments are too brutal or maybe addressable and depending on which will either then reject the paper, um, or invite you to resubmit anyway.

58
00:08:33,310 --> 00:08:37,840
So it's a, it's a, it's a process, you know, can be painful sometimes.

59
00:08:38,080 --> 00:08:42,280
It's most times, but, um, this is how the system works.

60
00:08:42,970 --> 00:08:50,590
So those original research papers, then they are the research synthesis paper, which is that we are to talk about today.

61
00:08:51,610 --> 00:08:58,390
And they can be either qualitative or quantitative and will, you know, point out the difference soon.

62
00:08:58,990 --> 00:09:04,180
And they actually can be peer reviewed or not. Sometimes they are not even peer reviewed.

63
00:09:05,650 --> 00:09:09,160
Uh, especially the qualitative ones may not be peer reviewed.

64
00:09:09,760 --> 00:09:20,920
Okay. And then the other big chunk of to literature, you know, editorial, editorial letters, opinions, they are typically not peer review.

65
00:09:23,920 --> 00:09:27,700
All right. So why why do we synthesize research?

66
00:09:27,730 --> 00:09:32,260
There are many motivations, really. One of them is that, um.

67
00:09:32,830 --> 00:09:37,690
As you can see. There is a lot of it out there.

68
00:09:37,930 --> 00:09:45,400
A lot of literature. Right. So this is the last year I checked.

69
00:09:45,550 --> 00:09:50,310
Um. But it was two years ago. But they have a lag.

70
00:09:51,090 --> 00:09:54,370
So don't think that I'm. No damn. They update my lights.

71
00:09:55,810 --> 00:10:00,220
So anyway, this this shows the growth in the number of journalists in a very.

72
00:10:01,300 --> 00:10:05,110
Common database that most of you may be familiar with.

73
00:10:06,010 --> 00:10:09,250
Vulnerable tablet. Have you heard about it?

74
00:10:09,880 --> 00:10:14,560
So it is sort of the electronic version of meddling, right?

75
00:10:16,110 --> 00:10:18,630
It is, is kept by the National Library of Medicine.

76
00:10:18,780 --> 00:10:33,059
The NIH know that look, there are so many journalists now index there and I guess since the 1990s with these movements of wide open access.

77
00:10:33,060 --> 00:10:36,180
But also a paper published.

78
00:10:36,180 --> 00:10:49,350
A paper published. System and also the predatory journals that there is lots of journals and obviously not all of them are good quality necessarily.

79
00:10:49,350 --> 00:10:59,220
Right. And consequently, the number of papers, which is these citations, the number of papers published in journals has increased really explanation.

80
00:11:00,260 --> 00:11:08,450
So I mean, you know, when I was doing HIV research and that's already, I guess, 20 years ago.

81
00:11:10,910 --> 00:11:16,910
The estimate was that for an HIV researcher to keep up with what was going on.

82
00:11:18,010 --> 00:11:22,510
Those people had to read at least 50 papers every day.

83
00:11:23,680 --> 00:11:26,259
So I mean, it is really mind boggling.

84
00:11:26,260 --> 00:11:36,370
It's impossible and let alone finding which ones were actually worth reading before, you have to spend their time on this.

85
00:11:36,620 --> 00:11:41,350
This was not very good. So so that that's that's one motivation because.

86
00:11:43,640 --> 00:11:51,350
Sometimes when sort of good researchers synthesize the review their literature for you in a review,

87
00:11:51,740 --> 00:11:55,250
they have already separated the grain from the chaff.

88
00:11:55,910 --> 00:12:00,830
So you don't have to read so much bad research and you just get sort of the essence.

89
00:12:00,830 --> 00:12:05,500
And so that's an important motivation, right? Okay.

90
00:12:06,250 --> 00:12:13,750
And then our other motivations, we will go back to with what are the types of research syntheses that is this was in your readings, right?

91
00:12:14,820 --> 00:12:20,910
So here you have five types of different syntheses, research synthesis.

92
00:12:20,910 --> 00:12:27,450
That is the first two narrative review and systematic reviews are qualitative meaning.

93
00:12:28,680 --> 00:12:36,360
They don aim to recalculate estimates of association based on what's already published.

94
00:12:36,720 --> 00:12:39,990
They simply describe what's already published.

95
00:12:40,050 --> 00:12:48,390
Okay. And the difference between narrative reviews and systematic reviews is that systematic reviews need to present a

96
00:12:48,390 --> 00:12:57,150
methods section with a protocol showing how they found the papers that they finally included in the systematic review.

97
00:12:58,150 --> 00:13:11,290
Okay. Now see through er1d dative types of research, synthesis studies and what you today quantitative.

98
00:13:11,540 --> 00:13:17,929
They are quantitative because the goal is to actually take the estimates of

99
00:13:17,930 --> 00:13:23,360
association that have been published in the individual studies and guess what?

100
00:13:23,690 --> 00:13:24,650
Put them together,

101
00:13:25,040 --> 00:13:38,420
summarize them quantitatively in a single estimate of association that represents the overall association as published in previous research studies.

102
00:13:39,440 --> 00:13:50,750
Right now, there are a number of of differences between this see the need that two studies by basically see is the most common the meta analysis.

103
00:13:51,390 --> 00:14:01,030
And again, we'll go through a couple of examples. And then there is pooled re-analysis in which we meta analysis.

104
00:14:01,040 --> 00:14:04,070
What people do is they take published papers.

105
00:14:04,250 --> 00:14:13,040
So as we will see, the unit of analysis is actually the person, not the group of people, but published papers.

106
00:14:14,180 --> 00:14:19,220
It's kind of interesting. In Bull Reanalysis.

107
00:14:20,030 --> 00:14:26,269
The people doing the analysis rather than taking the published papers actually have

108
00:14:26,270 --> 00:14:31,910
access to the original data from each research group that produced that paper.

109
00:14:32,840 --> 00:14:39,650
And they get those data and put them together using certain techniques and then reanalyze the original data.

110
00:14:40,280 --> 00:14:48,379
Right. Of many separate studies and then prospectively planned meta analysis is a

111
00:14:48,380 --> 00:14:55,520
technique in which the investigators from different groups before moving on to.

112
00:14:57,010 --> 00:15:00,130
Collect the data and analyze it. Actually agree.

113
00:15:01,100 --> 00:15:08,210
On. Harmonizing some of the data collection methods so that.

114
00:15:12,520 --> 00:15:17,980
The data are uniform and I'd be some of the data and that will allow some.

115
00:15:19,440 --> 00:15:25,800
Reanalysis after they conduct their studies by pooling original individual level data.

116
00:15:26,930 --> 00:15:30,110
Okay. So is that is that clear the difference between these three?

117
00:15:31,850 --> 00:15:37,730
Okay. So let's let's say a couple of words about negative reviews.

118
00:15:37,740 --> 00:15:44,700
And again, you know, nowadays many people tend to pooh pooh negative reviews.

119
00:15:44,700 --> 00:15:47,159
And, you know, thing is, there are reasons for that.

120
00:15:47,160 --> 00:15:54,120
But I don't think that that we need to be too, you know, tough with negative reviews because they can be pretty helpful.

121
00:15:54,570 --> 00:15:59,820
So they are, again, quality, qualitative assessment of published literature,

122
00:16:00,540 --> 00:16:07,739
and they're supposed to be a comprehensive integration of the current sort of state of the art of the research.

123
00:16:07,740 --> 00:16:08,730
QUESTION Right.

124
00:16:09,480 --> 00:16:19,170
And they are not actually limited to population of studies, but they can also include, you know, animal experiments, case reports, etc.

125
00:16:19,350 --> 00:16:27,900
The idea is to be as comprehensive as possible and to integrate the knowledge of a given area as well as as you can.

126
00:16:29,260 --> 00:16:36,580
Uh, in a way that, in fact, some of them could actually set the agenda for future research on a given topic.

127
00:16:37,270 --> 00:16:44,950
You know, um, or they can pick information from here and there actually come up with new theories,

128
00:16:44,950 --> 00:16:48,260
new hypotheses, new explanations of previous findings.

129
00:16:48,310 --> 00:16:55,810
So, um, and again, if you have a protocol that explains how you went and found the papers that.

130
00:16:56,930 --> 00:17:00,040
Uh. Uh, you wanted to. They can be systemic.

131
00:17:00,320 --> 00:17:04,410
This is the difference. Now there is there is obviously a problem.

132
00:17:04,420 --> 00:17:08,910
And this is an interesting example that illustrates that problem with narrative use.

133
00:17:09,570 --> 00:17:13,560
This is an exception. It's a historical excerpt from.

134
00:17:15,650 --> 00:17:27,160
From a narrative review. Of a problem, a question that that was baffling a cardiologist back in the 1970s or so.

135
00:17:27,760 --> 00:17:32,140
So there are some some medications that are called beta blockers.

136
00:17:32,350 --> 00:17:40,030
Right. So you'll get doctors words, a bit of look, a layperson definition of a better look, please.

137
00:17:42,730 --> 00:17:46,510
C.J. or Brian. So this your first?

138
00:17:52,680 --> 00:18:01,780
What? Exactly.

139
00:18:01,790 --> 00:18:05,920
So what are they typically used for in the treatment or of what?

140
00:18:22,700 --> 00:18:27,070
It can hurt itself by working too hard. How about hypertension?

141
00:18:27,080 --> 00:18:33,510
Is that still used for hypertension? No.

142
00:18:33,540 --> 00:18:37,630
You were not told that any more. Well.

143
00:18:38,620 --> 00:18:44,560
Are there doctors, uh, with a different opinion about the use of beta blockers for hypertension?

144
00:18:44,610 --> 00:18:49,930
Well, I may be totally outdated because, you know, you haven't seen a patient in over 25 years.

145
00:18:50,560 --> 00:18:59,680
Nowadays, I'm just. Let's call it dangerous physician. So can you believe I am actually license to practice in a certain country?

146
00:19:02,700 --> 00:19:08,069
I was not there, of course. But anyway, I mean, they used to be.

147
00:19:08,070 --> 00:19:15,210
Then let me take a historical approach. They used to be used in the treatment of hypertension.

148
00:19:16,050 --> 00:19:27,660
Really effective. They are so helpful in the treatment of stage fright when you are really sort of, you know, anxious about the performance.

149
00:19:27,660 --> 00:19:35,700
You know, if there is any violinist in the, in the classroom, um, and, you know, you, you know, your technique is perfect and all,

150
00:19:35,700 --> 00:19:43,500
but you are just so you take a beta blocker and then you, you and tremble and they, you know, the sweating also.

151
00:19:43,830 --> 00:19:47,640
So anyway but but back in the in the seventies.

152
00:19:50,150 --> 00:19:53,350
People were proposing, whether I be for what.

153
00:19:53,390 --> 00:20:00,200
But let's explain to us before we knew that people were suspecting that when somebody had had a heart attack,

154
00:20:00,860 --> 00:20:07,680
you know, to protect the heart from working too hard because he had suffered an injury and would be worse.

155
00:20:08,030 --> 00:20:12,750
They could give it a better look to make the hard life sort of easier.

156
00:20:12,770 --> 00:20:18,319
Right. But but again, there was there was a lot of debate about that.

157
00:20:18,320 --> 00:20:28,580
So one person wrote a negative review of of the studies that were available at the time and then concluded

158
00:20:28,580 --> 00:20:33,980
that it seemed perfectly reasonable to treat patients who have survived an infection with a bit of luck.

159
00:20:34,100 --> 00:20:40,579
That was the conclusion from just subjectively evaluating what was available at the time.

160
00:20:40,580 --> 00:20:45,350
Right. And then on this day, same year, the same year,

161
00:20:45,500 --> 00:20:55,880
another person in the British Medical Journal wrote those despite claims that they reduce arrhythmia as cardio will work and emphasize,

162
00:20:56,060 --> 00:21:03,020
we still have no clear evidence that the blockers improve long term survival and inflation, despite almost 30 years of clinical trials.

163
00:21:03,930 --> 00:21:09,299
So two people reviewing the same questions later.

164
00:21:09,300 --> 00:21:13,830
And the same question came to exactly opposite conclusions.

165
00:21:15,100 --> 00:21:23,670
You see? So that illustrates the main problem with narrative reviews, which is and I actually said the word subjectivity.

166
00:21:24,670 --> 00:21:32,140
Right. So it just depends on. Everybody's opinions and everybody's opinions are always, you know,

167
00:21:32,170 --> 00:21:38,410
potentially biased and consciously or unconsciously, depending on their own interest, their own experience, etc.

168
00:21:39,960 --> 00:21:45,810
But again, they do have a place in the in the literature and in our hearts.

169
00:21:46,080 --> 00:21:52,080
Yes. And so much so that even the the key kind of mainstream.

170
00:21:53,890 --> 00:21:58,660
Publishing houses in our own field have their own.

171
00:21:59,810 --> 00:22:03,060
Journalists devoted purely to narrative reviews.

172
00:22:03,090 --> 00:22:12,170
So some of you may be familiar with these Épidémiologique reviews, which is is published by the International.

173
00:22:12,950 --> 00:22:19,339
This is that it would be mitigating the same one that publishes the top epidemiology journal nowadays,

174
00:22:19,340 --> 00:22:27,170
which is the IEEE International Journal of Genealogy. And every year they publish these journal.

175
00:22:28,670 --> 00:22:36,459
Only with narrative reviews. Um, in the for the nutrition people there are not not only one.

176
00:22:36,460 --> 00:22:39,930
I mean, this is the well used to be the more uh.

177
00:22:41,060 --> 00:22:43,640
I guess the best known nutrition reviews.

178
00:22:43,970 --> 00:22:52,220
But there are many, many journals devoted only to publishing narrative reviews in individual but in any other.

179
00:22:52,520 --> 00:23:00,720
Hey, look. Immunology and immunology there. You have your own also journal devoted to publishing narrative.

180
00:23:01,020 --> 00:23:06,880
Okay. Right. So who has who has written and published a narrative review?

181
00:23:12,730 --> 00:23:22,750
Meta analysis. All right. So as opposed to narrative reviews, meta analysis belongs to the family of quantitative type of research synthesis.

182
00:23:23,350 --> 00:23:28,690
And the definition is that it is a statistical analysis of the results from independent studies,

183
00:23:29,080 --> 00:23:34,720
which generally aims to produce a single quantitative estimate of an association or an effect.

184
00:23:36,250 --> 00:23:40,750
The unit of analysis, as I mentioned before, is the published research study.

185
00:23:41,670 --> 00:23:46,950
It's not the individuals is not a group of people but these they published study.

186
00:23:48,860 --> 00:23:54,260
And for those of you who are history aficionado.

187
00:23:55,360 --> 00:24:00,130
Um, the first month the analysis was conducted by Karl Pearson.

188
00:24:00,880 --> 00:24:05,670
Does the last name ring a bell? Bill.

189
00:24:07,910 --> 00:24:12,180
Hmm. Okay. Yes.

190
00:24:12,180 --> 00:24:17,370
The Pearson correlation coefficient among many other contributions.

191
00:24:17,370 --> 00:24:25,980
Yeah. And the same guy. Uh, so over 200 years ago and it was on the studies on typhoid vaccine,

192
00:24:26,700 --> 00:24:35,400
the motivation was because many of the groups are far too small to allow for any definite opinion being formed at all.

193
00:24:36,450 --> 00:24:44,670
Okay, so that's actually another big motivation to do it, and that is that individual studies are very small and lack precision.

194
00:24:45,180 --> 00:24:52,980
So in the individual, you know, uh, motivations ratios have wide confidence intervals.

195
00:24:53,670 --> 00:25:02,790
That means there have since them to do a meta analysis. And there are other reasons, um, investigate rate exposures.

196
00:25:04,310 --> 00:25:09,710
Interactions with relics of right exposures or rare outcomes.

197
00:25:10,550 --> 00:25:21,190
Examine dose response relations. Maybe you you you may recall an example I provided a long time ago about.

198
00:25:22,670 --> 00:25:26,629
The problems with lack of variability in an exposure.

199
00:25:26,630 --> 00:25:32,240
I think I explain this example in the context of. Yes.

200
00:25:32,560 --> 00:25:35,560
EcoLogic studies. Thank use of Sophia.

201
00:25:37,600 --> 00:25:40,870
So it was about the meat intake, right?

202
00:25:41,110 --> 00:25:47,470
And maybe cancer or something. So when you when you do studies in a population.

203
00:25:49,360 --> 00:25:55,720
Or there is relatively little variability, say in the US, you know, meeting age relatively high on average.

204
00:25:56,170 --> 00:26:07,600
You may not be able to see an association with an outcome that you could only see if you compare extremes of the exposure distribution.

205
00:26:07,600 --> 00:26:11,680
Right. So. The same applies.

206
00:26:11,890 --> 00:26:16,000
We made analysis when you have that situation. In a meta analysis you could.

207
00:26:17,470 --> 00:26:23,140
Take estimates from, let's say, studies conducted in populations with.

208
00:26:24,210 --> 00:26:30,210
Overall larger variability in exposure and be able to actually see those response associations

209
00:26:30,450 --> 00:26:39,300
that you would miss if some studies lack the variation in exposure in different settings.

210
00:26:40,600 --> 00:26:45,780
All right. Another reason is to investigate heterogeneity between studies.

211
00:26:45,790 --> 00:26:49,150
Heterogeneity. What's heterogeneity?

212
00:26:49,840 --> 00:26:54,280
We may go back to these, but heterogeneity is basically the fact that sometimes.

213
00:26:56,220 --> 00:27:03,460
Two studies. Examining exactly the same question yields different results.

214
00:27:04,860 --> 00:27:12,560
For exactly the same question. You might even use that in the same same method, same definition of exposure, same definition of outcome.

215
00:27:12,570 --> 00:27:19,290
You come up with different answers, different numbers for your cumulative concerns, Rich.

216
00:27:20,710 --> 00:27:28,370
That's heterogeneity. So if you. Take all those studies together and apply certain techniques.

217
00:27:28,370 --> 00:27:33,230
And. The most important of which is common sense.

218
00:27:33,350 --> 00:27:40,490
Common sense techniques. But others. You might be able to infer why.

219
00:27:42,070 --> 00:27:47,080
The results are different across states and there are many explanations.

220
00:27:47,080 --> 00:27:56,440
And, you know, you should not be afraid of thinking outside the box we will go through is light on sources of heterogeneity but as we have.

221
00:27:57,590 --> 00:28:08,390
Okay. They take artifacts, you know, problems between studies and obviously to try to generalize results from singles sometimes to.

222
00:28:09,340 --> 00:28:15,730
Uh, implement policy. People will demand results from meta analysis.

223
00:28:16,590 --> 00:28:26,260
And you hope that some experts will know about how to limit analysis, have come up with a good one and say, hey, this is the smart thing.

224
00:28:26,440 --> 00:28:30,340
This thing does work, or this thing kills babies.

225
00:28:30,940 --> 00:28:34,620
So. Right.

226
00:28:34,650 --> 00:28:43,440
What type of studies can be made, analyzed, all of them, that the message from this slide is that all studies can be made to analyze.

227
00:28:45,430 --> 00:28:48,490
Some people might think, no, no. Or I don't know, perhaps.

228
00:28:50,140 --> 00:28:54,280
Only randomized trials can be made to analyze. But that's not true.

229
00:28:55,420 --> 00:29:04,230
Observational studies can also be made analyzed. And even the script, if there is any type of study, can be made on it.

230
00:29:05,560 --> 00:29:10,030
Right. And what are the elements of a meta analysis?

231
00:29:11,510 --> 00:29:16,399
Interesting enough, they're pretty much the same as the elements of any research study.

232
00:29:16,400 --> 00:29:24,720
Which in one of the. Flipped classroom lectures you will go through, you know.

233
00:29:24,790 --> 00:29:28,420
But I guess we have been through them somehow.

234
00:29:29,500 --> 00:29:40,900
You have to refine the study question. Obviously, uh, aims, you have to establish criteria to include or exclude studies like you would in,

235
00:29:41,680 --> 00:29:46,390
in an observational study of individuals to include or exclude, um.

236
00:29:48,200 --> 00:29:54,500
People write. Then you have to search for the studies and you have to collect the data.

237
00:29:54,770 --> 00:30:01,400
Right. And do quality assessment is is is very important.

238
00:30:01,970 --> 00:30:09,230
Extract the data that you need according to the aims that you have set and then analyze.

239
00:30:10,950 --> 00:30:20,630
And interpret your findings. So this one is on the exclusion and inclusion criteria.

240
00:30:20,640 --> 00:30:29,020
Meaning. Why is you have decided? Your topic and the types of studies that you want to analyze.

241
00:30:29,860 --> 00:30:38,950
These are the types of items that can make you exclude studies from your meta analysis or leave them on.

242
00:30:39,550 --> 00:30:46,870
They can be related to characteristics of the participants, let's say if you want to do a study on pediatric health.

243
00:30:46,870 --> 00:30:49,899
So obviously you want to limit that to kids.

244
00:30:49,900 --> 00:30:59,290
Perhaps that type of setting with you want to focus your meta analysis on population based studies or also clinical studies,

245
00:31:00,280 --> 00:31:04,720
the types of interventions, sometimes the scales of the exposures and outcomes.

246
00:31:05,170 --> 00:31:11,620
Um, the study design, you may want to focus your meta analysis on clinical or randomized trials only.

247
00:31:11,860 --> 00:31:18,850
Or you can. Decide to do a medley of observational studies or even both write a.

248
00:31:20,300 --> 00:31:30,650
Um, yeah, this is an example of that stick because that's, they've actually always has to be shown in your meta analysis.

249
00:31:31,070 --> 00:31:35,230
Has anybody conducted a meta analysis here? Okay.

250
00:31:38,120 --> 00:31:41,749
So this is from a. Yeah. MIT analysis.

251
00:31:41,750 --> 00:31:46,580
We did some well some years ago with Lewis Decker.

252
00:31:47,940 --> 00:31:54,660
And actually, if I may say a word about this, the motivation here was that, um.

253
00:31:56,580 --> 00:32:05,730
I had written a grant. I wanted to do a randomized trial of zinc supplements I've been working on.

254
00:32:06,000 --> 00:32:16,710
I did some work for kids who had HIV infection to see if zinc could help those kids.

255
00:32:18,300 --> 00:32:21,360
And so you got rejected.

256
00:32:21,360 --> 00:32:26,700
And the first time. And one of the critics was.

257
00:32:28,960 --> 00:32:36,220
Luke Zinc competes with iron ore absorption in the gut.

258
00:32:37,720 --> 00:32:42,070
Which means if you give zinc to kids who are likely to.

259
00:32:43,440 --> 00:32:47,640
We are in a setting where they have little irony in their diet.

260
00:32:48,120 --> 00:32:52,710
You are going to make them anemic, anemic because is the same mechanism.

261
00:32:52,750 --> 00:32:59,790
So if you get them, if you give them too much zinc. The kids will absorb all the zinc and the item will not have a chance.

262
00:32:59,790 --> 00:33:03,240
The little light on there might be will not have a chance to get absorbed because they compete.

263
00:33:03,600 --> 00:33:08,090
Is this a mechanism to. So I said, okay, well.

264
00:33:09,530 --> 00:33:16,660
Where is the evidence? So, I mean, I couldn't tell that to the reviewer because, you know, that was after the fact.

265
00:33:16,670 --> 00:33:22,330
The. So we went back sort of to lick our wounds.

266
00:33:23,380 --> 00:33:28,710
We went back to the literature and. It was all over the place.

267
00:33:29,460 --> 00:33:36,960
So the only thing that I mean, the evidence that we could look for was individual trials of zinc.

268
00:33:37,410 --> 00:33:45,900
And obviously, what we wanted to see us. Okay, in those trials of zinc, have they made kids anemic?

269
00:33:48,780 --> 00:33:56,020
All over the place. So we said frankly and on a personal note, I am not very fond of meta-analysis.

270
00:33:56,040 --> 00:34:04,380
No, but but you know, for this one we said, hey, it gives we probably want to carefully look at this and maybe do bit analysis.

271
00:34:04,770 --> 00:34:10,800
So we did that. And I have a few slides from that, um, experience.

272
00:34:11,550 --> 00:34:18,880
Um, and this shows the process of, of selecting the studies to include in the maintenance.

273
00:34:18,890 --> 00:34:28,110
Right. Um, so these are the, the, the first step is all the articles you screen as potentially eligible, right?

274
00:34:28,890 --> 00:34:33,050
And then these are the ones that you exclude overlapping study results.

275
00:34:33,720 --> 00:34:38,670
Note The authors write sort of duplicate papers on the same topic,

276
00:34:38,670 --> 00:34:44,970
and you have to be very careful because then, you know, you you will end up sort of double dipping.

277
00:34:46,010 --> 00:34:56,150
So review papers then retained and then some more exclusions because the intervention was not what we were looking for.

278
00:34:57,020 --> 00:35:04,190
We could not tell apart the effect of only these elements because they were not sort

279
00:35:04,190 --> 00:35:10,700
of apparently healthy living populations or they were not within the age bracket,

280
00:35:10,700 --> 00:35:16,730
you know. And at the end of the day, the final number included.

281
00:35:18,020 --> 00:35:22,080
All right. So after you have done that, then, uh.

282
00:35:23,650 --> 00:35:26,950
Or rather, how do you get here?

283
00:35:27,190 --> 00:35:37,780
So how do you search for studies? The reason I have this slide here is that if you decide to conduct a meta analysis,

284
00:35:37,780 --> 00:35:47,650
one of the things that you must make absolutely sure you do is to be you have to let out that little obsessive person.

285
00:35:48,540 --> 00:35:56,300
Uh, within yourself in that you have to make sure that you include and that you have found

286
00:35:56,340 --> 00:36:02,970
absolutely everything that can potentially have information on your question of interest.

287
00:36:03,480 --> 00:36:09,200
You have to be absolutely obsessive. The reason is that the results of a MIT analysis.

288
00:36:10,930 --> 00:36:15,610
Well, if it's bad, this would be public. And this.

289
00:36:17,120 --> 00:36:23,360
Who is going to be the author of the first hate mail you receive after the MIT analysis is published?

290
00:36:24,200 --> 00:36:27,280
Yes, they call it. Yes, but you.

291
00:36:39,780 --> 00:36:45,540
Okay. So that refers to to the to the quality of the study.

292
00:36:45,840 --> 00:36:50,970
And sometimes you can say the quality of the study as a criterion for exclusion.

293
00:36:51,880 --> 00:37:01,470
In our particular meta analysis, there were no such cases that I can recall, but you could set it as a as a criterion.

294
00:37:01,830 --> 00:37:08,940
So if they drove it, if they over adjusted, I'm going to exclude the meta analysis, the the study.

295
00:37:25,350 --> 00:37:29,340
Oh, yeah. Yes, of course. Yeah. There's a lot of training there.

296
00:37:30,490 --> 00:37:33,840
Yup. Yeah, good question.

297
00:37:33,840 --> 00:37:39,530
I mean, sometimes we take things for granted, right? But, uh, yeah, there's the awful truth.

298
00:37:39,540 --> 00:37:44,510
It's a lot of work. Okay, so. Yeah, so you have to be super comprehensive, I guess.

299
00:37:44,520 --> 00:37:48,930
Obviously, the first step is to go to the general databases that we are mostly familiar with.

300
00:37:49,320 --> 00:37:52,500
But for a meta analysis, you must not stop there.

301
00:37:52,920 --> 00:37:57,180
You have to implement other strategies. This is one of them.

302
00:37:58,410 --> 00:38:03,510
There's no bull or a reverse noble search. You know what is noble search?

303
00:38:05,310 --> 00:38:08,650
Snowball. Yes. Brian, when you.

304
00:38:19,260 --> 00:38:29,790
Right? Right. So basically, you well, I guess you can sort of to simplify the situation, you go to the references of the paper that you found,

305
00:38:29,790 --> 00:38:38,339
which is a good one, and then you see whether any of the cited papers there might be related to the study question.

306
00:38:38,340 --> 00:38:42,430
And maybe you have you may have missed on the first part of of data.

307
00:38:42,990 --> 00:38:49,150
How about that reverse noble search? So there's no more searches.

308
00:38:49,810 --> 00:38:53,080
What? The papers they found have signed it.

309
00:38:53,830 --> 00:39:08,990
What do you think the reverse snowball might be? Who has cited that beep, right?

310
00:39:12,130 --> 00:39:18,100
And now it's really relatively simple to use such a search and search.

311
00:39:21,010 --> 00:39:24,730
Uh, because we have.

312
00:39:28,070 --> 00:39:35,120
Even in pub me, if I think to the right hand side there is a, there is a link that says.

313
00:39:36,760 --> 00:39:44,230
Papers that have cited this or some you can just click there and they before that became

314
00:39:44,410 --> 00:39:51,850
available that featuring PubMed these was the best search engine for his four rivers noble.

315
00:39:52,560 --> 00:39:58,810
Um, I mean you've got obviously Google scholar as it's just the click.

316
00:39:59,110 --> 00:40:02,680
Right. So who has published papers here?

317
00:40:03,840 --> 00:40:07,140
Okay. Uh. Sophia.

318
00:40:08,750 --> 00:40:17,110
Uh. Uh. Let's see if I can figure out how to make this work.

319
00:40:19,940 --> 00:40:23,350
Are you on public? On target.

320
00:40:24,330 --> 00:40:31,730
Your papers. Look. So he's not on public.

321
00:40:35,540 --> 00:40:41,780
Okay. Well, let's give it a try. But he's not showing.

322
00:40:59,630 --> 00:41:10,320
Your last name is Kroger. Like this.

323
00:41:15,980 --> 00:41:20,900
Yeah. Okay. I want to make it more specific.

324
00:41:20,900 --> 00:41:25,150
We say. Does the author's name. Um.

325
00:41:28,080 --> 00:41:31,700
Oh, stuff like that. Okay.

326
00:41:31,710 --> 00:41:38,820
Well, how about, uh, you. Yes.

327
00:41:39,270 --> 00:41:42,280
Lindsay? Yeah. English. English.

328
00:41:43,300 --> 00:41:47,990
English. He's with an AA, right?

329
00:41:48,440 --> 00:41:54,410
He. Haha.

330
00:41:54,420 --> 00:42:04,320
Very nice. And it's pretty recent, though, so it may not be selected yet.

331
00:42:07,000 --> 00:42:12,840
Oh. Uh. Cited by.

332
00:42:12,840 --> 00:42:16,560
Aha. Very nice. Wow. You are very famous already.

333
00:42:20,230 --> 00:42:25,000
Less is is not yourself who has cited it or you know.

334
00:42:25,490 --> 00:42:37,410
So that's good. Yeah. Times people can be nutty to increase them anyway so that that's an example of reverse.

335
00:42:37,420 --> 00:42:41,380
We just need to reverse another search term that was in Tibet. Okay, great.

336
00:42:42,070 --> 00:42:55,210
Um. Okay.

337
00:42:55,470 --> 00:43:09,240
Um. I. Maybe I need to close this.

338
00:43:16,880 --> 00:43:22,990
Extend. Okay.

339
00:43:25,370 --> 00:43:30,740
That's not too painful. Yep.

340
00:43:31,320 --> 00:43:34,750
Yes. This is where we was. Okay.

341
00:43:35,700 --> 00:43:38,880
But again, you should not stop there even.

342
00:43:40,120 --> 00:43:43,750
You can also or should also.

343
00:43:44,770 --> 00:43:48,520
Look for registries of protocols and.

344
00:43:50,420 --> 00:43:55,820
And this is this is extremely important. And this brings me back to what I promised to tell you.

345
00:43:56,690 --> 00:44:01,580
But they failed. It was a it was an empty promise.

346
00:44:02,090 --> 00:44:10,760
In the randomized trials lecture. And I will tell you, you know, randomized trials, they all need to be registered.

347
00:44:10,880 --> 00:44:15,720
The protocols need to be registered. Um.

348
00:44:16,930 --> 00:44:25,300
Why? Well to prevent what we will discuss to end a problem that's called publication bias.

349
00:44:25,990 --> 00:44:35,090
So what happens is that. Every now and then, a randomized trial does not get published.

350
00:44:36,670 --> 00:44:42,700
In a in the form of a paper that is indexed in a database.

351
00:44:42,850 --> 00:44:49,450
Right. So in that case, you might still find relevant information.

352
00:44:50,480 --> 00:45:02,129
By going through. The protocol in the register with that protocol was the positive because sometimes results

353
00:45:02,130 --> 00:45:06,830
like preliminary results back that might be still usable for your research could have been,

354
00:45:06,840 --> 00:45:10,170
say published in form of an abstract or.

355
00:45:11,440 --> 00:45:15,940
You may even be able and this is fair to contact.

356
00:45:17,140 --> 00:45:26,430
There. The researchers themselves, and its chances are that they have conducted some analysis for any reason.

357
00:45:26,430 --> 00:45:35,520
They might have not been able to publish it, but they might still be able to share the results with you for you to include in the meta analysis.

358
00:45:37,900 --> 00:45:46,450
What do you think is the most common reason why results from a randomized trial may not be published?

359
00:45:52,560 --> 00:45:59,950
It's kind of funny. Well, other than that, that the investigator might be a slacker, etc.

360
00:45:59,950 --> 00:46:04,400
But, um. But to assume in good faith. Um.

361
00:46:08,870 --> 00:46:13,050
Well. Stephen.

362
00:46:15,550 --> 00:46:19,850
Right. Their findings were null or maybe even worse.

363
00:46:19,870 --> 00:46:26,350
What if their findings were against the parent in, you know.

364
00:46:27,980 --> 00:46:28,580
Thinking.

365
00:46:31,840 --> 00:46:40,960
Precisely so because that's that's the main, you know, I guess the main reason why the most common reason why sometimes results are not published.

366
00:46:41,800 --> 00:46:46,450
You know, the investigators may try a few times, but, you know, at the end, obviously,

367
00:46:46,450 --> 00:46:50,290
you have to move on with your life, you know, write your next grant or implement it.

368
00:46:50,290 --> 00:46:54,190
So you might might be left unpublished.

369
00:46:54,310 --> 00:46:56,110
Sometimes studies also fail.

370
00:46:56,500 --> 00:47:08,319
Um, so this is the most I would say in the US is perhaps the most common a register of, of, of protocols for randomized studies.

371
00:47:08,320 --> 00:47:18,280
Again it's say is a condition if you do not register your protocol in a, in a, uh, for randomized trial, trial in one of these registers,

372
00:47:18,910 --> 00:47:27,550
it will never be published because most old journals require you to provide the registration number at the time you submit your p.

373
00:47:28,210 --> 00:47:31,230
So, um. I.

374
00:47:33,030 --> 00:47:43,520
Uh, let me show you, uh. So, uh.

375
00:47:48,650 --> 00:47:53,990
Let's see if this is if this is registered by, um.

376
00:47:56,160 --> 00:48:02,270
The investigator would be sort of the easiest here. Okay.

377
00:48:03,080 --> 00:48:12,550
Let's see. Right.

378
00:48:12,730 --> 00:48:17,320
So, you know, I've done a few trials.

379
00:48:17,320 --> 00:48:22,300
And this this last one is from a is from my namesake.

380
00:48:23,830 --> 00:48:29,150
So you can ignore this one. It is very fine.

381
00:48:29,180 --> 00:48:34,009
We are actually sort of friends with him and he also works on pediatric stuff.

382
00:48:34,010 --> 00:48:42,169
So it's we sort of joke that we can put together our publications and sort of wisdom and and we want to

383
00:48:42,170 --> 00:48:49,940
do a study together with his son because his son has the same name and that would be just hilarious.

384
00:48:52,040 --> 00:49:04,670
So, for example, this, you know, this one, uh, his, that zinc study mentioned to you and it was completed a long time ago.

385
00:49:06,070 --> 00:49:12,780
He never got published. So. And he never got published because the study failed.

386
00:49:14,060 --> 00:49:21,980
But again you know I do have some it failed in the in the implementation of the study failed.

387
00:49:22,610 --> 00:49:25,910
Um, and it's an interesting study by the way.

388
00:49:26,200 --> 00:49:33,880
So, I mean, it's not that the interesting it's not like a Netflix series or something, but, um, you know, is that, you know,

389
00:49:34,010 --> 00:49:43,490
they propose a study and they started recruiting the kids and after a few kids they have recruited,

390
00:49:43,760 --> 00:49:49,610
one of my inclusion criteria was, uh, these were HIV infected babies.

391
00:49:49,610 --> 00:49:58,579
So one of the criteria was a see for cell counts because the babies had to be ready to begin antiretroviral treatment.

392
00:49:58,580 --> 00:49:59,780
And this was a gradual one.

393
00:49:59,780 --> 00:50:12,709
And then suddenly, uh, the local government health authorities changed the cut point of CD4 cell comes to begin babies on antiretroviral.

394
00:50:12,710 --> 00:50:16,310
And basically I was left out without anybody to recruit.

395
00:50:17,060 --> 00:50:24,380
So it's one of the sort of lessons of, of, um, and timeliness of, of randomized trials.

396
00:50:24,560 --> 00:50:31,700
Sometimes you do trials when they are perhaps no longer needed or, you know, it's just impossible to do them anyway.

397
00:50:32,480 --> 00:50:36,320
But but this is an example of a trial that, you know, is registered.

398
00:50:36,320 --> 00:50:42,139
I do have some data, I don't know, one, maybe a hundred babies that I managed to to recruit.

399
00:50:42,140 --> 00:50:52,340
And if somebody came and said, hey, we want to limit analysis on trials, on studies, on zinc and HIV, I can, I can share those data.

400
00:50:52,940 --> 00:50:57,580
Um, so that's, uh, that's does that work?

401
00:50:58,490 --> 00:51:01,710
So hum. Oh, yeah.

402
00:51:01,770 --> 00:51:04,820
But it's just sort of. But to this.

403
00:51:08,960 --> 00:51:18,720
Yep. And there are other sources that you should try not to miss.

404
00:51:19,500 --> 00:51:25,620
Um. Database bases from funding agencies.

405
00:51:25,620 --> 00:51:32,790
So sometimes this that is, you know, if there are no trials that need to be registered, but observational studies also,

406
00:51:32,790 --> 00:51:40,440
if they don't get published, you might still find them in the in the funding agencies registries and they may still have data.

407
00:51:40,980 --> 00:51:47,250
The nation obviously has, um, has an open database.

408
00:51:47,290 --> 00:51:50,660
Uh. For everything they.

409
00:51:51,810 --> 00:51:55,950
Fund because it's it's our money, taxpayers money.

410
00:51:56,040 --> 00:52:00,290
So everybody's entitled to know where that money is going.

411
00:52:00,300 --> 00:52:02,790
So all the studies funded by the NIH are there.

412
00:52:02,790 --> 00:52:11,909
And you should also use it to find potentially studies for meta analysis, something that that used to be called the great literature.

413
00:52:11,910 --> 00:52:19,889
Have you heard that term? The great literature, which is stuff that perhaps doesn't make it to peer reviewed journals,

414
00:52:19,890 --> 00:52:23,610
but that might still be helpful, like proceedings from conferences,

415
00:52:23,610 --> 00:52:33,180
theses dissertations or even contacting experts in the field to see, Hey, now, since you follow these, do you know what else is going on?

416
00:52:33,900 --> 00:52:44,260
Uh, which other studies might be conducted right now and or use more than one database and I don't know,

417
00:52:44,260 --> 00:52:49,980
a, a Google scholar, but um, embase base, for example,

418
00:52:50,370 --> 00:53:07,980
this is the e m is from excerpt up medica and let's use the name of the the European sort of go bonding database for science repository as Medline.

419
00:53:08,370 --> 00:53:10,710
So Medline is sort of the US based.

420
00:53:11,310 --> 00:53:22,230
Um, and, but Embase was the European and the overlap between these databases was in the 100% at least until uh, you know, a few years back.

421
00:53:22,800 --> 00:53:32,640
Meaning if you focus your search on the, on, on Medline, you might miss stuff that was only indexed in the European.

422
00:53:33,240 --> 00:53:39,319
And, um. You know, again, as a historical know that I may have mentioned these before,

423
00:53:39,320 --> 00:53:46,520
but before the advent of the Internet, I actually lived through that and believe that.

424
00:53:47,990 --> 00:53:57,050
So I think I tell you, these, these, um, these came to libraries, some libraries as every month as huge the.

425
00:53:58,030 --> 00:54:01,840
Books. And with the actual abstracts there.

426
00:54:02,560 --> 00:54:06,160
So. So you had to do sort of a manual search.

427
00:54:06,170 --> 00:54:14,710
And I mean, they had sort of an index of others, which was is a thinner book and then also an index of keywords.

428
00:54:15,130 --> 00:54:19,840
And using those, then you had to go to the actual book for the abstract every month.

429
00:54:20,740 --> 00:54:24,790
So, yeah, it was pretty painful. Yeah.

430
00:54:25,300 --> 00:54:29,430
Right. So these are the registers of. Of studies.

431
00:54:30,220 --> 00:54:33,530
Um. Al Green.

432
00:54:34,670 --> 00:54:41,450
All right. A few word on quality of that is, I guess, you know, was asking about.

433
00:54:42,800 --> 00:54:48,050
So it's always important to set the quality of the studies as a criterion for

434
00:54:48,050 --> 00:54:53,480
inclusion or inclusion or a good reason for doing some sort of separate analysis,

435
00:54:53,480 --> 00:55:01,760
excluding the poorest quality studies. And the question is, how do you assess the quality of the studies to include in the Met analysis?

436
00:55:02,420 --> 00:55:06,300
Well, good news for you. You have to take a bit.

437
00:55:06,320 --> 00:55:10,330
600. Uh huh. It was a bit of a joke.

438
00:55:10,820 --> 00:55:17,320
Uh, yeah. So, um, you could use a simple checklist.

439
00:55:17,470 --> 00:55:23,130
You could summarize the knowledge that you have accrued during your training here and just put together a checklist.

440
00:55:23,140 --> 00:55:24,430
Right. Um.

441
00:55:25,460 --> 00:55:35,810
In terms of the study design, what measures of what the quality of measures they have used, what the quality of the analysis have they over adjusted?

442
00:55:35,810 --> 00:55:39,170
Have they control properly for confounding, etc.? Right.

443
00:55:40,130 --> 00:55:40,720
Um.

444
00:55:41,780 --> 00:55:52,580
Or you can use existing checklists already from, you know, organizations of experts that have put together a number of possible criteria for you to.

445
00:55:54,820 --> 00:56:00,110
Check on whether this study has it or not, and some of them are even quantitative.

446
00:56:00,130 --> 00:56:04,410
You can assign a score of quality to you study of did you do this, etc.

447
00:56:06,340 --> 00:56:10,750
One thing that that's important also in general for, you know,

448
00:56:11,410 --> 00:56:18,790
not only for this stage but for any stage of analysis is that this is a tough thing to do by yourselves alone.

449
00:56:19,820 --> 00:56:25,879
So it's always advisable to, if you are embarking on the analysis,

450
00:56:25,880 --> 00:56:34,070
to have a collaborator because simply, you know, two sets of eyes are better than just one.

451
00:56:34,790 --> 00:56:42,750
And again, especially when something is going to become public, you'll have to be very careful with your own quality control.

452
00:56:42,770 --> 00:56:48,830
So these clarity of the studies, for example, assessment should be done by two independent people.

453
00:56:50,110 --> 00:56:52,210
Right. In case, you know,

454
00:56:52,270 --> 00:57:01,720
somebody perhaps is being too tough or somebody misses something and then you have to reconvene and resolve any potential discrepancies.

455
00:57:02,140 --> 00:57:05,890
Data extraction will use the quick word on this.

456
00:57:06,670 --> 00:57:11,139
You have to come up with a with a form with a c, r f.

457
00:57:11,140 --> 00:57:13,030
With a case report form. With a form.

458
00:57:14,880 --> 00:57:24,570
Having the feels for the variables that you are going to collect extracted from every single study as if you were conducting a study for people.

459
00:57:24,690 --> 00:57:28,860
So this one example. I'm not going to, uh, stop.

460
00:57:29,640 --> 00:57:33,760
Um. And then suppose you have already.

461
00:57:34,910 --> 00:57:42,030
You know, collected your data. How do you analyze, uh, meta analysis?

462
00:57:42,050 --> 00:57:45,650
I'm just going to say a few sentences here.

463
00:57:50,470 --> 00:57:57,440
This is a good one for those. The goal, the goal of the analysis.

464
00:57:58,700 --> 00:58:06,530
In a meta analysis is to. Weigh the studies according to their size.

465
00:58:08,080 --> 00:58:14,530
So to wait well rather than these days I took the weight the estimates of association from each one according to

466
00:58:14,530 --> 00:58:23,060
the size of the study before you put them together in a single estimate of association so that the larger studies,

467
00:58:23,410 --> 00:58:26,770
the estimates from the larger studies weight more.

468
00:58:28,000 --> 00:58:32,170
And the estimates from the smaller studies. That's the goal of the analysis.

469
00:58:32,580 --> 00:58:40,150
Innovate on that. Okay. Then you have to tabulate the results of including every individual study you will analyze.

470
00:58:40,570 --> 00:58:43,870
You find a measure of association. You typically.

471
00:58:46,790 --> 00:58:50,120
Describe also and present your data in a graphical form.

472
00:58:50,240 --> 00:58:56,180
I'll give you an example of you should explore source of heterogeneity if there is any.

473
00:58:57,020 --> 00:59:06,349
And then uh, you know, there are some advanced techniques, which is meta analysis of subgroups, some sort of fake modification type of analysis, etc.

474
00:59:06,350 --> 00:59:10,700
But I just want to show you so this is the way in which you tabulate the results.

475
00:59:10,700 --> 00:59:16,550
This, again, that meta analysis on zinc and. And hemoglobin.

476
00:59:17,120 --> 00:59:28,219
Uh, I'm going to skip through this, but basically you use, uh, there are measures, some measures of association that you can implement.

477
00:59:28,220 --> 00:59:31,490
Any standard of work can do that for you.

478
00:59:31,950 --> 00:59:39,139
Uh, and because they need to be weighted, you could even use something as simple as a mental handset to,

479
00:59:39,140 --> 00:59:43,080
uh, to weight each individual study and so on.

480
00:59:43,100 --> 00:59:46,190
But again, I'm not going to, uh, to dwell on this.

481
00:59:46,550 --> 00:59:49,700
I'm just going to do a little bit of advertisment here.

482
00:59:50,450 --> 00:59:55,850
We have a full course on meta analysis in the summer session in epidemiology.

483
00:59:57,550 --> 01:00:05,440
Which, by the way, any epidemiology student can take one cause in the summer session for free.

484
01:00:07,650 --> 01:00:15,660
And some of them are offered in a hybrid way, meaning you don't need to be here.

485
01:00:15,730 --> 01:00:19,600
I think the MIT analysis is going to be all for hybrid.

486
01:00:21,100 --> 01:00:24,850
By Ross D'Souza, one of my buddies from grad school.

487
01:00:25,880 --> 01:00:33,900
All right. Now, as you probably know, I often times like to emphasize.

488
01:00:35,350 --> 01:00:39,490
More. What you should not do rather than what you should do.

489
01:00:40,870 --> 01:00:47,890
Because, you know, I guess the by default then if you know what you should not do, maybe you'll do the right thing.

490
01:00:48,610 --> 01:00:54,010
So what are the wrong ways to summarize?

491
01:00:55,400 --> 01:01:03,320
Data from different studies. A popular one is to recalculate your measure of frequency.

492
01:01:04,130 --> 01:01:08,960
Simply adding up numerators and denominators from individual studies.

493
01:01:09,740 --> 01:01:11,300
And, you know, you could do that.

494
01:01:12,350 --> 01:01:21,260
You take all the numerators in the exposed all the denominators in the exposed across studies, and you just add them up.

495
01:01:22,070 --> 01:01:26,780
Then you do the same for the unexposed and you estimate the summary relative risk.

496
01:01:27,620 --> 01:01:33,630
What have you missed there? No, wait.

497
01:01:34,140 --> 01:01:37,620
You have not waited. That is according to their size.

498
01:01:37,920 --> 01:01:44,430
Okay, great. Wrong averaging out ratio ratio measures.

499
01:01:44,700 --> 01:01:51,630
So your cumulative incidence ratio in one study is two.

500
01:01:52,170 --> 01:01:57,390
In another study is four. And you're somebody is obviously three.

501
01:01:58,750 --> 01:02:08,730
The average of two and four. Why not, Lindsay? But you are not waiting it.

502
01:02:09,120 --> 01:02:20,400
Yeah, that's one thing. But the other is, is, is a mathematical mistake, which is that things that are in the multiplicative scale, you cannot others.

503
01:02:22,490 --> 01:02:29,110
You could only average I mean, if, you know, if they weighted the same estimates of that in the additive escape.

504
01:02:30,830 --> 01:02:34,120
Do average things in the multiplicative scale. What would you do?

505
01:02:34,450 --> 01:02:38,450
Anybody. Any month. My. Um.

506
01:02:39,930 --> 01:02:48,499
Uh. We sit here. You need to transform that.

507
01:02:48,500 --> 01:02:55,580
You probably need to take a log or something and then sort of transform them in a sort of additive scale.

508
01:02:55,940 --> 01:03:00,740
We will do that. But you should never do this. Do not average out ratio measures.

509
01:03:02,260 --> 01:03:11,860
Also do not count by the direction of the association, meaning good Danish studies five showed a positive association.

510
01:03:12,340 --> 01:03:17,560
Three showed an inverse association to wear. Therefore, my conclusion is that.

511
01:03:18,770 --> 01:03:25,010
Is a positive association because they win. As not.

512
01:03:26,970 --> 01:03:31,740
And the worst of all. Counting by statistical significance.

513
01:03:32,220 --> 01:03:39,150
Of those ten studies, six showed a p value of less than 0.05, and the others did not.

514
01:03:39,330 --> 01:03:42,510
Therefore, overall, the association is significant.

515
01:03:43,480 --> 01:03:50,780
What what is irritating of this? Quickly because we are running out of time.

516
01:03:55,310 --> 01:03:59,270
What's wrong with that? Other than no waiting, which is the first thing that's wrong.

517
01:04:04,810 --> 01:04:09,670
So what are we missing then? Huh?

518
01:04:11,430 --> 01:04:16,540
Yeah. Wait, what else? Go.

519
01:04:18,750 --> 01:04:22,479
Uh. Well, yeah, right. But what else? What, what?

520
01:04:22,480 --> 01:04:26,220
What, what? Why is this not informative? We said that in the.

521
01:04:27,940 --> 01:04:32,980
No direction. There is no direction. And we are those guys killing babies or saving babies?

522
01:04:33,440 --> 01:04:36,700
Well, suppose they are saving babies. What else are we missing?

523
01:04:39,120 --> 01:04:42,180
How many? Right. Thank you, Andrea. Yes, the magnitude.

524
01:04:42,860 --> 01:04:48,430
Okay, so never do that. That's terrible. Right. So a few words on heterogeneity.

525
01:04:48,440 --> 01:04:56,780
So as we said before, heterogeneity is variability in effect size estimate between studies that exceeds what you would expect by chance.

526
01:04:58,550 --> 01:05:03,530
So going back to my example of my own inspiration here.

527
01:05:04,630 --> 01:05:08,270
Um. Oh.

528
01:05:10,020 --> 01:05:26,580
This thing is red. I couldn't remember.

529
01:05:27,630 --> 01:05:33,780
So. All right. So this is the graphical representation of.

530
01:05:33,780 --> 01:05:40,230
Of results. Right. Uh, anybody knows how this type of graphics code.

531
01:05:43,400 --> 01:05:45,170
A florist is called a florist.

532
01:05:46,040 --> 01:05:56,360
It basically represents the estimates of association or effect in every single of the studies that I do included in the meta analysis.

533
01:05:57,020 --> 01:05:59,569
And at the end, the overall result,

534
01:05:59,570 --> 01:06:06,320
the overall summary estimate that has taken into account the weight and obviously the magnitude of each one of those.

535
01:06:06,650 --> 01:06:09,650
So in this in this meta analysis here,

536
01:06:10,640 --> 01:06:19,250
the measure of association was change in mean hemoglobin levels from baseline to the end of follow up at the end of the trial.

537
01:06:19,340 --> 01:06:27,740
Right. Was the same trial of zinc and. So here you have the vertical line.

538
01:06:29,000 --> 01:06:33,260
At zero, which is the value of the no association.

539
01:06:33,260 --> 01:06:38,780
The null zero means there is no effect of of zinc on hemoglobin.

540
01:06:38,790 --> 01:06:41,990
Right. And then for each study you have.

541
01:06:43,120 --> 01:06:50,240
Um. The. Point estimate, which is this dynamo, right?

542
01:06:51,050 --> 01:06:55,040
The confidence interval, which is this for that point estimate.

543
01:06:55,460 --> 01:07:01,460
And the box is a measure of the of the size of the study.

544
01:07:02,420 --> 01:07:08,530
So. I mean, obviously, it is related to the whiff of the confidence interval you see.

545
01:07:09,040 --> 01:07:15,910
For example, the larger studies like this one. Have a larger box around and have tiny confidence in them.

546
01:07:16,480 --> 01:07:20,050
So the larger the study, the larger the box, the narrower the confidence.

547
01:07:20,320 --> 01:07:24,170
Because more precise. Here's a question for you.

548
01:07:24,730 --> 01:07:29,200
Do you see any heterogeneity in general this plot?

549
01:07:35,280 --> 01:07:39,780
Maybe this estimate is only sort of, you know, out of the pattern.

550
01:07:41,010 --> 01:07:45,450
That's heterogeneity. But overall, I would say it's pretty homogeneous.

551
01:07:45,450 --> 01:07:49,500
Right. Any of these these studies is a bit of an outlier.

552
01:07:50,520 --> 01:07:57,870
So. Right. So the idea that, you know, you need these, then you have to investigate why why is this study?

553
01:07:57,870 --> 01:08:03,190
And I'll let. And here is the overall the summary estimate.

554
01:08:03,640 --> 01:08:08,530
So. His left. What was our conclusion from this meta analysis?

555
01:08:09,220 --> 01:08:12,610
Thus. Zinc supplementation.

556
01:08:13,710 --> 01:08:17,220
Decrease hemoglobin levels, produces anemia.

557
01:08:17,380 --> 01:08:23,310
Babies know if anything actually might slightly increase and more glowing.

558
01:08:24,370 --> 01:08:29,080
So that's what we wrote in the response to the grant.

559
01:08:29,530 --> 01:08:33,280
And then we got to that the second thing, and then I couldn't do the trial.

560
01:08:38,430 --> 01:08:42,149
All right. So these are the many sources of heterogeneity.

561
01:08:42,150 --> 01:08:45,210
And as I said, you all need to kind of think outside the box.

562
01:08:46,230 --> 01:08:52,710
But I would like to highlight that there can be heterogeneity due to.

563
01:08:53,930 --> 01:09:02,600
Problems with the studies. You know, studies of different qualities make different results because they have different qualities, but also.

564
01:09:04,170 --> 01:09:16,270
The you know, if if the sort of the causal action potential source of heterogeneity can be the fact that there is a effect modification.

565
01:09:17,270 --> 01:09:21,799
Meaning there might be an effect modifier in one population,

566
01:09:21,800 --> 01:09:28,910
in one study that interacts with the exposure in a certain way to produce an average result such as these.

567
01:09:29,960 --> 01:09:38,320
And if that same effect modifier does not exist in another population, there will not be interaction or even the prevalence is different,

568
01:09:38,330 --> 01:09:43,370
there will not be interaction with the exposure, and you will get a different overall estimate.

569
01:09:44,290 --> 01:09:47,570
Right. Yeah. Just one second.

570
01:09:47,990 --> 01:09:56,600
So example. Again, nutrition is pretty, pretty handy because it's it's a sort of kind of intuitive.

571
01:09:58,120 --> 01:10:03,609
Do you think Vitamin A supplements work better in populations where there's lots

572
01:10:03,610 --> 01:10:07,570
of kids with vitamin A deficiency or very few kids with vitamin D deficiency?

573
01:10:09,430 --> 01:10:10,509
When there is lots of kids.

574
01:10:10,510 --> 01:10:16,330
I mean, if you are deficient on something and you give the nutrient to the kids, I mean, they will, you know, benefit from it.

575
01:10:17,270 --> 01:10:20,830
So if you do a study in a population of vitamin E,

576
01:10:20,870 --> 01:10:26,150
in a population where there is a lot of vitamin A deficiency, likely you will get an estimate of effect.

577
01:10:26,630 --> 01:10:31,700
That's very impressive, right? If you the same sustainable population, there is no vitamin deficiency at all.

578
01:10:32,240 --> 01:10:37,160
You will not find nothing. You will find a one or you know, that's heterogeneity.

579
01:10:37,670 --> 01:10:41,120
And what is the modifier? Vitamin A deficiency?

580
01:10:43,160 --> 01:11:00,650
Right? Yeah. Quick question, because we have to wrap. All the.

581
01:11:02,670 --> 01:11:07,510
In. Okay.

582
01:11:07,530 --> 01:11:11,490
He's comparing between all of them. Yeah. Yeah. Good question.

583
01:11:12,060 --> 01:11:18,180
Yeah. Look, and this is kind of the the final note here.

584
01:11:18,810 --> 01:11:26,460
Publication bias. So what's publication? Well, publication bias is one of the, uh.

585
01:11:28,520 --> 01:11:36,680
Justifications to register studies, right? Publication bias is the omission of unpublished studies from meta analysis.

586
01:11:39,390 --> 01:11:42,820
Because. They haven't been published.

587
01:11:44,840 --> 01:11:54,889
And the problem is that due to the reasons why studies may not get published, you can actually anticipate the direction of publication.

588
01:11:54,890 --> 01:12:04,430
But. So results with significant findings and which are also in the direction expected by the community,

589
01:12:04,670 --> 01:12:09,560
the academic community or the policy committee might get published.

590
01:12:09,950 --> 01:12:20,760
They may be more favored to be, but, you know. Whereas studies that are null even if they are large.

591
01:12:22,290 --> 01:12:28,020
May have less chances of publication because we are all too human.

592
01:12:28,380 --> 01:12:34,950
Me is confirmation bias. We we like to to hear what we.

593
01:12:37,110 --> 01:12:40,680
Thing that we know already or what? What? But you know what we expect.

594
01:12:44,080 --> 01:12:54,130
So how do we assess publication bias? This is an interesting study and there are many ways, but I'm just going to tell you one common one.

595
01:12:55,570 --> 01:13:02,530
You can plot the results of the individual studies and in this case, an odds ratio.

596
01:13:03,620 --> 01:13:08,720
From cohorts they serve up against a measure of precision,

597
01:13:08,960 --> 01:13:17,030
which can be the size of the study or the variance of the of the estimate or the width of the confidence interval that a number of.

598
01:13:18,090 --> 01:13:21,340
So. If you do that.

599
01:13:21,360 --> 01:13:24,660
So each each note here represents the results of each study.

600
01:13:25,200 --> 01:13:33,900
So, for example, this dot means they found a strongly protective effect of a step two kinase in myocardial infarction.

601
01:13:34,230 --> 01:13:39,930
Right. But this study was a small one. Because the precision is low.

602
01:13:40,380 --> 01:13:47,560
Right. And so on. This one found a mildly protective effect.

603
01:13:47,970 --> 01:13:53,250
And it was the most precise of. All right. But guess what?

604
01:13:53,880 --> 01:13:57,810
There are quite a few studies here that found a harmful effect.

605
01:13:59,960 --> 01:14:07,520
Right. And also, as you can see. There is actually a shape of this representation.

606
01:14:08,240 --> 01:14:15,960
The shape is the shape of a final. So these types of representations are called funnel plots.

607
01:14:16,170 --> 01:14:21,730
Funny. Why do you think that?

608
01:14:22,770 --> 01:14:29,430
They are the base of the funnel. There are studies that are both harmful and protective.

609
01:14:35,280 --> 01:14:41,940
Because you lack precision, because of chance, instead of looking at the whole picture in general,

610
01:14:42,810 --> 01:14:48,150
you know, and studies with no precision can yield estimates that are all over the place.

611
01:14:49,330 --> 01:15:01,209
You know, that's okay. And in this case, well, this is the overall summary, is that she's just slightly protective.

612
01:15:01,210 --> 01:15:07,250
No, not not so much. So when you get this type of graph.

613
01:15:08,580 --> 01:15:13,310
There is really not strong evidence. For publication bias.

614
01:15:15,420 --> 01:15:21,470
If there was. You would be missing studies in the zone.

615
01:15:23,510 --> 01:15:26,670
Where people don't want to see that effect.

616
01:15:28,430 --> 01:15:38,090
So suppose if if doctors wanted to to see a strong protective effect of a certain kind on my.

617
01:15:39,200 --> 01:15:45,140
Then you can assume that these studies would be more likely to have been published than these studies.

618
01:15:45,380 --> 01:15:49,730
And then you would see, let's call an area of missing studies around here,

619
01:15:50,390 --> 01:15:55,010
because the smaller studies that have shown an adverse effect would not get passed.

620
01:15:55,340 --> 01:15:57,520
The editors would not want to see those things in.

621
01:15:58,550 --> 01:16:07,310
They would unconsciously or consciously want to promote publication of stories that are consistent with what they want to sort of, you know, present.

622
01:16:09,280 --> 01:16:12,490
So yeah, this is this is an example of that.

623
01:16:12,670 --> 01:16:18,160
If if the effect is modest, they're not mostly small studies that by and large, if by chance, are published.

624
01:16:19,030 --> 01:16:27,850
So these ones and then large studies with high precision that may report the null effect, but may still get published because they are left.

625
01:16:28,750 --> 01:16:31,570
Right, which would be this. So you will be.

626
01:16:32,780 --> 01:16:42,170
You will find a region of missing studies here, typically small studies that show an effect, but an association in the direction opposite to what?

627
01:16:43,250 --> 01:16:53,489
People want to hear. Now, how do we prevent publication bias?

628
01:16:53,490 --> 01:17:01,920
And this is an interesting set of propositions. So regarding the registration of ancillary studies at the beginning is one option.

629
01:17:03,210 --> 01:17:15,610
Um. There is another interesting proposition that has utility, you know, journalists good compromise to publish at the time of registration.

630
01:17:16,610 --> 01:17:24,260
Regardless of the results. And in fact, some journalists do that for very, very large and high profile trials.

631
01:17:24,560 --> 01:17:31,520
Possibly the COVID vaccine trials might have gone from that if if there is a very large

632
01:17:31,520 --> 01:17:37,760
study that has been funded and is is is going well in terms of its implementation.

633
01:17:38,180 --> 01:17:41,330
In theory, you could negotiate, I guess, with the.

634
01:17:42,500 --> 01:17:48,980
With the journal to. Guarantee publication of the stadium as long as it's well written.

635
01:17:50,000 --> 01:17:56,170
Regardless of whether he's not in it. How about look on that?

636
01:17:58,700 --> 01:18:02,750
The peer reviewed review blinded to the results.

637
01:18:04,760 --> 01:18:10,940
Based only. It's on the introduction, which is, you know, lays out the hypotheses and the methods.

638
01:18:11,780 --> 01:18:23,110
If the methods are sound, maybe the editors could lean toward acceptance of the paper regardless or before seeing the actual results.

639
01:18:23,120 --> 01:18:27,160
This one. I do not think this has been implemented. As far as I know.

640
01:18:28,780 --> 01:18:36,340
But promotion of new sources for publication of null results like some online journals and they actually exist.

641
01:18:37,420 --> 01:18:43,970
The Journal of Negative Results in Biomedicine. Journal of articles in support of the null hypothesis.

642
01:18:45,110 --> 01:18:52,580
And well, there are the journal of reproducible results of unlikely science and analysis of improbable research.

643
01:18:53,390 --> 01:18:59,040
Yeah. Well, right now, what are the limitations of meta analysis?

644
01:19:01,060 --> 01:19:07,570
This is the most important one. I mean, I guess this is perhaps one of the reasons why I personally am not very fond of Meta-analysis.

645
01:19:08,720 --> 01:19:14,510
But the quality of your summary estimate only depends on the quality of the individual.

646
01:19:15,170 --> 01:19:20,540
And sometimes there may not be enough studies of good quality that are worth putting together.

647
01:19:21,450 --> 01:19:30,089
Often things. The second is heterogeneity and there is a lot of heterogeneity and let's see if it's truly due to effect.

648
01:19:30,090 --> 01:19:36,630
Modification probably doesn't even make sense to attempt putting together results into a single list.

649
01:19:37,790 --> 01:19:44,599
Because simply that means whatever intervention that is has a different effect in different populations,

650
01:19:44,600 --> 01:19:48,469
depending on the background characteristics of those populations.

651
01:19:48,470 --> 01:19:53,720
Right. Another is that, you know, meta analysis tend to violate.

652
01:19:55,450 --> 01:19:57,550
One of the key conditions for.

