1
00:00:04,650 --> 00:00:12,330
So then today we are going to continue the discussion about, you know, using X, for example,

2
00:00:12,360 --> 00:00:21,750
squares to test a variety of different policies regarding to millions of people who are ration.

3
00:00:23,620 --> 00:00:27,339
So we are actually a bit ahead of the other session.

4
00:00:27,340 --> 00:00:30,820
So I think we can take our potholes and we can slow down a little bit.

5
00:00:32,170 --> 00:00:36,160
So let's get a refresh, remembering what we what we have done.

6
00:00:37,150 --> 00:00:39,970
So you talk about, okay, let's let's start from here.

7
00:00:42,100 --> 00:00:50,469
So there are two types of high voltage testing or whatever seen as one is sequential, one is partial, right?

8
00:00:50,470 --> 00:00:55,870
So we have the above about and we talk about partial testing in our last lecture.

9
00:00:56,470 --> 00:00:59,740
So the partner high voltage has the means that the term.

10
00:00:59,760 --> 00:01:07,770
So maybe here is a good. Yeah.

11
00:01:08,450 --> 00:01:08,830
It's just.

12
00:01:13,790 --> 00:01:23,510
So the partial testing means that, you know, force for regression model that we are trying to think that has not imposed multiple covariates.

13
00:01:24,140 --> 00:01:30,320
The particles testing means that you're looking at your casting either, you know,

14
00:01:30,320 --> 00:01:37,460
a single beta or maybe a subset of beta, say, adjusting for all the other covariates included too.

15
00:01:38,030 --> 00:01:42,769
So for example here, let's say we look at does x one contribute to the model,

16
00:01:42,770 --> 00:01:49,910
then that is to test whether beta one is equal to zero, given that the other covariates have already been included in the model.

17
00:01:50,330 --> 00:01:56,299
And that's that's part of a one part of has a similar you could test whether beta two

18
00:01:56,300 --> 00:02:01,640
is equal to zero given that beta model x one have three are already in the model.

19
00:02:02,060 --> 00:02:04,820
So these are what we call the partial casts.

20
00:02:06,980 --> 00:02:16,160
So for partial patterns, for example, if we're testing x11 that is not is while here the hypothesis is beta one equal to zero versus alternative.

21
00:02:16,160 --> 00:02:19,760
The beta one is not equal to zero, right? Again.

22
00:02:19,760 --> 00:02:24,229
So after one, even after the other two code errors are already included in the model.

23
00:02:24,230 --> 00:02:33,530
So here we're testing are we're looking out of the egg for some of the square by adding X1 into the model that already has x two and x three.

24
00:02:34,220 --> 00:02:46,250
And the AB statistic is the sum of squares, extra sum of squares divided by C Manhattan Square and the photo F distribution and similarly for X two.

25
00:02:46,250 --> 00:02:56,570
So we have one over this. And then a partial test could also be used to test about whether two covariates are

26
00:02:56,600 --> 00:03:01,910
important per dangerous given that the the other one as are to be included in the model.

27
00:03:02,120 --> 00:03:11,540
So this in terms of our the model we are feeding, for example, we can test whether it's true and be three, right?

28
00:03:11,540 --> 00:03:17,740
So we can pass whether these two are beta two and a beta three, whether or not in the deal,

29
00:03:17,760 --> 00:03:30,739
given that beta one is already employed and that's also a part of the pass that's actually testing must not have a loses versus not versus next year.

30
00:03:30,740 --> 00:03:34,100
Beta two is not an equal to zero four.

31
00:03:34,130 --> 00:03:43,720
Beta three is not an equal to zero. At least one of these two we might be able to the task as the host is actually the extra

32
00:03:43,800 --> 00:03:50,060
extra sum of the square by adding extra two and three into the model that only has as one.

33
00:03:51,200 --> 00:03:54,139
But here we need to be careful not to be afraid of it.

34
00:03:54,140 --> 00:04:02,420
So this tool comes from the fact that here we are casting a to beat us beta true and beta three they're both equal to zero.

35
00:04:02,570 --> 00:04:11,440
So that's why we have the directly on to numerator and denominator of this and I see that you've freedom is those p right?

36
00:04:11,450 --> 00:04:17,239
So the P in our cases we're going to count how many, how many debaters we have.

37
00:04:17,240 --> 00:04:21,680
That's number this number of people one, two, three, four, four data.

38
00:04:21,680 --> 00:04:28,820
So that's the is equal to four. So that is more freedom in the denominator minus Chivas and then is four.

39
00:04:29,330 --> 00:04:33,010
And so that's how we are.

40
00:04:34,250 --> 00:04:38,210
That's what a partial task means. And then we look at one example.

41
00:04:38,540 --> 00:04:46,820
So I don't need to go through the details of this example here and then we talk about the so-called sequential hazard.

42
00:04:48,050 --> 00:04:54,710
So I think here on the first letter of passing, last time I got a quite a few question I want to clarify a little bit.

43
00:04:55,460 --> 00:05:01,550
So let's suppose that now this is the model that would be our model fetch right there,

44
00:05:01,550 --> 00:05:07,430
including the function the parliament function for B, which is BMI in those body mass index.

45
00:05:08,000 --> 00:05:11,060
So we are including the following values up to order form.

46
00:05:11,300 --> 00:05:23,390
So this is the model we're trying to fit. And now last time we talk about the principle or the the balance between interpretation and the parsimony.

47
00:05:24,140 --> 00:05:38,300
So this is a commonly seen consideration in the European model, not only marginal model, but any moral of any models as with any kind of model.

48
00:05:39,170 --> 00:05:44,989
So we have to essentially what it means is that we have to account for the taking

49
00:05:44,990 --> 00:05:48,650
into account of the comparison to the model and interpretation of the model.

50
00:05:50,000 --> 00:05:57,140
So if a model is very complex, so of course, I mean the biological mechanism behind your data,

51
00:05:57,230 --> 00:06:00,710
you are a true pilot of because then that's very complex.

52
00:06:00,890 --> 00:06:04,550
We have no way of knowing the truth. So instead.

53
00:06:04,580 --> 00:06:11,930
SD or in any model of. We are trying to specify our model to model to to approximate the unknown, like very complex truth.

54
00:06:12,590 --> 00:06:24,920
So in this sense, we are adding a simplifying the the online unknown truth and a linear model is a way of simplifying the truth.

55
00:06:26,270 --> 00:06:29,690
So if you look at this model here.

56
00:06:30,850 --> 00:06:35,200
Right. So it has to alter for now compared to a model,

57
00:06:35,200 --> 00:06:44,590
let's say I have a model that does not have this loss in turn will only have to alter to then the simpler model is easier to interpret.

58
00:06:44,770 --> 00:06:51,940
Of course it's easier to interpret because it simply depends on the model is a function of BMI.

59
00:06:52,180 --> 00:06:57,120
It's much easier to imagine what are the functional for this model.

60
00:06:57,240 --> 00:07:01,250
For a model where you equal to under four, then it's more complex.

61
00:07:01,840 --> 00:07:09,160
It's a little bit hard to interpret exactly what the dependance of the response in this kind of classroom is,

62
00:07:09,310 --> 00:07:14,709
what the dependance is on the on the idea that is lies.

63
00:07:14,710 --> 00:07:18,130
And this is especially true later when we talk about interactions.

64
00:07:18,490 --> 00:07:22,330
So when you have different covariate, when you have age, for example, age, gender,

65
00:07:22,330 --> 00:07:27,790
you know, height, weight and, and in the race we have a bunch of variables in the model.

66
00:07:28,790 --> 00:07:38,340
Sometimes the effect of, for example, in factor of age on the, on the other response may be modified by another variable.

67
00:07:38,350 --> 00:07:44,350
So by gender, for example, let's say given gender as a if you look at within a sort of gender,

68
00:07:44,350 --> 00:07:48,280
their age effect, it becomes slightly different compared to another gender.

69
00:07:49,750 --> 00:07:54,400
So we are going to talk about that later, that that's so-called interaction.

70
00:07:55,030 --> 00:07:58,990
So when you have a bunch of variables, then there are different ways of including interaction.

71
00:07:59,560 --> 00:08:00,020
For example,

72
00:08:00,020 --> 00:08:10,270
you have five covariates missing and then the fluid reaction is any interaction between any pair of this life and that is going equal mathematic.

73
00:08:10,300 --> 00:08:14,470
We are going to have to to power five these many interactions positive interactions.

74
00:08:15,130 --> 00:08:19,450
So like for example interaction between age and gender interaction between age and height,

75
00:08:19,630 --> 00:08:26,380
age and weight, age and like others, and also between race and gender, race in height.

76
00:08:26,830 --> 00:08:35,250
So there are many interactions, but you have different interaction in the model that it really complicates the interpretation.

77
00:08:36,880 --> 00:08:45,820
So, so, so when we do a model, we have to be real careful how complex we want the model to be.

78
00:08:45,850 --> 00:08:53,470
Of course, the more complex model is the probably the more likely that we are able to approximate the truth better.

79
00:08:54,130 --> 00:09:00,020
However, on the other hand, when the model becomes too complex, it becomes very difficult to interpret this.

80
00:09:00,020 --> 00:09:03,990
It becomes more clear they don't have enough interactions.

81
00:09:07,120 --> 00:09:16,600
So that's why we well, we want the model to be parsimonious, which means that the models are we do not want the model to be over a complex.

82
00:09:17,680 --> 00:09:21,820
We will add the model two to be simple enough that we are able to interpret.

83
00:09:23,440 --> 00:09:36,520
So based on that now, if we look at this model terms of this model, then we will have to test whether, you know, each of these, for example, of this.

84
00:09:37,120 --> 00:09:43,840
Paul, if I have been whether that's needed in the in the model is not needed, I will simply remove it from the world.

85
00:09:45,190 --> 00:09:53,860
But because of the polynomial function, we will add to that in a sequential way based on the consideration that now if,

86
00:09:53,860 --> 00:10:00,580
let's say, if we have a model that does not have this two terms but only have to be to be too powerful.

87
00:10:01,330 --> 00:10:04,780
And this seems to be kind of a strange model.

88
00:10:05,110 --> 00:10:12,630
And so it's sometimes is hard to explain such a model to two other investigators like Peter is from other areas.

89
00:10:12,660 --> 00:10:16,960
Know what we mean by a model that has no single power.

90
00:10:17,110 --> 00:10:22,390
And so the power of that absolutely close to the power of the amount.

91
00:10:23,980 --> 00:10:30,100
But this is not saying that is is it's impossible to have a such model mathematically.

92
00:10:30,100 --> 00:10:33,570
It's totally fine to run if the truth is indeed such a model.

93
00:10:33,580 --> 00:10:42,430
And that is totally fine for me to build a second model. But it is just that, you know, this model is not as natural as a model that has,

94
00:10:42,580 --> 00:10:49,810
you know, has all these lower order polynomials in addition to the fourth one.

95
00:10:50,500 --> 00:10:54,879
So. So last time I mentioned that, what we talked about and what it has done,

96
00:10:54,880 --> 00:10:59,860
we said that it would have the first letter and this is equal to zero when this is equal to zero.

97
00:11:00,190 --> 00:11:04,360
And then given that this is already assuming that this is equal to zero.

98
00:11:05,770 --> 00:11:18,520
Especially. So we include, for example, we put off speaking to power for only when we include all these lower order puzzlement in the bottle.

99
00:11:18,760 --> 00:11:27,800
So if we pass that, if our test says that beta three is equal to zero, then we will just stop here.

100
00:11:27,820 --> 00:11:31,450
We're not going any further to build the model.

101
00:11:31,690 --> 00:11:39,210
So that's just a convention. That's just like a practice, a common practice that people you are to follow.

102
00:11:39,220 --> 00:11:42,940
But by no means is that by no means that, you know,

103
00:11:43,470 --> 00:11:54,660
it's it's that you cannot you cannot have a model that does not have a power of three, but it does have power for that.

104
00:11:54,670 --> 00:12:02,320
That's still if you if you do believe that, you know, this is like the the true model or the model that approximate the truth the best.

105
00:12:02,620 --> 00:12:10,800
And then go ahead and you can definitely do the second model, but it's just not a this is not a common practice.

106
00:12:10,810 --> 00:12:21,670
So in most cases, people would prefer having lower polynomials first before they concede or highlander on other functions.

107
00:12:22,360 --> 00:12:26,410
So that's about power, love and function.

108
00:12:27,790 --> 00:12:31,330
And then for Paloma function, we lost the collision testing one.

109
00:12:35,320 --> 00:12:46,180
One of the used case first world has and that is the polio virus will be able panel to include all the functions of a variable.

110
00:12:46,190 --> 00:12:51,170
Another one I go to is the inclusion of interactions.

111
00:12:51,190 --> 00:12:56,550
Again, we will later talk about the inclusion of interactions and a foreign policy function.

112
00:12:56,560 --> 00:13:00,190
We do what we can at a sequential level of testing.

113
00:13:00,190 --> 00:13:03,520
So first, we test whether beta one is equal to zero.

114
00:13:04,360 --> 00:13:15,820
We're not. So we consider the extra sample square by adding X1 into a model that has no covariates, that has only the intercept.

115
00:13:17,120 --> 00:13:22,940
And we carried out a test. So this is our pass testing, whether we want people to zero or not.

116
00:13:23,600 --> 00:13:27,440
And then if H0 is rejected, then we move ahead.

117
00:13:27,440 --> 00:13:33,920
And then it hasn't really been a tool of the beta for the 700 polynomial, whether that's equal to zero or not.

118
00:13:34,190 --> 00:13:46,070
That's by adding B square into a model into the model that has an intercept and A and B, and then if this is rejected, that will move forward.

119
00:13:46,400 --> 00:13:51,320
In you pass when you should add a beta, three, three, three is equal to zero.

120
00:13:52,220 --> 00:13:55,400
But if this is not a reaction, let's see here.

121
00:13:55,450 --> 00:14:01,970
If we if if we fail to reject does not have elicits.

122
00:14:02,300 --> 00:14:06,030
So that means that the p square of this term is not a significant.

123
00:14:07,400 --> 00:14:11,270
Then in most cases, people will stop here, they will not move forward.

124
00:14:12,230 --> 00:14:15,620
And then we will just state because we fail to reject this.

125
00:14:15,830 --> 00:14:20,300
So it is not a bother. The psychomotor value is not significant.

126
00:14:20,600 --> 00:14:23,780
So we will stay with the model in this model.

127
00:14:23,900 --> 00:14:28,040
So we will not go further. It will not go further.

128
00:14:28,700 --> 00:14:32,800
However, this is not saying that you have to stop here.

129
00:14:32,810 --> 00:14:40,250
I mean, if you have to have reason to think that, you know, there may be B square, sorry, B cube, that is significant.

130
00:14:40,250 --> 00:14:43,300
Maybe significant because do go ahead and pass that.

131
00:14:44,180 --> 00:14:48,280
But it's just a common practice that once you see that this is not significant,

132
00:14:48,440 --> 00:14:56,810
a significant out of water because we have to stop one goal and forever, we have to stop somewhere and the rest must move that hat.

133
00:14:56,840 --> 00:15:00,170
Well, this is not significant. You already people to stop here.

134
00:15:04,720 --> 00:15:11,300
Okay. So that's sequential testing. Okay.

135
00:15:11,320 --> 00:15:15,190
So this is another example.

136
00:15:15,190 --> 00:15:23,530
We we went over this. Let's see now. Oh, yeah.

137
00:15:23,560 --> 00:15:26,950
And then this is the thing where we. Okay, so this is where we start.

138
00:15:26,950 --> 00:15:30,219
The last time we didn't have time to finish this necessarily.

139
00:15:30,220 --> 00:15:36,460
Right? So here, this is a subtle but a quite important.

140
00:15:39,900 --> 00:15:45,740
A subtle but quite important. In fact, actually.

141
00:15:46,010 --> 00:15:55,610
So when we when we encounter a statistic mean we have to what we call absolute honesty, we have to calculate to see my house square.

142
00:15:55,610 --> 00:16:01,280
And that's a very important part of that. Now this becomes quite relevant.

143
00:16:01,280 --> 00:16:04,850
This sort of one become quite prevalent for sequential testing.

144
00:16:05,810 --> 00:16:11,330
Again, sequential testing has a wide application.

145
00:16:11,510 --> 00:16:14,629
When we look at what we try to include interaction in the model.

146
00:16:14,630 --> 00:16:18,980
So this is a very important table of testing.

147
00:16:19,250 --> 00:16:22,730
So we have to be here. I think we have to be a little bit careful.

148
00:16:25,020 --> 00:16:31,610
So suppose now we are we are building Sergeant Moore so why is equal to this additional.

149
00:16:31,710 --> 00:16:34,860
And this is the model we are where we are building.

150
00:16:36,900 --> 00:16:47,220
And then let's see. Now we are casting. We're testing here where it hasn't been a full three.

151
00:16:48,300 --> 00:16:55,020
Where has the weather beaten to equal to Peter three is equal to zero versus the alternative

152
00:16:55,020 --> 00:17:09,990
that you know at least at least one of beta two and the beta three is not is not a zero right.

153
00:17:10,260 --> 00:17:19,140
Of so well it hasn't such anomalies and then well we can start with f statistic.

154
00:17:20,830 --> 00:17:24,250
So in the numerator we have to add for some one square.

155
00:17:24,310 --> 00:17:32,889
So that's by adding you add two and three into the model that already has a zero beta one

156
00:17:32,890 --> 00:17:39,640
or intercept into the x one and then dividing by the corresponding lot of your freedom.

157
00:17:39,640 --> 00:17:46,480
That's two because we are attempting to balance now the numerator and denominator of this.

158
00:17:46,480 --> 00:17:53,740
See my square. If you look at this my had a square in this room, had a square is actually for the full model.

159
00:17:54,280 --> 00:17:56,800
So this is computed based on the full model.

160
00:17:57,160 --> 00:18:06,430
That is the model containing the contents that all the five x that is got the this full model that we are considering.

161
00:18:08,470 --> 00:18:11,470
This is the list. You must learn how we are using.

162
00:18:11,830 --> 00:18:16,210
So because it is calculated based on this full model.

163
00:18:16,550 --> 00:18:21,040
In a full model there are one, two, three, four, five, six, six.

164
00:18:21,040 --> 00:18:27,660
Be this. That's why here in the denominator we have a six figure freedom, right.

165
00:18:28,060 --> 00:18:30,070
That's that seems to be quite natural.

166
00:18:31,660 --> 00:18:42,850
However, in some textbook when people calculate in this our statistic now here we use a star to denote the distinction.

167
00:18:44,920 --> 00:18:54,550
Sorry. So the numerator is the same.

168
00:18:57,130 --> 00:19:04,030
We have the same numerator in the same order here, but the denominator, the denominator this become differs.

169
00:19:06,010 --> 00:19:12,640
This denominator is actually a Sigma Square patch for this model.

170
00:19:30,300 --> 00:19:39,290
If you look how well this denominator is is SSD for this model divided by.

171
00:19:39,440 --> 00:19:45,140
Yeah, minus four. We have a minus four because here we have one, two, three, four, four valence now.

172
00:19:46,800 --> 00:19:54,720
Well, the rationale behind this the argument behind this ouster is that, okay, so now I am casting.

173
00:20:00,990 --> 00:20:06,780
Because this is sequential casting. So I'm testing beta two equal to a three, equal to zero.

174
00:20:06,810 --> 00:20:11,580
That's actually after we have already tested. Beta one is equal to zero or not.

175
00:20:11,850 --> 00:20:18,810
This is one of testing. So suppose we have heard it has to be a y is equal to zero or not and we will fail to reject that.

176
00:20:18,900 --> 00:20:23,790
Oh sorry. We are tackling that so that we already included beta one for x1 in the model.

177
00:20:24,000 --> 00:20:27,330
Now we're positive whether beta through it will be a three is equal to zero.

178
00:20:28,560 --> 00:20:37,620
And at this stage, when we pass this model, this is the beta four in beta five, but we haven't reached that yet.

179
00:20:38,160 --> 00:20:42,570
Now we are doing this equation. So we have already seen, you know, one already has the beta one.

180
00:20:42,960 --> 00:20:46,170
Now we are moving to better to beta three and beta for a beta five.

181
00:20:46,200 --> 00:20:53,580
We haven't reached them yet. So, so we shouldn't worry about when we test more this bit until get a 3 to 0.

182
00:20:53,850 --> 00:20:59,400
So that's the argument behind using this seamless word.

183
00:20:59,970 --> 00:21:04,770
That's the seamless word corresponding to this model that does not include ads for X Factor.

184
00:21:06,840 --> 00:21:11,850
So that's the argument behind this on this collision our start.

185
00:21:14,460 --> 00:21:34,230
However, here we want to point out that we is better actually to use this F instead of this avatar actually in SAS and in R, you know, in SAS.

186
00:21:35,970 --> 00:21:41,940
And they are you know, once you see them in last I mean unless I guess of.

187
00:21:44,380 --> 00:21:51,190
Unless you specifically ask SAS or ARM to our approach the f star.

188
00:21:51,460 --> 00:21:55,660
Otherwise the f you will see. That's that's the statistic.

189
00:21:55,900 --> 00:22:02,080
That's the first version of the master using a summerhouse work based on the full moon.

190
00:22:02,230 --> 00:22:05,530
Based on all the communities you're considering.

191
00:22:09,270 --> 00:22:16,680
So the reason I'm doing this and the reason that we said that, you know, this app is better than App Store.

192
00:22:17,130 --> 00:22:27,750
The reason is that for this app to have for this either this app were ever star to have evidence fusion because eventually

193
00:22:27,870 --> 00:22:34,020
what we carried out have all this test we need to the app to follow some of those fusion so that we can capture the p value.

194
00:22:35,040 --> 00:22:38,160
So for this. Yeah. For App Star to follow this evidence fusion.

195
00:22:40,410 --> 00:22:49,670
This CMA had to square the CMA to square whether it is in this version or is calculated in this way.

196
00:22:50,040 --> 00:22:53,590
So they have to be a cracked estimate of the truth.

197
00:22:53,640 --> 00:22:56,910
You must square your model. For a moment.

198
00:22:58,870 --> 00:23:07,540
Now, if we compare this model, a model House expert, X5 with this model, the model that does not have as four and five.

199
00:23:08,050 --> 00:23:19,770
Now this two models. Now the smaller model has the risk that it's Mississippi's Mississippi's fight because it does not include ads for X five.

200
00:23:20,160 --> 00:23:24,540
So let's let's imagine a scenario where the truth indeed.

201
00:23:25,320 --> 00:23:29,010
Well, let's say the true white indeed depends on X for X file.

202
00:23:29,250 --> 00:23:34,720
So another race, in other words. But the larger model is individual model than the smaller one.

203
00:23:34,740 --> 00:23:37,920
This is not a crack because you include ads for ads. Fine.

204
00:23:38,370 --> 00:23:49,710
So it's not a crack model. Now, when you calculate the system square hat based on like in this way, Calvin, that's based on an incorrect model.

205
00:23:50,400 --> 00:23:54,060
Now, this Susquehanna is in crack estimate of Sigma Square.

206
00:23:54,750 --> 00:24:03,330
So as a result, this star does not no longer follow this kind of distribution anymore and no longer follows this out of this building.

207
00:24:05,070 --> 00:24:11,310
So that's the risk that we while of using this version of me.

208
00:24:12,720 --> 00:24:15,870
So you have a square. So that's again,

209
00:24:16,260 --> 00:24:21,329
the risk is that if this smaller model is incorrect now the denominator of the

210
00:24:21,330 --> 00:24:25,290
capital account of a similar square used in the denominator is not a crack.

211
00:24:25,290 --> 00:24:33,310
It's not an unbiased estimate of similar choosing less square anymore so that this start started a longer follow this after solution.

212
00:24:36,150 --> 00:24:41,340
Now using the larger model, using the first like a full model, but it doesn't include as 4x5.

213
00:24:41,490 --> 00:24:53,850
We do not have that risk. The reason is that now, even if in the case where let's say x4x5, there indeed have no revisions should with this one.

214
00:24:54,240 --> 00:24:57,420
So in other words, beta for a beta five we are indeed zero.

215
00:24:57,420 --> 00:25:01,950
So so this the smaller model is already the true is artificial model.

216
00:25:03,490 --> 00:25:07,300
And then by including the ads for x five.

217
00:25:07,930 --> 00:25:17,350
If we got a miracle memories that are not related to why we just introduce a noise I so this means that we will have some noise

218
00:25:17,350 --> 00:25:23,830
introducing the model but it really this this is still a crowd model is just a beta for equal rate of five equal to zero.

219
00:25:24,640 --> 00:25:35,110
So it will not affect so that the resulting sigma had a square, you know, as made it based on this larger model,

220
00:25:35,890 --> 00:25:46,110
although it's not in a most precise estimate of Sigma Square because you introduce some extra noise by including extracts five.

221
00:25:46,510 --> 00:25:50,300
However, it is still an unbiased estimate of three similar square.

222
00:25:51,570 --> 00:25:59,590
So. So that means that a using the larger model is always safer when you estimate this Sigma Square,

223
00:25:59,920 --> 00:26:03,520
which we can calculate in the C, must work in a denominator.

224
00:26:04,150 --> 00:26:09,100
So it's safer to come from a Sigma Square using a larger model because larger model

225
00:26:09,880 --> 00:26:15,130
because the larger model has a higher chance of being in space by a simpler model.

226
00:26:16,480 --> 00:26:28,090
So that's the reason why, you know, we will and not only us, but, you know, our in a sense allows you ask them specifically to calculate this AB star,

227
00:26:28,510 --> 00:26:36,040
analyze the results in C they are based on this F where the C one square is calculated based on the longer model.

228
00:26:37,990 --> 00:26:50,280
So this is a. You know, again, while this is a subtle thing, but it's quite important because ah, in a sense they do.

229
00:26:50,820 --> 00:26:56,760
Our approach to if you ask them to, if you ask them, they do our protocols, our style.

230
00:26:57,060 --> 00:27:03,020
So it's very important to know the difference between F in f star and how to interpret this.

231
00:27:03,030 --> 00:27:13,020
We are going to see a very detailed example in a few minutes to try to show the difference, like how to interpret and how different healthy from R.

232
00:27:13,920 --> 00:27:22,530
But here this is not just a general description of f f starts this to a statistic.

233
00:27:26,310 --> 00:27:32,000
Okay. Any questions about this to start?

234
00:27:39,830 --> 00:27:46,700
Okay. Oh, yeah.

235
00:27:46,700 --> 00:27:55,069
I think we have already covered this slide. So another thing is that this star, well, it has a very nice interpretation.

236
00:27:55,070 --> 00:28:01,130
If you look out of whatever Starr is, if Starr is looking for someone square.

237
00:28:06,150 --> 00:28:11,309
The extra sample square by adding adds to while by adding two and three into a

238
00:28:11,310 --> 00:28:19,230
model that has intercepted heads one and that actually compared to the SLC look.

239
00:28:19,410 --> 00:28:25,770
The error total error was almost graph as we measured this error, this extra sample square.

240
00:28:26,190 --> 00:28:33,880
This can be considered as the reduction in terms of the errors almost in errors almost where by adding this to covariance.

241
00:28:33,950 --> 00:28:43,050
Right, extrapolate the reduction in the errors on the script so that if you look at the reduction, well, this is the result.

242
00:28:43,350 --> 00:28:47,340
If we forget about the degree of for a minute, but let's forget all them for a minute.

243
00:28:48,270 --> 00:28:55,320
If we just look at the end for some of us where this is a reduction in the error sample square and this is the total error somehow square.

244
00:28:55,620 --> 00:28:59,109
In this model, the model has one answer.

245
00:28:59,110 --> 00:29:06,630
We paid it. So this f star has a very nice interpretation is the percentage in terms of the reduction

246
00:29:07,050 --> 00:29:13,950
of the error somewhat square because it's the reduction divided by the total error.

247
00:29:14,820 --> 00:29:19,760
So if that is the percent percent reduction in error, someone square.

248
00:29:19,980 --> 00:29:27,390
But of course this is the growth rate and adjusted because both are actually divided by car slugging your freedom.

249
00:29:27,870 --> 00:29:40,290
So F star has this interpretation. However, again, however we want to emphasize that F star because it calculates the Selahattin the CMA,

250
00:29:40,770 --> 00:29:48,390
the CMA square using this reduced model, using this reduced model does not include no additional covariates.

251
00:29:49,320 --> 00:29:54,300
So we have the risk that there is a risk that this model is incorrect.

252
00:29:55,470 --> 00:30:01,320
Is incorrect because the true model might indeed fall by the true what, but indeed depends on x for x five.

253
00:30:01,320 --> 00:30:10,680
We have in the test that yet, but at this stage we have no way of saying that a y cannot depend on adds to our x five and so so now.

254
00:30:11,400 --> 00:30:15,330
So there is a risk that this similar model is increasing as specified.

255
00:30:15,840 --> 00:30:27,420
Now if this is incorrect, then this calculation here gives us an incorrect estimate of Sigma Square and it seems we're choosing Square.

256
00:30:27,900 --> 00:30:34,139
So that makes this f star while in that case F starting to longer follow and after disputed.

257
00:30:34,140 --> 00:30:41,370
So then the subsequently you know when we calculate p value draw confusion for it has to

258
00:30:41,370 --> 00:30:47,160
miss not have been actually even zero then that a conclusion would be might be incorrect.

259
00:30:47,850 --> 00:30:56,520
So that's the risk we have. Well we use F star so it's safer to actually to calculate this s that has to

260
00:30:56,520 --> 00:31:01,140
calculate the denominator using this larger model because the larger model,

261
00:31:03,750 --> 00:31:09,959
even if expert x five are not needed while then including not just adding noise.

262
00:31:09,960 --> 00:31:18,840
As for noise to the model, it doesn't. It doesn't, but it still gives you a valid estimate of seriously.

263
00:31:19,680 --> 00:31:24,040
So let's say. Yeah.

264
00:31:24,140 --> 00:31:27,290
That's what this list is about.

265
00:31:27,980 --> 00:31:32,990
It's the questionnaire. So we're doing this because we said these are modeling complex processes.

266
00:31:33,800 --> 00:31:38,150
Why are we ever okay using the estimate of sigma squared?

267
00:31:38,710 --> 00:31:43,540
It doesn't quite make sense why it's different using the smaller model versus the bigger model, even though.

268
00:31:46,990 --> 00:31:50,470
I think that that's just like the assumption.

269
00:31:51,010 --> 00:32:00,820
Why are we trusting? Very trusting that that model that we specified the largest model is a gives a good estimate for the Sigma have house.

270
00:32:01,030 --> 00:32:04,090
Oh, okay. So you're saying that.

271
00:32:04,420 --> 00:32:07,329
So your question let me just briefly, if I may, for our standard.

272
00:32:07,330 --> 00:32:12,010
So your question is like, why are we trusting that a seamless square to calculate based on those is the truth?

273
00:32:12,760 --> 00:32:16,330
Yeah, it makes sense from the assumptions, but also like why?

274
00:32:16,660 --> 00:32:20,799
Why doesn't the same logic that says that the small model might not work?

275
00:32:20,800 --> 00:32:27,370
Well, that's may or might not be an unbiased estimate. Why doesn't that also extend to the larger model of.

276
00:32:28,600 --> 00:32:32,319
Yeah, that's right. Okay. So I that that's a really good question.

277
00:32:32,320 --> 00:32:37,020
So. Yeah, of course.

278
00:32:37,020 --> 00:32:43,599
I mean, well, we saw that the smaller model when we use the smaller model, we have the risk of having, you know, this model misbehaving.

279
00:32:43,600 --> 00:32:47,010
And so so that calculation, obviously my square here is not valid, right?

280
00:32:47,490 --> 00:32:54,719
So for the larger model, we also have this risk. There's no way of saying that this is the correct model.

281
00:32:54,720 --> 00:32:58,170
And then the estimates were calculated based on those is is cracked.

282
00:32:58,980 --> 00:33:01,710
But it's just that, you know, if we compare this tweet.

283
00:33:02,100 --> 00:33:09,630
So, I mean, this one is the model that has all the covariates we have or this is the model actually, we are trying to build it.

284
00:33:09,810 --> 00:33:13,620
We are trying to pass. Whether I mean, indeed, we need a such large model.

285
00:33:17,310 --> 00:33:25,860
So given that I mean, look here, here, this this absolutist or aster or the subsequent of how old is this we're doing?

286
00:33:26,260 --> 00:33:32,459
That's given that we are interested in this lark, in this larger model, that's the model we're focusing on.

287
00:33:32,460 --> 00:33:39,480
We're looking at. We are not saying that, you know, this is our total.

288
00:33:39,500 --> 00:33:43,940
It is the truth. So if it is not, say if it is not.

289
00:33:45,620 --> 00:33:49,770
And from a practical point of view, it is not resolved because I mean,

290
00:33:49,790 --> 00:33:55,279
we do not believe such a simple linear model describing the true and the true biological chasm.

291
00:33:55,280 --> 00:34:02,990
Right. It is not. But however, here we need to keep in mind that everything we do is, that is, does an approximation to the truth.

292
00:34:03,350 --> 00:34:10,420
So let's say that, you know, there's nothing that this is not true of.

293
00:34:10,820 --> 00:34:13,960
This is this is not the truth. So this is not true model.

294
00:34:14,300 --> 00:34:18,950
Then this, you see, must have is, of course, is not a crack estimate of Sigma Square.

295
00:34:19,880 --> 00:34:25,820
And in that case this f a well it also will not follow an evidence fusion.

296
00:34:26,060 --> 00:34:38,720
That's that's just just the same as you know when this model is in practice this by this F star will not follow this and this fusion by.

297
00:34:41,890 --> 00:34:48,219
But the look, when we do that, like when we do this whole thing,

298
00:34:48,220 --> 00:34:54,930
we sort of have to be under the assumption that, you know, this larger model is cracked.

299
00:34:55,240 --> 00:34:59,950
It's just like a movie. So if we forget about this, have almost nothing for a minute.

300
00:35:00,250 --> 00:35:09,520
Let's just say that if you read a paper or if you're writing a real paper or you doing a product or you have data.

301
00:35:10,610 --> 00:35:18,379
And now let's say this by a linear regression model and you build a few regression model to the data and you got as a means of data,

302
00:35:18,380 --> 00:35:23,120
and then you interpret the estimate and then you write a report.

303
00:35:23,930 --> 00:35:30,650
So all the things you do, they are based on the assumption that the model fit is a crack model.

304
00:35:31,490 --> 00:35:35,410
Without that assumption, all the subsequent things we do, they are meaningless.

305
00:35:35,750 --> 00:35:43,530
And so we have to assume that, okay, so this model, Alice, is a reasonable model and a mathematically model interface that is cracked.

306
00:35:44,020 --> 00:35:51,020
It has a crack model. And then based on the crack model, we fit the model to the data and then we do all the following interpretation.

307
00:35:51,650 --> 00:35:58,400
So that's an assumption that we have to make in order to to proceed is similar here.

308
00:35:58,940 --> 00:36:02,930
So this is the model we are looking at the murder model.

309
00:36:03,590 --> 00:36:09,910
So we want to look at this model and to see whether we can reduce it by paths by doing, let's say, or two, it has to.

310
00:36:10,850 --> 00:36:19,009
M So we sort of assume that from the beginning that the model is correct because otherwise I mean, from the very beginning, if you think,

311
00:36:19,010 --> 00:36:26,870
oh, this model is, is absolute crack, so it's not a truth at all, oh, that would be we probably wouldn't look at this model anymore.

312
00:36:27,350 --> 00:36:33,800
So we would start with this model. So we start with this model and we focus on this model, this larger model.

313
00:36:34,160 --> 00:36:38,050
This is sort of under the assumption we have her to mate. This is a crack about.

314
00:36:38,510 --> 00:36:41,629
And then we're trying to see through a sequential casting,

315
00:36:41,630 --> 00:36:48,440
we're trying to see whether indeed each theta in this model is necessary, squared is necessary.

316
00:36:50,930 --> 00:36:58,280
Yeah. So I think this is true for not only for maybe intervention but also for not only for regression, but also for,

317
00:36:59,330 --> 00:37:04,670
for other ways on other models is when you do the analysis will specify a model,

318
00:37:04,700 --> 00:37:12,140
although that we know that it cannot be the truth because it's whatever model we have is an approximation.

319
00:37:13,220 --> 00:37:18,710
But when we proceed, we try to apply, you know, the theory of statistics.

320
00:37:19,220 --> 00:37:22,520
We have to mathematically we have to make the assumption that this is the true model

321
00:37:23,360 --> 00:37:28,340
in order to carry out all the hypothesizing Carol and all the other interpretations.

322
00:37:30,500 --> 00:37:38,350
Hopefully that answer your question. Yeah. Okay. Okay.

323
00:37:38,350 --> 00:37:52,110
Any other questions? Okay.

324
00:37:52,110 --> 00:37:54,720
So now let's take a look at one example.

325
00:37:57,980 --> 00:38:12,170
So here we have a study that is concerning to examine the the role of age and of course wage as predictors of inferred systolic blood pressure SBP,

326
00:38:12,170 --> 00:38:22,940
BP. So this is let's say this age is ages X one and so and the birth weight and this is x two and the SBP, this is x three.

327
00:38:23,980 --> 00:38:26,270
Sorry, this is why here this is the response y.

328
00:38:26,960 --> 00:38:38,300
And we want to build a linear regression model to study how Y is associated with both x1x2 and a first visit.

329
00:38:41,530 --> 00:38:45,250
So first, this is the result of.

330
00:38:45,790 --> 00:38:58,900
Well, this is to ask me the crude. The crude oil we meet our does it the crude effect of birth weight birth weight of SBP.

331
00:39:02,790 --> 00:39:07,780
And we want to see whether there is any significant. So this is a very simple model result.

332
00:39:07,800 --> 00:39:13,100
We are not looking to beat a lot of why you go to work,

333
00:39:13,110 --> 00:39:23,700
you make an SBP blood pressure equal to beta zero plus beta one times B double team birth weight loss capsule.

334
00:39:23,700 --> 00:39:30,209
This is the model we are looking at. And of that we fitted this model, we see that.

335
00:39:30,210 --> 00:39:36,300
Okay, so we we have I mean, all these outputs in this table here we have in previous examples,

336
00:39:36,300 --> 00:39:42,810
we have shown how to interpret each single number in this like all the numbers in this in this output.

337
00:39:42,810 --> 00:39:48,410
So we're not going to go over that again. If you want to get a reference number,

338
00:39:48,420 --> 00:39:58,350
you can go to previous recording and you will find that we have discussed all the meaning of every single number in this table or in this output.

339
00:39:58,890 --> 00:40:04,230
But now if we look at the covariance this table here, we have the estimating fact.

340
00:40:04,230 --> 00:40:10,530
Well, this is estimated beta. What is what hat and this is the standard error of beta.

341
00:40:10,530 --> 00:40:16,740
What hat and this is the beta one had a divided by the standard error of beta one at.

342
00:40:17,820 --> 00:40:22,100
And this is the Peabody. This is the that.

343
00:40:23,080 --> 00:40:26,920
Right. And this is actually well, this P-value is for testing this.

344
00:40:26,920 --> 00:40:37,240
If we're testing the hypothesis that a beta one equals zero versus beat up one parent beta one, it's not even zero.

345
00:40:40,060 --> 00:40:50,740
Based on this report we see that here are the P-value is .08 and if we use .05 as the cutoff, this is larger than 0.05.

346
00:40:51,370 --> 00:40:54,310
And then the conclusion is that, okay, the, the,

347
00:40:54,460 --> 00:41:01,510
the body weight and the body of birth weight does not have a significant effect on systolic blood pressure.

348
00:41:01,780 --> 00:41:06,579
Right. So we failed to reject that. It means we we failed to reject this novel.

349
00:41:06,580 --> 00:41:14,980
And so we fail we fail to reject reject this novel as is really worse.

350
00:41:15,250 --> 00:41:19,750
The brother does not have does not have a significant impact on as we've seen.

351
00:41:19,750 --> 00:41:28,240
So that's the the marginal or the crude estimate for this is fairly easy.

352
00:41:29,950 --> 00:41:36,790
And then the next slide here, we we look at the effect of birth weight on SBP adjusted for age.

353
00:41:37,810 --> 00:41:40,330
Now this is a model that we want to address for age.

354
00:41:40,330 --> 00:41:57,310
So this is model where we look at beta zero plus beta one being t w gamma plus beta two h or let me just to make this even more,

355
00:41:58,630 --> 00:42:06,820
this is beta b, I do not appear to be in this beta eight has been on beta for age and plus absolute.

356
00:42:09,320 --> 00:42:19,190
Well, this is the model we are feeding that. And then if you look at this table here, like the birth, if you look at this rule.

357
00:42:19,430 --> 00:42:22,670
So, again, this is this is the estimate of beta.

358
00:42:24,040 --> 00:42:31,420
Be better be that and this is the standard error of these have been at the end of this.

359
00:42:31,840 --> 00:42:39,460
This number here this t none of this is beta B you have divided by standard ever beta B patch.

360
00:42:40,420 --> 00:42:51,910
And then this is the p value. P value for testing for it has to be equal to zero versus the p not equals zero.

361
00:42:56,660 --> 00:43:05,600
Okay. And then we see that if we look at this p value, what is point too old to lie again?

362
00:43:05,600 --> 00:43:09,050
If you compare this 2.05, it's much smaller and 1 to 5.

363
00:43:09,920 --> 00:43:15,920
So this means that, well, we show the regional obviousness and so we know that a lot.

364
00:43:15,920 --> 00:43:30,090
And this is. This means that indeed there is a significant, significant effect of birth weight of SBP adjusting for age now.

365
00:43:30,100 --> 00:43:33,890
Now we are adjusting for age. Right. We need to be very careful.

366
00:43:33,900 --> 00:43:35,760
This is after adjusting for age.

367
00:43:37,410 --> 00:43:47,610
So now we see that we have contrary condition, a conclusion without adjusting for age, mostly since seems that birth weight has no significant effect.

368
00:43:48,000 --> 00:43:52,480
But after adjusting for age, we know there is a significant effect based on looking.

369
00:43:53,160 --> 00:43:56,490
It seems now that we have control of our degree.

370
00:43:56,940 --> 00:44:02,760
Conclusion. So this is really we do have an explanation for this.

371
00:44:05,070 --> 00:44:09,030
So here but first let's look at the estimating effect.

372
00:44:09,900 --> 00:44:21,960
So without adjusting for age, without adjusting for age, this is the estimated effect of 4157.

373
00:44:23,340 --> 00:44:30,260
And after adjusting for age. This is estimated effect 2.1 to 5.

374
00:44:31,910 --> 00:44:39,020
We see that these two numbers, they are not too far away. One is 415711125.

375
00:44:39,470 --> 00:44:42,740
Not far, Rick. So a natural question to ask.

376
00:44:42,950 --> 00:44:50,870
The first question to ask is whether this numbers are indeed whether these two effects are indeed different in this two models.

377
00:44:52,250 --> 00:44:58,910
Of course, numerically, they are different. Five, they started a new arrival is not a same number as point one, two, five.

378
00:44:59,750 --> 00:45:03,800
But when we say whether they are different, what we're asking the question whether they are different.

379
00:45:04,100 --> 00:45:09,640
What we mean is whether they are as different a statistic, whether there is a significant difference,

380
00:45:09,660 --> 00:45:17,060
no significant difference between these two numbers, because remember that the data we have there is always noise.

381
00:45:18,170 --> 00:45:22,310
And what we see when we when we try to say two things or two quality, they are different.

382
00:45:23,140 --> 00:45:32,030
Always have to take into account that there is noise in the data. And we cannot say, oh, these two are absolutely different, were absolutely the same.

383
00:45:32,370 --> 00:45:35,780
There's no way that we can make such a conclusion because of noise in the data,

384
00:45:36,110 --> 00:45:39,650
because if you leave on data set, you might have a different conclusion.

385
00:45:40,640 --> 00:45:48,110
So the only thing we can say is that, okay, so they are they are statistically significantly different.

386
00:45:48,110 --> 00:45:56,870
Rosalind, is it is well, is it the difference is that whether the difference is statistically significant or not.

387
00:45:57,380 --> 00:46:04,400
So so here, let's take a look at whether a statistically whether this to estimate as many facts, whether they are different.

388
00:46:05,330 --> 00:46:13,760
The way to look at whether they are different is to construct and confidence in a for example, based on this one,

389
00:46:14,000 --> 00:46:19,760
we can construct a confidence interval because we have we have as median value and also we have standard error.

390
00:46:20,690 --> 00:46:29,090
We can construct a confidence interval and then see whether or not a confidence interval includes this estimate of the fact.

391
00:46:30,340 --> 00:46:37,360
If it does include this estimated effect that it is small, these two estimating facts they are not significantly different.

392
00:46:38,530 --> 00:46:47,590
But if this one falls inside the accommodation interval based on the other effect that we see, this two effects, they are significantly different.

393
00:46:48,550 --> 00:46:52,750
That's how we how we look at it, whether they are significant.

394
00:46:52,750 --> 00:47:01,750
Different. Okay. So let's let's finish this calculation and then and then take a break.

395
00:47:01,820 --> 00:47:09,190
Okay. So let's say we compare the simple and might have amino radical revision estimates for birth weight.

396
00:47:09,910 --> 00:47:14,070
And we ask the question, are they really different other.

397
00:47:16,270 --> 00:47:29,170
The way to look at whether they are different is to look at 95% of COVID colonies in our world for,

398
00:47:29,500 --> 00:47:37,270
let's say, for, you know, for we feeding the housing power.

399
00:47:43,060 --> 00:47:53,290
But look, 95% confidence interval here in our model, the estimated value is .1255.

400
00:47:54,070 --> 00:47:57,100
So that's point one, two, five, five.

401
00:47:58,360 --> 00:48:02,080
95% confidence interval as a plus minus.

402
00:48:04,310 --> 00:48:15,320
Oh. Plus minus. Okay, so here we have a key.

403
00:48:17,120 --> 00:48:22,999
We have. If you were freed on an important point, nine, five, seven, you can have the times.

404
00:48:23,000 --> 00:48:26,860
The center in our center area of the.

405
00:48:29,930 --> 00:48:43,880
All the 95% confidence interval is greater be at plus minus t the gore freedom quite a975 times

406
00:48:44,330 --> 00:48:52,700
a standard error of beta v at this is equal to the estimated B to B that's that's 4 to 1,

407
00:48:52,700 --> 00:48:55,730
two, five, five plus minus the center error.

408
00:48:56,570 --> 00:49:04,130
That's that's that's this number two here 8.034348.

409
00:49:04,490 --> 00:49:10,210
So that's one three, four, three, four, right?

410
00:49:12,050 --> 00:49:15,500
So that's that's the 95% confidence interval.

411
00:49:17,110 --> 00:49:22,719
So. So then the T here in the D are freedom.

412
00:49:22,720 --> 00:49:28,870
We can finally the from the table. That's how many that's actually on.

413
00:49:29,290 --> 00:49:39,070
Yeah. Minus T right. That's in these sentences I think is for 15 I so I'm saying oh we don't say but we can figure it out.

414
00:49:39,190 --> 00:49:45,100
Right. So that's actually, that's actually this three.

415
00:49:45,670 --> 00:49:54,200
Yeah. The 13. I'm sorry. Yeah.

416
00:49:54,220 --> 00:50:00,100
So it's tedious. Go to a theme to our freedom. So that's amnesty.

417
00:50:00,160 --> 00:50:05,950
That's 13. So this this is a t 13 that you are freed up for the nine, seven, five.

418
00:50:06,460 --> 00:50:11,200
So you can of course, you can look back and look at this number from latitude to table.

419
00:50:11,200 --> 00:50:17,620
We're using our vocabulary this but let's not do that let's just use approximation that's that's again we call that a whether your

420
00:50:17,620 --> 00:50:25,779
freedom is large and TS just approximate can be presented by normal this fusion although this do or freedom may not be large enough.

421
00:50:25,780 --> 00:50:31,540
But let's forget about that for a minute. Let's just use the 1.6 we're used to as a approximation.

422
00:50:31,960 --> 00:50:40,260
So here we just want to illustrate that idea. Let's not worry too much about the exact value of this on.

423
00:50:41,420 --> 00:50:47,070
Of this team. Let's see, we use to to approximate this person.

424
00:50:49,140 --> 00:50:55,080
Well, there are two accounts measures we used here. One is that we use normal to approximate this transfusion.

425
00:50:55,470 --> 00:51:00,450
And then for normal is 1.6. But. But we just use tool to replace that.

426
00:51:01,020 --> 00:51:06,420
Okay. So let's say we use two here and then this is 1.2, five, five plus minus.

427
00:51:06,840 --> 00:51:11,400
This is .068.

428
00:51:11,530 --> 00:51:23,910
That's right. And then if we calculate this, well, let's make a even further approximation, let's say plus or seven.

429
00:51:25,740 --> 00:51:30,110
But please don't do this at your exam. Okay. Yeah, you know.

430
00:51:30,150 --> 00:51:33,820
Exactly. To do that, as you would imagine, again, it was taken off here.

431
00:51:34,520 --> 00:51:39,240
And I am doing a very sloppy job here.

432
00:51:39,480 --> 00:51:43,710
So I'm just trying to show that. So the idea is to simplify the calculation.

433
00:51:43,950 --> 00:51:54,720
So that's a plus -27. So that's actually 0.05.

434
00:51:56,690 --> 00:52:02,240
Five. Five. Right. To. 1955.

435
00:52:02,630 --> 00:52:13,460
Okay. So this is 95% confidence interval for for you to be in the adjusted model, in the rescue model.

436
00:52:13,670 --> 00:52:27,380
Now, if you look at Discovery's confidence interval and then we look at the beta in the unadjusted model in the island testing model, that's .157.

437
00:52:28,550 --> 00:52:41,600
All right. So since the 12157, 6.157 as the false return is on this interval.

438
00:52:44,220 --> 00:52:52,480
Force is understandable. So that better gun than the other conclusion is that I'm.

439
00:52:54,810 --> 00:52:58,140
Then the conclusion is that so the two events.

440
00:52:58,560 --> 00:53:08,610
The two events. Or not significantly different.

441
00:53:18,630 --> 00:53:23,680
Are not significant different. So that's how we look at it, whether there is a difference.

442
00:53:23,880 --> 00:53:31,020
So of course we can do the other way around and here we can strike 95% confidence interval for faith to be in the aggressive mode.

443
00:53:31,620 --> 00:53:41,760
You can actually look out at the 95% confidence interval for for the beta in the US model and let us see whether the other beta falls inside.

444
00:53:42,820 --> 00:53:47,400
You should you should reach that evolutionary region of the same conclusion.

445
00:53:47,990 --> 00:53:56,150
There are maybe in some extreme, extremely rare cases where there may be this to two ways they give you different.

446
00:53:56,520 --> 00:54:02,639
But generally speaking, this should give you the same conclusion. But anyway, this is how we tell whether this true facts.

447
00:54:02,640 --> 00:54:08,580
They are significantly different. So any question about this?

448
00:54:13,950 --> 00:54:21,030
Okay. If not, then let's take a 6 minutes break and then we will resume the actual final five.

449
00:54:26,690 --> 00:54:27,180
You said he was going to come forward.

450
00:54:29,762 --> 00:54:35,332
This is an example that I really like to go into the details because, you know,

451
00:54:35,582 --> 00:54:44,722
it shows how to interpret the articles later the season and later in the week that we're going to happen.

452
00:54:45,752 --> 00:54:48,552
And also what you're seeing here is evidence of stocks. Yes,

453
00:54:49,062 --> 00:54:56,122
I think we really need to slow down here and pay attention because this is this is quite important

454
00:54:56,142 --> 00:55:03,062
example to make sure that we all have a good understanding of how to interpret the results.

455
00:55:03,452 --> 00:55:11,972
Okay. So all we have we just have seen that, you know, there's two effects advancing age and not adjusting for age.

456
00:55:11,972 --> 00:55:16,351
We see that there's two effects. We are not seeing that little difference. And that's our conclusion.

457
00:55:16,352 --> 00:55:24,452
By constructing confidence, you know, we see that they're not evened or there are not a significant difference of this is well, this.

458
00:55:26,182 --> 00:55:34,492
No significant difference can also be interpreted and explained by looking at the the correlation between the two covariates,

459
00:55:35,002 --> 00:55:42,892
the birth weight and age. If we calculated their correlation, I mean, if you are you can always page views our account of the correlation.

460
00:55:43,882 --> 00:55:48,022
The correlation. We test whether the correlation is equal to zero or not.

461
00:55:48,052 --> 00:55:53,302
So here we are. Where has a rather just correlation.

462
00:55:53,302 --> 00:56:00,232
We use use and we use the rule of, you know, the correlation. So the current work has to whether this correlation is equal to zero or not.

463
00:56:06,362 --> 00:56:08,792
And while, of course, so far in this course,

464
00:56:08,792 --> 00:56:14,762
we haven't really talked about the formal cast forecasting this coronation, but that's not about the main point.

465
00:56:15,002 --> 00:56:23,312
Let's forget about let's not worry too much about that. But let's say that a theory we use are we carried out a sort of test and the p value is .69.

466
00:56:23,792 --> 00:56:28,442
This is very large if you have it. This means that we will not have the liabilities.

467
00:56:29,042 --> 00:56:35,822
This means that the two variables their correlation is low is not a significant difference from zero.

468
00:56:36,602 --> 00:56:41,942
So this then means the correlation, even if there exists a correlation coefficient is going to be very small.

469
00:56:41,942 --> 00:56:48,632
Is very small. So indeed, if we look at the symbol, correlation is .106.

470
00:56:49,052 --> 00:57:02,852
This is not a large correlation. So this correlation, this very small correlation between birth, weight and age explained why the two effects,

471
00:57:03,042 --> 00:57:10,142
like the effects of birth weight without asking for age and with a drastic adjustment for age.

472
00:57:10,892 --> 00:57:17,281
Like why? There's two effects. They are quite a similar. The reason again, while this is easier to interpret,

473
00:57:17,282 --> 00:57:24,482
if you again think of this linear figure of linear regression as a projection in the future, try to understand this geometrically.

474
00:57:25,952 --> 00:57:29,462
So here now let's say you have two x, let's forgot.

475
00:57:29,542 --> 00:57:36,482
Forget about the intercept for a minute. You have two x, you have age and and a birth weight or birth weight and age.

476
00:57:37,022 --> 00:57:40,432
You have two x. Just imagine this age. Imagine this.

477
00:57:40,442 --> 00:57:43,772
This 2x2 be the two x axis.

478
00:57:43,862 --> 00:57:47,312
That is the span before this plane.

479
00:57:47,612 --> 00:57:55,792
And the fourth and the correlation is very small means that this tool, they are orthogonal to each other like in 3D.

480
00:57:55,862 --> 00:57:59,852
But to fit into imagine interpretation is more like there are a thousand for each other.

481
00:58:01,232 --> 00:58:08,191
So let's say they are indeed orthogonal. Let's just this is this is X or this is this is birth weight.

482
00:58:08,192 --> 00:58:18,272
And this is, you know, this is age. And I said, and then you have this y and effect on the beta.

483
00:58:18,542 --> 00:58:22,382
You assume you try to estimate the beta for BMI.

484
00:58:22,412 --> 00:58:29,042
That's the production of this vector Y on this be a B birth weight on this axis.

485
00:58:29,702 --> 00:58:34,112
Right. And if this is orthogonal to, you know, to to edge,

486
00:58:34,592 --> 00:58:42,902
then just imagine that whether you are predicting the Y ultra BMI, whether you have this age or not,

487
00:58:43,112 --> 00:58:51,032
it won't affect the production of BMI because and because these two are both them versus if they are not orthogonal,

488
00:58:51,032 --> 00:58:54,962
less MSA less, let's say this is this is BMI, but they are not orthogonal.

489
00:58:54,962 --> 00:58:58,021
But this is age. There is a there's animals is not okay.

490
00:58:58,022 --> 00:59:01,412
So my belief is they're not alone.

491
00:59:01,982 --> 00:59:07,861
So because they are not orthogonal, then then if you look at let's say this is one year per day,

492
00:59:07,862 --> 00:59:13,862
this Y until until BMI alone, in a way you have an age.

493
00:59:14,372 --> 00:59:16,952
And again for united of a BMI,

494
00:59:17,432 --> 00:59:30,022
the potential BMI is going to change because because these two they are not not orthogonal because there is a because the age I mean,

495
00:59:30,092 --> 00:59:38,282
has because they are not orthogonal. So the age part of age, in fact, is going to coincide with that with a BMI.

496
00:59:39,362 --> 00:59:44,612
That's that's that's that's a geometrical interpretation.

497
00:59:44,942 --> 00:59:52,772
Like why this small correlation between the BMI and sorry, now BMI,

498
00:59:53,162 --> 01:00:01,652
the birth weight and age Y with small condition leads to this to very similar, in fact, as made in the end.

499
01:00:02,282 --> 01:00:11,222
And because a small correlation means that while these two are are almost orthogonal to each other if they are orthogonal

500
01:00:11,532 --> 01:00:20,972
now whether you're predicting the Y on birth weight alone will never feel black and white on both age and and birth weight.

501
01:00:21,482 --> 01:00:29,102
And the production on birth weight is not going to be affected by whether you are predicting at all age or not.

502
01:00:30,102 --> 01:00:34,052
Okay. So this is this is a geometric interpretation.

503
01:00:35,012 --> 01:00:43,132
This is actually quite general. So if you have this is generally so if you have a bunch of covariance, not all,

504
01:00:43,152 --> 01:00:50,702
not matter at all, but if you have multiple covariates and if one covariate,

505
01:00:50,822 --> 01:00:57,662
let's say, has very small correlation to the others, very small as a has no correlation with the others.

506
01:00:58,022 --> 01:01:02,012
So so that mathematically it is orthogonal to all of the other covariates.

507
01:01:02,882 --> 01:01:08,632
And then if you look at if you feed the model. That has no other coverage, just this coverage alone.

508
01:01:08,812 --> 01:01:15,682
If you look at the effect and also not a model that has this coverage and all plus all the other coherence,

509
01:01:16,102 --> 01:01:22,102
if you look at those two effects, they should be very, very close and they should be the same.

510
01:01:22,602 --> 01:01:28,402
But about with the numerical error, with the noise than the numerical, you are going to see very close.

511
01:01:28,552 --> 01:01:37,522
You've got an estimate. So that explains why, you know, the birth weight in these two models with and without addressing for age,

512
01:01:37,762 --> 01:01:39,982
we see that of those two factors that are quite a close.

513
01:01:43,712 --> 01:01:54,662
However, now, another question we need to what you might ask is, well, says the two events are quite close,

514
01:01:54,672 --> 01:01:59,442
but why do we see, you know, quite a different conclusion in the end?

515
01:01:59,462 --> 01:02:10,742
Because after adjusting for age, the P-value is very small, but without adjusting for age, the P value is larger than point of weight.

516
01:02:11,042 --> 01:02:15,632
So in the end we had to draw different conclusions. So why?

517
01:02:15,632 --> 01:02:19,382
Why do we have this? Why do we have this?

518
01:02:19,982 --> 01:02:23,432
This is actually resulted from the fact that.

519
01:02:28,042 --> 01:02:34,832
Without adjusting for. Just what's happening.

520
01:02:35,722 --> 01:02:41,572
Yeah. So without adjusting for age, if you look out here, if you look at this number,

521
01:02:41,932 --> 01:02:50,152
this is the the residual center error or this is actually do not mean square out of the ABC.

522
01:02:51,262 --> 01:02:56,752
Oh, sorry. This is the square root of MSE. So this is asking the square root of that.

523
01:02:58,072 --> 01:03:03,982
So if you look at the square root of MSE without adjusting for H is 6.2.

524
01:03:05,422 --> 01:03:08,692
6.2. But if you look at the.

525
01:03:13,132 --> 01:03:19,372
The German embassy after adjusting for age that to become significantly smaller thanks to the socialist model.

526
01:03:19,612 --> 01:03:23,392
As part of our coupon four. So we are the worst.

527
01:03:26,822 --> 01:03:45,062
So the embassy in the model adjusting for age is equal to put together one point forward seven.

528
01:03:47,882 --> 01:03:51,842
Two one, two, four, seven, nine square.

529
01:03:53,272 --> 01:04:03,332
And I will see you model without. Without adjusting for age that is equal to.

530
01:04:07,172 --> 01:04:18,832
That's 6.2. Okay.

531
01:04:25,762 --> 01:04:30,082
So let's say this is I don't know the exact value.

532
01:04:30,562 --> 01:04:41,781
It's probably closer to point to 2.3 years old, roughly, maybe this number and this is probably 36, maybe 37.

533
01:04:41,782 --> 01:04:54,682
Right. Let's say this number, we can see that, you know, I see our quality is quite different for this one is the reason we see such a difference.

534
01:04:55,262 --> 01:05:02,902
And I say is that because actually. The reason for this difference is age.

535
01:05:03,082 --> 01:05:06,322
Is that a very good predictor in this case, age?

536
01:05:06,322 --> 01:05:09,532
It's a very good predictor of SBP.

537
01:05:11,552 --> 01:05:15,842
Because age is a very good predictor, not adjusting for age.

538
01:05:15,872 --> 01:05:23,342
If you put an age in the model. It will explain lots of variation in why.

539
01:05:24,662 --> 01:05:34,202
That's why in the end you'll see a much smaller and I see because it explains age is able to explain a large proportion of the variation in what.

540
01:05:35,582 --> 01:05:41,502
So so in Cuba, including the U.S., using this very substantial drop.

541
01:05:41,532 --> 01:05:44,912
And as you know, because.

542
01:05:46,912 --> 01:05:52,552
Because age is not for because age.

543
01:06:14,792 --> 01:06:20,011
Because it explains all our corroboration. Why not?

544
01:06:20,012 --> 01:06:23,972
If you recall how we calculate the center area of data.

545
01:06:25,112 --> 01:07:07,522
The center error of data. If you recall the standard arrow beta, that's the corresponding diagonal element.

546
01:07:07,612 --> 01:07:13,402
I mean, here, this description a little bit vague because it depends on which which beta you're looking at.

547
01:07:13,912 --> 01:07:17,302
And so, I mean, this is a matrix.

548
01:07:17,302 --> 01:07:22,402
So you need to look at the elements. These are the the the viruses of.

549
01:07:26,232 --> 01:07:27,792
The Squirrel Whisperer and the squirrel.

550
01:07:27,802 --> 01:07:35,882
This is the corresponding diagonal elements of this matrix, recall of the various covariance matrix of beta have.

551
01:07:35,892 --> 01:07:43,992
That is, as we see my squared times the inverse of x transpose x and so we have derived that in previous matrix.

552
01:07:46,392 --> 01:07:55,142
So now after adjusting for age, you have a much smaller see how to square and I see you have much smaller mass.

553
01:07:56,142 --> 01:07:59,202
That means you have a much smaller center error for data.

554
01:08:01,392 --> 01:08:07,122
And indeed, if you look hard enough to center error for. Now.

555
01:08:08,372 --> 01:08:14,642
The sin era. After adjusting for age, there's porn, all three pornography.

556
01:08:14,642 --> 01:08:18,662
And without adjusting for age, that's porn to all eight.

557
01:08:20,192 --> 01:08:26,012
So it's reduced by more than half those in Iran are going to reduce by more than half.

558
01:08:26,612 --> 01:08:34,562
The reason, again, is because age is a very good predictor of SBP, so it's clear loss of generation.

559
01:08:35,192 --> 01:08:45,112
So in the end, if you include age in the model, then your MSI become much smaller or any other words this semester have become much smaller.

560
01:08:45,722 --> 01:08:52,562
So in the end of all you have much smaller screen or center error has made for better and because of this reduction is

561
01:08:52,562 --> 01:09:03,722
center error now for the p value but because center error become much smaller so so when you calculate that the p value,

562
01:09:04,142 --> 01:09:09,812
the key is based on dividing by center error by ts beta divided by center error.

563
01:09:10,322 --> 01:09:16,352
Now the beta does not change too much. It shouldn't is towards a center and we've got to reduce by a lot.

564
01:09:17,012 --> 01:09:22,172
Of course this t value got increased by a lot after electrons break.

565
01:09:23,592 --> 01:09:26,862
Might have got increased by a lot of that means it becomes more extreme.

566
01:09:26,892 --> 01:09:31,602
Right. Either works for the right or work for the left because they have more extreme on.

567
01:09:32,142 --> 01:09:34,812
So that way to counter the pivot in the pivot to become much smaller.

568
01:09:35,652 --> 01:09:43,702
So that's why here we have we have we see this much smaller P-value compared to the first few days without the French.

569
01:09:45,332 --> 01:09:51,482
This explains why the attraction for age.

570
01:09:51,692 --> 01:10:00,962
Suddenly we see that we see a much smaller p value and leading to the conclusion that there is significant effect of birth weight on SBP.

571
01:10:01,262 --> 01:10:04,262
But without adjusting for age, there is no significant effect.

572
01:10:05,342 --> 01:10:09,512
So there is no combination. So everything has an explanation behind it.

573
01:10:10,252 --> 01:10:18,572
I was just trying to figure out explanation to this seemingly contradictory observation.

574
01:10:22,612 --> 01:10:33,952
Any questions about these? Okay.

575
01:10:34,582 --> 01:10:39,442
So that's that. That's the move so far.

576
01:10:39,442 --> 01:10:44,632
We explained why that. So in fact they are quite similar in a way.

577
01:10:45,142 --> 01:10:51,202
But despite a very similar effects, we see the p values to be quite different and the conclusions are different.

578
01:10:51,382 --> 01:10:58,822
So so far we have explained this two things and then let's take a look at the sum of squares.

579
01:11:00,952 --> 01:11:07,492
So first we look at the sequential sum of square. This is in incense.

580
01:11:10,322 --> 01:11:13,652
It's called the Thai one of someone's quick.

581
01:11:15,112 --> 01:11:19,312
And this is this is not our sequential, somewhat scripted.

582
01:11:22,212 --> 01:11:33,072
So if we fit a model of, let's say in India, are we fit Lenovo's model and we use another function to gauge the ANOVA table, this is a table.

583
01:11:34,872 --> 01:11:41,852
Right in this. Well, maybe first, let's look at another table with only birth weight in it.

584
01:11:42,152 --> 01:11:45,602
There's no age. Look at the side of the table. There's only birth weight.

585
01:11:47,102 --> 01:11:51,482
Only birth weight. And then in this case, we have the freedom.

586
01:11:51,482 --> 01:11:55,202
This corresponds to birth weight, of course, with much more freedom.

587
01:11:55,532 --> 01:12:01,012
Then we have 14 you freedom for the for the error sample square.

588
01:12:04,652 --> 01:12:12,602
And then if we look at the Soho square here, this sum of square, this is SS.

589
01:12:15,552 --> 01:12:21,022
Better be. Even Beta zero.

590
01:12:22,852 --> 01:12:31,462
The other word, this is the squares with some square by adding pathways into our model that has only intercept but has nothing else.

591
01:12:31,822 --> 01:12:39,782
So this is what this small square needs. And of course, this this one is the error some elsewhere.

592
01:12:40,822 --> 01:12:46,442
I see. This is terrorism. This is quite a symbol of.

593
01:12:48,702 --> 01:12:56,262
And then this F value, this F value here this is actually ask you to read.

594
01:12:57,302 --> 01:13:01,922
Right. Divided by speaker freedom which is one then divided by SC.

595
01:13:04,172 --> 01:13:12,092
You make it a cleaner beta y and a better B, you get a zero and better be divided by as do our freedom.

596
01:13:12,092 --> 01:13:19,662
That's 14. That's how this is calculated.

597
01:13:22,992 --> 01:13:27,402
And then you have the corresponding p value and this one should follow at those vision.

598
01:13:27,402 --> 01:13:32,322
One in 14. You are afraid of other novelists.

599
01:13:38,252 --> 01:13:45,412
Okay. So this is the interpretation of the numbers in this table that has only been published.

600
01:13:45,422 --> 01:13:49,592
He has only two. Now, let's look at the above table.

601
01:13:51,032 --> 01:13:54,332
This people here, we have two covariates, birth, weight and age.

602
01:13:55,052 --> 01:14:04,802
And this is a sequential. Yeah.

603
01:14:04,942 --> 01:14:14,052
Sorry. Yeah, that's right. So. So the output from this ANOVA, I mean, this this output here, this is this is a sequential based on sequential test.

604
01:14:14,062 --> 01:14:19,522
So if we look at this number here, this number is the S.

605
01:14:19,532 --> 01:14:23,122
S beta be even zero.

606
01:14:24,742 --> 01:14:28,792
So when we actually birthweight into a model that has only intersect.

607
01:14:30,142 --> 01:14:38,362
And of this number here, this is SS theta h given theta zero and date of the.

608
01:14:39,112 --> 01:14:46,852
So this is not the extracellular squared by adding h into a motor that already has birth weight.

609
01:14:47,782 --> 01:14:53,212
And again, of course, intercept. This is what this model is.

610
01:14:53,812 --> 01:15:06,202
And then this is the SS as an average sample squared, but it has been a zero theta be in beta h.

611
01:15:09,902 --> 01:15:28,632
Okay. And then if you look at this, if this f better here, this is SARS better be given better zero divided by zero freedom.

612
01:15:28,672 --> 01:15:48,322
That's by one. And then divided by SC beta zero beta p eight divided by the more freedom in this case not 13.

613
01:15:48,472 --> 01:15:53,152
Do you want freedom? This is how this f value is calculated.

614
01:15:55,162 --> 01:15:58,672
And similarly, for age. This age. This number here.

615
01:15:59,602 --> 01:16:03,642
This is calculated by. As asked.

616
01:16:04,902 --> 01:16:10,331
Being a human being a zero to be divided by one day.

617
01:16:10,332 --> 01:16:16,192
We are freedom because we adding you are adding just pay the law to the law as has.

618
01:16:18,762 --> 01:16:21,792
Bitterly divided by the same.

619
01:16:27,162 --> 01:16:30,922
Change. Classified information.

620
01:16:33,822 --> 01:16:38,241
Think. And that's how this too.

621
01:16:38,242 --> 01:16:46,062
Our values are calculated. Divided by 1340.

622
01:16:48,122 --> 01:16:52,602
Because here we have to declare covariance included. This was 30.

623
01:16:57,332 --> 01:17:00,872
Yeah. Here we have for him because we only included we.

624
01:17:07,022 --> 01:17:10,052
So the total sum or size I think is and equal to 16.

625
01:17:11,132 --> 01:17:15,272
So here you have 13 because you have interceptor, you have birthweight, you have age.

626
01:17:15,572 --> 01:17:22,612
So in most fitness, that's equal 17. I guess so.

627
01:17:22,882 --> 01:17:30,172
So the function automatically or not function automatically does a sequential so.

628
01:17:30,682 --> 01:17:39,292
So that means like if we switch the order of the age and the birth weight in the input, it will increase the sequence.

629
01:17:39,592 --> 01:17:49,162
Oh yeah, definitely so. That's a good question. On the next line of work. Oh, I thought the other way was the greatest.

630
01:17:53,722 --> 01:17:57,982
Okay. And then. Well, this is how the F values are calculated.

631
01:17:58,762 --> 01:18:03,712
And then if you look at the P value, this p value over here, the first event,

632
01:18:04,552 --> 01:18:21,442
this is making a mess here on this value, this key value is actually testing whether beta p is equal to zero.

633
01:18:21,952 --> 01:18:34,312
Does is it a b is not equal to zero. So this is without adjusting for age because birth weight is vital at this stage.

634
01:18:34,732 --> 01:18:38,242
There we have it. And yet we are just looking at the birth weight.

635
01:18:38,572 --> 01:18:42,052
So this is testing whether Peter Beattie is equal to you or not.

636
01:18:43,272 --> 01:18:56,262
But this part of the second part out of here this is actually this is testing made of a is equal to zero versus beta a is not equal to zero.

637
01:18:56,832 --> 01:19:03,072
Now, this is actually adjusting for. Now here we need to keep in mind this is actually adjusting for.

638
01:19:07,372 --> 01:19:14,212
For being WG preservation because we have already included birth weight in the body.

639
01:19:24,332 --> 01:19:30,362
I'm sure you guys you guys can see this, though. So the other worse this is this is testing.

640
01:19:30,872 --> 01:19:40,652
This is actual testing. The better be in the model Y equal draw beta zero plus grade on field has B guaranteed plus abs.

641
01:19:41,132 --> 01:19:50,191
This is testing the better be this on the whereas this this whole thing this is

642
01:19:50,192 --> 01:19:58,152
for the moto y equal to beta zero plus beta and B the WTI plus beta eight times,

643
01:19:58,172 --> 01:20:07,922
age plus times. This is testing the beta eight in this model after adjusting for age because this is sequential.

644
01:20:16,372 --> 01:20:23,392
But then for the past it's still using the, the, the standard deviation for the whole.

645
01:20:24,082 --> 01:20:34,812
Yeah, right, exactly. This is what we meant by, you know, the AB are the the R and a SAS, they actually automatically use the F is that AB star.

646
01:20:34,822 --> 01:20:40,312
If you look at what are we are dividing here these are the the square for the

647
01:20:40,492 --> 01:20:45,622
lager model for for model that includes both being for both birth weight and.

648
01:20:48,412 --> 01:20:55,942
So when we look at this based on the hypotheses requesting, when do our statistics stop corresponding to our CTE statistics?

649
01:21:15,412 --> 01:21:25,222
In both of these two cases, the F corresponds to T, but here, I mean, we have to be very careful which tedious version.

650
01:21:25,882 --> 01:21:30,892
Well, in the end corresponds to.

651
01:21:31,582 --> 01:21:37,722
So here this f value, this AB statistic that's testing for this novel.

652
01:21:37,722 --> 01:21:46,432
This is for this model based on this model. So it corresponds to the T statistic for case of the same hypothesis.

653
01:21:47,522 --> 01:21:53,262
And look at this model. And this after the second half value.

654
01:21:53,772 --> 01:21:56,202
But it's also equivalent to a Houston mystique.

655
01:21:57,012 --> 01:22:05,982
But it is that it is equivalent to that statistic for testing this novel and this hypothesis in Brazil.

656
01:22:07,512 --> 01:22:15,132
So in other words, if you fit this model, if you look at the Houston history for a.

657
01:22:16,552 --> 01:22:22,422
And if you splurge, you will get the staff. Okay.

658
01:22:23,482 --> 01:22:31,102
So because here we are kind of both our stories that are testing just on a single covariate like this.

659
01:22:31,312 --> 01:22:35,512
The first one passed the beat up of the birth weight effect.

660
01:22:35,752 --> 01:22:41,192
The second one has just eight effect. So because there has been a single effect, so both.

661
01:22:41,192 --> 01:22:49,672
And so there is no corresponding keystone as they are to square on some statistic from which to statistic.

662
01:22:49,972 --> 01:22:53,212
And that's what we need to do a little to correct. So,

663
01:22:53,722 --> 01:23:03,752
so the first one gets that statistic when you finish this model and we will look at the two studies D for beta B that's that corresponds to the first.

664
01:23:03,772 --> 01:23:08,932
And by the second half that corresponds to the fate of this model.

665
01:23:09,232 --> 01:23:17,642
You look at the beta eight because one is just so that trend will continue if we add more than two covariates.

666
01:23:18,802 --> 01:23:25,272
Yes, that's right. That's right. So. So yeah.

667
01:23:25,752 --> 01:23:33,852
So if you have if you have more on it's just sort of similar to has this I'm not sure if I should probably

668
01:23:33,852 --> 01:23:41,122
I I'm hesitating a little bit to give a even more complex example because this is already large audience.

669
01:23:41,132 --> 01:23:44,552
I mean, because there are already too many numbers to, to interpret.

670
01:23:45,312 --> 01:23:53,592
I really want to add further confusion. But yeah, but you are similarly if you have more clarity added.

671
01:23:55,872 --> 01:24:02,741
Yes, but the with the for the birthweight but with the values still be exactly the square of

672
01:24:02,742 --> 01:24:10,182
the T since for that to calculate after using the standard error of the whole model.

673
01:24:10,182 --> 01:24:15,732
Whereas for the t statistic that's only using the partial model.

674
01:24:18,252 --> 01:24:22,272
Um. Okay. Well, that's a good question. They are equivalent.

675
01:24:22,272 --> 01:24:28,272
So that's why I said we have to be very careful like there. But F is equivalent to lynch t statistic.

676
01:24:29,082 --> 01:24:34,092
Okay. So, so again, so this F here because this is sequential.

677
01:24:34,092 --> 01:24:41,412
So the first gap here when we look at this, if there is no H yet, we haven't talked about AB, we just looking at F.

678
01:24:41,442 --> 01:24:45,762
So that's why this F corresponds to the T for this model.

679
01:24:46,572 --> 01:24:51,162
Okay. The model that has only birthweight and that if you fitted this model,

680
01:24:51,582 --> 01:24:58,662
if you look at the Latin correspondent, there's been a B and that's well, then this app is a square of that.

681
01:25:00,592 --> 01:25:07,812
But the second hour here now, this is actually adding edge into a model that ATA has birthweight.

682
01:25:09,322 --> 01:25:12,412
That means you're looking at you're looking at this model.

683
01:25:13,672 --> 01:25:20,092
You're adding age into the world and you're looking at the corresponding age.

684
01:25:20,782 --> 01:25:27,592
And that's equivalent to smoking. This model you're looking at that the teen corresponding to age, in fact.

685
01:25:30,252 --> 01:25:33,762
And this, of course, is different and this time is different.

686
01:25:33,762 --> 01:25:40,772
Of course, these differ from the the tea. In this model.

687
01:25:41,662 --> 01:25:47,942
Right. Is that going to be hard? Because because this is a rising birth rate and this is not addressing for work related.

688
01:25:48,602 --> 01:25:55,922
And if we're both, you have a statistic, right? So for this one, you have a T for this map and for this Y, you have a T for Veda.

689
01:25:57,002 --> 01:26:06,902
So so that's why I said here, I mean, both f they are equivalent to some T, but we need to be very careful like which t they're equivalent to.

690
01:26:13,492 --> 01:26:19,452
Well, I hope that this does not add further confusion and.

691
01:26:22,202 --> 01:26:30,512
But if you know, what we are trying to do is we're trying to we're trying to interpret all the numbers in the tables.

692
01:26:31,502 --> 01:26:42,122
We try to see like what number represents what they're doing and any other questions.

693
01:26:47,222 --> 01:26:51,172
Made. Okay.

694
01:26:52,372 --> 01:26:58,252
So that actually I think that now we have interpreted all the numbers in this table.

695
01:26:58,372 --> 01:27:01,572
Okay. So then let's move on to the next table.

696
01:27:01,582 --> 01:27:06,982
So here this is shows that are on the order matters where you carry out a sequential test.

697
01:27:07,402 --> 01:27:11,092
So this table here the jargon first.

698
01:27:11,102 --> 01:27:16,282
Well, this is the model that has h although I so we are not to make a revision.

699
01:27:16,282 --> 01:27:19,982
You can actually compare this to this table here.

700
01:27:20,002 --> 01:27:24,232
I mean, you can do an all in a revision of every single number in the table.

701
01:27:24,712 --> 01:27:29,542
But this is actually looking at the age along at age along to the model.

702
01:27:30,262 --> 01:27:36,921
And then this output here, this is actually by adding age to a model.

703
01:27:36,922 --> 01:27:40,942
Oh, sorry. Adding birth weight to a model that already has H.

704
01:27:42,502 --> 01:27:47,032
Right. So we are adding birth weight in the model that has H.

705
01:27:47,542 --> 01:27:56,662
And then we have see that if we compare this model of this table with the previous table, we can make it.

706
01:27:57,652 --> 01:28:04,912
This is probably not sure if you guys can still see does you have your laptop all?

707
01:28:05,002 --> 01:28:10,432
So I think you probably can you can look at your laptop, but you can definitely see that the numbers here,

708
01:28:10,762 --> 01:28:16,222
for example, above the first wave here, this this birth wedge, the sample square is 130.

709
01:28:17,182 --> 01:28:22,492
But here the birth weight, the sample square is 82 and they are definitely different.

710
01:28:23,272 --> 01:28:31,132
And also here in the first table above the age, some square is for 460 and here is 508 and they are definitely different.

711
01:28:32,902 --> 01:28:35,991
But that residual sample square there are never the same.

712
01:28:35,992 --> 01:28:40,072
So both are 79.9 and of course this should be the same.

713
01:28:40,072 --> 01:28:48,411
The reason is that it doesn't matter whether you first at age or first and a birth weight, but eventually you add a into the model.

714
01:28:48,412 --> 01:28:55,072
Of course. Then if you look at the original Selma Square for the model that has both covariance the residual someone special do the same.

715
01:28:56,332 --> 01:28:59,872
So that's why we have this the same of two models.

716
01:29:03,282 --> 01:29:09,132
And then similar. Just two syllables, almost exactly the same as the for the previous table.

717
01:29:09,162 --> 01:29:16,722
You can't interpret the sum of squares, you know, the the f value and the the corresponding p value.

718
01:29:18,912 --> 01:29:26,592
And maybe that's maybe not just to make it a just to make it a super clear.

719
01:29:26,592 --> 01:29:35,952
So this f about this is as an SS, this is adding beta age into a model that has only intersect.

720
01:29:37,132 --> 01:29:40,402
And divided by this one. They were afraid on nine divided by eight,

721
01:29:40,402 --> 01:29:47,222
as I see that has been a zero beta age and beta be divided by 13 and that's this and

722
01:29:47,272 --> 01:29:55,852
that and this fellow this is as as adding therapy into a model that has beta age,

723
01:29:56,302 --> 01:30:10,132
very flat rate of zero in beta h and divided by one bigger freedom, not even as I see zero age 18 divided by three.

724
01:30:10,462 --> 01:30:16,522
Now that's how this F is calculated and then this p value.

725
01:30:17,692 --> 01:30:25,072
This is as we're testing this testing h zero beta h equal to zero versus h one.

726
01:30:25,072 --> 01:30:36,682
Beta age is not equal to zero and this is for the model y is equal to beta zero plus beta h h plus absolute.

727
01:30:40,262 --> 01:30:53,312
Right. And again this p value this is actually testing h zero paid be equal to zero versus alternative better be not equal to zero.

728
01:30:54,152 --> 01:31:02,191
But this is for the model y equal to better zero plus beta h plus.

729
01:31:02,192 --> 01:31:06,692
There have been 19 governmental caps.

730
01:31:19,792 --> 01:31:27,201
And if you look at, well, not only the sample square, if you look at the sample square and if you look at the f p values and these

731
01:31:27,202 --> 01:31:31,412
values are definitely different from the previous table when you first add B,

732
01:31:31,502 --> 01:31:33,552
the birth weight and then range.

733
01:31:33,952 --> 01:31:42,862
So this indeed shows that first sequential testing that the order that you add variable into the into the mode on that does matter.

734
01:31:43,792 --> 01:31:48,772
So it will give you different. Different it might give you different.

735
01:31:54,782 --> 01:31:58,892
Now because well, again, I don't know whether maybe I should add.

736
01:31:59,312 --> 01:32:04,982
So again, because, for example, for this age now this F and this P, this is testing for age.

737
01:32:04,982 --> 01:32:11,342
In fact, however, this is testing working, in fact, in the model that has only age.

738
01:32:12,602 --> 01:32:21,422
However, in a previous case, if you look at this, this aid here, this gas and this P value,

739
01:32:21,452 --> 01:32:28,382
this is testing for a defect in the model and also has already has the growth rate.

740
01:32:29,762 --> 01:32:33,642
So this is not the difference in.

741
01:32:36,712 --> 01:32:40,132
In fact, of the order when we have never.

742
01:32:46,322 --> 01:33:04,362
Okay. Any questions? And then while this next slide is really not, there's nothing new here.

743
01:33:04,372 --> 01:33:12,021
This is just numerical verify that, you know, the SAS and ANOVA, this function uses F rather than using F star.

744
01:33:12,022 --> 01:33:19,161
We have pointed this out because by the way, this is this is exactly the same table as before.

745
01:33:19,162 --> 01:33:22,792
So birth weight refers to have birth weight and then have age.

746
01:33:23,422 --> 01:33:27,742
That's added to this table here. So first and then wait and then have age.

747
01:33:28,102 --> 01:33:32,302
So as we measured so when we even when we cast birthweight,

748
01:33:33,422 --> 01:33:40,102
when we test of other repeatability is equal to zero or not, that means that looking on this model has no age.

749
01:33:40,432 --> 01:33:47,182
But even when testing this, we are still dividing the sum of squared errors.

750
01:33:47,182 --> 01:33:53,322
Almost got real students still dividing births errors almost clear based on the formula that's used.

751
01:33:54,392 --> 01:33:59,052
Well, that's based on the model that has both birthweight and a reality.

752
01:33:59,062 --> 01:34:06,222
First, this whole thing, this is the so-called f f statistic, but not of of star.

753
01:34:06,642 --> 01:34:12,052
And that's because in a denominator we're dividing massively square has made it based on a football.

754
01:34:16,362 --> 01:34:19,842
Yeah. That's that's what this life is trying to say.

755
01:34:20,232 --> 01:34:29,532
We have already pointed this out and that's sequential, some of square and then we have partial some square.

756
01:34:30,752 --> 01:34:33,812
So this is in SAS.

757
01:34:33,812 --> 01:34:37,112
This is the so-called time three someone squared.

758
01:34:39,852 --> 01:34:47,261
Besides, as it's called, two times three summits where the party is almost where you are,

759
01:34:47,262 --> 01:34:51,432
you can get parties almost where by at this particular function.

760
01:34:56,222 --> 01:35:01,142
So I think I now I want to take a previous and I said that, you know,

761
01:35:01,142 --> 01:35:08,342
you have to be you had to look at how assassin are exactly what type of square, what time square you want to get.

762
01:35:08,962 --> 01:35:13,052
And if you don't, how long are you automatic again? I yes.

763
01:35:13,292 --> 01:35:18,472
I guess the second part is on the square. So let let's forget about that.

764
01:35:18,482 --> 01:35:18,722
I mean,

765
01:35:19,022 --> 01:35:27,992
here we do have I guess my point is that we do have deferred a comment you can use to get either part of the summer square or square was almost here.

766
01:35:28,262 --> 01:35:32,882
So you need to specify that specifically what type of things were engaged.

767
01:35:33,182 --> 01:35:39,272
So by this line of comment, it are you will get the partial sum square.

768
01:35:41,402 --> 01:35:47,502
Now, if you look at this table, the partial sum of square. The part was almost written.

769
01:35:48,402 --> 01:35:57,522
If you look at this table here, it has a very similar structure to the previous look for gas cigarets elsewhere.

770
01:35:57,792 --> 01:36:01,542
While the difference is that we have one more line corresponding to the Intersect,

771
01:36:02,052 --> 01:36:06,612
so for now, the interpretation of these numbers are quite different.

772
01:36:07,212 --> 01:36:11,892
Now let's let's take a look at the interpretation. Now this some square for birth weight.

773
01:36:12,252 --> 01:36:16,991
This number here, this is well, this is partial sample square.

774
01:36:16,992 --> 01:36:21,582
So this is SS So you don't be.

775
01:36:24,352 --> 01:36:31,042
Adding Birthweight into a model that already has intercept and A and an H.

776
01:36:32,812 --> 01:36:37,281
Okay, this is what this solid square means. And similar of this.

777
01:36:37,282 --> 01:36:46,692
A sample square. This is as. At age intermodal that ATA has birthweight.

778
01:36:48,672 --> 01:36:54,372
So in other words, it doesn't matter whether birth weight appears first or age appears first.

779
01:36:54,882 --> 01:36:59,262
Now there's the order doesn't matter anymore because each one is, you know, partial.

780
01:36:59,282 --> 01:37:05,692
It's actually is adding that a corresponding or into a quarter of an RNA has all the traits.

781
01:37:08,902 --> 01:37:11,962
So yours now for this model.

782
01:37:13,392 --> 01:37:20,712
It doesn't matter whether this is by birth weight, a place, age or age plus birth weight, you will be at the same table.

783
01:37:21,072 --> 01:37:27,192
But it's just that there's two lives. I mean, there was there was each order, but of course, both numbers would be the same.

784
01:37:29,502 --> 01:37:38,892
And then you have look, everybody, now this f value, this is assessed to be even better,

785
01:37:38,892 --> 01:37:48,402
0 to 8 divided by one divided by, as I see it in zero eight, continue to be divided by three.

786
01:37:54,422 --> 01:38:09,752
And similar to this number. This is as as beta a human being 08ab divided by one divided by as as we had in zero eight.

787
01:38:10,472 --> 01:38:20,842
They had been divided by 30. So both are partial ads for some of square.

788
01:38:22,872 --> 01:38:32,172
And the similarity that if you look at this first p value here, this P value, this is this demand is tested.

789
01:38:33,492 --> 01:38:40,832
Now, this problem is testing. Testing to be equal to zero versus parent.

790
01:38:40,842 --> 01:38:48,642
B is not equal and this segment to be valid.

791
01:38:49,302 --> 01:38:57,482
This is testing beta eight equal to zero versus beta.

792
01:38:57,482 --> 01:39:09,612
Okay. Does not equal the. And a fourth are actually for the moto mods based on the auto.

793
01:39:16,532 --> 01:39:30,352
He. Based on his formal.

794
01:39:35,682 --> 01:39:39,612
So so this is actually the particle particle.

795
01:39:39,612 --> 01:39:49,212
And so the the p value corresponding to birth weight that's asking whether this B is equal to zero advancing for H.

796
01:39:50,932 --> 01:39:57,112
And this does P-value for age, thus testing whether this better day is equal to zero.

797
01:39:58,252 --> 01:40:09,162
After adjusting for for birth weight so both p values are for have always found that after adjusting for the other covariance notice for for.

798
01:40:11,392 --> 01:40:16,552
So that's the the interpretation of the results based on the part of the sample square.

799
01:40:19,162 --> 01:40:29,752
Okay. So we do have a few slides, but I think we will start here and then we will continue the first.

