1
00:00:08,840 --> 00:00:11,760
Hello. Welcome to this lecture on

2
00:00:11,760 --> 00:00:14,385
hypothesis testing with
continuous outcomes.

3
00:00:14,385 --> 00:00:16,950
This lecture is a SQL to
my previous lecture on

4
00:00:16,950 --> 00:00:18,570
ANOVA in which I presented

5
00:00:18,570 --> 00:00:21,555
a visual examination
of how ANOVA works.

6
00:00:21,555 --> 00:00:25,515
Now, as we say a picture
can be worth 1000 words.

7
00:00:25,515 --> 00:00:28,770
For those of us who love
graphical descriptions of data,

8
00:00:28,770 --> 00:00:31,910
this module is an
exploration of the analysis

9
00:00:31,910 --> 00:00:35,795
of variance using pictures
instead of formulas.

10
00:00:35,795 --> 00:00:38,270
So our learning
objectives are first to

11
00:00:38,270 --> 00:00:40,625
visualize how analysis
of variants works

12
00:00:40,625 --> 00:00:43,550
and then to visualize how
sampling variability impacts

13
00:00:43,550 --> 00:00:46,700
the conclusions that we make
from analysis of variance.

14
00:00:46,700 --> 00:00:47,880
Now, in front of you,

15
00:00:47,880 --> 00:00:49,650
you see a figure that
I've adapted from

16
00:00:49,650 --> 00:00:52,355
Kim's paper published in 2017.

17
00:00:52,355 --> 00:00:55,715
This figure represents data
collected from three groups.

18
00:00:55,715 --> 00:00:57,680
The red circles represent
data from group

19
00:00:57,680 --> 00:01:00,460
1 the blue triangles
from group 2,

20
00:01:00,460 --> 00:01:03,160
and the green stars for group 3.

21
00:01:03,160 --> 00:01:05,480
There is also a grand
mean represented by

22
00:01:05,480 --> 00:01:07,280
the solid dark square which is

23
00:01:07,280 --> 00:01:08,870
the average across all of

24
00:01:08,870 --> 00:01:13,870
the observations regardless
of their group membership.

25
00:01:13,870 --> 00:01:16,270
Now, we highlight the fact that

26
00:01:16,270 --> 00:01:18,460
these data come from
three different groups.

27
00:01:18,460 --> 00:01:20,815
There is a red circle
for the first group,

28
00:01:20,815 --> 00:01:22,840
there's a green circle
for the third group,

29
00:01:22,840 --> 00:01:25,165
and a blue circle for
the second group.

30
00:01:25,165 --> 00:01:27,040
Each of those groups has

31
00:01:27,040 --> 00:01:28,899
their own individual mean

32
00:01:28,899 --> 00:01:31,765
represented by the solid
circle for group 1,

33
00:01:31,765 --> 00:01:33,475
the solid triangle for group 2,

34
00:01:33,475 --> 00:01:37,800
and the solid star for group 3.

35
00:01:37,800 --> 00:01:40,240
In ANOVA, the first thing we

36
00:01:40,240 --> 00:01:42,910
do is compare the three means

37
00:01:42,910 --> 00:01:44,800
of the three groups relative to

38
00:01:44,800 --> 00:01:47,320
the grand mean across
all three groups.

39
00:01:47,320 --> 00:01:49,480
So in other words, we
look at the length of

40
00:01:49,480 --> 00:01:52,450
the three arrows in
the third diagram.

41
00:01:52,450 --> 00:01:55,070
The larger those lengths are,

42
00:01:55,070 --> 00:01:57,545
the more distinct that
groups are from each other.

43
00:01:57,545 --> 00:01:59,375
The more likely we have evidence

44
00:01:59,375 --> 00:02:01,955
that there is a difference
among the groups.

45
00:02:01,955 --> 00:02:04,970
The three solid
arrows are compared

46
00:02:04,970 --> 00:02:07,670
relative to the size
of the dashed arrows.

47
00:02:07,670 --> 00:02:10,070
The dashed arrows
represent how spread

48
00:02:10,070 --> 00:02:13,790
out each group's data is
from the mean of that group.

49
00:02:13,790 --> 00:02:16,370
The larger the dashed arrows are,

50
00:02:16,370 --> 00:02:18,680
the more noise there
is in our data and

51
00:02:18,680 --> 00:02:21,395
the harder it is to
see a difference.

52
00:02:21,395 --> 00:02:23,540
So I now present to
you on the left and

53
00:02:23,540 --> 00:02:26,000
example of three samples
of data that would

54
00:02:26,000 --> 00:02:28,160
lead to a small F-statistic

55
00:02:28,160 --> 00:02:30,895
which then would produce
a large P-value.

56
00:02:30,895 --> 00:02:33,500
I would not reject the
null hypothesis that there

57
00:02:33,500 --> 00:02:36,260
is a difference among the
groups in the population.

58
00:02:36,260 --> 00:02:39,635
Visually we can see that
we have a very hard time

59
00:02:39,635 --> 00:02:41,210
discerning how many groups of

60
00:02:41,210 --> 00:02:43,985
data we might have
in this example.

61
00:02:43,985 --> 00:02:47,990
This is emphasized by the
solid arrows which are

62
00:02:47,990 --> 00:02:52,235
quite small relative to the
length of the dashed arrows.

63
00:02:52,235 --> 00:02:54,440
The dashed arrows
represent the noise or

64
00:02:54,440 --> 00:02:56,480
the amount of overlap
among the groups.

65
00:02:56,480 --> 00:02:59,300
The amount of
overlap is too large

66
00:02:59,300 --> 00:03:01,115
relative to the differences

67
00:03:01,115 --> 00:03:03,895
in the means with the grand mean.

68
00:03:03,895 --> 00:03:06,150
The mean square between would be

69
00:03:06,150 --> 00:03:09,770
small and the mean square
error would be large.

70
00:03:09,770 --> 00:03:13,040
The ratio of these two
numbers would be small.

71
00:03:13,040 --> 00:03:17,065
Therefore, I don't have
evidence to reject the null.

72
00:03:17,065 --> 00:03:21,200
In contrast, here's an example
of data that would lead to

73
00:03:21,200 --> 00:03:25,310
a large F-statistic and
correspondingly a small P-value.

74
00:03:25,310 --> 00:03:27,350
Visually, we can see
immediately that

75
00:03:27,350 --> 00:03:29,870
we have three distinct
groups of data.

76
00:03:29,870 --> 00:03:32,060
This is quantified
by the length of

77
00:03:32,060 --> 00:03:33,800
the solid arrows which again is

78
00:03:33,800 --> 00:03:35,855
the distance that
each group's mean

79
00:03:35,855 --> 00:03:38,465
is from the overall grand mean.

80
00:03:38,465 --> 00:03:40,550
These three arrows are

81
00:03:40,550 --> 00:03:44,390
quite large relative to the
dashed arrows which are very

82
00:03:44,390 --> 00:03:47,360
small because the
data in each group is

83
00:03:47,360 --> 00:03:50,855
clustered very tightly around
their corresponding mean.

84
00:03:50,855 --> 00:03:53,450
In this case, the mean
squared between would be

85
00:03:53,450 --> 00:03:56,515
large and the mean squared
error would be small.

86
00:03:56,515 --> 00:03:59,210
Thus my F-statistic
would be large because

87
00:03:59,210 --> 00:04:02,860
the numerator is much larger
than the denominator.

88
00:04:02,860 --> 00:04:08,125
Now, analysis of variance has
one very specific question.

89
00:04:08,125 --> 00:04:10,160
That question is whether any of

90
00:04:10,160 --> 00:04:11,630
the populations have means

91
00:04:11,630 --> 00:04:13,325
that are different
from each other.

92
00:04:13,325 --> 00:04:16,625
So if I conclude the answer
to this question is yes,

93
00:04:16,625 --> 00:04:19,535
I now have a new question
I have to address.

94
00:04:19,535 --> 00:04:22,210
If the populations
have different means,

95
00:04:22,210 --> 00:04:24,560
I want to know which pairs of

96
00:04:24,560 --> 00:04:28,250
populations actually have
the different means?

97
00:04:28,250 --> 00:04:31,130
The second question can
be addressed by simply

98
00:04:31,130 --> 00:04:34,325
doing several two-sample
t-tests with the data.

99
00:04:34,325 --> 00:04:36,080
However, before we do that,

100
00:04:36,080 --> 00:04:37,670
we need to make one adjustment to

101
00:04:37,670 --> 00:04:41,330
the P-values that are
produced from these t-tests.

102
00:04:41,330 --> 00:04:42,650
This adjustment is known as

103
00:04:42,650 --> 00:04:44,390
a Bonferroni correction and

104
00:04:44,390 --> 00:04:49,422
will be discussed further
in a later lecture.

