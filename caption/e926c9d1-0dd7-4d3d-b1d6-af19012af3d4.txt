1
00:00:04,050 --> 00:00:12,510
Here we are. All right. Rid of this good business, folks.

2
00:00:12,780 --> 00:00:18,300
All right, so I emailed a handful of you yesterday.

3
00:00:18,360 --> 00:00:27,179
I took I looked at the homework number once and trying to find a plot out of each of the 11 data sets getting necessarily the best plot,

4
00:00:27,180 --> 00:00:30,720
but eight plots. That gave us an idea of what's going on in these data.

5
00:00:31,140 --> 00:00:34,170
And I think that will help inform what we're going to do the rest of the semester.

6
00:00:36,670 --> 00:00:44,170
So let's go through the 11 datasets quickly. The first one was Ambulance Our Debt, 80 folks.

7
00:00:44,980 --> 00:00:48,600
They had multiple sclerosis clinical trial here. We had two groups of folks.

8
00:00:48,610 --> 00:00:53,530
Some got randomized to 14 and some did not.

9
00:00:54,310 --> 00:00:59,830
The outcome was measured at weeks two, four and six. There was a baseline, but they subtract the baseline out.

10
00:00:59,830 --> 00:01:05,620
So they didn't change from baseline. And they have three repeated changes over time in each of two groups.

11
00:01:07,570 --> 00:01:13,150
And so the homework assignments I saw all plot like this for the means over time.

12
00:01:15,070 --> 00:01:18,580
The blue line is fan protein and the green line is placebo.

13
00:01:20,260 --> 00:01:25,630
Very interesting. A disadvantage, I would imagine. A very disappointing result for the folks in this clinical trial.

14
00:01:26,560 --> 00:01:33,220
Right. Fan four being fan protein got better and then it just went back to where the baseline group ended up at the end of the study.

15
00:01:35,320 --> 00:01:40,059
How would should time be modeled when you fit a regression model to these data?

16
00:01:40,060 --> 00:01:49,690
How should say, should? How would you fit time? Those working on the dataset.

17
00:01:50,440 --> 00:01:56,060
How are you fitting time? There's a line, a good plan here.

18
00:01:57,020 --> 00:02:02,510
No trading time is continuous is probably not the best approach here.

19
00:02:03,560 --> 00:02:05,300
We only have three time points.

20
00:02:06,050 --> 00:02:13,070
Everything seems to be going off in different directions, so there's no reason why you can't treat time as categorical with these data.

21
00:02:13,910 --> 00:02:21,680
Right. I've got a dummy variable that says time for versus time to add a dummy variable for six versus to two is the intercept, for example.

22
00:02:22,700 --> 00:02:30,040
Then I've got two groups. So. Two time, one group and then two interaction terms.

23
00:02:30,050 --> 00:02:33,770
That's five terms. I think that's a reasonable fit for regression model.

24
00:02:34,670 --> 00:02:40,670
You could fit. Time is continuous. You could say, Well, I like curves, so I'm going to do a quadratic because each of them has a different curvature.

25
00:02:41,420 --> 00:02:45,350
If you did that, that's fine. You could do all three of those.

26
00:02:45,350 --> 00:02:53,030
And how would you pick among those three? Any ideas?

27
00:02:54,260 --> 00:03:03,290
AIC and BSE. Right. If things are nested, you can always use an AIC value to compare any models to each other to help you decide.

28
00:03:05,430 --> 00:03:13,830
So these data, I think just putting in a linear term for time probably isn't the smartest way to compare it how time is going out on the industry.

29
00:03:14,100 --> 00:03:17,700
So there's feedback there. Again, I didn't take off.

30
00:03:17,700 --> 00:03:21,450
I didn't have some take off points for how time was considered.

31
00:03:22,500 --> 00:03:29,729
I don't know if I will, but as we're moving along, we need to be a little more thoughtful on how we're doing the stuff.

32
00:03:29,730 --> 00:03:39,690
And this is why we're doing this today. Deficit number two looked at AZT, the treatment, AZT, and those with a similar asymptomatic HIV.

33
00:03:40,290 --> 00:03:41,640
And so they didn't count it.

34
00:03:41,790 --> 00:03:52,440
They measured CD4 positive cell counts and a total of four time points in a whole lot of folks either getting AZT or getting a placebo.

35
00:03:52,560 --> 00:03:57,030
So, again, two arms from a randomized trial, measuring something over time.

36
00:03:59,700 --> 00:04:02,880
And this is what the plot looks like and those homework assignments.

37
00:04:05,270 --> 00:04:13,130
Again, a pretty disappointing result that by the time you get to the end here, things are pretty much overlapping each other again.

38
00:04:15,320 --> 00:04:18,650
It's a little bizarre that this was a randomized. Do we have baseline in these data?

39
00:04:20,570 --> 00:04:25,080
Oh, they didn't give you the baseline. Okay. It's kind of strange that they don't care.

40
00:04:25,760 --> 00:04:31,430
Right. The things we're randomized, that should start out at the same point. That's because we don't have time.

41
00:04:31,430 --> 00:04:34,670
Zero. How are we going to handle time here?

42
00:04:40,240 --> 00:04:45,190
Makes sense. Anybody?

43
00:04:45,220 --> 00:04:49,000
Yes. A lot of observations. You can probably just do categorical data again.

44
00:04:49,450 --> 00:04:55,350
We have tons of people in the study. There's no reason why you can't treat time aid as a dummy.

45
00:04:55,360 --> 00:04:58,540
Is the reference rate in all future time points with dummy variables.

46
00:04:59,080 --> 00:05:06,490
So we've got four dummy variables there, plus an intercept plus a group variable, plus interactions with the dummy there.

47
00:05:07,720 --> 00:05:13,120
So that seems plausible. Again, I don't think time as a line here is probably the best plan.

48
00:05:13,540 --> 00:05:16,989
Again, you might say, well, you know, the placebo group, there's that dip,

49
00:05:16,990 --> 00:05:22,180
but then they're going up and within sampling variability, maybe there is a line that gets through those points.

50
00:05:22,180 --> 00:05:31,900
Right? And again, this is one of the spot I like to make the spot first to see what makes sense in terms of how things are are going.

51
00:05:32,200 --> 00:05:37,570
You probably never see that in a spaghetti plot of, well, it's a thousand people.

52
00:05:40,000 --> 00:05:46,510
The third data set here, we've got this outcome called b, r s, the brief psychiatric rating scale.

53
00:05:47,410 --> 00:05:51,220
And so this is basically a measure of the severity of symptoms.

54
00:05:52,180 --> 00:05:58,100
And there are a series of 18 questions and each goes from 1 to 7 and they get a total score.

55
00:05:58,720 --> 00:06:02,890
So I believe higher scores indicate more severity of the outcome.

56
00:06:03,460 --> 00:06:10,780
And so each patient had this measurement before treatment week zero and then at weekly intervals for eight weeks.

57
00:06:10,900 --> 00:06:18,610
So a lot of time points. And this is what's I saw in homework assignments for these data.

58
00:06:20,020 --> 00:06:23,930
This one's easy. Is there a group difference?

59
00:06:26,450 --> 00:06:34,630
Is there a time difference? Is there a difference, which is I expect to find a regression model that gives you a significant group difference?

60
00:06:35,650 --> 00:06:43,470
Probably not. No matter what we do in this class, we're probably not going to find out that there is any difference between the groups.

61
00:06:43,480 --> 00:06:53,110
There's a slight change over time. Linearity seems pretty easy to do here, and the slopes are pretty consistent between the two groups.

62
00:06:54,850 --> 00:07:02,140
So again, all the models will give the similar estimates, unbiased estimates, but inference is affected.

63
00:07:03,160 --> 00:07:13,680
So. Again, this wasn't part of the homework assignment to be careful with this plot here.

64
00:07:14,490 --> 00:07:20,880
When you say confidence balance, how did you estimate the standard error of these things?

65
00:07:22,330 --> 00:07:28,320
It's an error. Probably assumed independence because you hadn't learned how to deal with correlation yet.

66
00:07:29,820 --> 00:07:36,300
So those confidence fans are probably wrong. Just remember that when you do these sorts of plots, when you start talking about variability,

67
00:07:37,530 --> 00:07:41,640
make sure you know whether or not a correlation is being accounted for or not.

68
00:07:42,060 --> 00:07:48,930
But does it really matter here? Even if we fix things, I think there's an easy problem here.

69
00:07:48,960 --> 00:07:52,470
We've got a variable continuous time and an interaction.

70
00:07:54,750 --> 00:07:58,350
Now. Again, I am not telling you what the right answer is.

71
00:07:59,400 --> 00:08:06,570
I am not here to tell you what the right answer is. I'm telling you what I see. And as long as you can defend what you're doing, that's just as good.

72
00:08:07,680 --> 00:08:10,670
We have more groups in this dataset here.

73
00:08:10,710 --> 00:08:19,860
This is a clinical trial, looking at different treatments, combinations of HIV, one reverse transcriptase inhibitors.

74
00:08:20,040 --> 00:08:26,670
So we've got. So nobody in. Plus side Damascene and then some other combinations here.

75
00:08:27,090 --> 00:08:34,499
So for groups it looks like folks who are measure of baseline and then every eight weeks during the follow

76
00:08:34,500 --> 00:08:40,620
up and they log transform the counts for us again I guess because they're worried about a lot of skewness.

77
00:08:43,100 --> 00:08:47,390
Here are the four groups and the averages over time in the tour groups.

78
00:08:49,010 --> 00:08:54,960
What are we going to do here? Are we going to treat time?

79
00:09:01,190 --> 00:09:09,170
I mean, it is. If you fit line through the purple group, you're going to get a pretty flat line.

80
00:09:10,250 --> 00:09:14,570
Probably doesn't describe. What's going on with this group?

81
00:09:18,740 --> 00:09:22,010
That's what our folks are going to do by next Wednesday.

82
00:09:23,300 --> 00:09:32,910
That's the time variable here. Anybody are categorical.

83
00:09:33,350 --> 00:09:36,520
And then I heard something else. No, I'm sorry. I know what you said.

84
00:09:38,050 --> 00:09:43,830
It's like a categorical. Okay, so categorical. Okay. Alright, so we've got one, two, three, four, five time bytes.

85
00:09:43,840 --> 00:09:48,780
Right. So you could have four categories. So I'm sorry, four dummy variables.

86
00:09:48,790 --> 00:09:59,780
Right. So you've got an intercept for dummy variables, three dummy variables for group and then for dummy variables for each group.

87
00:09:59,800 --> 00:10:03,940
Right. There's a lot of parameters in that model. I'm saying that's a bad model.

88
00:10:07,270 --> 00:10:11,770
How many folks were in the study? I don't even know. Didn't say the description.

89
00:10:12,850 --> 00:10:17,380
So you have to keep that in mind. Can I estimate a model like that with the amount of data I have at hand?

90
00:10:20,650 --> 00:10:28,000
Again, if I wanted to reduce the number of parameters, it looks to me like everybody has the chance to go up right away and then come down.

91
00:10:29,080 --> 00:10:32,650
Although the red group doesn't have that up, it just continues to go down.

92
00:10:34,000 --> 00:10:42,250
So I would consider a piecewise linear model where the first time there's a slope, and then after the second time point, there is another slope.

93
00:10:43,420 --> 00:10:50,890
That's that's just two parameters instead of four. It's another way to do it and so have to do it that way.

94
00:10:52,990 --> 00:10:57,460
But again, I think just fitting a line here is probably not a good plan.

95
00:10:58,210 --> 00:11:02,710
You're going to miss some of that up and down pattern that obviously differs between the groups.

96
00:11:03,130 --> 00:11:09,040
Right. Obviously, the purple group is seeing the greatest increase, the greatest benefit in the beginning,

97
00:11:10,090 --> 00:11:14,320
which, again, all the treatments seem to be dying off over time. So keep that in mind.

98
00:11:14,800 --> 00:11:20,870
Don't just throw timing as a continuous variable when we're trying to learn the things in these class, this class.

99
00:11:20,980 --> 00:11:30,100
All right. Number five, cholesterol gallstone study treatment for gallstones is known to also increase cholesterol.

100
00:11:30,820 --> 00:11:37,090
So 113 folks randomized to either a schedule or a placebo.

101
00:11:38,380 --> 00:11:43,330
This is where you can see these data sets are sort of made pretty for classroom work.

102
00:11:44,020 --> 00:11:48,940
It should have been a randomized trial, but we've got 65 and 148 in the other.

103
00:11:49,930 --> 00:11:54,970
That means where the rest of the people. It's probably 5050 randomization.

104
00:11:55,480 --> 00:11:59,530
So we probably took some folks out for noisy data drop out and so forth.

105
00:12:00,310 --> 00:12:10,510
Serum cholesterol measured in these folks at baseline and then four more time right so five timepoints two groups and we see something like that.

106
00:12:10,630 --> 00:12:13,900
And I saw Mark and this dataset.

107
00:12:13,900 --> 00:12:17,830
I think we've got a very simple model. You could get a vertical.

108
00:12:17,830 --> 00:12:21,820
There's no reason why you can ever say categorical if you don't like getting in a straight line.

109
00:12:22,510 --> 00:12:25,840
But linearity doesn't seem too far off here. Right.

110
00:12:26,590 --> 00:12:34,510
So it could easily I can easily see a model that has for continuous time slope and then an interaction term.

111
00:12:35,900 --> 00:12:41,020
As you try to figure stuff out. There's one day I we're going to right.

112
00:12:41,030 --> 00:12:45,050
This is number six. We have 1 to 3 groups.

113
00:12:46,190 --> 00:12:51,410
I got an email about the set of set because it is is missing a little bit information.

114
00:12:52,100 --> 00:12:58,670
So we had three groups of individuals. This subset of data contains data for visit one and three through six.

115
00:12:58,680 --> 00:13:02,210
So I don't know why two couldn't be part of the dataset.

116
00:13:03,840 --> 00:13:10,170
Visits three through five were two weeks apart and visits 6 to 6 weeks have to visit.

117
00:13:10,200 --> 00:13:16,980
But what about visit one? What how long did visit one go between that and visit three?

118
00:13:18,870 --> 00:13:24,570
No idea. So then you can decide?

119
00:13:27,030 --> 00:13:32,819
I don't know. That's like, again, the computer likes to space everything out.

120
00:13:32,820 --> 00:13:36,630
Here it is. One, two, three, four or five. But is that span between one and three?

121
00:13:37,110 --> 00:13:40,830
Really? Twice as far as the others, we don't really know.

122
00:13:42,960 --> 00:13:48,730
So. But if you're just going to fit a straight line through the entire set of data, I don't think that's too problematic.

123
00:13:49,030 --> 00:13:55,899
Again, you're making assumptions here, and the reason I kept both of these spots, too,

124
00:13:55,900 --> 00:14:01,630
is remember when you have the scale on your y axis, these two plots are showing the same thing.

125
00:14:01,900 --> 00:14:05,950
One is showing and connecting the means. The other one is showing the box plots over time.

126
00:14:07,060 --> 00:14:11,920
The scales are not the same. And this isn't a criticism of whoever did this.

127
00:14:12,700 --> 00:14:16,990
I'm just saying it looks like, wow, it really drops over there.

128
00:14:17,800 --> 00:14:23,630
The number like kind of decays a little. That's right. And that's just because of the scale that is shiny to this part.

129
00:14:23,680 --> 00:14:31,010
So always be careful with that, too. But could be categorical, could be aligned.

130
00:14:31,020 --> 00:14:36,000
Yes. I have a question for number six because I know I think in the description says

131
00:14:36,770 --> 00:14:41,340
is it five to visit six is so different the time like 3 to 5 or two weeks ago.

132
00:14:41,370 --> 00:14:45,330
True, yes. So the scale of the sex. Sex us is misleading.

133
00:14:45,900 --> 00:14:55,980
Yeah. Like if we had chosen to do like the line plot, would it make sense to scale like 5 to 6 further on the x axis?

134
00:14:56,070 --> 00:14:59,490
You could have. But then how do you scale one and three? Because we don't really know.

135
00:14:59,610 --> 00:15:03,060
What I did was I just drew a line set invalid. Okay.

136
00:15:03,120 --> 00:15:07,919
You could do that, too. Again, you've got a visit down there.

137
00:15:07,920 --> 00:15:13,469
And as long as it doesn't say time, right? As long as you tell me those are the visits, that helps me a little bit.

138
00:15:13,470 --> 00:15:18,360
But that's a really good point. Right. It looks like three, four or five and six four space are equally and they weren't.

139
00:15:18,720 --> 00:15:23,370
You could contrive our two space things better. Yeah.

140
00:15:24,150 --> 00:15:27,750
This is the challenge with these datasets. So.

141
00:15:32,190 --> 00:15:38,670
However, what what should the time variable be? Should it be the visit number or should it be the number of weeks?

142
00:15:41,350 --> 00:15:47,830
If you want to use number of weeks, we don't know right between visit one and visit three.

143
00:15:48,490 --> 00:15:55,630
You can make something up. You can take lots of different values and average over them and try to be really clever.

144
00:15:57,550 --> 00:16:05,020
I probably would go and visit. I would probably go with visit number in these data simply because I don't know all of the timing.

145
00:16:06,640 --> 00:16:11,469
However, whenever you use visit number, as you just said, remember that when you fit a line,

146
00:16:11,470 --> 00:16:16,750
then you are saying that each visit is equally spaced from each other over time.

147
00:16:19,080 --> 00:16:28,400
Can't just do with that assumption as you will. You might want to stick with categorical time, categorical visit number as your time variable you.

148
00:16:34,260 --> 00:16:39,360
Again, if you have big plans this weekend and you already fit time a certain way for Homer too.

149
00:16:39,960 --> 00:16:43,590
I am not telling you to go kill yourself, fix everything.

150
00:16:44,760 --> 00:16:49,950
Right. And just as we're all discussing here, everybody, everybody's doing this semester.

151
00:16:49,950 --> 00:16:56,790
So we're all learning here. Don't undo everything on Sunday or Monday night.

152
00:16:57,990 --> 00:17:00,479
Number seven, grip strength rights.

153
00:17:00,480 --> 00:17:13,920
We've got clinical trial, two treatments, two groups, grip strength, one, two and three visits after initiation of treatment at baseline.

154
00:17:13,920 --> 00:17:18,209
Okay, so baseline one, two and three. We don't know when those visits.

155
00:17:18,210 --> 00:17:23,680
Where do we. Isn't that cool? So what are we going to do here?

156
00:17:23,890 --> 00:17:28,230
So there are the visits. This is what I saw in homework assignments.

157
00:17:32,350 --> 00:17:35,920
Yes. They're going to really matter. What we do with these data doesn't matter how we model things.

158
00:17:37,000 --> 00:17:40,120
There isn't much going on. Maybe there's some increases over time.

159
00:17:41,080 --> 00:17:47,380
It's pretty small and there obviously is no difference between the groups nor their slopes.

160
00:17:47,470 --> 00:17:51,630
So again, categorical is fine.

161
00:17:51,640 --> 00:17:55,570
I have no problem with fitting a linearity through visit as a continuous variable.

162
00:17:56,380 --> 00:18:02,770
It's one parameter again assuming that the visits were equally spaced.

163
00:18:03,880 --> 00:18:07,660
Then treating it as a continuous variable seems perfectly plausible.

164
00:18:08,830 --> 00:18:11,680
But again, a very not a not a very exciting dataset to analyze.

165
00:18:12,980 --> 00:18:17,330
I don't know what the correlation looks like, but there's not a lot going on in this study.

166
00:18:19,400 --> 00:18:22,460
1991. 1991.

167
00:18:23,120 --> 00:18:27,440
What is it? Wow. So maybe I shouldn't use that data.

168
00:18:27,830 --> 00:18:33,560
All right. Oh, longitudinal measures and data set number eight.

169
00:18:34,370 --> 00:18:40,670
Oh, here's the good one. All right. Those of you who got this dataset, I can't wait to see what we've come up with over the semester.

170
00:18:41,540 --> 00:18:50,660
Three outcomes measured on a set of 41. So a small, small sample, 41 folks, 21 experienced panic attacks and 20 of them do not.

171
00:18:51,750 --> 00:18:56,360
And so this is not a randomized study and this is observational, right. They didn't randomize panic attacks.

172
00:18:57,230 --> 00:19:00,870
The first outcome is the score on an anxiety scale.

173
00:19:01,610 --> 00:19:05,750
Other outcomes. So I ask them to focus on pulse rate. Pulse was the outcome.

174
00:19:05,990 --> 00:19:16,610
Okay. So they measured someone's pulse at several points in time for six, eight, ten, 11 minutes after the study started.

175
00:19:17,330 --> 00:19:24,470
The challenge of is that sense is that they were intervened upon in different ways over time.

176
00:19:26,250 --> 00:19:33,870
So they were spoken to about the type of the topic of anxiety at six, eight and 10 minutes.

177
00:19:34,560 --> 00:19:41,220
They were asked to hyperventilate again, asking them to hyperventilate to see how it affects everything about them, their anxiety, their pulse.

178
00:19:41,790 --> 00:19:45,719
At 16, 17, 18 and all the other chimes were sort of a washout, period.

179
00:19:45,720 --> 00:19:50,670
We call it clinical trials, a moment of rest to try and undo what was just done to the person.

180
00:19:51,120 --> 00:19:56,400
So they're resting. Then there then people talk to them about what's causing the anxiety.

181
00:19:57,210 --> 00:20:01,970
Then they're allowed to rest. Then the rest of hyperventilate. And then they're told to rest.

182
00:20:06,630 --> 00:20:13,540
Look at the fact. What are we going to do here?

183
00:20:15,020 --> 00:20:19,820
We've got two groups. We have quite a few time points.

184
00:20:21,700 --> 00:20:26,620
And not a lot of people think and I did not intend this. This is a great anxiety.

185
00:20:26,620 --> 00:20:34,719
They told someone last year this would be a great final exam dataset, but we're going to play with it now with 41 individuals and two groups.

186
00:20:34,720 --> 00:20:40,150
That's not a lot of data to try and estimate a lot of things. So we've got a lot of side plates.

187
00:20:40,150 --> 00:20:43,690
One, two, three, four, five, six, seven, eight, nine, ten, 11 time points.

188
00:20:44,320 --> 00:20:47,500
So categories that would be ten dummy variables already.

189
00:20:48,730 --> 00:20:53,240
And if you want to do interactions. That's a lot.

190
00:20:53,570 --> 00:21:00,140
That's a problem. So how do we allocate time in of this model?

191
00:21:04,970 --> 00:21:16,100
Any little creative things we can do here? Yeah, some of this makes any sense, but if you have a bunch of wash out times, why don't you get rid of it?

192
00:21:16,340 --> 00:21:25,740
Get rid of the wash out times and then do categorical. So you would keep in talking about times and hyperventilation times.

193
00:21:26,780 --> 00:21:29,880
Get rid of the wash out so that we can.

194
00:21:32,610 --> 00:21:36,930
All right. So we're talking about time six, eight and ten, 16, 17 and 18.

195
00:21:38,310 --> 00:21:41,340
So this period, I guess you could see it in the plot.

196
00:21:41,340 --> 00:21:48,510
What we're talking about, we're talking about this period of time here, in this period of time here, and then as categorical time.

197
00:21:48,510 --> 00:21:58,440
So you're saying would you put all those times into one category like you would have a a speaking time in a hyperventilation time?

198
00:21:58,800 --> 00:22:03,240
You just have two categories. Okay. I have no problem with that.

199
00:22:04,440 --> 00:22:08,430
I would go and think further and why wouldn't I have three time categories?

200
00:22:08,730 --> 00:22:11,750
Rest talking and anxiety. Okay.

201
00:22:13,750 --> 00:22:22,390
I like that better. Both of those ideas. Better than fitting some spleen or some crazy curves through these data.

202
00:22:23,620 --> 00:22:28,449
Because I don't think the time isn't due to time, that the bumps are not due to time.

203
00:22:28,450 --> 00:22:32,020
They're due to changes and state of what's going on for these folks.

204
00:22:33,370 --> 00:22:36,750
So this is a I would never designed this study like this.

205
00:22:36,760 --> 00:22:43,110
I guess they're trying to get as much as they can out of one individual who has panic attacks by again,

206
00:22:43,420 --> 00:22:49,510
this is sort of a crime, what we call a crossover study. They got one treatment, which is the talking about the anxiety.

207
00:22:50,020 --> 00:22:54,500
Then they had a crossover and then they had another treatment, which is hyperventilation.

208
00:22:54,520 --> 00:22:57,550
Usually we try to randomize that. So there isn't an ordering effect.

209
00:22:58,000 --> 00:23:02,800
Right. Did the do the talking about it somehow influence the hyperventilating later?

210
00:23:03,460 --> 00:23:06,880
We don't know because everybody had the same orders anyway.

211
00:23:07,840 --> 00:23:11,000
So that's a this is a cool data set.

212
00:23:12,130 --> 00:23:15,520
There's got to be some clever categorization of time here.

213
00:23:17,830 --> 00:23:21,370
Right. And then? Then by then, it's not too bad.

214
00:23:23,860 --> 00:23:27,640
And probably not too huge of a group difference.

215
00:23:27,660 --> 00:23:30,520
Right. We see the two groups basically going in the same pattern.

216
00:23:32,030 --> 00:23:40,750
So maybe there's a baseline difference there from people who have panic attacks, perhaps started a higher pulse than others who don't.

217
00:23:42,880 --> 00:23:49,330
Otherwise, it's not too much of a treatment group or I shouldn't call treatment groups is to introduce groups.

218
00:23:50,290 --> 00:23:55,540
But again, time's going to have to be some sort of category here. And it certainly does should not.

219
00:23:56,240 --> 00:24:03,490
You again, I guess you could you could try and fit every category of type and see what happens.

220
00:24:03,730 --> 00:24:08,870
But again, that's a lot of variables. I don't think it's even going to fit you.

221
00:24:09,460 --> 00:24:14,710
What is that? Let's do that. One, two, three, four, five, six, seven, eight, nine, ten, 11.

222
00:24:14,710 --> 00:24:17,920
Did I say 11, ten points? So intercept ten dummy variables.

223
00:24:17,920 --> 00:24:23,979
That's 11 parameters. Group is 12 group times time it's like 20.

224
00:24:23,980 --> 00:24:27,160
Some variables will be smaller than ten.

225
00:24:28,450 --> 00:24:31,780
But not by much. So they've got to try that.

226
00:24:32,000 --> 00:24:35,750
Yeah. Okay. Good. Don't.

227
00:24:39,050 --> 00:24:50,120
Number nine plaque. So two folks, a dental clinical trial, 109 folks screamed for a minimum of 20 something subjects,

228
00:24:50,120 --> 00:24:54,469
were randomized to one of two new mouth rinses or to a controls.

229
00:24:54,470 --> 00:25:02,640
So three groups. And during the study, they used their assigned mouth rats twice a day for twice a day for six months.

230
00:25:02,720 --> 00:25:06,800
Wow. Would you use a mouth? Rinse twice a day, every day for six months?

231
00:25:07,310 --> 00:25:11,600
No. Come on. Anyway, that's a hard study to write.

232
00:25:12,380 --> 00:25:16,220
The plaque was scored at baseline three months and six months. Only three time points.

233
00:25:16,520 --> 00:25:24,050
Not a lot of follow up time wise, using the score of plaque despite severity higher scores indicator.

234
00:25:24,080 --> 00:25:29,720
Right. So again, if mouth rinses are good, we should see these plaque scores lower.

235
00:25:29,840 --> 00:25:32,900
The severity of plaque. Bingo.

236
00:25:36,840 --> 00:25:40,860
There is what we see. Again, it was randomized. We had a baseline measure we should see.

237
00:25:40,860 --> 00:25:46,030
The baseline means being pretty close to each other because everybody started out and it's right.

238
00:25:46,120 --> 00:25:48,240
We controlled for everything else by randomizing.

239
00:25:49,110 --> 00:25:55,400
We didn't see a drop at three months and a drop at six months, and I don't see why you wouldn't set categorical time here.

240
00:25:55,410 --> 00:25:59,370
It's only two variables pretty straightforward versus one.

241
00:25:59,550 --> 00:26:08,420
You could find a slope, I suppose. Fascinating how klatch goes down with that control.

242
00:26:10,830 --> 00:26:14,520
After three months, people using a control mouse rents had less plaque.

243
00:26:15,810 --> 00:26:20,040
What do we call that? Something in clinical trials.

244
00:26:20,840 --> 00:26:29,280
It's the placebo effect. When folks know they're on a clinical trial, they behave differently than they do when they're not on a clinical trial.

245
00:26:30,020 --> 00:26:34,290
Right. You're going to have you be in a clinical trial for weight loss.

246
00:26:35,660 --> 00:26:38,900
You're probably going to exercise a little bit more, right?

247
00:26:39,140 --> 00:26:44,510
You're probably going to think a little bit more about what you're doing regardless of what treatment RV you're in.

248
00:26:45,110 --> 00:26:48,080
So it's fascinating that there was the step at three months.

249
00:26:49,400 --> 00:26:56,360
Again, if I were in a study, a study looking at my teeth, I'd brush my teeth more often, regardless of the mouth rinse.

250
00:26:56,600 --> 00:27:00,410
I mean, I do breath. You should brush your teeth twice a day, every day of your life.

251
00:27:01,820 --> 00:27:08,460
If you're not, start doing it again.

252
00:27:09,170 --> 00:27:17,190
Not a difference at baseline for the group, but certainly there should be a there is a difference between the placebo and the two treatment arms.

253
00:27:17,210 --> 00:27:23,240
I don't know if it's significant or not statistically significant, but I would expect any model you sit here,

254
00:27:23,240 --> 00:27:31,100
regardless of the correlation structure, you would hopefully see that the mouth, for instance, might have an effect behind that of the placebo.

255
00:27:32,450 --> 00:27:36,080
But this is why we also this is why it's really important in trials to have a control arm.

256
00:27:38,710 --> 00:27:44,200
The two mouth rinses don't look nearly as good compared to the control as if that blue line were gone.

257
00:27:47,060 --> 00:27:50,960
So controls are important. Always tell your investigators controls are important.

258
00:27:52,580 --> 00:28:01,760
All right. This I'll call it twisters, for lack of a better pronunciation of the Toronto Western Spasmodic tour across rating scale.

259
00:28:02,810 --> 00:28:10,820
So these are individuals with cervical dystonia. Higher scores of this scale indicate worse symptoms.

260
00:28:11,300 --> 00:28:15,530
And we're giving folks the trigger.

261
00:28:15,530 --> 00:28:23,090
It's really a placebo. 500 units of botulism toxin, botox, or 10,000 units of Botox.

262
00:28:25,100 --> 00:28:34,070
Botox has shown to have incredible benefits for things around muscles, muscle spasms.

263
00:28:35,210 --> 00:28:39,290
I worked in a clinical trial for many years for women with urinary incontinence.

264
00:28:41,570 --> 00:28:47,480
Botulism tightens up a lot of things, and it's very effective for not just for facial muscles.

265
00:28:48,140 --> 00:28:56,000
So anyway, but this was for cervical dystonia. Again, another problem that hopefully some botulism toxin actually helps to resolve.

266
00:28:56,480 --> 00:29:02,570
So three groups measured with an outcome at baseline and then five more time points.

267
00:29:02,570 --> 00:29:12,390
And you were told those time points, they weren't equally spaced, 3 to 4, 8 to 12 and 16 to 20 4 to 29.

268
00:29:12,860 --> 00:29:18,140
And so this is kind of cool. So here are the three groups.

269
00:29:18,890 --> 00:29:25,450
So. And how often do they do that is treated once.

270
00:29:30,710 --> 00:29:34,030
Anyway, here we are baseline pretty much.

271
00:29:34,040 --> 00:29:37,520
You know, if we put senators around these things, they probably overlap.

272
00:29:38,930 --> 00:29:44,659
We quickly see that women shouldn't say women and go with in to treatment

273
00:29:44,660 --> 00:29:50,180
groups red and green there and we see a quick benefit and severity goes down.

274
00:29:52,460 --> 00:30:02,180
But disappointingly, eventually they have a higher mean score than seem, or at least yet both of them do.

275
00:30:02,810 --> 00:30:12,790
So how do we muddle time here? Any thoughts on your work?

276
00:30:12,870 --> 00:30:13,920
You could do piecewise linear.

277
00:30:14,280 --> 00:30:19,649
You could say, Well, I think after the first point there's a knot maybe, or after the first two time points, I don't know where they're not.

278
00:30:19,650 --> 00:30:23,550
Again, you got to use your own judgment as to where you want to put that, not for the change point.

279
00:30:24,630 --> 00:30:27,750
You could have, I think two lines, right, going down and then going up.

280
00:30:28,620 --> 00:30:36,989
And that model would allow you to say if, in fact, there was a quick benefit of the treatment and then maybe and then again,

281
00:30:36,990 --> 00:30:42,720
if the slopes are different after the match, it actually tells us that the benefit is going away.

282
00:30:43,170 --> 00:30:47,549
So that's a nicely interpretable model. I think more than again, you could fit curvature.

283
00:30:47,550 --> 00:30:52,590
You could say, I'm just going to fit time and time squared. And that's harder for me to interpret.

284
00:30:52,620 --> 00:30:57,840
How do you explain to an investigator that things are significant or not?

285
00:30:58,050 --> 00:31:04,010
When you have a squared term in there so you could treat categorical.

286
00:31:04,080 --> 00:31:09,870
I think we probably have enough data here. I don't know how many individuals.

287
00:31:10,200 --> 00:31:18,810
37, 36, 36. Yeah. You could probably figure out a verbal here as well and the vocabulary dataset.

288
00:31:19,510 --> 00:31:24,510
Right. So here we're looking at boys and girls, old time, 1975.

289
00:31:26,970 --> 00:31:33,420
That's almost 50. Can you believe it? That's almost 50 years old. All right, test it out.

290
00:31:33,420 --> 00:31:38,700
A vocabulary section. Oh, God. Do you want to look at vocabulary of boys and girls?

291
00:31:41,370 --> 00:31:42,730
Because girls. Come on, come on.

292
00:31:42,780 --> 00:31:47,550
They complete their physical girls earlier than boys, he said to prevent that, trying to determine whether the same is true for them.

293
00:31:48,360 --> 00:31:55,409
Okay. So we've measured these kids at grades eight, nine, ten and 11.

294
00:31:55,410 --> 00:32:00,719
So 14 points. Again, we're going to treat grade as time.

295
00:32:00,720 --> 00:32:11,010
I guess it's sort of a visit, but it's about they probably spend about a year apart and this is what we see.

296
00:32:14,700 --> 00:32:18,600
So not much going on there no matter what you try to do here.

297
00:32:20,160 --> 00:32:23,490
I think we have a variable. The time slope is going to be sufficient.

298
00:32:24,360 --> 00:32:29,640
You can treat as categorical if you want. And probably no interaction.

299
00:32:29,910 --> 00:32:36,180
I don't see any difference there. White p values like point eight or nine year.

300
00:32:36,870 --> 00:32:38,130
It's really nothing going on there.

301
00:32:41,240 --> 00:32:49,800
So handful of data sets and a handful, maybe two or three, where I think a single linear component of time is sufficient.

302
00:32:49,820 --> 00:32:53,510
All the others, I think, require categorical or something else.

303
00:32:55,290 --> 00:33:04,980
So. And once we have decided upon that, now we've got to decide what the best correlation structure is to give us the best inference.

304
00:33:05,670 --> 00:33:12,990
All right. The questions. Yes, but I was just there.

305
00:33:13,050 --> 00:33:25,200
Usually up there. What's happening? I was curious if there was anything else about either the organization or component of reports like mine.

306
00:33:27,690 --> 00:33:31,950
Good question. So he was pretty generous with the grading.

307
00:33:35,580 --> 00:33:40,110
I really despise our output as the report.

308
00:33:42,180 --> 00:33:47,700
They don't like to see a coefficient table with asterisks and ten decimal points as my table in a report.

309
00:33:49,050 --> 00:33:53,910
And I would tell you to get rid of that now because if you don't now at six nine, you're going to hear it again.

310
00:33:56,610 --> 00:34:00,809
I know it takes a little bit of time to take numbers out of our head,

311
00:34:00,810 --> 00:34:05,850
put them into a nice table, but get into the habit of not having six decimal places.

312
00:34:07,050 --> 00:34:12,030
I rarely need more than two decimal places in any study that I've ever been in or something.

313
00:34:13,230 --> 00:34:17,160
So stay away from the cutting and pasting from R into into a document.

314
00:34:17,190 --> 00:34:23,040
I know you guys like to use our markdown and other sorts of things, but you got to fix the decimal places.

315
00:34:26,370 --> 00:34:27,990
I like words over numbers.

316
00:34:29,280 --> 00:34:35,520
If English isn't your first language and you don't want to write a paragraph about what you see, start practicing that skill.

317
00:34:36,710 --> 00:34:44,390
You're going to need it next semester. You're going to need it in your life if you continue to work with that group that uses English.

318
00:34:48,180 --> 00:34:51,630
Someone told me and again, I haven't always done this in this class enough.

319
00:34:53,880 --> 00:35:00,960
Describing your results as not just giving a p value. Describing your results as telling me what the effect size is right.

320
00:35:01,140 --> 00:35:09,560
The two groups have a mean difference of two to the p value of whenever I focus on more than just p values.

321
00:35:09,570 --> 00:35:13,480
Right. You can get a p value less than .05 for a very, very tiny coefficient.

322
00:35:13,500 --> 00:35:16,800
If you have enough data. Right. Is that clinically useful?

323
00:35:18,360 --> 00:35:24,630
Um. Most individuals not know what to do with t statistics and statistics and chi square statistics.

324
00:35:25,140 --> 00:35:31,290
Right. If I go to one of my best collaborators and say I get a chi square of ten, you know, congratulations to.

325
00:35:31,980 --> 00:35:35,520
What am I supposed to do with that? Right. So don't.

326
00:35:35,520 --> 00:35:41,220
Don't give statistics. They're just used to look up a p value in a normal distribution.

327
00:35:41,580 --> 00:35:44,600
Right. So stay away from giving me statistics.

328
00:35:44,610 --> 00:35:50,850
I don't need those other things that you wonder you did.

329
00:35:54,630 --> 00:35:59,520
Again, we're at a point in our lives now in statistics where we're going to be more than number crunchers,

330
00:36:00,490 --> 00:36:08,219
that we're going to explain what our gave to us, what says gets to us so that other people can understand it.

331
00:36:08,220 --> 00:36:15,300
So they're going to know the homework assignment. And I'm not going to tell some hyung now to look for only paragraphs.

332
00:36:16,710 --> 00:36:18,240
We don't have enough time for that kind of stuff.

333
00:36:18,480 --> 00:36:24,180
But get into the habit of searching to explain your results with more than just cutting and pasting from our end.

334
00:36:24,180 --> 00:36:27,809
Giving me a p value. Plots are wonderful.

335
00:36:27,810 --> 00:36:30,930
I love plots. Plots are a great sport.

336
00:36:30,930 --> 00:36:39,150
Everything we do should support everything we do. There was no everybody tried to stay within the page limits.

337
00:36:39,840 --> 00:36:48,130
And I think we're able to again, I didn't go through all 55 of them in great detail, but yeah.

338
00:36:49,470 --> 00:36:53,310
So keep that in mind. Again, I have two goals for this semester.

339
00:36:53,320 --> 00:36:57,390
One is to teach you this material, and two is to get you ready for 699.

340
00:36:58,430 --> 00:37:04,550
It's kind of trossachs and exciting. Not enough times that we go, Wow, we didn't realize that.

341
00:37:04,910 --> 00:37:09,640
They didn't learn that in six 5050. I don't know how to explain stuff in report.

342
00:37:09,660 --> 00:37:15,830
So over the course of six homeworks you should be so darn ready for 699.

343
00:37:18,470 --> 00:37:18,980
All right.

344
00:37:19,640 --> 00:37:28,760
You know, as I'm developing this course, this was my cool idea this year was to give people the same data sets, but, you know, randomize you guys.

345
00:37:29,780 --> 00:37:34,160
I don't know how exciting it's going to be to analyze this data set the third time.

346
00:37:35,420 --> 00:37:40,310
This data set the third time and tell me once again that there's nothing going on.

347
00:37:40,850 --> 00:37:44,060
Maybe it's more interesting for you guys to have different datasets, right?

348
00:37:45,680 --> 00:37:49,010
So anyway, but this is where we're at right now, too.

349
00:37:49,560 --> 00:37:52,850
You're going to see of the two boring ones and I apologize.

350
00:37:56,930 --> 00:38:02,180
But remember, we don't always get significant results in the things that we do.

351
00:38:03,130 --> 00:38:12,710
I don't always want to think that every homework assignment should produce a small P value where we can see 10 to 3.

352
00:38:12,740 --> 00:38:18,350
All right. I want to show you what I was doing with homework two with my dataset.

353
00:38:20,660 --> 00:38:27,650
Because if you are not using continuous time, I mean, I'm going to show you how many is a piecewise linear.

354
00:38:29,550 --> 00:38:32,600
I was trying to figure out like, how do you take things in and out of the model?

355
00:38:32,610 --> 00:38:36,030
Should I have the interaction and so forth?

356
00:38:36,240 --> 00:38:41,250
And I discovered that it takes a little more thought than just throwing a homework assignment at you.

357
00:38:42,210 --> 00:38:52,140
Let's start over. So my homework.

358
00:38:52,590 --> 00:38:56,040
Number two more. No stimulation. Oh, this is all the simulation stuff.

359
00:38:57,840 --> 00:39:00,930
Oh, no, we're not there. We're going to.

360
00:39:02,470 --> 00:39:07,360
Here we go. All right. So I have this dataset called Pippy.

361
00:39:08,980 --> 00:39:17,310
Make the while long wait in line for I'm going to make the plot like I just showed you guys for everybody here.

362
00:39:17,800 --> 00:39:21,100
It's not very good, is it? There you go.

363
00:39:29,120 --> 00:39:35,750
Oh. There we go.

364
00:39:36,950 --> 00:39:41,300
So again, we had three groups of individuals measuring the spike value in the plasma,

365
00:39:42,260 --> 00:39:48,140
two hyperinsulinemia, one was non hyperinsulinemia and one was hyperinsulinemia.

366
00:39:48,230 --> 00:39:52,070
And we had a control group and we measured them over time and we saw this distinct

367
00:39:52,640 --> 00:39:56,660
drop in all three groups and then they just sort of came together after 2 hours.

368
00:39:57,470 --> 00:40:06,650
And so I probably want to treat time as categorical, but what I'm going to do right now and I could again,

369
00:40:06,650 --> 00:40:13,280
I could use AIC for two different models, categorical versus piecewise linear, and see which one has a lower FC.

370
00:40:14,660 --> 00:40:19,309
I'm going to stick with piecewise linear again.

371
00:40:19,310 --> 00:40:25,670
Purpose of my doing this show is to show you some of the things I learn as I download these libraries.

372
00:40:26,360 --> 00:40:31,430
There's all kinds of stuff in these libraries that we won't use, but some of it's useful.

373
00:40:32,590 --> 00:40:36,460
So it's a two model time and he has a piecewise linear model with.

374
00:40:38,600 --> 00:40:43,979
And not at 2 hours. So that means my first time variable.

375
00:40:43,980 --> 00:40:56,000
It's as time as it is. And then the second time variable takes time minus two and then zeros it out for anybody less than two.

376
00:40:57,440 --> 00:41:03,080
So this overall slope over right at four folks for the time points down to.

377
00:41:04,210 --> 00:41:12,420
First thing I did is assume independence, right? So I'm going to say, oh well yes, that's how I'm function and ah, I've got a group variable,

378
00:41:12,880 --> 00:41:18,310
a time variable with these two piecewise linear pieces and the interaction.

379
00:41:20,190 --> 00:41:24,240
It's a decent number of coefficients and I got a nice big error because I didn't run that.

380
00:41:25,670 --> 00:41:28,960
Here we go. So there's my first model meeting.

381
00:41:33,600 --> 00:41:44,900
What have we got? Correct.

382
00:41:46,700 --> 00:41:51,770
Again, I have given interpreting the group's variable.

383
00:41:51,800 --> 00:41:57,920
Remember, be careful. Interpreting the group in the time variable strictly is not possible because there's an interaction.

384
00:41:59,090 --> 00:42:05,390
I shouldn't say that. It's they all go together, right? So the intercept is the placebo group at time zero.

385
00:42:06,020 --> 00:42:09,800
And then we have the differences for the other two groups at time zero.

386
00:42:10,850 --> 00:42:16,429
And then we have this first piecewise slope for the placebo group, and then the second one.

387
00:42:16,430 --> 00:42:18,800
So it's going down and then it's going up.

388
00:42:19,640 --> 00:42:25,250
And then we have the components for the other two groups that we add on to what we see for the placebo group.

389
00:42:27,290 --> 00:42:31,609
An inference here yet is not quite all of these standard errors we know are probably

390
00:42:31,610 --> 00:42:36,589
wrong because I've assumed independence and probably shouldn't be using these.

391
00:42:36,590 --> 00:42:42,080
Right. So then I need a sandwich. Standard error if I don't fit the correlation structure.

392
00:42:42,530 --> 00:42:47,090
One approach to this just to fix is to go with these estimates as they are.

393
00:42:48,210 --> 00:42:51,330
And you know, I discussed this with someone after class on Monday.

394
00:42:52,140 --> 00:42:57,150
I have told you that coefficient estimates are unbiased regardless of the correlation structure.

395
00:42:57,840 --> 00:43:03,220
That doesn't mean you always get the same coefficient estimates regardless of the correlation structure.

396
00:43:03,240 --> 00:43:08,460
Remember, the correlation structure goes into the estimation. It's x transpose sigma inverse.

397
00:43:10,340 --> 00:43:18,930
They all come from a distribution that has been better, but you don't know where in that distribution those coefficients are going to be, right?

398
00:43:19,460 --> 00:43:23,300
So I'm not telling you that every approach will give you the same coefficients.

399
00:43:23,690 --> 00:43:29,670
I'm just telling you they're unbiased. And this is why these theories we have, they're great, right?

400
00:43:29,720 --> 00:43:36,590
I have a method that leads to an unbiased estimate, but from my one dataset, I don't know how far they are from data.

401
00:43:37,100 --> 00:43:42,170
That's why we like smaller and smaller standard errors. Right? So that I can be more more confident, close to the truth.

402
00:43:42,770 --> 00:43:46,160
So these I could go with these I want to fix.

403
00:43:47,540 --> 00:43:54,000
And I do that by loops by sitting there with standard error again.

404
00:43:54,620 --> 00:43:59,630
Let's break this function down. It's V code C our.

405
00:44:02,890 --> 00:44:12,110
It is so so from my model and tell it where the clustering lies.

406
00:44:12,120 --> 00:44:16,560
Where does the correlation lie? And again, there are lots of different types of sandwich estimates.

407
00:44:16,590 --> 00:44:20,910
We're going to start with the basic one. Remember, this is a matrix.

408
00:44:22,680 --> 00:44:29,050
I have all of these. We just. Like Windows.

409
00:44:31,170 --> 00:44:35,700
It's not true. Oh, there we go.

410
00:44:36,660 --> 00:44:42,989
Right. So this produces a matrix of one, two, three, four, five, six, seven, eight, nine.

411
00:44:42,990 --> 00:44:46,800
By nine. A nine by nine matrix.

412
00:44:48,000 --> 00:44:53,580
Of those variances along the diagonal, as well as correlation of the parameters.

413
00:44:55,060 --> 00:45:03,340
Right. And I'm going to take the diagonal of that matrix, because that gives me simply the the variance of each parameter.

414
00:45:03,340 --> 00:45:08,410
And then I'll take the square root to get the standard errors. And so that's where all of that is.

415
00:45:12,290 --> 00:45:19,460
Now you could take the coefficients from your independence model.

416
00:45:19,940 --> 00:45:23,960
Divide them by your sandwich. Standard errors, get 80 statistic.

417
00:45:24,500 --> 00:45:29,120
Look that up in a normal distribution or a T with a certain degrees of freedom if you want to do all that.

418
00:45:31,000 --> 00:45:37,180
The library with sandwich generators allows you to do that all with a function called Celia test.

419
00:45:37,990 --> 00:45:42,729
So all Celia test really does is it takes the original coefficient table and it

420
00:45:42,730 --> 00:45:45,970
slaps out of the sandwich standard errors for the model best that you didn't like.

421
00:45:47,320 --> 00:46:00,700
And so we a test of this model correlation structure, sandwich variance estimate, our method clustering.

422
00:46:02,320 --> 00:46:05,610
And then again, there are lots of different tests. Right.

423
00:46:05,620 --> 00:46:11,140
The degrees of freedom I told you about, the degrees of freedom are really solid in these sorts of models.

424
00:46:12,010 --> 00:46:18,370
So all you're doing is telling this computer what the null distribution is to get the p value for you.

425
00:46:20,380 --> 00:46:29,320
And I just stick with a naive t test because I'm naive. And let's see if I can shorten that.

426
00:46:34,350 --> 00:46:39,320
Joining us. Shoot.

427
00:46:51,350 --> 00:46:55,800
They didn't like that. They just went for shaving.

428
00:46:56,730 --> 00:47:01,440
Well, let's. Which.

429
00:47:06,860 --> 00:47:14,409
Right. So again well it did was gives me the same coefficient estimates that changed the senators and it gave

430
00:47:14,410 --> 00:47:21,410
me a statistic and then look those key statistics up in a distribution and there are two values.

431
00:47:21,430 --> 00:47:31,110
Again, if I did this that I'll be next to each other. So correcting for the correlation through a standard through a sandwich variance estimate.

432
00:47:32,440 --> 00:47:37,780
What I see is that the for correlation terms, those are very interesting for interaction terms.

433
00:47:38,740 --> 00:47:46,420
They don't appear to have much significance in this model after I control for the time piecewise and the group

434
00:47:46,420 --> 00:47:54,940
at baseline so I may consider again my description of these data might be that there are baseline differences,

435
00:47:55,690 --> 00:47:59,050
there are changes over time it goes down and then it goes up.

436
00:48:00,280 --> 00:48:09,040
But that that pattern is the same among all three groups. There is a significant interaction based upon a sandwich variance estimate.

437
00:48:09,850 --> 00:48:18,580
And then if I wanted to keep track of some stuff, so I kept track of the coefficients from my model and the standard errors.

438
00:48:19,120 --> 00:48:24,489
I also got the AIC and BSC for this model as well as the R-squared value,

439
00:48:24,490 --> 00:48:32,320
which is a default in the lemma because ordinarily squares R-squared as is already defined for us.

440
00:48:37,470 --> 00:48:40,560
And then I decided to put it on one big table. We'll look at that in a second.

441
00:48:40,980 --> 00:48:48,600
So that's model one. Now, I want to get an idea maybe as to whether I should consider modeling the correlation structure.

442
00:48:49,270 --> 00:48:54,000
Again, the residuals should help me figure that out. That was inside of the sandwich variance estimate.

443
00:48:54,750 --> 00:48:58,320
But I can also do it this way. I can collect the residuals from the model.

444
00:48:59,880 --> 00:49:03,180
Again, UAD means get the unique IDs within the dataset.

445
00:49:05,280 --> 00:49:13,110
The largest number of observations in each group essentially. And try to keep each person's residuals and combining them into this the sigma matrix.

446
00:49:13,710 --> 00:49:17,280
So again, you don't need to do this. I'm just showing you things.

447
00:49:17,280 --> 00:49:21,929
You can adopt yourself should you wish. This is what we did with the leg.

448
00:49:21,930 --> 00:49:26,460
One in the leg. Two correlations. Fight group. Just to add something a little bit different here.

449
00:49:26,610 --> 00:49:32,130
And for each each person, I'm getting how many observations they had.

450
00:49:34,070 --> 00:49:37,610
The residuals. And then again, I'm doing the transpose.

451
00:49:40,150 --> 00:49:44,320
And then just counting them all together over each person.

452
00:49:44,620 --> 00:49:47,680
So let me start over here with her for good.

453
00:49:49,110 --> 00:49:54,629
All right. All right. So I've created a correlation matrix, and this is a really useful function.

454
00:49:54,630 --> 00:50:01,050
If you haven't seen it an hour, it takes a covariance matrix and turns it into a correlation matrix, which I know you can all do.

455
00:50:01,920 --> 00:50:08,880
Right. We know how corporate governance and correlation are related to other, but there's a function for us just lazy.

456
00:50:09,720 --> 00:50:16,710
And there we go. So that's what the correlation structure looks like as the residuals right over time.

457
00:50:18,660 --> 00:50:24,580
So what do we do with that? You know, maybe it's stronger and then it goes away.

458
00:50:24,610 --> 00:50:29,229
Maybe one day, you know, maybe that first one's overly estimated.

459
00:50:29,230 --> 00:50:30,760
Maybe it's pretty constant.

460
00:50:33,220 --> 00:50:40,240
So I don't really have a clear idea as to whether exchangeable our air one is a better way to go here, at least looking at this.

461
00:50:41,140 --> 00:50:44,860
So let's try let's do the exchangeable correlation approach.

462
00:50:45,400 --> 00:50:54,190
So now the function is GLS. It looks just like the syntax in Elm, except you need to specify a correlation structure.

463
00:50:55,460 --> 00:51:05,020
And so again, this is the syntax in our correlation compound, the symmetric form tilde one within it it is the clustering variable.

464
00:51:05,590 --> 00:51:10,719
And as some folks show me in class and I went to the Internet and checked artisan care with Tilde,

465
00:51:10,720 --> 00:51:14,020
one is it could be tilde s, it could be tilde tomorrow.

466
00:51:14,950 --> 00:51:18,670
It ignores that. But Tilde one just means intercept, you know.

467
00:51:19,180 --> 00:51:30,310
So anyways, that gives me a compound symmetric correlation structure clustering observations by ID and way it goes.

468
00:51:32,290 --> 00:51:37,840
We can get a sandwich standard error beyond that model based shared error should we wish.

469
00:51:39,190 --> 00:51:45,460
You just want to make that point clear. Again, if you believe your model, if you believe council compound symmetry is a good way to go.

470
00:51:45,760 --> 00:51:49,270
So don't do a sandwich editor. Go with your model and be done.

471
00:51:49,840 --> 00:51:52,870
But we can we can do a sandwich, Senator, for that one as well.

472
00:51:54,690 --> 00:52:00,840
I'm going to get the coefficient estimates, the same with senators and then the AC ABC for that model.

473
00:52:01,710 --> 00:52:08,490
There is no R-squared that I'm aware of in girls. I couldn't find one as I was developing this class.

474
00:52:09,150 --> 00:52:18,540
So I'm giving you the code that I use to get an R-squared value. So again, I just use the equations that are in my slides.

475
00:52:21,360 --> 00:52:29,400
So again, I asked you what is a known model? We're trying to compare my model to a no model to get a value for how much variability is explained.

476
00:52:30,240 --> 00:52:35,130
And so the no model is simply above all else with an intercept and nothing else.

477
00:52:35,460 --> 00:52:39,930
I'm going to call that my no model. And then a number of observations.

478
00:52:39,930 --> 00:52:44,430
And then R-squared, as again, is simply based upon a comparison of log likelihoods.

479
00:52:44,430 --> 00:52:47,770
There's a function in our call log, like The Good Stuff.

480
00:52:47,820 --> 00:52:57,240
So there you have a formula for error squared for any challenge that you want to set up be if you want the correlation coefficient.

481
00:52:58,890 --> 00:53:10,140
It took me 5 hours, 4 hours trying to find someone on the Internet who showed folks how to get the correlation coefficient at a glance.

482
00:53:10,890 --> 00:53:13,950
So this code is gold, right?

483
00:53:15,120 --> 00:53:19,650
I don't know. This this was in no documentation that I ever found anywhere.

484
00:53:19,660 --> 00:53:26,070
But anyways, so there it is. If you want the actual roll coefficient and the exchangeable correlation matrix, there it is.

485
00:53:30,320 --> 00:53:34,250
Where do they stop? So let's fit the no model.

486
00:53:35,270 --> 00:53:40,639
There's no reservations. There are squares. So this model has an R-squared of 7.5.

487
00:53:40,640 --> 00:53:44,480
Sex, right. Goes from 0 to 1. There's.

488
00:53:45,130 --> 00:53:48,900
I called it row two for model number 202.

489
00:53:48,910 --> 00:53:55,590
So the correlation coefficient is 8.578 doesn't seem to off loops.

490
00:53:55,690 --> 00:54:00,190
I want to scroll there .578 doesn't seem so off from what I saw here.

491
00:54:01,600 --> 00:54:05,710
I feel like I'm doing the right thing in my code. At least get the right number.

492
00:54:13,690 --> 00:54:24,120
Which a tourism. Oh, rounds.

493
00:54:24,390 --> 00:54:29,380
These rounds. That's sigma.

494
00:54:29,390 --> 00:54:33,920
That's the variance. It that's the variance covariance matrix of observations in the same person.

495
00:54:35,110 --> 00:54:39,759
Right. Remember, there's correlation and there's covariance. And so that's the variance covariance matrix.

496
00:54:39,760 --> 00:54:44,799
And then if you want to turn that into a correlation matrix uses the C of E to see our covariance,

497
00:54:44,800 --> 00:54:49,570
to see to correlation, and we typically call that in our matrix.

498
00:54:53,440 --> 00:54:56,700
Right. You know what it was? I told you a rose was point five, seven, eight.

499
00:54:56,710 --> 00:55:00,760
So this is a diagonal of ones and points five, seven in the diagonals.

500
00:55:01,870 --> 00:55:07,570
So again, I knew what row was by looking at this, but I wanted it.

501
00:55:07,630 --> 00:55:15,220
I just wanted that one number and I wanted to keep it. And it was impossible to figure that out in all the documentation that I've had.

502
00:55:15,370 --> 00:55:20,500
All right. So now I've got a compound symmetry approach and I'm going to do the same thing with air one.

503
00:55:21,410 --> 00:55:28,600
Everything looks the same again, except now I tell it that I want an air one structure within person within it.

504
00:55:31,010 --> 00:55:34,070
Everything else is the same. I could get a sandwich. Standard error.

505
00:55:34,100 --> 00:55:42,110
Ask you to do that. Just to practice. Brazil results CBC And I'm going to get the R squared for that model, right?

506
00:55:42,140 --> 00:55:51,890
So I got a correlation coefficient 1.57 H on the case, I guess hopefully the case.

507
00:55:51,890 --> 00:55:56,890
Let's see what happens here. So what does the correlation matrix look like from an error one set here?

508
00:55:58,910 --> 00:56:02,660
Looks like that ticking over time.

509
00:56:06,910 --> 00:56:14,560
Okay. I don't know what model I'm going to go with quite yet. So I put the results from all three models into a comma separated file.

510
00:56:16,380 --> 00:56:25,610
So that really trumps. It's.

511
00:56:28,110 --> 00:56:38,520
So here is here are the coefficients standard errors, sandwich, standard errors, AC, the AC and the R squared from the independence OS model.

512
00:56:39,870 --> 00:56:45,390
Right. So AC in 551, let's use AC as a metric R squared 2.34.

513
00:56:46,770 --> 00:56:56,670
Here's my model. With compound symmetry, we see a higher R squared, we see a lower AC at 445.

514
00:56:58,720 --> 00:57:06,060
And we have an even lower AOC. And I better R-squared if I have the model with error.

515
00:57:06,060 --> 00:57:18,920
One error of correlation. So maybe I'll say AC by AC criteria, and I think that auto regressive seems to be the best approach.

516
00:57:19,910 --> 00:57:29,629
I saw it in the residuals, so forth. So if I then decide that this is my model than here I am right at the intercept.

517
00:57:29,630 --> 00:57:37,130
I've got coefficients for the difference between the groups that baseline the two piecewise linear and again I'm not seeing.

518
00:57:37,250 --> 00:57:41,780
I can look at these numbers. Right. We want usually a t statistic of two or higher.

519
00:57:41,810 --> 00:57:44,060
That's going to give me a p value of less than five.

520
00:57:44,630 --> 00:57:52,610
We have a normal distribution, so none of these are telling any story or that I have any significant differences between the groups over time.

521
00:57:58,560 --> 00:58:05,040
So then we have to start talking about I would like to get coefficient estimates if I want coefficient

522
00:58:05,040 --> 00:58:11,700
estimates and I don't think the interaction is important then I should take it out of the model altogether.

523
00:58:12,000 --> 00:58:17,070
So I'm going to see what I should do here. So the first thing I did just again,

524
00:58:17,310 --> 00:58:29,010
I am eyeballing a bunch of P values and I'm saying that all four of these coefficients right here don't have any signal,

525
00:58:29,130 --> 00:58:32,850
at least that I can see based upon the ratio of estimate to standard error.

526
00:58:34,020 --> 00:58:38,430
I can do a model comparison. I can compare a model with the interaction with and without.

527
00:58:38,860 --> 00:58:43,560
And so you get one p value, one p value for all four interaction terms and once.

528
00:58:45,220 --> 00:58:49,660
And so I'm going to use the function on Nova in our last meeting when.

529
00:58:52,680 --> 00:58:56,760
And once again, I'm doing marginal.

530
00:58:56,790 --> 00:59:05,340
I want to see how each of the affects in the model is significant with the others in the model, the marginal after controlling for the other things.

531
00:59:06,060 --> 00:59:12,150
And so not surprisingly, there is a group difference. As I said, the two time variables are significant,

532
00:59:13,080 --> 00:59:19,380
but I don't see any interaction for the first piecewise time component and I don't see any importance of the interaction in the sector.

533
00:59:19,980 --> 00:59:27,120
Sorry I went to p values. I keep forgetting there's two time variables here. So again, this is all telling me that these probably are irrelevant.

534
00:59:27,150 --> 00:59:33,210
So I'm going to remove the interaction terms and now I've got a model three B that just has group.

535
00:59:34,270 --> 00:59:38,700
Time, one and time to. And now I'm going to do one another and over.

536
00:59:40,960 --> 00:59:45,670
And I see that the group effect at baseline is just barely there.

537
00:59:46,360 --> 00:59:49,590
And we do see some important time effects going down and then going up.

538
00:59:49,720 --> 01:00:03,850
Right. If you decide to go with the sandwich standard error, how do you do this?

539
01:00:03,880 --> 01:00:07,030
How do you pick and choose variables to come in and out of models?

540
01:00:09,640 --> 01:00:19,330
And I just again want to reemphasize that the sandwich standard error code library has this function c0f test.

541
01:00:20,510 --> 01:00:26,510
Which is allowing you to test coefficients in the model using the sandwich centers instead of the model three as standard errors.

542
01:00:27,140 --> 01:00:31,340
So they did all that. And then I came up with my final model.

543
01:00:36,980 --> 01:00:40,930
Three P. I'm serious.

544
01:00:40,930 --> 01:00:42,220
And I should the.

545
01:00:57,660 --> 01:01:07,290
To get a coefficient table in l m it's called coefficients and you've probably noticed this, but in gls some jerk decided to call it Tambor.

546
01:01:09,780 --> 01:01:15,090
So every time I do this, I don't get the table. I want it really as a table.

547
01:01:15,300 --> 01:01:20,700
It's not a coefficient table. But anyway, so here's what I would propose as my final model.

548
01:01:20,760 --> 01:01:26,459
Right? It looks like there is a mean of four in the first group and that there is a

549
01:01:26,460 --> 01:01:30,360
slight increase in group two and a slightly larger increase for group three.

550
01:01:30,960 --> 01:01:38,570
And then I see that all three groups tend to go down by point six units and then they go up after that 6.9.

551
01:01:39,270 --> 01:01:42,690
Right. Point nine on top of negative six of 1.3.

552
01:01:43,730 --> 01:01:54,540
Yes. And that's my final model. I will get different coefficient estimates from a different model with a different correlation structure.

553
01:01:54,990 --> 01:02:00,690
So again, I want to emphasize that the different models will give you different coefficients,

554
01:02:00,690 --> 01:02:05,700
but they're all unbiased, which doesn't do a lot of good when you only have one dataset.

555
01:02:07,240 --> 01:02:12,570
But that's how I approach this dataset. I'm giving you this code to approach your data the same way.

556
01:02:13,500 --> 01:02:19,040
I am not asking you to go line by line and do this exactly with your data.

557
01:02:19,580 --> 01:02:25,899
Just use your judgment. And again, my goal next week is in two weeks, I guess, right?

558
01:02:25,900 --> 01:02:31,680
If you're going to turn it in, you're going to get graded, see what kind of correlation structures folks are coming up with.

559
01:02:34,310 --> 01:02:38,420
All right. And filling up our dates with our vote. Make sure you play football.

560
01:02:39,530 --> 01:02:43,040
We're here until 20 after. I'm happy to answer questions on the homework.

561
01:02:46,760 --> 01:02:53,510
Next Wednesday, we're supposed to already cover a new topic. I always love covering a new topic when folks and others have examined the old topic.

562
01:02:56,870 --> 01:03:02,419
I'm going to pull you guys for information. I'm going to ask you what you still don't understand.

563
01:03:02,420 --> 01:03:06,370
What you don't think I've done a good job of covering my stand with my boss?

564
01:03:06,570 --> 01:03:15,010
Know it's a woman talking. And if you want to slow me down on Wednesday by covering stuff we've already covered, I'm happy to do that.

565
01:03:15,020 --> 01:03:24,380
So I will ask for input. If I didn't know input. We will plow forward with the models with that.

566
01:03:24,430 --> 01:03:30,770
Enjoy the weekend. Michigan might actually have a challenge more than they have had in football.

567
01:03:30,800 --> 01:03:34,430
They might actually be a team that to score a touchdown this week.

568
01:03:36,260 --> 01:03:50,880
Enjoy. Fall. Fall came really fast. We're going to take down for.

569
01:03:53,370 --> 01:03:56,860
Capital can also be easily.

