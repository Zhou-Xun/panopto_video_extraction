1
00:00:00,390 --> 00:00:05,050
We'll look back on your time. Oh, yeah, yeah, yeah.

2
00:00:05,870 --> 00:00:33,840
Oh, stop it. So they're telling us to see what it's like to play.

3
00:00:46,370 --> 00:00:53,120
Oh, hold it. Hold it right there.

4
00:00:53,450 --> 00:00:58,720
So I was like, No, I'm great.

5
00:01:02,880 --> 00:01:07,700
Okay, let's get started. Two more people.

6
00:01:08,570 --> 00:01:17,670
Oh. I just won't confirm with you about the schedule of your presentation.

7
00:01:18,780 --> 00:01:42,280
And so. So Thursday we have three teams that are going to give presentation team to first, second team three and the third team six.

8
00:01:42,580 --> 00:01:46,450
Is that right? On Thursday, December 1st.

9
00:01:47,890 --> 00:01:55,120
So this is a group of eight and we have group to have a game seven next week on the World Cup.

10
00:01:56,170 --> 00:02:03,970
So so December six, we have team four first, team five second and team one last.

11
00:02:05,850 --> 00:02:11,290
So. Okay, so clear that. Okay. Well.

12
00:02:16,460 --> 00:02:23,330
I finish grading your homework? Sorry. I was a little worried at the time.

13
00:02:23,330 --> 00:02:27,740
I signed the homework. Number three. I thought there's a lot of it.

14
00:02:28,700 --> 00:02:39,229
Sort of how? See that hesitation or sort of fear of doing common theater and similar.

15
00:02:39,230 --> 00:02:46,370
But I mean, this is probably the best homework I've ever graded.

16
00:02:46,370 --> 00:02:55,520
I mean, everybody got it and they can do it. I saw that maybe my article somehow is awful code, but somehow it worked.

17
00:02:55,910 --> 00:03:03,319
And you you produce the result that so and I think that this is very excellent

18
00:03:03,320 --> 00:03:10,040
homework you have done so it's so you guys can become a filter become a smoother.

19
00:03:10,040 --> 00:03:19,100
I should have told more on that I was a little bit conservative and did not realize that you are able to do this.

20
00:03:19,430 --> 00:03:27,170
Okay. I mean, at that time I see some kind of fear or some hesitation in handling this.

21
00:03:27,200 --> 00:03:34,970
Yeah. So I had a question that thinking back from the homework, some of the questions we had in a student truth value,

22
00:03:35,060 --> 00:03:37,700
some of the questions we didn't know what the true value was, right?

23
00:03:39,200 --> 00:03:46,990
Why would you use a common filter, common school instead of just like longitudinal regression?

24
00:03:47,000 --> 00:03:52,219
Like what? What added benefit is anything you can get the theta parameter and like the rate.

25
00:03:52,220 --> 00:03:59,060
But is that like the main purpose of the common filter. Common smoother you get that like theta t the lake in process of your event.

26
00:03:59,720 --> 00:04:02,900
Okay, that's basically what I taught in the class. Okay.

27
00:04:03,440 --> 00:04:07,379
So, so for longitudinal data, you should have multiple times.

28
00:04:07,380 --> 00:04:14,780
Four is right? Yeah. Here you only have one. Mm hmm. So, strictly speaking, this is now longitudinal data.

29
00:04:14,780 --> 00:04:16,150
Yes, right. Yeah.

30
00:04:16,160 --> 00:04:27,319
But as I said that Scott Ziegler did this team method, right, where he basically treated this dependance structure as a nuisance structure.

31
00:04:27,320 --> 00:04:30,680
He is only interested in modeling the mean structure.

32
00:04:30,680 --> 00:04:36,530
As you can see that if you only use 80 to make prediction, the prediction quality is very low.

33
00:04:36,950 --> 00:04:40,459
Mm hmm. Right. So. So if you have a common filter,

34
00:04:40,460 --> 00:04:52,300
smoother that you basically consider the theta t process that gives you some unexplained sort of process or information that can now be model by 80.

35
00:04:52,760 --> 00:04:57,920
So at end of day, you have better prediction than just using 18 to prediction, right?

36
00:04:58,220 --> 00:05:06,830
Oh, okay. So the common filter, common simulation of the model gives you your predicted number of deaths per group at that time.

37
00:05:06,830 --> 00:05:13,460
But for the common filter, common smoothing is a way of getting possibly a better prediction than what just the model is,

38
00:05:13,530 --> 00:05:20,450
is not only the better prediction, but also give you a better way to model the temporal process.

39
00:05:20,780 --> 00:05:26,210
Right? So when you use T, you don't have residual, right?

40
00:05:26,240 --> 00:05:33,810
You don't know how the residual looks like. But when you have this stage based model theta t is more like a residual, right?

41
00:05:33,830 --> 00:05:39,560
So you have a T term that capture whichever the covered process that you've called the model.

42
00:05:39,920 --> 00:05:45,830
They have a set of t that basically describe what is not being mauled by the or 80 process.

43
00:05:46,130 --> 00:05:49,610
So you have two pieces together that to tell you the full story.

44
00:05:50,490 --> 00:06:00,910
Right. So so in that case you can capture more variability and also you have better way to handle the process because I only covered this situation.

45
00:06:00,930 --> 00:06:11,600
We have stationary process, but I did work a lot of on the now stationary suit of T where you can build up some covers into theta to process.

46
00:06:11,720 --> 00:06:17,660
How do you model this sort of inhomogeneous process or now stationary process?

47
00:06:17,660 --> 00:06:21,770
And that's another sort of flexibility.

48
00:06:21,770 --> 00:06:27,139
You can extend a curve model, but we do not go down this sort so depth.

49
00:06:27,140 --> 00:06:35,000
But certainly there is something, something people have work out for some solution, I guess.

50
00:06:35,000 --> 00:06:40,100
Yeah. Yeah. Mm hmm. Okay.

51
00:06:40,100 --> 00:06:50,149
So I were returning home of three and so far I have post dataset from three teams now.

52
00:06:50,150 --> 00:06:53,540
I received the data from additional two teams this morning.

53
00:06:53,540 --> 00:07:04,759
I will post it today and, and I'm waiting for the data from the last of the team I and anyway so I think we will

54
00:07:04,760 --> 00:07:11,510
start to see the data you capture and how they are related to the infectious disease stuff.

55
00:07:12,020 --> 00:07:15,550
So. Any questions?

56
00:07:16,270 --> 00:07:19,930
Now I have a quick question regarding spatial spatial analysis.

57
00:07:20,070 --> 00:07:28,360
Are you okay? Yeah. When you're doing Myron's, I guess you're looking at the random allocation of an attribute on a map.

58
00:07:28,600 --> 00:07:32,500
Right. Does that actually have to be continuous? Yeah.

59
00:07:32,830 --> 00:07:34,780
I mean, the current version, yes.

60
00:07:35,260 --> 00:07:45,880
But but then if you there are so general a generalized version of that where it allows you to have discrete or categorical things.

61
00:07:46,060 --> 00:07:46,780
Yeah. Yeah.

62
00:07:46,930 --> 00:07:57,520
Just like entropy in the entropy or mutual information by Shannon was started as a continuous by bigger if there was a discrete version of that and.

63
00:07:58,580 --> 00:08:03,420
Merci. Also has a discrete version for that number.

64
00:08:05,500 --> 00:08:10,030
Do you want. Come on. The spatial data coding like.

65
00:08:10,540 --> 00:08:14,540
Sure. And then share with some of the tips.

66
00:08:14,540 --> 00:08:18,460
Then he used to process the the data for formwork for.

67
00:08:18,520 --> 00:08:26,410
Do you want to come up front and share some of the tips with the team or is it more like the tier of the course?

68
00:08:26,780 --> 00:08:32,340
But um, I think there's two main tips.

69
00:08:32,740 --> 00:08:44,710
So for the first question, we want to do like a spatial plotting in G plot and the CSV file that is on canvas when you turn it on.

70
00:08:44,840 --> 00:08:57,230
So you want to use the word where you could open the homework such that, okay, actually this crack.

71
00:08:57,580 --> 00:09:02,690
What? And that's the point.

72
00:09:04,890 --> 00:09:21,050
Okay. So the critical part in Bomber four is that you're able to define the polygons and the spatial polygons of those countries.

73
00:09:21,750 --> 00:09:24,840
Yes. So you want. So let me just.

74
00:09:26,160 --> 00:09:37,860
So you want to make a heat map for a specific variable over a three counties in Michigan in the CSP file for the data as it opens right now.

75
00:09:38,340 --> 00:09:42,600
When you write the CSP files for one of the variables,

76
00:09:42,870 --> 00:09:48,090
one of the variables in order to kind of get the boundaries for each count is called the multiple polygon object.

77
00:09:48,480 --> 00:09:55,820
And in order to use a multiple polygon object, it needs to be in that specific data set.

78
00:09:56,550 --> 00:10:00,420
But as you see here, if this is the state boundary. But if you go further.

79
00:10:01,920 --> 00:10:06,900
Oh, yeah. So sorry. So this is here is the county county boundary and then here is the state boundary.

80
00:10:07,110 --> 00:10:13,170
When you pass this in R, it's going to be passed as string in character format.

81
00:10:13,530 --> 00:10:23,100
So you need you get you need to actually have the data be read in or in the format of multiple polygon object.

82
00:10:23,430 --> 00:10:33,330
And the best way I found to do this was to use the Tigers library in R specifically using.

83
00:10:43,790 --> 00:10:53,189
It's like map data function where you put the either state on or county depending on what levels you want,

84
00:10:53,190 --> 00:11:00,420
and it will give you the format in such a way that it is now a multi polygon object.

85
00:11:01,380 --> 00:11:05,490
A country like U.S. because we have the.

86
00:11:05,490 --> 00:11:08,490
So yeah, you say county instead of state and gives you the county boundaries.

87
00:11:08,500 --> 00:11:16,559
Okay. Yeah. So all you would have to do is you like map data county and then you can filter based

88
00:11:16,560 --> 00:11:21,960
on which region you want and get the specific county and subregion that you need.

89
00:11:25,640 --> 00:11:29,650
For that. Okay. Question.

90
00:11:31,050 --> 00:11:39,040
So actually, I want to restate that this gets you the specific format for Jeju Park mapping.

91
00:11:39,610 --> 00:11:44,170
This doesn't get it for problem two. This is for like problem one. In order to get it.

92
00:11:44,650 --> 00:11:48,160
When you plot on Gigi plot, you want each coordinate of each place.

93
00:11:48,160 --> 00:11:50,650
So this gives you like a row for each coordinate.

94
00:11:52,390 --> 00:12:01,600
So, like one county, Oakland County, give you the vortex of Oakland County, all of the vortexes, each vertex being around the data center.

95
00:12:03,460 --> 00:12:06,940
And this I don't actually know if it's in the Tigers library or not.

96
00:12:06,940 --> 00:12:12,100
This might just be in Base Park, this specific line of code.

97
00:12:12,760 --> 00:12:16,930
The Tigers Library is for problem two, which I was getting a bit ahead of myself.

98
00:12:19,060 --> 00:12:24,370
So that's how you get that. So the multiple polygon object is actually a different function.

99
00:12:31,670 --> 00:12:42,380
Using the Tiger's library, and specifically this function count called counties here.

100
00:12:43,250 --> 00:12:46,460
This is what you'd want to use to get the multiple polygon object.

101
00:12:47,240 --> 00:12:56,690
Do note that when you download this multi polygon object, the new dataset you're now working with is on an F data table.

102
00:12:56,840 --> 00:13:00,350
It's not like a table or data table anymore.

103
00:13:00,350 --> 00:13:06,200
It's a different type of data structure. So you have to sometimes like play around with the different functions.

104
00:13:06,470 --> 00:13:14,630
But the merge function still works to merge this county's table with the data you download directly from the vaccine hesitancy file.

105
00:13:15,290 --> 00:13:19,640
And then you can start problem queue.

106
00:13:19,670 --> 00:13:29,030
Once you have that multi polygon object specifically, now you can run Moran's eye test and do the spatial analysis.

107
00:13:29,990 --> 00:13:36,440
There's a few extra functions that you'd need for this in order to like, find the neighbors.

108
00:13:37,220 --> 00:13:46,700
And this is the first function poorly queue and B which finds a neighbors list based on regions with continuous boundaries.

109
00:13:47,180 --> 00:13:51,380
And this function you have to supply it in that multi polygon object.

110
00:13:51,950 --> 00:13:57,530
All of the multiple drag on objects and counties you're looking at in this will spit out a neighbors list for you.

111
00:13:59,300 --> 00:14:06,020
As I talked with the professor, the main attribute that's important here is queen,

112
00:14:06,680 --> 00:14:12,470
because queen means that your setting, that only one shared point has to exist.

113
00:14:12,830 --> 00:14:20,180
So, for example, when you're looking at Michigan counties on a map,

114
00:14:20,810 --> 00:14:29,360
you'd want something like Alcona and that like these two to be neighbors, to be neighbors to one another.

115
00:14:29,750 --> 00:14:33,590
This is the function that allows you to share that boundary lines.

116
00:14:33,590 --> 00:14:41,270
And yes, so that's the one you use and that gets you your neighbors list here.

117
00:14:41,810 --> 00:14:54,440
And then you then want to use the different functions such as Miranda's test that there is in the S p depth library.

118
00:14:54,440 --> 00:15:09,290
As mentioned in the homework here though, as an extra hint, you need to specify here the list W which is not the neighbors list.

119
00:15:09,650 --> 00:15:16,580
You have to process the neighbors list again in order to define adjacency matrix.

120
00:15:16,580 --> 00:15:19,910
Yes, an adjacency matrix based on also the weights that you want.

121
00:15:20,180 --> 00:15:24,770
And in order to do that and I think they might give an example here,

122
00:15:24,860 --> 00:15:34,519
you use this function and B to list W where you put in the first argument is the neighbors that you get from the previous function

123
00:15:34,520 --> 00:15:46,310
and then you specify a style where W means that your row standardizing your adjacency matrix or B is when you're not standardizing.

124
00:15:47,570 --> 00:15:51,680
But I tried both and there's very little difference between the two and this homework.

125
00:15:57,330 --> 00:16:03,680
And then for the last problem, you use this function in our do you know little area.

126
00:16:03,970 --> 00:16:12,330
Yeah. Yes. On ESP outer alarm. Do you note it was originally in the SB that package, but that package became too big.

127
00:16:12,330 --> 00:16:17,430
So the authors decided to deprecate the function and move it to a different package called the Spatial Rack.

128
00:16:19,230 --> 00:16:23,300
But if you look for the vignette for SpeedUp, it is still in there.

129
00:16:23,310 --> 00:16:28,770
They never took it out of the vignette, so it makes it seem like it's in the package when in fact it isn't anymore.

130
00:16:30,420 --> 00:16:34,980
Because when they want to remove it, there are something that they have to sort of.

131
00:16:35,820 --> 00:16:42,660
It's not like simple removal. It's they keep there because they want to have this integrity of the package.

132
00:16:42,690 --> 00:16:52,380
Yes. I guess that when they remove the function, some or some function will be removed, which is going to cost them damage to the other functions.

133
00:16:53,400 --> 00:16:55,110
So so that's why they keep there.

134
00:16:55,110 --> 00:17:04,740
But it's just like kidney transplant or kidney transplant is not the way that you take out the kidney is like adding new kidney on the old kidney.

135
00:17:05,010 --> 00:17:11,550
So they still have the old one there. But take a new in a special rec function.

136
00:17:11,590 --> 00:17:16,919
You know that the package that you can and when using this function,

137
00:17:16,920 --> 00:17:21,030
it's very it's very similar to the alarm function you're used to where you give a formula,

138
00:17:21,030 --> 00:17:26,219
you give the data, but now you also give that list w command that I showed in the Moran test.

139
00:17:26,220 --> 00:17:30,770
And that's pretty much all there is to do for that function as well.

140
00:17:30,780 --> 00:17:34,480
So it's very straightforward, like air and like neither request.

141
00:17:34,890 --> 00:17:44,430
Yes. Once you have the The Weekly Matrix, unless you want to run common filter, I'm not available yet.

142
00:17:45,360 --> 00:17:54,060
So I did have a question. So you could you can't use the result that you can't use the A's from this model to do common filter coming smoother?

143
00:17:54,480 --> 00:17:57,750
No. Why is that? Like one good. Nobody has a work on the phone.

144
00:17:59,550 --> 00:18:03,210
Okay. Your principle. You can work on common field recovers user.

145
00:18:03,240 --> 00:18:06,950
Right. But nobody has to work out this formulas yet.

146
00:18:07,470 --> 00:18:12,930
Maybe somebody had, but there's no our package yet.

147
00:18:13,950 --> 00:18:19,530
So it's common filter in common similar primarily used with just simple linear regression model.

148
00:18:20,240 --> 00:18:25,440
Is also work for the spatial dependance situation because my favorite comes closer

149
00:18:25,980 --> 00:18:32,940
in principle is a recursive calculation for prediction in the time domain,

150
00:18:33,660 --> 00:18:41,250
but it's for the error data analysis. Maybe not that sort of ideal, but today I'm going to talk about yield statistic.

151
00:18:41,280 --> 00:18:50,660
Okay. We are that probably is common field in couple schools or in a special domain would be more probably suitable thing to do.

152
00:18:50,700 --> 00:19:00,720
Yeah. And so for this function, the spatial function does the required the response should be continuous.

153
00:19:01,620 --> 00:19:05,690
Like, is there like a g version of it where it doesn't require to be.

154
00:19:05,710 --> 00:19:11,850
Yes, yes, yes. Or you can use different link functions. Yes, I yes, I think that is a good question.

155
00:19:11,850 --> 00:19:14,940
I think this one only works for continuous.

156
00:19:15,210 --> 00:19:18,390
Okay. Because people have done extensive today.

157
00:19:18,390 --> 00:19:25,320
I will give you example and Jan come and I did one work for the categorical one and so on.

158
00:19:25,950 --> 00:19:30,779
Yeah, because one of the questions I had when I was doing this problem is hesitancy is bounded by 0 to 1.

159
00:19:30,780 --> 00:19:36,960
So when you consider the continuous variable, technically allowing it to go below zero as a proportion.

160
00:19:37,290 --> 00:19:40,890
Mm hmm. But this function takes continuous. I don't know if that you could.

161
00:19:41,190 --> 00:19:44,220
You need a link function in order to bounce it between zero and one.

162
00:19:44,730 --> 00:19:52,350
Yeah. I think the difficulty here is how do you come up with a dependency matrixes for categorical variable?

163
00:19:53,010 --> 00:19:58,440
I mean, the marginal y. It's easy. You have to do a ramp, right? Do you want to analyze the involves?

164
00:19:58,440 --> 00:20:03,240
You model the meaning of the outcome. It's first group more different.

165
00:20:03,480 --> 00:20:09,450
More different. The difficult part is how do you come up with a dependance structure that's,

166
00:20:10,200 --> 00:20:15,149
you know is a little more challenging that that's something I like to talk about

167
00:20:15,150 --> 00:20:21,120
today but the questions that clear at some tips or in hints and from then.

168
00:20:23,580 --> 00:20:36,410
Okay, great. Thank you so much. Okay.

169
00:20:38,480 --> 00:20:43,760
Some of you opened so many windows. Yeah. Okay.

170
00:20:46,780 --> 00:20:50,860
It takes time to save it. Yeah, it was a big thing.

171
00:20:51,220 --> 00:20:57,490
Yeah, it is. Thanks for Team Six for this very nice data.

172
00:20:57,730 --> 00:21:04,870
It's quite handy for us to use it for homework.

173
00:21:06,010 --> 00:21:09,280
Okay, so let me go to what I want to talk about.

174
00:21:39,940 --> 00:21:56,620
Okay. So that's what we need to talk about. So we have covered this area of data analysis, which is quite a, you know, relevant to your test.

175
00:21:56,620 --> 00:22:08,680
You do this modeling. And basically, if you have surveillance data from CDC, most of the time the resolution is at a county level.

176
00:22:09,130 --> 00:22:12,130
So that error data analysis is very relevant.

177
00:22:12,160 --> 00:22:18,910
But in reality, of course, we could go, you know, even higher resolution, spatial data,

178
00:22:19,480 --> 00:22:27,250
if you do this model this afternoon and the job candidate friend who she is going to talk about,

179
00:22:27,430 --> 00:22:34,540
sort of this sensor like sensor tracking data, like you track this sort of in fact,

180
00:22:34,540 --> 00:22:46,480
she using this sort of personal wearable tracking device and looking at the dynamic network, how drivers somehow pass around the community.

181
00:22:46,490 --> 00:22:54,190
So that's kind of location specific measurement or individuals aspects of specific measurement.

182
00:22:54,580 --> 00:23:01,960
So that requires a little bit more spatial modeling because your data resolution becomes much higher.

183
00:23:02,320 --> 00:23:06,160
So this part is really just something called the geo statistic models.

184
00:23:08,050 --> 00:23:13,320
So this is a very, very big field, actually, two statistics.

185
00:23:14,560 --> 00:23:19,910
It has a lot of, you know, important topics in the spatial data analysis.

186
00:23:20,410 --> 00:23:23,469
So this originally developed for the mining industry.

187
00:23:23,470 --> 00:23:34,150
For example, in the oil industry, you taxes, people wander, where are you going to, you know, have an experimental hole to to to, you know,

188
00:23:34,150 --> 00:23:40,690
take this soil sample out to for the analysis to make sure that either underneath has

189
00:23:40,690 --> 00:23:47,740
some kind of like trash or you were some of the mines or something I looking for.

190
00:23:48,100 --> 00:23:58,209
So so people are looking at how how did you use spatial sampling and trying to find the didn't identify location at which you are

191
00:23:58,210 --> 00:24:07,620
going to collect the samples for the full analysis so that you can identify locations for some particular sort of mining purpose.

192
00:24:07,810 --> 00:24:12,640
Right. So this has been a long time in that mining industry.

193
00:24:13,090 --> 00:24:22,840
So Jill, that's the name of your statistics from so it's from geography or geology where people are doing this spatial data analysis.

194
00:24:22,840 --> 00:24:32,979
And so it covers a lot of the you know, this sort of spatial modeling approach is based on analyzing this location,

195
00:24:32,980 --> 00:24:38,770
based on measurement data like error data is like aggregated data for a certain polygon.

196
00:24:39,040 --> 00:24:43,989
So you have average your mean total from a polygon here.

197
00:24:43,990 --> 00:24:49,270
You basically have the explosive data from particular location, for example,

198
00:24:49,300 --> 00:24:57,490
data from household data from individual or data from a particular sort of store or,

199
00:24:57,640 --> 00:25:04,360
you know, the below some, some intersection of, you know, rows and so on, so forth.

200
00:25:04,720 --> 00:25:11,020
And that also data are coming from this very high of particular locations.

201
00:25:11,470 --> 00:25:19,760
So you want to really mall this and how this spatial sort of history to novelties or

202
00:25:19,900 --> 00:25:26,799
change in certain variables all come and and you're trying to understand the relationship

203
00:25:26,800 --> 00:25:36,400
you do this interpolation or you know for example you have monitor locations for PM

204
00:25:36,400 --> 00:25:43,870
2.5 apparently you cannot have this monitors and monitor like PM 2.5 every each year.

205
00:25:44,120 --> 00:25:54,159
You cannot afford for that. So what you are having here is maybe you are spatially distributed all the PM time people like air pollution

206
00:25:54,160 --> 00:26:02,410
monitors stations around the region and you trying to do some kind of creaking or interpolation to figure out,

207
00:26:02,590 --> 00:26:07,270
to estimate or to protect the air pollution at a particular location.

208
00:26:07,420 --> 00:26:14,410
You like to know, for example, you want to know what's the pollution level of the hospital and University of Michigan

209
00:26:14,920 --> 00:26:19,630
so that you can know the patient exposure right at that particular location.

210
00:26:20,260 --> 00:26:31,140
Of course, you don't have this monitor. PM 2.5 monitor located right in the hospital, but you have a monitor in the airport, Digital Blue Airport,

211
00:26:31,280 --> 00:26:40,150
some research stations around it so that you do this sort of interpolation or creating and some uncertain destination.

212
00:26:40,720 --> 00:26:45,430
And also people use this sort of station or a random fuze and.

213
00:26:45,910 --> 00:26:52,239
Like, Oh, very well. Graham covers function creaking and all sort of things.

214
00:26:52,240 --> 00:26:57,760
Like people are, you know, work out to marvel this special temple here, particularly in YouTube statistics,

215
00:26:57,760 --> 00:27:07,480
people are interested in modeling career structure or very grim because if you want to do prediction or a preview, you know, interpretation.

216
00:27:07,570 --> 00:27:11,620
DEPENDANCE It's very important, right? If they are you pregnant,

217
00:27:11,770 --> 00:27:23,320
you don't need to there's no way you can utilize a never a data point to make a prediction for for for the measurement at that particular location.

218
00:27:23,770 --> 00:27:33,790
Because of dependance and because of dependance, you are able to utilize neighboring data points to make a prediction for a particular location.

219
00:27:34,120 --> 00:27:43,210
So working out or estimating covariance structure or dependencies actually is very essential in this kind of achieve statistical analysis.

220
00:27:43,360 --> 00:27:51,099
Okay. So people also use so all sorts of things, you know, the simulation model or parameter estimation course.

221
00:27:51,100 --> 00:27:57,540
Now you mentioned data, it's one. So the very important application in this field where you look out pixels, right?

222
00:27:57,550 --> 00:28:09,070
So particularly the measurement of some kind of like form are expression or, you know, intensity at particular location in brands.

223
00:28:09,400 --> 00:28:17,200
And also you look at this spatial transcriptome and also look at some intensity of and particular location.

224
00:28:17,620 --> 00:28:24,650
So this has very wide application besides not to use statistic, but it started from Charleston.

225
00:28:24,850 --> 00:28:31,240
But this whole like spatial data analysis we're talking about, most of you refer to this kind of analysis.

226
00:28:31,450 --> 00:28:43,720
Okay? So you also look at pulling processes and know that COX Pulling process, it's very popular in this field to analyze that spatial process.

227
00:28:44,260 --> 00:28:48,970
I was surprised that people did not mention similar automata.

228
00:28:49,060 --> 00:28:57,880
So that's, in my view, is not a way to model this kind of spatial dependance spatial dynamics.

229
00:28:58,300 --> 00:29:08,830
Yeah. So basically that to use that is a colossal statistic used to analyze or predict value as we should be so spatial or spatial temporal phenomena.

230
00:29:08,950 --> 00:29:21,280
Okay. So, so in summary, that basically what we are trying to achieve here is, you know, we, we first have a location based on measurement data.

231
00:29:21,790 --> 00:29:25,690
So this is high resolution data, then error data.

232
00:29:26,380 --> 00:29:34,240
So we, you know, want to use the two boxes to model the spatial distribution based on spatial recorded

233
00:29:34,240 --> 00:29:40,390
data in which correlations are captured by spatial covariance matrix like a very,

234
00:29:40,660 --> 00:29:44,140
very grim isolate precision matrix like we usually talk about.

235
00:29:44,650 --> 00:29:52,630
So you so in statistical always have covariance matrix and have precision mystery position matrix is inverse of coherence matrix.

236
00:29:52,960 --> 00:29:59,350
So very grim is somewhat out of analog to precision matrix.

237
00:29:59,560 --> 00:30:04,810
Okay give us a better way to is easier to estimate in this spatial data setting.

238
00:30:04,960 --> 00:30:15,520
Okay. So one of the primary goals is to produce model based interpolation of measurement by so-called accretion and it is uncertain estimation.

239
00:30:15,520 --> 00:30:28,749
So this is know very important thing to to do and in your statistic and so people use random

240
00:30:28,750 --> 00:30:35,500
fields to understand which cover may explain spatial variation or a spatial heterogeneity.

241
00:30:35,500 --> 00:30:38,890
So there is some modeling involved. Okay.

242
00:30:39,130 --> 00:30:43,150
Let me just start with a very simple sample. So here is a river.

243
00:30:43,330 --> 00:30:53,110
Okay? And then so you have some kind of pollution rock from river or from lake from some water system.

244
00:30:53,110 --> 00:31:03,560
Okay. So that you use the, you know, sort of different colors to indicate different level of this concentration of certain pollutant,

245
00:31:03,640 --> 00:31:16,570
for example, that I'm just made up. Right? So the data, they are more at higher sort of pollution level of this heavy metal coming from this river.

246
00:31:16,960 --> 00:31:23,740
So now you do the sampling from locations, so you have different contours.

247
00:31:24,040 --> 00:31:29,229
The contours indicate, you know, the level of this pollution.

248
00:31:29,230 --> 00:31:32,770
Right. Or, you know, this contamination. Okay.

249
00:31:32,980 --> 00:31:40,630
Now, you know, those are the you know, that location based measurement of the plume, the pollution level or the content.

250
00:31:40,810 --> 00:31:45,220
So contamination from this system. So now you want to use those.

251
00:31:46,060 --> 00:31:54,370
Sort of the fixed number or finite number of locations at which you collect the soil sample.

252
00:31:54,370 --> 00:31:59,730
You send the sample to a lab and analyze it in whatever the instrument available.

253
00:31:59,910 --> 00:32:04,790
Lab So you calculate this concentration level of the pollution,

254
00:32:04,790 --> 00:32:12,099
then you try to map back in the entire space because of course that you you wonder

255
00:32:12,100 --> 00:32:17,500
about what is the contamination or soil concentration level at this location.

256
00:32:18,550 --> 00:32:22,510
Of course, you can directly measure, but you haven't measure it, right?

257
00:32:22,510 --> 00:32:32,680
You only measure this number of points. But you wonder what is the continuous map of this sort of contamination in the entire space?

258
00:32:32,980 --> 00:32:35,380
So that requires that you do clean.

259
00:32:35,800 --> 00:32:46,209
So basically you do interpolation using the final number or location data from final locations to create a continuous

260
00:32:46,210 --> 00:32:56,620
map in the entire space so that you can have this kind of sort of continuous imaging of what's going on there,

261
00:32:56,620 --> 00:33:09,639
pollution around the river. So you can certainly summarize this as the margin distribution here is sort of the concentration level.

262
00:33:09,640 --> 00:33:14,860
You have some, you know, this pollution level, high pollution, low pollution.

263
00:33:14,860 --> 00:33:22,150
And based on the data point and you can calculate that the the dependance using the data, the very low gram,

264
00:33:22,690 --> 00:33:34,899
you can imagine that if you really wanted to interpolation to calculate the contamination level at this point which you do not have any data collected

265
00:33:34,900 --> 00:33:41,860
from there you need to really the dependance of those of this model and to

266
00:33:41,860 --> 00:33:46,930
you work to predict the point that you want to know and you don't have data.

267
00:33:48,880 --> 00:33:57,130
So that's very clear goal. And sometimes you could summarize this in a different way.

268
00:33:57,130 --> 00:34:06,820
You can summarize as this, you know, one, one single point or you can summarize as a multiple points.

269
00:34:06,820 --> 00:34:18,640
You define a sort of cover. And for example, you use this size or use this size, you can get a little bit like so running beings,

270
00:34:19,030 --> 00:34:23,530
running being type of the sort of smoothing, right?

271
00:34:23,530 --> 00:34:31,350
So, so this is you have your data, you can big beam the data to calculate the average with this,

272
00:34:31,600 --> 00:34:37,780
with the data points with the data point 14 to that big here that you have

273
00:34:37,780 --> 00:34:42,459
this free tangle as sort of your being the spatial data that you can calculate

274
00:34:42,460 --> 00:34:47,410
the average essentially this kind of is like a smoothing out of your distribution

275
00:34:47,830 --> 00:34:53,550
and of course the the larger size of this and smooths the distribution.

276
00:34:53,560 --> 00:35:06,010
Okay. Right. So so this is more like your kernel estimation, this running being sort of smoothing technique to generate the,

277
00:35:06,580 --> 00:35:09,310
you know, the the distribution that will catch.

278
00:35:09,340 --> 00:35:16,690
Of course, the counter here represents sort of the level of the concentration while contamination like you look at.

279
00:35:18,280 --> 00:35:21,999
But most importantly in this kind of data analysis,

280
00:35:22,000 --> 00:35:31,780
we're interested in spatial dependance because that's essential piece that you need you work to do creating for interpolation.

281
00:35:31,780 --> 00:35:40,570
Okay, here is, you know, a small region where you have, you know, the task is different colors represent different level of,

282
00:35:41,050 --> 00:35:47,110
you know, expression where a concentration, those are location at which you collect your data.

283
00:35:47,410 --> 00:35:57,850
Okay, now what are you trying to do here is, of course, besides actually the the expression level or, you know, the concentration level of each point,

284
00:35:58,390 --> 00:36:05,830
this circle, you want to know what what is the dependance between this pairwise dependance of those locations?

285
00:36:06,310 --> 00:36:16,930
So that pairwise dependance is the minimum information that you need in order to develop the or, you know, prediction interpolation.

286
00:36:17,830 --> 00:36:25,960
So you need to estimate that. Okay. So in this spatial correlation sort of analysis,

287
00:36:26,410 --> 00:36:38,950
there's a little bit sort of issue in comparison to time series state in time to state a time Canada and time is well defined one dimensional index,

288
00:36:39,550 --> 00:36:44,650
right? You know that one minute different is versus one hour difference.

289
00:36:45,100 --> 00:36:47,350
Classic and different. 5 minutes, one minute.

290
00:36:47,640 --> 00:37:00,820
And it's very, very clear, conceptual, but in a spatial location, because you know this on your head, if you look at 2D space, right, you have this.

291
00:37:01,360 --> 00:37:05,110
So to coordinates sometimes it's very tricky.

292
00:37:05,120 --> 00:37:15,549
How do you define actually separation of two thoughts which distant views like there are many different ways to choose distance.

293
00:37:15,550 --> 00:37:23,340
For example, you can choose equidistant, which is very straightforward and direct, but sometimes people use from the hand of distance figure.

294
00:37:23,350 --> 00:37:31,089
We can use also the difference. The distance in to deep space is not the uniquely define.

295
00:37:31,090 --> 00:37:41,320
I mean depends how how you going to make your own choice right but the distance and then you know the following

296
00:37:42,220 --> 00:37:50,470
spatial analysis it will depend on the choice of the distance or you know this point separation and there are many,

297
00:37:50,470 --> 00:37:55,870
many choices available in practice. So that's one complication.

298
00:37:56,200 --> 00:38:04,120
But of course you can only this is the one that people use for a problem most commonly in practice, but you could use other types.

299
00:38:04,120 --> 00:38:14,850
It is the latter thing is this more like air one so dependance they are larger distance between two points, they're less correlated developments.

300
00:38:15,130 --> 00:38:18,910
So this is more like something like very naturally,

301
00:38:19,930 --> 00:38:32,650
you know and this is may now be true in the image you do that if you if you look at the some kind of functionality you know,

302
00:38:33,370 --> 00:38:40,840
because all brain has this symmetry sort of a structure if one point is out and probably,

303
00:38:41,110 --> 00:38:52,510
you know, in the other side of the brand is also it's not completely, you know, following this kind of simple decaying sort of pattern,

304
00:38:52,720 --> 00:39:01,060
you know, so if you have some symmetry structure that can violate this kind of Montana, see as we see.

305
00:39:01,060 --> 00:39:08,590
But in the geo statistic, this is a dependent structure is widely used in practice.

306
00:39:08,590 --> 00:39:18,489
But if you want to have some of this consideration of biology or some kind of background signs

307
00:39:18,490 --> 00:39:25,120
that or you think this one is not reflective to actually the degree of the signs that we're,

308
00:39:25,120 --> 00:39:28,540
you know, press on the neurology or something like that,

309
00:39:28,660 --> 00:39:34,389
that you can modify this according to whatever you think is appropriate to actually use scientific.

310
00:39:34,390 --> 00:39:38,090
QUESTION So what is the very ground?

311
00:39:38,110 --> 00:39:41,830
Very great. Actually, it's very simple. Let's define as this way.

312
00:39:42,070 --> 00:39:48,910
Okay, so as I say, the virulent is the one we use to calculate the dependance.

313
00:39:50,170 --> 00:39:56,770
And so, so first, the first thing we learn in practice is covariance matrix.

314
00:39:56,770 --> 00:40:05,050
But this is not covariance matrix. You can see that this is a basically you give me a distance,

315
00:40:05,410 --> 00:40:15,340
I'll cut word point separation inch whichever way you define I look at the x and whatever x right.

316
00:40:15,730 --> 00:40:19,260
That has a distance with the h. Okay.

317
00:40:19,270 --> 00:40:24,979
So. So maybe I have a point here or a point here.

318
00:40:24,980 --> 00:40:32,180
Point here and a point here. Right. So for any given access, for example, this is my ex.

319
00:40:32,420 --> 00:40:36,360
Right. So I look at the one that points out this same same distance.

320
00:40:36,620 --> 00:40:40,760
This two points would have the same distance with this point.

321
00:40:41,000 --> 00:40:48,380
That should be kind of based on the house's. What do you calculated, actually, is the expression of this point?

322
00:40:48,410 --> 00:40:52,850
So what is the measure of this contamination or, you know,

323
00:40:53,600 --> 00:41:01,790
some some pollution level or some something like the outcome of the treaty interest minus the point of here and here.

324
00:41:02,240 --> 00:41:08,270
And then you do the square of that. And this point should not be called because it's not at a distant age.

325
00:41:08,540 --> 00:41:16,219
Okay. So if you look at all the points that are distilled by each and put them together,

326
00:41:16,220 --> 00:41:21,410
this is the sample size that you have and the mineral level you have on your parallel.

327
00:41:21,920 --> 00:41:27,560
But in reality, if you have a lot of points in the space, like in the brain medicine situation, right,

328
00:41:28,520 --> 00:41:34,970
and all the pixels are regularly distributed in space, you could have a lot of points that have the same distance.

329
00:41:35,450 --> 00:41:42,740
So so this sample size depends on the density of your points at which the data are measured.

330
00:41:43,670 --> 00:41:48,730
But anyway, this is the formula you calculated why you need to do one half.

331
00:41:48,740 --> 00:41:54,910
Half? Well, this is simple, because you can have some points plus or some points minus each,

332
00:41:55,430 --> 00:42:04,520
because in the space you're not operating each space or lower to a space that gives you the same sort of distance.

333
00:42:05,000 --> 00:42:08,870
So there's no clear direction plus minus.

334
00:42:09,080 --> 00:42:12,290
So in this spatial space. So.

335
00:42:12,290 --> 00:42:15,530
So that this is the one that you calculated.

336
00:42:16,250 --> 00:42:21,050
So under this station, there are two situation. As I said, this is now.

337
00:42:21,530 --> 00:42:22,630
Okay. This is very.

338
00:42:23,810 --> 00:42:33,560
So in the stationary second order of station R2 means that this process has the same meaning as the various covers or these covers.

339
00:42:34,040 --> 00:42:40,640
Dependance only depend on the the lag doesn't depend actually location.

340
00:42:41,180 --> 00:42:45,620
Okay. Under this kind of assumption inverse of this.

341
00:42:46,430 --> 00:42:53,900
So this is inverse of are. So in that sense, basically development is your precision matrix.

342
00:42:54,500 --> 00:43:04,940
Okay. So so the second order station, in our view means that the dependance only depend on the lac or this doesn't depend on actual location.

343
00:43:05,120 --> 00:43:13,550
Okay. So in this case, there is a clear relationship between coherence and zero, but basically dark universe each other.

344
00:43:14,240 --> 00:43:19,459
Of course, when you go to zero, then you have the very when is zero?

345
00:43:19,460 --> 00:43:30,500
But the deviance is not okay. The virus is actually the, you know, the the house as down here.

346
00:43:30,680 --> 00:43:33,710
Right. So, so that's the variance.

347
00:43:35,840 --> 00:43:44,690
So talking about frigging. Well, I already introduced the idea of this is very essential thing people do if they are used that just because

348
00:43:44,690 --> 00:43:53,120
they want to sort of propagate those green points in this space to create a continuous map in of space.

349
00:43:53,540 --> 00:43:57,949
So basically, you are doing this prediction for every point you like or, you know,

350
00:43:57,950 --> 00:44:03,320
some target points you like using the surrounding points or neighboring points in the prediction.

351
00:44:04,070 --> 00:44:07,970
Okay. The the way to do that is block. Okay.

352
00:44:08,360 --> 00:44:13,969
Destiny, unbiased prediction. You did this common filter after you getting the homework through.

353
00:44:13,970 --> 00:44:23,720
You did after the prediction. That's exactly the prediction you do in the temple sort of of space.

354
00:44:23,930 --> 00:44:27,020
Now, suppose now you have all this surrounded point.

355
00:44:27,350 --> 00:44:31,970
How do you create the block? Best in the unbiased prediction.

356
00:44:32,000 --> 00:44:42,830
Right. So that you need to know or calculator basically at least is the inverse variance weighted method to do this.

357
00:44:43,280 --> 00:44:51,290
Hmm. And so one thing I should point out is that the current accreting based on block is not

358
00:44:51,290 --> 00:44:58,760
robust because this method is based on the error to loss bias and minimize the variance.

359
00:44:59,660 --> 00:45:09,260
So you I have an outlier here. You don't have outlier, but if you have an outlier right, then the block will be heavily influenced by outlier.

360
00:45:09,620 --> 00:45:13,000
The outlier will contribute a lot to the frequency.

361
00:45:13,910 --> 00:45:20,240
So so people have been talking about how do you create a robust.

362
00:45:20,980 --> 00:45:28,630
From using, for example. Rather than minimize the difference in minimizing the absolute difference.

363
00:45:28,650 --> 00:45:31,590
Or someone using their media or other ways.

364
00:45:32,360 --> 00:45:44,350
If you if you are now going to go to multivariate normal distribution, then you could go very t distribution to a you create some robust cream.

365
00:45:44,670 --> 00:45:59,100
Okay. So one time I was talking to someone that, you know, we have Gaussian random fields that's very well established in the spatial geo statistic,

366
00:45:59,940 --> 00:46:07,210
which is not robust because Gaussian distribution is based on error to loss based on the variance.

367
00:46:07,230 --> 00:46:15,420
Right. But if you really want to have a robust geo statistic, you can have a multi very t random field.

368
00:46:15,450 --> 00:46:22,150
That's something that people have not developed yet, but that's something, you know, you could do it.

369
00:46:22,170 --> 00:46:28,560
That's basically gave you a robust create order prediction.

370
00:46:29,130 --> 00:46:32,610
Okay. So how do you do that creaking?

371
00:46:32,790 --> 00:46:38,339
Okay. So the creaking is this like this on this this nasty?

372
00:46:38,340 --> 00:46:47,040
Of course, the variances cancel out. So what you're doing here is really you look at, you know, there there are multiple ways to do it.

373
00:46:47,160 --> 00:46:56,590
Okay. So the first way is num parametric plane also called hot back sort of of in the hot sort of creaking.

374
00:46:57,240 --> 00:47:01,200
So what we are trying to do here is you find the nearest neighbor polygons.

375
00:47:01,440 --> 00:47:10,290
Okay. And then you, you know, you you do this nearness, neighboring sort of computation or prediction, right?

376
00:47:10,710 --> 00:47:12,330
So, for example, in this case,

377
00:47:12,750 --> 00:47:21,180
which one is the nearest the one you have to that are nearest so that you can use the average of this to to predict this?

378
00:47:21,990 --> 00:47:27,870
Okay. So if you have another data point right here, of course, that this one will be the nearest to this one.

379
00:47:28,290 --> 00:47:37,050
So if this one got like five from zero, you're praying for this point, which you don't directly measure would be 5.0,

380
00:47:37,350 --> 00:47:43,170
because that's the the way they are going to to nearest neighboring polygon imputation.

381
00:47:43,710 --> 00:47:53,550
What in the case that you have multiple points that have, you know, say, the distance or they are all nearest neighbors, you just tick average.

382
00:47:53,790 --> 00:47:58,280
Okay. That's one one thing you know, too.

383
00:47:58,350 --> 00:48:05,240
And another one is that instead of using a nearest neighbor, you can create a buffer.

384
00:48:05,340 --> 00:48:10,410
Like you see that, hey, I'm going to create a rectangle.

385
00:48:11,580 --> 00:48:17,160
This is two kilometers wide and two km sort of.

386
00:48:18,480 --> 00:48:22,630
This is the tangle that the region that I'm going to include.

387
00:48:22,650 --> 00:48:31,980
So you create that in that you define your net and that could be a circle, could be a rectangle, whichever shape you want to define.

388
00:48:32,250 --> 00:48:36,380
And any points sort of captured by this nut.

389
00:48:36,730 --> 00:48:43,270
Right. That that you just take an average for those data points that people observe within that.

390
00:48:43,530 --> 00:48:46,800
Okay. The average will be developed like that. Okay.

391
00:48:46,980 --> 00:48:50,100
So that's local mean sort of memory values.

392
00:48:50,520 --> 00:48:58,260
It's not nearest neighbor, you just neighboring according to certain, not to your design to define neighboring boundaries.

393
00:48:58,410 --> 00:49:02,310
Okay. You totally take a local meeting to do that. Okay.

394
00:49:02,340 --> 00:49:07,860
Most popular way to do this is that you want to include more data points in there,

395
00:49:09,030 --> 00:49:18,809
sort of of the prediction of cricket instead of only using local structures like nearest neighbor polygons or,

396
00:49:18,810 --> 00:49:28,950
you know, this local meeting of local mean value of neighbor values that is all based on local data points around the target location.

397
00:49:28,980 --> 00:49:32,400
You like to figure out? I'd like to predict.

398
00:49:32,510 --> 00:49:38,999
Right. But if you want to evolve more data points than you just will according to this,

399
00:49:39,000 --> 00:49:45,150
that basically says that these far away from this location, you give them in lower weight.

400
00:49:46,020 --> 00:49:54,570
Right. So the distance is the weight. Right. So basically larger di lower weight.

401
00:49:55,470 --> 00:49:59,760
Right. Because larger, distant, more relevant data.

402
00:50:00,840 --> 00:50:11,790
Because according to this principle of the dependance, right, you have monotonic dependance, larger distance implies weaker dependance.

403
00:50:12,060 --> 00:50:18,990
So that you want to have the distance as a clear factor to just before the week that you want to do.

404
00:50:19,560 --> 00:50:24,040
And then. This is actually in a better divided and in locations.

405
00:50:24,430 --> 00:50:27,460
In this case, you can involve all locations, right?

406
00:50:27,550 --> 00:50:38,680
So any data points that your locations at which you have your data and then you weighted according to the distance to the target location.

407
00:50:38,990 --> 00:50:40,690
Okay. In this case. Right.

408
00:50:40,690 --> 00:50:51,620
So you not only just look at the value, but you weight their contribution according to the relative distance one over being one or d,

409
00:50:51,710 --> 00:51:02,410
do they need the near the, you know, the closer to the higher point, more contribution to the app with the average?

410
00:51:02,800 --> 00:51:11,500
Okay, that's the most popular way to do it are you can prove that this one is actually in the simple case as block B Okay,

411
00:51:11,500 --> 00:51:21,580
this version is blocked so that you can prove that you're stationary too, but you can generalize this with a little bit more complicated situation.

412
00:51:22,780 --> 00:51:33,610
So here is a result that if you use nearest neighborhood in a prediction, you can see that a lot of structures are very local, useful.

413
00:51:33,640 --> 00:51:37,840
For example, you know, here you have low local things.

414
00:51:39,160 --> 00:51:41,440
So for example, there is nothing going on here.

415
00:51:41,440 --> 00:51:49,419
And then you almost have the name of people, you know, if you have some data points, then you start to see some changes, right?

416
00:51:49,420 --> 00:51:54,190
Because everything is local here.

417
00:51:54,190 --> 00:52:02,500
You can see that it's a little bit more dynamic because so you use entire data in a so it really depends on distance.

418
00:52:02,560 --> 00:52:12,430
For example, here you start to see a little bit of this because you have some kind of yellow diffusion structure here,

419
00:52:12,430 --> 00:52:16,120
here, here that kind of affects some of the prediction here.

420
00:52:16,510 --> 00:52:20,660
So that is, you know, a little bit more spread out.

421
00:52:21,670 --> 00:52:26,500
The pattern is more global than that structure.

422
00:52:26,620 --> 00:52:32,410
Okay. So which way to use, really? It depends on how you understand the underlying process.

423
00:52:32,770 --> 00:52:33,700
It was situation.

424
00:52:33,700 --> 00:52:44,500
Maybe, you know, this local structure is something you want to go for because you believe that this process is very content in the local environment.

425
00:52:44,860 --> 00:52:52,540
But if you look at this wide type free world, like in a river contamination situation, right,

426
00:52:53,260 --> 00:53:06,190
there's no like artificial human builder structured to present or to intervene some kind of penetration of the pollution you have very wide.

427
00:53:06,520 --> 00:53:13,600
It's a process in practice that the global one problem is more natural process you would use to do the creaking.

428
00:53:13,780 --> 00:53:18,460
Okay, so that's the way thing you do in practice.

429
00:53:18,640 --> 00:53:32,340
Okay. So just give you a one example that I work with Gen and when it was a your student Joe technically speaking was my still,

430
00:53:32,350 --> 00:53:40,450
I mean took courses from me and so yeah so we were called a project to get our couple project together.

431
00:53:40,930 --> 00:53:45,940
So this is a project that we tried to model this malaria prevalence in Gambia.

432
00:53:46,750 --> 00:53:51,790
So by was my former Ph.D. student.

433
00:53:51,790 --> 00:54:00,970
So what we're trying to do here is trying to come up with some sort of estimation process to model this spatial cluster data.

434
00:54:01,180 --> 00:54:11,580
Okay, this paper was published by Matrix. So what we're trying to do here is that as part of the answer to this,

435
00:54:12,010 --> 00:54:17,020
they asked me the question whether or not this whole thing can be work on categorical, discrete data.

436
00:54:17,350 --> 00:54:22,809
So that's something we're thinking about at that time, too. So how do you scouted this?

437
00:54:22,810 --> 00:54:29,080
Gaussian random field is essentially a field process for a continuous variable.

438
00:54:29,590 --> 00:54:36,129
So now if you want to do something, live a more general like serum type thing, how do you generalize that?

439
00:54:36,130 --> 00:54:39,910
So so you know that I've been working on a copula.

440
00:54:41,050 --> 00:54:51,010
So copula is the model is I mean, the model for dependance copula is known for its capacity to model components.

441
00:54:51,880 --> 00:55:00,970
I was very weird that copra has been widely used in financial sort of industry like modeling.

442
00:55:00,970 --> 00:55:09,670
Do risk management due to Malda Temple did not make transaction data all the time

443
00:55:09,670 --> 00:55:17,380
sustained but popular as a model for dependance has barely used in spatial data.

444
00:55:18,040 --> 00:55:21,520
But while only spatial. In spatial data.

445
00:55:21,630 --> 00:55:32,080
I created this very essential question because a lot of spatial data analysis is actually coming down to the problem of modeling,

446
00:55:32,080 --> 00:55:39,320
estimating dependent structure. And I was quite surprised that why popular as is very important.

447
00:55:39,340 --> 00:55:45,159
So costs of models for depends have been little applied to most spatial data.

448
00:55:45,160 --> 00:55:56,350
That's something we're trying to figure out. And and you know, that's something we start with this malaria data and infectious disease you know that.

449
00:55:58,120 --> 00:56:01,270
So you say compose a likelihood method to deal with it.

450
00:56:01,780 --> 00:56:06,160
We propose a new method called compulsive estimate equation.

451
00:56:06,550 --> 00:56:09,280
I talk about compulsive likelihood and very beginning of lecture.

452
00:56:09,280 --> 00:56:15,760
That's probably the place how to apply the compulsive likelihood to solve a high dimensional problem.

453
00:56:15,910 --> 00:56:19,300
Okay, just let me walk you through about this work.

454
00:56:20,710 --> 00:56:26,020
I can skip this one and start to talk about malaria in fashion statement.

455
00:56:26,020 --> 00:56:32,860
Okay, so here is the data collected from Gambia.

456
00:56:32,860 --> 00:56:40,750
So there is a river from here going down here, salicylic, acid and efficacy.

457
00:56:40,810 --> 00:56:45,400
This is the river and the thought represent a village.

458
00:56:45,790 --> 00:56:49,990
There are 65 villages. Okay, sample from this study.

459
00:56:50,620 --> 00:56:55,060
This is study conducted by people from Johns Hopkins.

460
00:56:55,330 --> 00:56:59,889
Okay. So of course, you have multiple households or families living in village.

461
00:56:59,890 --> 00:57:05,170
So in total to some old 33 children sample from 65 villages.

462
00:57:05,650 --> 00:57:10,870
So each town represent one village. So you have multiple families living one day.

463
00:57:11,410 --> 00:57:20,400
So, you know, they're malaria infested. These is mostly sort of of so passed through mosquitoes, right?

464
00:57:20,470 --> 00:57:29,290
So so this mosquito is major sort of the carrier of the virus and so so so that it causes, you know,

465
00:57:29,290 --> 00:57:35,590
some of the diarrhea, fever and a lot of complications, particularly for children in Africa.

466
00:57:35,680 --> 00:57:41,259
So the cost of deaths as well. So in the previous analysis,

467
00:57:41,260 --> 00:57:53,650
people are just talking about sort of there's a dependance among the children because this data collected from 2035 children,

468
00:57:53,860 --> 00:58:01,600
there are especially correlated spatially distributed around the river, Gambia River in this region.

469
00:58:02,230 --> 00:58:06,160
So what why when we start to analyze data, we simply problem.

470
00:58:06,160 --> 00:58:11,800
There are two levels of dependance. What is the dependance and the village?

471
00:58:12,340 --> 00:58:20,410
Because you see that the villages are sort of, you know, scattered around very cluster.

472
00:58:20,420 --> 00:58:28,390
You can see that the here is the older you have one class, a village, another cluster here, here, here and the two side of the river.

473
00:58:30,250 --> 00:58:34,390
So that this is the spatial dependance between villages.

474
00:58:34,600 --> 00:58:41,800
Okay, that, of course, for the people that children live in the same village, they have very close distance.

475
00:58:42,430 --> 00:58:46,240
So it's hard to model their spatial dependance, as a matter of fact.

476
00:58:46,810 --> 00:58:50,500
So. So but they're correlated. They share comedy.

477
00:58:50,500 --> 00:58:57,010
Vermont, if this place has a lot of Greenland, then probably there have higher population mosquitoes.

478
00:58:57,430 --> 00:59:01,090
Then children are more likely to buy from mosquitoes.

479
00:59:01,390 --> 00:59:04,810
So there are more likely to be infected by malaria.

480
00:59:05,260 --> 00:59:11,409
Oh, so, so, so we're thinking about there's several two levels of dependance.

481
00:59:11,410 --> 00:59:14,110
What is the spatial dependance between villages?

482
00:59:14,380 --> 00:59:23,560
Clearly you can see that villages are spatially distributed and then you have this privilege coordination where we all know how to handle the.

483
00:59:23,590 --> 00:59:28,000
But there's some strong correlation among those children living in the same village.

484
00:59:28,360 --> 00:59:37,329
They share a lot of common ground sort of environment so that, you know, we create two level of dependance.

485
00:59:37,330 --> 00:59:43,060
One is the village level, another one is between children.

486
00:59:43,450 --> 00:59:43,720
Okay,

487
00:59:44,290 --> 00:59:54,990
so there the outcome is categorical is persons absence of malaria paradise that that's the one that you get this result from the block drop right?

488
00:59:55,060 --> 01:00:04,450
You get brought over there you can test whether or not that there's a malaria parasite still in the part of the of your lab work.

489
01:00:04,900 --> 01:00:11,979
So so the outcome is not is binary data then you have coverage you wonder what

490
01:00:11,980 --> 01:00:18,640
are the risk factors that causes this higher risk of being affected by malaria.

491
01:00:18,790 --> 01:00:23,350
So so. This data is complex in the way that you have spatial dependance.

492
01:00:23,360 --> 01:00:27,370
Two types of spatial dependance. At the same time, you have careers.

493
01:00:27,610 --> 01:00:38,110
You are interested in studying the risk. Okay. What are things that are associated with the risk of the malaria paradise?

494
01:00:38,440 --> 01:00:46,900
That's something we want to do. Okay, so actually this data has been analyzed by quite a number of famous biostatistician, not by us.

495
01:00:47,080 --> 01:00:53,860
Okay. The first choice would be that you do this each year and Kamal Right.

496
01:00:54,220 --> 01:01:01,240
You know, Kamal like you write. So the condition of the regression model, you introduce random effects, then you,

497
01:01:01,240 --> 01:01:06,760
you put the different effects into a Kamal that you can use anything to do it.

498
01:01:07,060 --> 01:01:13,150
Okay, this is doable. So the s here is village, a location, spatial location.

499
01:01:13,450 --> 01:01:21,220
I am in the eyes. Children. Okay. I stopped living in this s village.

500
01:01:21,370 --> 01:01:25,299
Okay, so you can introduce this kind of generalization.

501
01:01:25,300 --> 01:01:31,510
Either mix it back in model where random facts follows, a is conditional alternative model.

502
01:01:32,050 --> 01:01:39,430
Okay, so here you would have 65 random effects because you have 65 villages.

503
01:01:39,880 --> 01:01:42,970
Here you introduce village level.

504
01:01:44,920 --> 01:01:56,889
So dependents. Okay. And one thing in this kind of spatial data analysis that is very challenging is that you don't have independent replicates.

505
01:01:56,890 --> 01:02:05,800
Right? Everybody is correlated with another you know, people are calling always you don't have independent rep replicas.

506
01:02:06,340 --> 01:02:10,870
So 2035 children are spatially correlated.

507
01:02:10,870 --> 01:02:14,080
They're all correlated. There's no independence here.

508
01:02:14,290 --> 01:02:18,729
So that makes the whole analysis bit sort of challenging.

509
01:02:18,730 --> 01:02:23,920
So that's why a lot of people trying to use Bayesian method to where you do not really need to,

510
01:02:24,430 --> 01:02:29,740
you know, or come with sort of large row of large number type of theory.

511
01:02:30,490 --> 01:02:37,930
Okay, so Bayesian model is pretty popular, so people use this sort of car model city.

512
01:02:38,740 --> 01:02:46,209
So this is the model T model where Divvy is kind of this quote dependent structure.

513
01:02:46,210 --> 01:02:53,380
That's the Peter Peter article. This very, you know, did one of the pioneer in spatial modeling.

514
01:02:53,740 --> 01:03:07,720
He did this work for this so the dimension of V as to solving of 33 by two some 35 Tucson 35 by Tucson 35 dimension that's this so if you want to,

515
01:03:07,900 --> 01:03:08,310
you know,

516
01:03:08,980 --> 01:03:19,030
estimate this this parameter in the in the model that you have to deal with this inversion of this to solve them by to the matrix all the time.

517
01:03:20,530 --> 01:03:24,790
So we were trying to say, you know, among those method,

518
01:03:24,860 --> 01:03:34,550
what could be a little bit faster can be scalable to larger sort of more locations or more a sample size.

519
01:03:34,570 --> 01:03:43,600
Because you can if you have easily reached like ten solid of where even 100 solid individuals, if you use a method you need to worry about,

520
01:03:43,660 --> 01:03:52,060
you know, inverting a covariance matrix of tens or even want to solve the issue because everybody's correlated in a spatial data.

521
01:03:52,540 --> 01:03:57,580
So this stability is a big issue in the spatial data analysis, right?

522
01:03:58,630 --> 01:04:04,990
So we want to see how could you come up with something a little bit more structured and have faster computing?

523
01:04:05,290 --> 01:04:11,229
So that's the D we we again I start to work on together like this.

524
01:04:11,230 --> 01:04:16,660
So we use Gaussian copula. So basically we have this marginal regression.

525
01:04:16,840 --> 01:04:23,080
So S stands for village, I stands for a child in the village.

526
01:04:23,500 --> 01:04:32,920
So you use logistic regression model, you use logistic regression model, right to specify your risk.

527
01:04:32,950 --> 01:04:40,810
What is zero one precedents or absence of memory paradise in sample.

528
01:04:41,140 --> 01:04:44,350
So you handle just a regression model, then you have some risk factor.

529
01:04:44,350 --> 01:04:48,580
I will tell you what kind of corners that have been collected in this study.

530
01:04:49,060 --> 01:04:59,800
So from here you can calculate the this CDF component distribution function from this binomial distribution per unit distribution.

531
01:05:00,190 --> 01:05:06,820
So we have a CDF that goes in copula, allow you to put this CDF into this model.

532
01:05:08,260 --> 01:05:18,160
So F is the binomial distribution CDF okay from children one from in the village one.

533
01:05:18,700 --> 01:05:22,210
Okay. So this one. Is Children too solid.

534
01:05:22,320 --> 01:05:25,720
Oh 35 from Village 65.

535
01:05:26,110 --> 01:05:33,930
So you have this two song, the 30 501 and Tucson, the 35 distribution,

536
01:05:33,940 --> 01:05:38,110
F distribution, each one followed by an alternative distribution, ground zero one.

537
01:05:38,770 --> 01:05:45,670
But they are spatially correlated. So you use Gaussian copula, which is a model to help you model dependance.

538
01:05:46,090 --> 01:05:54,460
And this covariance matrix is the matrix that describes dependance of this Tucson 35 dimension.

539
01:05:54,850 --> 01:05:58,239
Okay. So now you have a perpetual model.

540
01:05:58,240 --> 01:06:07,840
We have a fully parametric model parameterized the model that you can't first of all, specify the dependent structure, unlike the ran effects model,

541
01:06:09,160 --> 01:06:16,090
because you have this copula situation, your have the freedom to specify what kind of correlation you want to put into the model.

542
01:06:16,520 --> 01:06:21,849
So here. Right. So we can put this between cluster spatial correlation.

543
01:06:21,850 --> 01:06:25,630
Basically, the cluster means events.

544
01:06:25,900 --> 01:06:35,080
Okay. So you have 65 villages. You can create a dependance a structure for 65 villages.

545
01:06:35,110 --> 01:06:39,370
That's the one measure of dependance that's spatially correlated.

546
01:06:39,550 --> 01:06:51,120
So you have this a coefficient to describe the the rate of decay of this, the farther away of two villages, the lower distance you want.

547
01:06:51,550 --> 01:06:56,740
Basically, this is spatial correlation function that I just introduced.

548
01:06:57,550 --> 01:07:06,020
So you have the real parameter that defines the magnitude eight, defines how fast the decay of that curve looks like for spatial dependance.

549
01:07:06,040 --> 01:07:11,290
So this is one layer of dependance that you like to put this in village.

550
01:07:11,290 --> 01:07:16,240
You use complex symmetry. Okay. You know, you could agree with this.

551
01:07:16,720 --> 01:07:18,270
For children living in the same village,

552
01:07:18,280 --> 01:07:25,330
they're pretty much equally correlated if you don't have additional information about their actual location of household.

553
01:07:26,080 --> 01:07:35,920
Right. You don't know exactly the, you know, their spatial sort of structure or provide that.

554
01:07:35,920 --> 01:07:42,130
You can choose compound symmetry if the children live in the same village are equal is sort of correlated.

555
01:07:42,430 --> 01:07:46,810
So this is another layer of dependance.

556
01:07:47,300 --> 01:07:51,700
So you can have this in village dependance and between villages dependance.

557
01:07:52,090 --> 01:08:00,700
And then you do this chronic product connect canonical product of this two course matrix to specify the signal.

558
01:08:02,680 --> 01:08:12,520
So, so that's what you do. And then how do you estimate this high dimension likelihood you have to solve in 35 sort of likelihood.

559
01:08:12,550 --> 01:08:18,470
So what I we did here is, you know, we do this compulsive likelihoods, right?

560
01:08:18,500 --> 01:08:31,120
So instead of working on Tucson and 35 dimensional likelihood, you construct all the, you know, pairs work on only the pairwise correlations.

561
01:08:31,930 --> 01:08:35,920
So pairwise likelihood, okay.

562
01:08:36,370 --> 01:08:43,060
Then in this case that you only need to worry about two by two correlation pictures.

563
01:08:45,040 --> 01:08:50,620
So we did the labor filter to, you know, do two types of this pairs.

564
01:08:51,130 --> 01:08:57,850
One pair is missing village pair. So two children living in the same village you form up here.

565
01:08:58,450 --> 01:09:05,180
So this pair will basically tell you what's the correlation for the visiting village correlation.

566
01:09:05,200 --> 01:09:11,320
So this pair will contribute to that. And then you have cross pair, cross village pair.

567
01:09:11,980 --> 01:09:18,400
So one child living in this village, another child live in this village and this pair in data.

568
01:09:18,870 --> 01:09:23,500
The likelihood function will tell you what is the spatial dependance.

569
01:09:24,250 --> 01:09:31,360
Okay. So because this two persons are spatial recorded essentially.

570
01:09:31,450 --> 01:09:45,280
Okay. So we create a, you know, this logistic regression like normal equation was best move equation based on this between village pairs.

571
01:09:45,580 --> 01:09:53,890
And then you have another one based on the only existing village estimating function or this score function from logistic regression.

572
01:09:53,950 --> 01:09:58,000
Okay. Based on the innovation village pairs.

573
01:09:58,270 --> 01:10:07,719
So put them together. And then Haggerty in the jobs of over 98, he said, you just direct.

574
01:10:07,720 --> 01:10:14,380
Add them together. Okay. So basically treat this two types of pairs that are independent.

575
01:10:14,860 --> 01:10:19,720
There are no weight associated with this. Two sort of types of pairs.

576
01:10:20,460 --> 01:10:28,620
But what we did here is because we know that they are this this is very strong assumption that.

577
01:10:29,700 --> 01:10:33,270
So this pair and this pair are correlated.

578
01:10:33,420 --> 01:10:42,600
As I said, everyone is calling each other. So you cannot assume that this pair and this pair are independent, as Patrick, I should say,

579
01:10:42,690 --> 01:10:52,350
because anyway, Hugh Patrick agreed was a dissertation advisor of Patrick Haggerty.

580
01:10:52,470 --> 01:10:55,710
I should be careful anyway. But that's what he did.

581
01:10:55,950 --> 01:10:59,130
And anyway, he did this. And. But we said that.

582
01:10:59,160 --> 01:11:06,580
Well, you say that those are correlated. You should try to find a way to introduce cohesion and improve the work.

583
01:11:06,650 --> 01:11:12,440
Right. So what we did is this quote, what efforts function?

584
01:11:12,450 --> 01:11:18,660
What is that essentially that you instead of adding them together, you stack them together.

585
01:11:19,200 --> 01:11:22,380
Instead of adding them together, you stack them together.

586
01:11:22,570 --> 01:11:29,040
Okay, that's a vector. Of course, this when you stack in particular, you have a vector that has hired dimension,

587
01:11:29,040 --> 01:11:36,270
that number of parameters that you have over identified over identification situation.

588
01:11:36,600 --> 01:11:43,470
So how to overcome that problem? Well, this is the million dollar idea from Lars Hansen who wants Norbert Price, right.

589
01:11:43,740 --> 01:11:50,250
2013 based on the 1982 famous paper in the econometric.

590
01:11:50,610 --> 01:11:55,380
Is it that you stop adding it here and, you know, solving the root of this?

591
01:11:55,710 --> 01:11:57,750
You just put them in quadratic form.

592
01:11:58,020 --> 01:12:08,160
And to minimize this, okay, minimize this with the theta, you still get the unique solution and you get all the nice properties you like.

593
01:12:08,640 --> 01:12:13,050
Okay. You stack them together and you introduce the weighting matrix.

594
01:12:13,530 --> 01:12:19,649
And the weight matrix basically will allow you to put the quarters structure between

595
01:12:19,650 --> 01:12:25,770
these two pieces in them all in the estimation that will help you to improve your power.

596
01:12:28,080 --> 01:12:31,709
So that's what we did. And using this, you know,

597
01:12:31,710 --> 01:12:43,800
generalized method moment idea and then we calculate this weighting matrix using the slop sampling and then how much time I have.

598
01:12:43,800 --> 01:12:45,390
I only have 5 minutes or so.

599
01:12:46,200 --> 01:12:54,540
So the stop sampling has been why this study in spatial statistics like Carsten Schurmann, they all talk about soft sampling.

600
01:12:54,930 --> 01:12:59,060
So how do you stop sampling basically and net effect.

601
01:12:59,550 --> 01:13:06,060
So then you randomly put that in this so that each data points the special location, right?

602
01:13:06,060 --> 01:13:09,300
So you create left whichever the shape you want.

603
01:13:09,660 --> 01:13:15,180
You just randomly through the left in the space and see which points have been captured by this net.

604
01:13:15,390 --> 01:13:20,220
Okay. So all of those data points captured by that will be used in your estimation?

605
01:13:20,500 --> 01:13:27,360
It's like bootstrap, right? So so that you come constantly through this net into the space,

606
01:13:28,230 --> 01:13:38,070
let's say 1000 times over when some sometimes that you can have one, some sort of subset or some samples out of this samples.

607
01:13:38,640 --> 01:13:45,600
Then you use this once on some samples to get once all the different type data set like bootstrap samples.

608
01:13:45,600 --> 01:13:49,590
And you can calculate the covariance matrix as you wish.

609
01:13:50,880 --> 01:13:58,020
Okay. So that's the one we we calculated. And back to the data analysis.

610
01:13:58,230 --> 01:14:01,260
Okay. If you want to see the detail, we can read the paper. Okay.

611
01:14:01,920 --> 01:14:05,640
So we have this spatially clustered binary.

612
01:14:06,090 --> 01:14:13,480
So we have two level of correlations. Not only spatial recorded, but you have this in village core issue.

613
01:14:13,770 --> 01:14:17,100
It's a little bit more than typical, the spatial correlation.

614
01:14:17,880 --> 01:14:22,740
So outcome is presence or absence of the material paradise.

615
01:14:23,280 --> 01:14:27,450
Then you have covered level of children level quarters like their age.

616
01:14:27,810 --> 01:14:40,740
Do they use that net? Okay. So, you know, so that this is a very effective way to, you know, protect mosquitoes during this sleep event.

617
01:14:41,190 --> 01:14:46,010
And then you have the village level covers about the greenness.

618
01:14:46,020 --> 01:14:53,490
This certainly is covered. But, you know, important because that is something related to the population of mosquitoes.

619
01:14:54,180 --> 01:15:01,020
And then, you know, you do this dependency modeling, as we described.

620
01:15:01,020 --> 01:15:04,350
And so here is the result.

621
01:15:04,560 --> 01:15:13,410
Okay. So first of all, we compare our resolve is the result generated by Patrick Nicotine from your done.

622
01:15:14,220 --> 01:15:19,890
So for example here we all the significance he found.

623
01:15:20,160 --> 01:15:29,610
There's some simple, some math ignoring the correlation between different pairs, like different types of pairs.

624
01:15:30,390 --> 01:15:38,430
So, for example, if you find that this is significant to this date, location East is significant.

625
01:15:39,060 --> 01:15:42,870
And we also find that significant.

626
01:15:43,500 --> 01:15:47,640
And we also find that age is significant. Okay.

627
01:15:48,180 --> 01:15:51,960
But, you know, he did not find that.

628
01:15:52,140 --> 01:15:58,430
Okay. So so in the 95 confidence interval we found, is this significant?

629
01:15:58,430 --> 01:16:01,890
But he did not. Is this a redefining. Well, actually.

630
01:16:02,340 --> 01:16:06,149
Back to Peter Dingle.

631
01:16:06,150 --> 01:16:11,490
And he did find this. The age is significant.

632
01:16:12,150 --> 01:16:15,420
We with our results also confirm that finding.

633
01:16:17,220 --> 01:16:23,880
And then in the east part we found significant but the Peter Diggle did not find.

634
01:16:24,240 --> 01:16:31,710
So essentially Dingle's analysis and Patrick analysis are somehow complementary each other,

635
01:16:32,160 --> 01:16:37,140
but our results actually capture all the significance that both the novices captured.

636
01:16:37,560 --> 01:16:44,910
Why is that? Because we use this billion dollar a year generalized lack of movement, the place, a winning idea.

637
01:16:45,180 --> 01:16:48,300
So that's a very powerful idea. As a matter of fact.

638
01:16:49,350 --> 01:16:54,450
So it's it's a very simple idea, but very powerful idea, I think.

639
01:16:56,140 --> 01:17:07,590
So, you know, in a spatial data analysis, there are all of the interesting data structures that require a lot more sort of effort and modeling.

640
01:17:07,590 --> 01:17:13,080
And so that is novel approach, particularly to now for the spatial transcript.

641
01:17:13,470 --> 01:17:23,330
So people will kind of spatial image and data. And I think that this is a very important area for future methodology problem.

642
01:17:23,460 --> 01:17:28,200
And and this statistic, I call it your copula.

643
01:17:28,860 --> 01:17:34,530
This we call it your copula. This is the one that gives you flexibility.

644
01:17:34,530 --> 01:17:36,149
You can analyze continuous data.

645
01:17:36,150 --> 01:17:49,140
You can analyze categorical data using this in a compulsive likelihood, which is computationally efficient and, you know, statistically efficient.

646
01:17:49,390 --> 01:17:56,969
Right. So it's quite nice method and it's something I really enjoyed working on and I think

647
01:17:56,970 --> 01:18:05,580
it has very good future of promise to handle a lot of complex spatial data analysis.

648
01:18:07,600 --> 01:18:17,110
Okay. That's all I want to see and give you a snapshot of what we have done for this in the more complex data like cluster spatial data.

649
01:18:17,110 --> 01:18:25,360
And so this things can be, you know, applied or extend to other settings of spatial data.

650
01:18:25,670 --> 01:18:37,940
Yeah, that's all I want to see. Returning home, which means it's not.

651
01:18:41,380 --> 01:18:57,620
You're going to get three out of people. Active.

652
01:19:01,960 --> 01:19:07,860
You scared? Chapter.

653
01:19:20,000 --> 01:19:34,290
Sure. What do they want you to shoot?

654
01:19:46,020 --> 01:20:12,900
Suzanne. I did receive only four portraits.

655
01:20:12,990 --> 01:20:17,210
Weren't I supposed to have six? Right. Oh, I knew you made me.

656
01:20:17,410 --> 01:20:23,340
You? Yeah, you know, I didn't. Okay, you can email me right back.

657
01:20:23,850 --> 01:20:32,280
The report. Right? The report? Yeah, this is a report. So I supposed to receive six and I only for one.

658
01:20:40,530 --> 01:20:57,220
Oh. It.

659
01:21:05,760 --> 01:21:10,139
So we have the job candidate to give the cemetery similar.

660
01:21:10,140 --> 01:21:13,980
I have to postpone the office are two 430.

661
01:21:14,130 --> 01:21:18,630
If you want to talk to me. Okay. I will be my office.

662
01:21:20,910 --> 01:21:21,220
Okay.

