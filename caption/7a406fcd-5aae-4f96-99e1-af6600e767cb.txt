1
00:00:02,680 --> 00:00:14,150
The last. Oh, boy.

2
00:00:14,980 --> 00:00:18,930
Morning, everybody. How we all doing?

3
00:00:20,150 --> 00:00:28,380
If you made it to the last the last day, uh, hopefully your finals going well.

4
00:00:29,560 --> 00:00:37,710
There's been a few questions. Come in. That's great. Just a reminder, it's due Tuesday at noon.

5
00:00:38,370 --> 00:00:42,420
So you still have a few days to get that finished up.

6
00:00:45,430 --> 00:00:50,710
Do you want to have general questions on it? I'm not going to answer anything specifically, but.

7
00:00:52,220 --> 00:00:56,510
Yes. Logistic regression. Can I go over it?

8
00:00:56,650 --> 00:01:02,270
Yeah. Well, in what sense?

9
00:01:06,380 --> 00:01:12,500
We did. We did. And? Well, you need to use proc logistic.

10
00:01:12,500 --> 00:01:19,070
Right. So.

11
00:01:20,560 --> 00:01:23,740
Is the question more in terms of interpretation or.

12
00:01:24,310 --> 00:01:38,520
Yeah. Well. The big difference between a logistic model and a linear model is that the initial outcome is a log odds.

13
00:01:39,270 --> 00:01:44,260
Right. So. In that sense.

14
00:01:45,420 --> 00:01:53,250
If you consider the outcome as log odds. The rest of it is linear, and the interpretation is the same as a linear model.

15
00:01:56,090 --> 00:02:00,020
But in that respect, if you're looking at the betas of your model results.

16
00:02:01,550 --> 00:02:07,060
Instead of a one unit increase being a beta increase in the outcome.

17
00:02:07,070 --> 00:02:13,880
Right. It's a one unit increase is a beta increase in the log odds.

18
00:02:15,240 --> 00:02:24,780
And that's that's really all there is to it, except that you can make that better by exponentially eating the beta.

19
00:02:25,500 --> 00:02:31,350
And when you do, you get an odds ratio. Logistic will give you the odds ratio as well.

20
00:02:32,910 --> 00:02:39,150
Now the big difference there is when you exponential rate, it is no longer a linear relationship.

21
00:02:40,740 --> 00:02:48,570
Instead of a one unit increase being an added increase in outcome, it's now a multiplier.

22
00:02:50,250 --> 00:02:53,940
Okay. So a one unit increase in that predictor.

23
00:02:55,390 --> 00:03:01,930
Corresponds to an odds ratio multiplier to the odds of the outcome.

24
00:03:04,140 --> 00:03:09,040
So if you're predicting. I don't know, smoking status.

25
00:03:10,010 --> 00:03:13,100
From. Age, right?

26
00:03:15,510 --> 00:03:22,430
Your your odds ratio for age. Let's say it's two, right?

27
00:03:23,270 --> 00:03:26,690
That would indicate that for every one year increase in age.

28
00:03:28,550 --> 00:03:35,060
The odds of. Say smoking increase by a factor of two.

29
00:03:36,890 --> 00:03:44,250
Right. And so that's the big difference. It's a multiplier as opposed to an additive.

30
00:03:45,830 --> 00:03:49,700
Increase. Does that make sense?

31
00:03:52,170 --> 00:03:57,030
So that's kind of the main focus there and the main difference between that and a linear model.

32
00:04:01,440 --> 00:04:05,440
All right. It's not clear to everyone.

33
00:04:08,800 --> 00:04:15,700
So when you're interpreting it, when you're interpreting results, you need to be clear that it's a multiplier and not additive.

34
00:04:17,400 --> 00:04:21,750
Okay. Right.

35
00:04:26,430 --> 00:04:35,200
Anything else? There is some stuff in those logistic slides about.

36
00:04:38,030 --> 00:04:45,050
Kind of making predictions with that model. That's cool stuff, but not as important.

37
00:04:45,590 --> 00:04:52,880
Obviously, it's not in the final. You seen the final. So I certainly look over those as part of the slides, but it's not crucial.

38
00:04:56,670 --> 00:05:01,220
All right. Um.

39
00:05:01,730 --> 00:05:10,800
Let's see. What else do I have to say? Uh, I'd appreciate if you did your course evaluations.

40
00:05:11,190 --> 00:05:16,350
So if you haven't done those, I'd appreciate you doing that. Y'all know how to do that, right?

41
00:05:17,760 --> 00:05:22,420
Yeah. Good. So if you can do that, that would really be helpful.

42
00:05:24,490 --> 00:05:27,820
And today we're going to talk about complex sampling.

43
00:05:29,500 --> 00:05:33,430
This is something that. Is often overlooked.

44
00:05:35,360 --> 00:05:38,390
We kind of learned today that everything we've done so far is.

45
00:05:39,680 --> 00:05:45,800
Kind of wrong. Not. Not truly, but in a sense it is.

46
00:05:47,090 --> 00:05:51,950
So we'll learn that today. So there's not any further questions, Will.

47
00:05:53,170 --> 00:05:57,250
Keep going on that. There's no lab today. So you're.

48
00:05:58,060 --> 00:06:01,630
You're free. Free and clear. After.

49
00:06:01,660 --> 00:06:06,580
After day. Of course. You need to finish your final. You don't have to.

50
00:06:06,610 --> 00:06:10,390
You don't have to look at me anymore. So I guess that's a bonus.

51
00:06:14,190 --> 00:06:18,890
No questions. All right.

52
00:06:19,760 --> 00:06:27,370
So. First couple learning objectives here is to understand the various types of sampling design.

53
00:06:28,860 --> 00:06:33,060
And to understand the impracticality of simple random sampling.

54
00:06:35,140 --> 00:06:40,870
Which is the sampling, the general type of sampling that we're more familiar with.

55
00:06:41,840 --> 00:06:49,700
So let's discuss it. Simple random sampling. Well it's the simplest probability sample when elements from a population of NW.

56
00:06:51,470 --> 00:06:55,610
Each unit in the sampling frame has the same chance of being included in that sample.

57
00:06:57,660 --> 00:07:08,910
And. Simple random sample of equal size and has the same probability of being chosen, which is one over and choose.

58
00:07:08,910 --> 00:07:12,990
N. Now SARS.

59
00:07:13,290 --> 00:07:20,220
Simple, random sampling. It really does remove any possibility of bias, whether conscious or unconscious.

60
00:07:20,340 --> 00:07:26,920
So it is the best way to sample. However, as we'll see, it's not always practical.

61
00:07:28,610 --> 00:07:32,280
So. It's not even feasible.

62
00:07:32,520 --> 00:07:40,100
A lot of times. How you would actually do one, how you would actually draw a simple random sample.

63
00:07:41,550 --> 00:07:43,650
It's just number your elements one to end.

64
00:07:45,370 --> 00:07:54,490
And using random number generator of some sort that by computer some table and let that randomness choose your sample.

65
00:07:55,870 --> 00:08:05,870
It's really as simple as that. The units in sample frame whose unique numbers match the random numbers that are selected by that random generator.

66
00:08:06,830 --> 00:08:10,690
That's your sample. SAS can do this for you.

67
00:08:12,450 --> 00:08:19,020
With Practice Survey Select. So today we're going to be learning all these survey procedures.

68
00:08:19,320 --> 00:08:25,080
We'll start with survey, and that has to do with complex sampling or other type of survey design.

69
00:08:28,700 --> 00:08:33,610
However, as I mentioned. Simple random sampling is rarely used.

70
00:08:35,330 --> 00:08:39,860
Well, for one thing, in large populations, it's really not practical or even possible.

71
00:08:39,860 --> 00:08:47,660
Assign every element a number. If we're talking about our population as the United States, for example.

72
00:08:48,800 --> 00:08:54,200
That's 300 some million people. You can't really assign everyone a number.

73
00:08:55,860 --> 00:09:00,350
Practically anyway. It would also be very, very expensive to do so.

74
00:09:03,830 --> 00:09:07,530
It can also result in samples that are spread out over very large areas.

75
00:09:07,580 --> 00:09:13,850
So if your population is the United States and you randomly selected people, you'd have people all over the country.

76
00:09:15,230 --> 00:09:23,900
And it'd be hard to track them down. Additionally, if subdomains are rare in your population.

77
00:09:24,930 --> 00:09:34,010
They're going to be even more rare in your sample. So if you had some disease of interest or some group of interest.

78
00:09:35,880 --> 00:09:40,740
That's rare in your population and you try to take a sample of your population.

79
00:09:41,130 --> 00:09:47,340
You might not get anyone with that particular rare disease or rare group, right?

80
00:09:49,820 --> 00:09:53,150
So that's another thing to keep in mind with simple random sampling.

81
00:09:56,080 --> 00:09:59,580
So. We have alternates.

82
00:10:01,330 --> 00:10:06,350
There is systematic sampling, stratified, random sampling, single stage cluster sampling,

83
00:10:06,350 --> 00:10:10,520
multistage cluster sampling, probability, proportional size and so on.

84
00:10:12,220 --> 00:10:19,160
These are more practical designs. But they have two important factors.

85
00:10:20,340 --> 00:10:27,030
To consider the inclusion probabilities for the elements and sets of elements may be unequal.

86
00:10:27,750 --> 00:10:37,770
So now not everyone has the same chance of being selected. In the sampling unit can be different from the population elements of interest.

87
00:10:38,830 --> 00:10:45,910
Which means really that these are going to complicate your estimation for variance.

88
00:10:50,660 --> 00:10:57,530
And if you don't make any adjustments, it's going to lead to bias in your estimation, in statistical tests.

89
00:10:59,440 --> 00:11:04,290
Okay. Which means we're going to have to make those adjustments if we use these methods.

90
00:11:07,410 --> 00:11:11,880
So the first type that I'll talk about here is systematic, random sampling.

91
00:11:12,810 --> 00:11:17,220
It's commonly used as an alternate alternative to SARS because it's simple.

92
00:11:18,480 --> 00:11:21,660
It basically just selects every element from a random start.

93
00:11:27,480 --> 00:11:30,660
Its procedural tasks are simple and the process can be checked easily.

94
00:11:32,200 --> 00:11:34,540
Which is something that's difficult for us. Or is.

95
00:11:35,950 --> 00:11:42,100
It assigns each element in population the same probability of being selected and there's only k possible samples.

96
00:11:43,550 --> 00:11:47,330
This is going to assure that the sample mean will be unbiased.

97
00:11:48,820 --> 00:11:55,270
In the population mean when the number of elements in the population and is equal to k times the number of elements in the sample.

98
00:11:57,620 --> 00:12:05,420
Okay. If our end is not exactly in, we can kind of do a circular sampling.

99
00:12:06,700 --> 00:12:13,570
Which we'll talk about. So to do this type, just select a random starting point.

100
00:12:15,320 --> 00:12:20,350
In your population. And choose every element.

101
00:12:23,890 --> 00:12:29,110
If you assume it's circular, you can kind of connect the end back to the beginning and keep going.

102
00:12:31,940 --> 00:12:35,420
So if you have this population abcd.

103
00:12:36,450 --> 00:12:40,370
Okay, you could select the random starting points a e.

104
00:12:41,970 --> 00:12:45,960
Choose a E and then every fourth element. So your samples I be.

105
00:12:50,290 --> 00:12:57,520
Each probability sampling is guaranteed in the scheme. Equal probability, should say, is guaranteed in the scheme.

106
00:13:00,210 --> 00:13:06,620
So it's nice because of that. However, it can give unrealistic estimates.

107
00:13:08,330 --> 00:13:13,760
In particular when the elements in the frame are listed in cyclical manner with respect to a survey variable.

108
00:13:15,420 --> 00:13:18,270
And the selection interval coincided with the listing cycle.

109
00:13:19,590 --> 00:13:27,900
So, for example, if one selects the 40th patient coming to a clinic in the average daily patient load is about 40 in that hospital.

110
00:13:29,200 --> 00:13:34,320
The resulting sample would contain only those who come to the clinic at some particular time of day.

111
00:13:35,400 --> 00:13:44,870
Right. And that. Might be bad because, you know, people in the evening might be worse off than people in the morning, for example.

112
00:13:45,020 --> 00:13:48,260
Right. A morning visit, usually something that's planned.

113
00:13:48,830 --> 00:13:53,510
Right. You've got an appointment, whereas something in the evening might be worse.

114
00:13:54,290 --> 00:13:59,600
It's might be an emergency. So that cyclical nature can cause problems.

115
00:14:05,910 --> 00:14:13,980
Another way of viewing systematic random sampling is equivalent to selecting one cluster from case systematically formed clusters of elements each.

116
00:14:16,120 --> 00:14:22,210
The sampling variance between clusters cannot be estimated estimated from one cluster selected.

117
00:14:23,290 --> 00:14:26,590
So we're going to require special, special strategies for that.

118
00:14:31,700 --> 00:14:40,820
A modification to overcome some of these problems with systematic sampling is the repeated, systematic, repeated sampling.

119
00:14:41,540 --> 00:14:49,850
So instead of taking a systematic sample with one pass through, you might take several passes through.

120
00:14:50,920 --> 00:14:54,220
By choosing several different starting points.

121
00:14:55,510 --> 00:15:00,370
And. In choosing multiple cycles through.

122
00:15:03,400 --> 00:15:10,050
It's going to guard against those periodicity problems. But also allows variance estimation directly from the data.

123
00:15:10,440 --> 00:15:18,180
So by taking more than one cycle through, you can measure the differences between those.

124
00:15:19,140 --> 00:15:22,740
Those groups to better estimate your variance.

125
00:15:22,950 --> 00:15:27,310
Variance. So that makes sense.

126
00:15:31,890 --> 00:15:42,820
We also have stratified random sampling. This is something that is used in complex sample designs, like an Heinz that we've been using all semester.

127
00:15:45,260 --> 00:15:51,830
In this scheme, the units in the sample theme frame are first divided into groups which we call strata.

128
00:15:53,560 --> 00:15:57,130
And then you can take a separate sample, random sample from each stratum.

129
00:15:58,070 --> 00:16:04,580
To form your total sample. The idea is to keep similar units together.

130
00:16:05,110 --> 00:16:08,730
Those gender, race groups and so on.

131
00:16:09,230 --> 00:16:14,140
Those can be your strata. And often it's the combination of many of those things.

132
00:16:18,060 --> 00:16:24,930
In this design units need not have equal chance of being selected, and some strata may be deliberate, deliberately oversampled.

133
00:16:25,560 --> 00:16:28,890
So this is going to solve that problem where if you have a rare.

134
00:16:30,040 --> 00:16:32,170
Group or a rare disease of interest.

135
00:16:33,510 --> 00:16:42,450
It's going to solve the problem of not having enough people in your sample to analyze because you can deliberately oversample that particular group.

136
00:16:47,420 --> 00:16:56,990
And that's one of the advantages. Stratification can also reduce the variability of sample statistics.

137
00:16:58,250 --> 00:17:05,540
Over a simple random sample. And therefore your sample size might not be.

138
00:17:06,830 --> 00:17:11,270
Need to be as big to to get nice results.

139
00:17:12,760 --> 00:17:22,140
Okay. This reduction in variability is going to occur when the units in the stratum are similar, but there is variation across strata.

140
00:17:24,420 --> 00:17:32,250
So in other words, it's going to this reduction occurs when the variables used form the strata are related to the variable being measured.

141
00:17:33,740 --> 00:17:39,070
So here's a simple demonstration of that. Suppose we want to estimate average weight.

142
00:17:41,460 --> 00:17:45,800
There are a population of six people. Right.

143
00:17:45,980 --> 00:17:49,790
Obviously, this is a simple example, but it will illustrate what I mean here.

144
00:17:51,400 --> 00:17:54,370
These six people consists of three males and three females,

145
00:17:55,120 --> 00:18:03,670
and the females populate their weights are 110, one 2130 and the male weights are 160, 170, 180.

146
00:18:06,020 --> 00:18:12,000
If I was to take a simple random sample. Of the six people.

147
00:18:13,120 --> 00:18:15,760
Of two. So my sample is going to be two people.

148
00:18:18,440 --> 00:18:25,370
The smallest possible estimate we could get with a simple random sample is this is the average of the two smallest weights.

149
00:18:26,600 --> 00:18:31,910
Right, 110 and 120, the average is 115. So if we were doing that, we could.

150
00:18:33,810 --> 00:18:38,520
By random chance, get an estimate of 115.

151
00:18:39,420 --> 00:18:43,410
We could also get something as high as 175. The average of the two largest.

152
00:18:46,030 --> 00:18:51,820
So our variance, our range of possible estimates is 115 to 175.

153
00:18:53,630 --> 00:19:00,740
But if we stratify first into gender and choose one person from each strata.

154
00:19:03,160 --> 00:19:09,280
Then the smallest estimate we could get would be the smallest female and the smallest male.

155
00:19:10,800 --> 00:19:18,120
Which would be 135 average. Or the largest we could get would be the largest female and the largest male.

156
00:19:18,120 --> 00:19:24,390
130 plus 182 is 155. So we've drastically reduced our variance there.

157
00:19:26,290 --> 00:19:32,460
By choosing. From Strava instead of completely at random.

158
00:19:34,850 --> 00:19:46,330
So it makes sense. So that's a big advantage. There's also post stratification.

159
00:19:47,780 --> 00:19:54,080
So the formation of the strata requires the information of the stratification variables to be available.

160
00:19:56,730 --> 00:20:00,330
In your sampling frame. But you might not actually know this ahead of time.

161
00:20:03,090 --> 00:20:08,180
However, you can still, still do it. In post, essentially.

162
00:20:09,140 --> 00:20:19,280
So, for example, stratification by race is usually desirable, but the racial identification is often not available when you start your your survey.

163
00:20:21,100 --> 00:20:26,200
So in this case, you can attempt to take race into account in the analysis after the sample is selected.

164
00:20:28,420 --> 00:20:34,300
And this is done by adjusting the sample distribution so that they match the population distribution.

165
00:20:40,500 --> 00:20:52,560
Cluster sampling. One of the some of the difficulties with SARS include that the sampling frame is not readily available for large areas.

166
00:20:54,460 --> 00:20:59,620
So. And you can't list every element.

167
00:20:59,620 --> 00:21:04,050
We've already talked about that. So one of the solutions is.

168
00:21:06,420 --> 00:21:09,510
To sample elements based on geographical areas instead.

169
00:21:10,480 --> 00:21:13,630
For which the sampling frames are readily available and can be easily constructed.

170
00:21:16,000 --> 00:21:21,010
So for example. First take a random sample of census tracks.

171
00:21:23,890 --> 00:21:27,910
Then neighborhood blocks within each tract or rim can be randomly selected.

172
00:21:29,710 --> 00:21:33,400
And then within the selected neighborhood blocks, a list of households can be prepared.

173
00:21:33,710 --> 00:21:36,940
A sample of households can be selected systematically from the list.

174
00:21:37,930 --> 00:21:44,800
So instead of having to make a list of everybody in your population, you kind of choose some random census tracts,

175
00:21:45,070 --> 00:21:51,460
choose some random neighborhoods from that census tract, and then that small piece you can make a list of,

176
00:21:51,880 --> 00:21:55,420
and you randomly select or systematically select from that.

177
00:21:57,870 --> 00:22:03,600
So much more practical. Okay.

178
00:22:04,820 --> 00:22:09,320
Once you've got your households, you can then probably randomly select someone in that household.

179
00:22:10,240 --> 00:22:13,250
To be your final. Sample.

180
00:22:14,640 --> 00:22:23,700
Observation. So in this example, the census tracks neighborhood blocks, and the households are the clusters we would call clusters.

181
00:22:24,860 --> 00:22:35,840
Uses the sampling units. And then because we went from tracks to neighborhoods to households, we would call this a multistage cluster sample design.

182
00:22:39,240 --> 00:22:46,940
Okay. This is also something that many of our publicly available survey data uses, like NHANES.

183
00:22:51,140 --> 00:22:54,140
This definitely complicates statistical estimation, however.

184
00:22:58,790 --> 00:23:02,810
So, for example, a simple random sample of unequal size clusters.

185
00:23:03,080 --> 00:23:10,780
These elements in the smaller clusters being more likely to be selected in your sample than large larger clusters.

186
00:23:13,450 --> 00:23:18,549
So these are these complications are handled by using special selection methods such

187
00:23:18,550 --> 00:23:23,530
as probability proportional to size sampling or by special analytical methods.

188
00:23:23,830 --> 00:23:30,340
We don't have to go too deep into that. We're going to see how we deal with this in SAS in particular.

189
00:23:33,340 --> 00:23:37,000
The other thing that's really important to take into account is sample weights.

190
00:23:39,200 --> 00:23:46,340
So because not everyone has an equal chance of being selected anymore, we want to account for that with a weight.

191
00:23:49,340 --> 00:23:55,430
The way it's going to estimate the number of individuals in the population for which each randomly selected person represents.

192
00:23:58,200 --> 00:24:02,520
Which means simply that the base sampling rate is the inverse of the selection probability.

193
00:24:04,280 --> 00:24:09,080
One over the probability. That the person selected.

194
00:24:11,450 --> 00:24:20,660
And then you can make adjustments to that weight for things like non-response coverage, bias and truncation of extreme sampling weights.

195
00:24:22,870 --> 00:24:30,710
This is a rather complicated. Calculation to do.

196
00:24:31,970 --> 00:24:43,080
But. Almost all of our good sampling surveys like Heinz have statistical experts who create these weights for you.

197
00:24:44,460 --> 00:24:48,000
They're already there. They're in your NHANES data.

198
00:24:50,060 --> 00:24:55,709
Okay. So here's kind of the idea of what I mean by sample.

199
00:24:55,710 --> 00:25:02,280
Wait. This would be a base weight. So it doesn't account for any kind of coverage or non-response or things like that.

200
00:25:03,330 --> 00:25:06,540
So if your population is broken into four strata.

201
00:25:08,760 --> 00:25:16,420
With big in. Numbers in each strata and your sample has small N in each strata.

202
00:25:17,950 --> 00:25:22,150
You can simply divide begin by little n to get your weight.

203
00:25:26,120 --> 00:25:32,370
So in that stratum a there. Each of the 98 people in your sample.

204
00:25:34,060 --> 00:25:40,090
Each one of those persons represents 4.08 persons in your population.

205
00:25:44,630 --> 00:25:48,860
Okay. So that's the basic idea of a weight.

206
00:25:52,900 --> 00:25:58,300
And as I've been alluding to, most public datasets are not drawn from simple random samples.

207
00:26:01,600 --> 00:26:06,460
But Sass assumes simple random samples.

208
00:26:10,600 --> 00:26:13,600
So as I mentioned, everything we've done so far is wrong. Just forget.

209
00:26:16,160 --> 00:26:27,650
Not really. So we're going to have to account for stratification and clustering and and the weights.

210
00:26:29,660 --> 00:26:32,979
Because if we don't. There was unequal selection probabilities.

211
00:26:32,980 --> 00:26:39,090
Second risk of type one error. And because they're not representative.

212
00:26:40,530 --> 00:26:44,040
If we don't use the weights, we're going a risk of type two error.

213
00:26:49,370 --> 00:26:55,090
So key takeaways of this section. Simple random sampling is generally impractical for many reasons.

214
00:26:56,060 --> 00:27:00,770
Stratification, clustering and sample weights are ways to overcome shortcomings of simple random sampling.

215
00:27:03,910 --> 00:27:07,840
And SAS assumes simple random sampling for standard procedures.

216
00:27:08,900 --> 00:27:13,490
So if you are using survey data with a complex sampling design like, again, Haynes.

217
00:27:14,830 --> 00:27:22,030
You really should account for that. Designed to make proper inferences. Okay.

218
00:27:23,510 --> 00:27:30,460
There's a lot of really good. Publicly available survey data out there.

219
00:27:31,680 --> 00:27:35,940
And Heinz. The Health and retirement study is one that.

220
00:27:37,480 --> 00:27:41,080
The Institute of Social Research runs here at University of Michigan.

221
00:27:41,110 --> 00:27:47,210
It's an amazing data set. Probably my favorite one here is things like surface.

222
00:27:48,290 --> 00:27:51,880
Which is another publicly available one like an Heinz.

223
00:27:52,400 --> 00:27:58,030
There's lots of them out there. So there's no shortage of large public data.

224
00:27:58,390 --> 00:28:01,660
It's free to use that you can use for for your studies.

225
00:28:03,370 --> 00:28:06,880
But all of them use a complex sampling design.

226
00:28:10,410 --> 00:28:14,550
This is a point that is very often overlooked by many analysts.

227
00:28:18,640 --> 00:28:26,910
Which is why we're talking about it here. I thought it was important. So how do we account for complex sampling design in SAS?

228
00:28:27,390 --> 00:28:35,250
Well, we have lots of survey procedures. We have survey means, survey FREEK, survey rag, survey logistic.

229
00:28:35,730 --> 00:28:38,730
So just the survey versions of all these things that we've learned already.

230
00:28:43,810 --> 00:28:50,650
So that's what we want to learn. For the rest of the lecture today, how to use those procedures.

231
00:28:52,070 --> 00:28:58,850
And also how to deal with the sub populations because it's a little bit different when you have a complex survey design.

232
00:29:02,160 --> 00:29:05,340
So they're all going to allow you to account for those three items stratification,

233
00:29:05,340 --> 00:29:10,530
clustering and weighting with the strata cluster and weight statements.

234
00:29:11,810 --> 00:29:16,430
All the survey procedures allow those three statements.

235
00:29:20,690 --> 00:29:30,280
Okay. Let's talk more about the survey design and enhance its multi-stage, clustered and stratified design in two year cycles.

236
00:29:33,040 --> 00:29:37,750
With the weights. It's representative of the non institutionalized civilian U.S. population.

237
00:29:41,350 --> 00:29:44,560
If you don't use the weights, it's not. So what that really means.

238
00:29:45,790 --> 00:29:50,400
It does oversample minorities. Persons below poverty level.

239
00:29:51,180 --> 00:30:03,660
And kids and old people. And these are the variables that you can use to account for the complex sampling design.

240
00:30:04,200 --> 00:30:10,350
So the strata variable is SDM as the MVS tray.

241
00:30:11,600 --> 00:30:14,810
The cluster variable DMV, PSU.

242
00:30:18,080 --> 00:30:24,180
And the weight. At least one of the weights it includes is this WTI two year.

243
00:30:25,330 --> 00:30:35,270
This is if you're using the full tier sample. However, it's important to realize that if you're not using the full two years sample,

244
00:30:35,310 --> 00:30:40,920
if you're using some some sample of that enhanced data, you're going to need to use a different weight.

245
00:30:45,510 --> 00:30:54,330
For example, there's a lot of the measures like height and weight, things like that are measured and what's called their mobile exam center.

246
00:30:54,870 --> 00:31:01,620
So they actually have like a van that goes around the country and to the people's houses.

247
00:31:01,620 --> 00:31:08,580
And it's it actually measures their height and their weight and blood pressure and things like that.

248
00:31:10,090 --> 00:31:17,270
So it actually takes physical measurements of the person. But that's not done for every single person in the survey.

249
00:31:19,360 --> 00:31:23,169
So there is an MSE. Wait, the mobile exam center.

250
00:31:23,170 --> 00:31:26,500
Wait that you would use instead if you're using any of those measures.

251
00:31:29,440 --> 00:31:39,520
If you're if you're using lots of variables like weight combined with something that's in the full sample, you should always use the smallest.

252
00:31:40,530 --> 00:31:47,559
The weight for the smallest combined dataset. So at the mobile exam center.

253
00:31:47,560 --> 00:31:52,010
Wait, is. That sample is smaller than the full two year sample.

254
00:31:52,340 --> 00:31:59,620
So you would want to use the MSE weight if you're using. One of those variables and there's other subsample weights as well.

255
00:32:02,840 --> 00:32:09,460
The syntax for survey means. Is basically the same as regular proc means.

256
00:32:10,640 --> 00:32:12,320
The big difference is that.

257
00:32:13,620 --> 00:32:22,080
Well, two things you can include the cluster strategy statements and the class statement is replaced with a domain statement.

258
00:32:26,590 --> 00:32:30,550
So here's an example. Suppose you want to look at mean age by gender.

259
00:32:30,850 --> 00:32:38,890
The 2013 wave of enhanced. So we can invoke the procedure practice or remain stealth the data dataset name.

260
00:32:39,240 --> 00:32:43,760
In this case, Demo H. We're estimating the mean age.

261
00:32:43,760 --> 00:32:51,600
So the VAR statement, we have the age variable. And we're looking at that by gender.

262
00:32:51,610 --> 00:32:57,840
So that's the domain and gender variable. And then we include our closest strata in weight statements.

263
00:32:58,260 --> 00:33:07,079
With those variables I listed above on the other slide. So it's it's easy.

264
00:33:07,080 --> 00:33:10,320
It seems easy enough to account for this stuff. So.

265
00:33:11,430 --> 00:33:16,960
You probably should. And here's the results you get from that.

266
00:33:17,710 --> 00:33:22,870
So it'll tell you the number of strata, your 15 strata, the number of clusters, 30.

267
00:33:25,080 --> 00:33:28,560
We've got 10,000 observations. You see the sum of weights.

268
00:33:29,940 --> 00:33:38,340
311 million approximately, which is in fact the approximate U.S. population in 2013.

269
00:33:41,260 --> 00:33:48,310
Okay. And that's exactly what it should be. Some of some of your weights should equal the population size.

270
00:33:52,720 --> 00:33:59,380
And here we see we've got the overall mean for age and standard error and so on.

271
00:33:59,710 --> 00:34:03,190
And it's broken down by gender here. So the mean.

272
00:34:04,500 --> 00:34:15,550
Age. For. Males is 36 and a half about and the mean age for females is 38.4.

273
00:34:19,420 --> 00:34:23,910
Okay. Now it's important to see the difference here.

274
00:34:24,720 --> 00:34:28,420
Oh, sorry. It also gives you these nice graphs. It's just convenient.

275
00:34:29,380 --> 00:34:32,710
It's important to see the difference between PROC means and survey means.

276
00:34:34,370 --> 00:34:41,480
So if you do not take into account the complex sampling design and you run proc means on the same exact data.

277
00:34:42,680 --> 00:34:49,490
You'll get a mean. Age for males of 30 and females 32.

278
00:34:52,110 --> 00:34:57,090
Where if you do take it into account, it's much increased 36 and 38.

279
00:35:00,990 --> 00:35:07,730
It's quite a bit of a difference. Okay.

280
00:35:10,700 --> 00:35:18,170
It's a couple of other things that you see by default here in proc means it give you the standard deviation in and max.

281
00:35:19,160 --> 00:35:24,320
We're survey means we'll give you a standard error and a 95% confidence interval by default.

282
00:35:25,850 --> 00:35:30,260
You can get the other things with options, but this is what it gives you by default instead.

283
00:35:32,860 --> 00:35:43,180
Okay. Another important thing to note is that if you have any people in your in your sample with non positive weights,

284
00:35:43,510 --> 00:35:46,930
they will be taken out of your survey means.

285
00:35:48,370 --> 00:35:56,890
Sample. In this case, in this example, there isn't anybody that had a non positive weight.

286
00:35:57,730 --> 00:36:03,830
You can see the ends are identical between the two. But there are some situations where that might not be the case.

287
00:36:06,940 --> 00:36:11,740
A person might have a zero wait because they simply.

288
00:36:14,650 --> 00:36:19,370
Aren't in your population of interest. Right.

289
00:36:19,370 --> 00:36:26,150
So in Heinz it's very specific to say it's representative of the non institutionalized U.S. population.

290
00:36:26,480 --> 00:36:29,690
So maybe it's someone who is institutionalized, right?

291
00:36:30,020 --> 00:36:35,980
They might give them a zero. Wait, if that's the case. We're in the health retirement study, which is one I use a lot.

292
00:36:36,790 --> 00:36:42,960
It's supposed to be representative of. Non institutionalized adults over the age of.

293
00:36:43,950 --> 00:36:51,130
50. That's for older adults. So but they do survey spouses as well.

294
00:36:51,160 --> 00:36:58,330
So if you had someone who is 52 and their spouse was 48, they'd still be surveyed, but they'd have a zero.

295
00:36:58,330 --> 00:37:07,210
Wait. Now that spouse would have a zero wait. So there are circumstances where where you might have zero waits.

296
00:37:11,180 --> 00:37:14,930
All right. So that survey means it's a good survey freak.

297
00:37:17,140 --> 00:37:23,780
It's again, essentially the same as freak. When the cluster share and weight statements included.

298
00:37:26,330 --> 00:37:31,100
So for an example, it's suppose you want to explore the cross tab between history of asthma and gender.

299
00:37:32,410 --> 00:37:43,080
In that same in Haines wave. So we can set up our table statement just like with proc freq with the asthma variable times the gender variable.

300
00:37:46,000 --> 00:37:52,270
Here. I've included the row and column options. Because survey freak does not give those to you my default like proc freak does.

301
00:37:53,910 --> 00:37:56,040
So if you want those, just include these options.

302
00:37:57,620 --> 00:38:03,530
And of course I had to cluster straight and weight statements again with those same variables specified.

303
00:38:06,090 --> 00:38:16,550
Right. And here's the results. Again, it gives you the number of Strava clusters, observations and some weights, which is the same as it was before.

304
00:38:19,380 --> 00:38:22,980
And we've got our breakdown of asthma.

305
00:38:23,880 --> 00:38:30,030
Condition one or two. Gender, one or two. And the frequencies, the weighted frequencies.

306
00:38:31,320 --> 00:38:33,900
You got your percents here. The percentages, your weighted percent.

307
00:38:36,170 --> 00:38:42,080
And because I included the row in column options, those percents are there as well.

308
00:38:45,890 --> 00:38:51,300
Okay. It is slightly laid out different than regular freak, as you noticed.

309
00:38:51,900 --> 00:39:02,120
And here's kind of a side by side comparison. So instead of kind of this cross tab box.

310
00:39:03,410 --> 00:39:08,890
It's more of a tabular form. Kind of a row tabulated.

311
00:39:11,270 --> 00:39:14,749
Format. But it gives you the same same information.

312
00:39:14,750 --> 00:39:20,430
And it. Just remember that the kind of that first listed variable is is your.

313
00:39:21,580 --> 00:39:25,390
Is your row. And that second one is your column.

314
00:39:29,630 --> 00:39:33,970
Okay. Again, it doesn't give you a row and column percents by default.

315
00:39:34,780 --> 00:39:38,260
And again, people with non positive waste we would be excluded.

316
00:39:39,510 --> 00:39:42,960
From this from this analysis, if you had any.

317
00:39:43,500 --> 00:39:47,800
In this case? We don't. All right.

318
00:39:50,890 --> 00:39:57,730
This is really especially going to be important when analyzing oversampled populations.

319
00:40:00,640 --> 00:40:05,770
So here's an example where I'm just looking at the frequencies of race.

320
00:40:07,690 --> 00:40:16,170
In. With free and practice survey free so once doesn't take into account the weights and the other does.

321
00:40:19,300 --> 00:40:29,530
And you can see, as I mentioned in hangs over samples minority groups which include non-Hispanic, black, Asian and Latino groups.

322
00:40:32,000 --> 00:40:36,050
And you can see that there's a humongous difference in freak.

323
00:40:36,650 --> 00:40:40,550
We would estimate there to be 30%.

324
00:40:40,760 --> 00:40:44,300
6% of that is white.

325
00:40:46,110 --> 00:40:52,050
Where if we take into account the weight, it's actually 62%, almost double.

326
00:40:55,630 --> 00:41:01,340
Right. That's a huge difference. And the minorities go the other direction, obviously.

327
00:41:01,340 --> 00:41:08,450
So in in the sample there's 17% maximum Mexican-American where in the population.

328
00:41:09,350 --> 00:41:14,080
So estimated to be 11%. Okay.

329
00:41:16,700 --> 00:41:21,710
So there's there's major, major differences there. When you're analyzing oversampled populations.

330
00:41:25,930 --> 00:41:37,800
All right. So what this really means if you if you're actually trying to make inference about the population.

331
00:41:38,880 --> 00:41:46,650
This is really crucial piece to do if you're really only interested in analyzing the sample of people.

332
00:41:47,330 --> 00:41:53,610
Or you don't need to do this. But if you are actually trying to make some kind of inference about the population, this is important.

333
00:41:55,750 --> 00:41:59,050
All right? Yeah, they are. Important part is subpopulations.

334
00:42:00,550 --> 00:42:04,840
When running these analyzes and data with complex sampling.

335
00:42:05,960 --> 00:42:11,570
For specific subpopulations, you need to use domain analysis.

336
00:42:14,080 --> 00:42:22,210
What this really means is you do not really, really do not want to restrict your data set, only those individuals of interest.

337
00:42:23,270 --> 00:42:32,080
So a lot of times we are a study question might be only for say I don't males right.

338
00:42:33,360 --> 00:42:39,930
And what we would do a lot of times is just, you know, subset our sample to males because that's all we're really interested in.

339
00:42:41,450 --> 00:42:48,710
Right. And then do our analysis from there. But if you're trying to account for complex sampling design, you do not want to do that.

340
00:42:50,710 --> 00:42:57,300
Okay. You need the entire sample in order to get the proper variance estimation.

341
00:43:00,150 --> 00:43:11,190
So in other words. The calculation that SAS is doing is still going to use all those weights and and values for the cluster and sample.

342
00:43:11,400 --> 00:43:16,710
For those people who aren't in the domain, you're actually interested in still going to use them.

343
00:43:17,730 --> 00:43:21,540
And needs to use them to make the proper estimation. Okay.

344
00:43:23,120 --> 00:43:31,720
So here's a an example. What I'm talking about here. So let's look at the mean BMI for Mexican-Americans who have ever smoked.

345
00:43:32,770 --> 00:43:39,870
In that same wave of engines. If we subset our data to Mexican-Americans who have ever smoked.

346
00:43:42,130 --> 00:43:45,920
And run. Survey means on it.

347
00:43:46,850 --> 00:43:50,600
These are the results this top. So you get a mean of 30.

348
00:43:51,210 --> 00:43:55,550
The standard error is about 0.23. But.

349
00:43:56,710 --> 00:44:00,040
If you run it with a domain statement.

350
00:44:02,810 --> 00:44:08,260
A domain on race. Right. And keep the entire sample.

351
00:44:10,270 --> 00:44:16,600
You'll get you'll get values for every combination here of the smoking and the and the race groups.

352
00:44:16,640 --> 00:44:21,460
Right. So the domain here is actually on race and cigarets smoking.

353
00:44:23,130 --> 00:44:29,880
You'll see in the Mexican American group here. The mean estimated is identical.

354
00:44:30,720 --> 00:44:34,230
So that's not a problem. But the standard error.

355
00:44:35,530 --> 00:44:42,150
Is now larger than it was before. Now, this isn't a huge difference.

356
00:44:42,660 --> 00:44:51,880
In this example. But it could be much bigger and it may influence your inferences.

357
00:44:53,190 --> 00:45:00,420
Right. So this is this is important to to understand as well.

358
00:45:01,260 --> 00:45:06,150
We don't want to exclude anybody when we're we're using these weights and complex sampling design.

359
00:45:10,020 --> 00:45:16,500
Otherwise you risk. Underestimating your standard error, which might cause you to make.

360
00:45:18,110 --> 00:45:22,620
Inferences that are. Significant when they're not.

361
00:45:24,390 --> 00:45:27,630
Okay. That makes sense.

362
00:45:30,970 --> 00:45:34,230
All right. How about some regression?

363
00:45:37,170 --> 00:45:40,230
So this is going to work pretty much the same as prior glm.

364
00:45:40,770 --> 00:45:49,420
You can use a class statement unlike regular proc rig. So here we're predicting BMI from age, race and gender.

365
00:45:50,650 --> 00:45:53,890
You can see the set up is exactly the same as Glen.

366
00:45:57,750 --> 00:46:06,310
Okay. Another thing to notice on this case, though, is I used a different weight.

367
00:46:08,130 --> 00:46:16,230
I use the MSE weight, and that was because BMI is something that's measured from height and weight, which were done in the mobile exam center.

368
00:46:17,240 --> 00:46:22,960
So this is a subpopulation that I'm I'm looking at here. Okay.

369
00:46:23,490 --> 00:46:27,710
A subsample. Of the mobile exam center people.

370
00:46:30,760 --> 00:46:37,050
And here's the results. So you get similar results as you would with regular practice or practice learn.

371
00:46:40,750 --> 00:46:48,430
Okay. Here's a side by side comparison. While there are estimates, aren't hugely different.

372
00:46:49,610 --> 00:46:53,930
You may make some inferences that aren't quite correct. For example, on gender there.

373
00:46:55,200 --> 00:47:03,010
It looks like male gender was statistically significant. Where if we use survey rig it's.

374
00:47:04,280 --> 00:47:09,560
Not quite meeting a point five threshold. Okay.

375
00:47:14,140 --> 00:47:19,210
And you've also got survey logistic, which works exactly the same as logistic.

376
00:47:21,800 --> 00:47:27,410
So in this one, we're predicting smoking status by age, race and gender.

377
00:47:31,650 --> 00:47:37,030
Right. And here's the results.

378
00:47:37,930 --> 00:47:46,480
You get results that are very similar to proc logistic. The side by side.

379
00:47:48,020 --> 00:47:52,730
Shows you that. Well, estimates are, again, a lot different.

380
00:47:53,690 --> 00:47:58,310
They are different enough. To where you might have some inferences that.

381
00:47:59,360 --> 00:48:06,520
That are different. Such as? Hispanic race there gets swapped in general.

382
00:48:06,880 --> 00:48:10,120
You'll notice that the kind of the significance is attenuated.

383
00:48:11,790 --> 00:48:16,450
When you account for this. That's not always the case, but often the case.

384
00:48:22,130 --> 00:48:28,030
Okay. Any questions about any of that?

385
00:48:30,950 --> 00:48:37,250
So key takeaways there. And as survey procedures allow you to account for compact sampling design.

386
00:48:39,590 --> 00:48:45,920
And. This is the end. Truly, this is the end.

387
00:48:48,830 --> 00:48:51,910
Questions. No.

388
00:48:53,460 --> 00:48:56,310
Well. Enjoy your winter break. Good luck on your final.

