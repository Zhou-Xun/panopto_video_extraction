1
00:00:02,390 --> 00:00:06,820
Okay. So.

2
00:00:07,860 --> 00:00:11,590
Let's check in on where we are. We are.

3
00:00:12,990 --> 00:00:24,560
Finishing here now 16 today. So we are here and we only have two more handouts.

4
00:00:25,860 --> 00:00:31,110
He's not 17, which I'm pretty sure will at least open and start today.

5
00:00:31,110 --> 00:00:34,440
So he might as well download that handout. 17.

6
00:00:35,610 --> 00:00:45,900
You have that ready to go and then hand out 18. So just so that you have something to work on if you're working ahead.

7
00:00:48,390 --> 00:00:53,760
By the end of today, you should be able to do one of the homework problems.

8
00:00:55,130 --> 00:01:01,130
On Homework six. I actually I think the lab this week helped set that up, so you might want to wait until you've had your lab session.

9
00:01:02,190 --> 00:01:06,179
But this first homework problem. Actually this one.

10
00:01:06,180 --> 00:01:09,840
You can do this first problem you can do.

11
00:01:09,930 --> 00:01:17,340
Just after today's lecture, I'll tell you when we get to that slide and I'll talk about this just a little bit.

12
00:01:18,240 --> 00:01:22,740
And homework two is the one that's going to be very helpful with the lab.

13
00:01:22,750 --> 00:01:26,400
So we will finish the handout.

14
00:01:26,490 --> 00:01:30,830
16. That this problem is based upon.

15
00:01:30,830 --> 00:01:36,350
But there's some manipulation of the data set that I think the lab will be invaluable for.

16
00:01:38,440 --> 00:01:43,260
So this is going to be this is the final homework for the course.

17
00:01:43,270 --> 00:01:51,700
So if you are working along with the pace of the lectures in class, you are going to be doing really great.

18
00:01:52,030 --> 00:01:55,750
But this is this homeworkers due towards the end of the term.

19
00:01:58,220 --> 00:02:03,070
All right. And then problems. Three and four.

20
00:02:04,440 --> 00:02:07,530
We won't quite be ready for after today, but.

21
00:02:08,980 --> 00:02:13,150
But soon. But soon. Okay.

22
00:02:15,140 --> 00:02:21,220
Uh. Let's see. Well, actually, let me just pop back down here.

23
00:02:21,580 --> 00:02:26,290
So homework five is due right before the break. And homework sticks.

24
00:02:26,320 --> 00:02:30,220
I just popped up a problem for homework. Six is due December the seventh.

25
00:02:32,010 --> 00:02:35,770
Right. Okay.

26
00:02:43,230 --> 00:02:47,910
So oops, I should have followed my own advice and got these handouts ready to hand out 16.

27
00:02:51,730 --> 00:02:55,820
And. We left off.

28
00:03:04,060 --> 00:03:08,170
Pretty earlier on. So somewhere around Slide 14 is what I had in my notes here.

29
00:03:09,640 --> 00:03:16,300
So we had talked, we'd started talking about let's just do a little refresher since it's easy to lose track over the weekend.

30
00:03:16,780 --> 00:03:20,950
We started talking about this data set that was a clinical trial.

31
00:03:23,360 --> 00:03:27,920
In HIV positive participants.

32
00:03:29,000 --> 00:03:34,730
And actually, I think they had progressed to AIDS to be as an entry criteria for this study.

33
00:03:35,060 --> 00:03:40,700
And there were two different treatments that were under investigation, one that was a triple therapy,

34
00:03:41,270 --> 00:03:48,830
and then there were many combinations of two of the three that were combined into kind of a dual therapy group.

35
00:03:49,430 --> 00:03:57,090
So were the treatment variables called triple triple goals one, if they got the triple and and zero if they got one of the duals.

36
00:03:57,800 --> 00:04:01,459
And then we just have a few covariates that were playing within this data set.

37
00:04:01,460 --> 00:04:04,850
This is the same data set that you will be using for your homework.

38
00:04:05,690 --> 00:04:13,550
So familiarity with this data set is kind of like a good thing to be listening for, but we only have a few covariates we're going to be playing with.

39
00:04:14,950 --> 00:04:30,430
Age male week and the outcome log city for count over time and so city for counties measure of your body's immunity system.

40
00:04:31,550 --> 00:04:37,580
This is coming from a non doc. So take that in mind. You know, this is not an explanation of what this variable is.

41
00:04:38,060 --> 00:04:44,510
And so the goal of the treatment was to kind of maintain the body's ability to have a healthy immune system.

42
00:04:45,350 --> 00:04:55,759
And so changes over time that are good in these participants might be just maintaining it at the current level or improving city four counts,

43
00:04:55,760 --> 00:05:03,110
increasing them. Those are all good things. And the natural progression of the disease is that this goes down over time.

44
00:05:04,760 --> 00:05:11,150
And so the hope is that Triple IS has a better trajectory over time than the dual therapies.

45
00:05:13,130 --> 00:05:20,210
All right. So that's the data set. And we kind of looked at some spaghetti plots.

46
00:05:21,660 --> 00:05:25,979
But by therapy group. So it's a huge dataset.

47
00:05:25,980 --> 00:05:38,160
So you're looking at the hairball here and we have a kind of a smoother through the individual spaghetti plots so that the triple therapy group,

48
00:05:38,160 --> 00:05:41,489
there might be a shape to this that's not just a straight line.

49
00:05:41,490 --> 00:05:50,160
So we're going to be playing with a spline term near about 16 weeks as an example of how to model changes over time.

50
00:05:52,820 --> 00:05:56,000
And I see you really just catching up again.

51
00:06:00,570 --> 00:06:10,590
So remember, one of the goals, one of the reasons this is such a popular model is that people can be measured at variable times,

52
00:06:10,770 --> 00:06:16,080
not all grouped together at time zero, eight weeks, 16 weeks, the way the protocol planned.

53
00:06:16,500 --> 00:06:23,250
But if people miss visits or if they are, you know, instead of at eight and 16 weeks, they get a measure at 12 weeks.

54
00:06:24,240 --> 00:06:31,470
The mixed effects models take that into account in a very natural way because of the way you model variability.

55
00:06:32,010 --> 00:06:35,190
And I'm going to go ahead and start here because I think this is.

56
00:06:37,760 --> 00:06:42,800
Fine to repeat. So random effects models.

57
00:06:44,190 --> 00:06:51,569
I are trying to model the variability in the outcomes over time by talking about each individual's having

58
00:06:51,570 --> 00:06:59,640
their own regression trajectory and modeling the variability of those trajectories between people.

59
00:07:00,150 --> 00:07:04,770
And so there's two different types of regression models that are included in mixed effects model.

60
00:07:05,160 --> 00:07:11,610
The one is the one that you're interested in writing your paper about on average, what's going on with the population mean?

61
00:07:12,300 --> 00:07:15,360
And for this study, one of the most important stories was,

62
00:07:15,780 --> 00:07:23,970
is the triple therapy helping keep the trajectory of CD4 counts over time in a healthy space compared to the dual therapy.

63
00:07:24,720 --> 00:07:34,000
And so the population model. Is describing with the overall or average Turner trajectory is four different Couvert profiles,

64
00:07:34,000 --> 00:07:40,870
and this is going to look in the output and feel in the output as if you're just interpreting a linear regression.

65
00:07:41,170 --> 00:07:48,520
With the population mean model, you've accounted for the correlation between measures in the same person correctly.

66
00:07:48,520 --> 00:07:51,250
So the P values will be corrected for that.

67
00:07:52,660 --> 00:07:58,890
So that's the usual model you're used to thinking of with standard linear regression, the way you write your manuscript worthy sense.

68
00:07:58,910 --> 00:08:04,170
Is it going to feel very familiar to that?

69
00:08:05,520 --> 00:08:10,169
And so the average outcome for the population mean is going to be an intercept

70
00:08:10,170 --> 00:08:14,490
and then parameters that are multiplied by their corresponding covariates.

71
00:08:17,240 --> 00:08:22,910
And the covariance, the covariates are allowed to change for different dependent outcome measures.

72
00:08:23,750 --> 00:08:32,160
So the most common example and the example that is key in this particular analysis is the time of the measurement.

73
00:08:32,180 --> 00:08:37,130
So at each outcome, the time of the measurement is an important covariate.

74
00:08:38,380 --> 00:08:40,590
And it will be individual for each person.

75
00:08:40,630 --> 00:08:47,230
It won't necessarily always be week zero eight, but whatever the measurements were for that outcome over time for that person.

76
00:08:48,470 --> 00:08:57,320
And when we talk about this part of the model and in the software uses this jargon as well, we call this the fixed effects part of the model.

77
00:08:59,470 --> 00:09:06,610
So the random affects parts of the model are trying to take into account the variability of these trajectories over time.

78
00:09:07,360 --> 00:09:15,819
And so we actually have an individual level meeting model that includes the population effects.

79
00:09:15,820 --> 00:09:26,320
And these random effects and this individual level mean model is trying to estimate what the average trend or trajectory is for person I like.

80
00:09:26,470 --> 00:09:29,800
It's almost like their own linear regression line, but not quite.

81
00:09:32,480 --> 00:09:35,900
So in addition to the population model or terms that just affect a person,

82
00:09:35,900 --> 00:09:41,570
I and I'm submerging a lot of subscripts here, but when I need to talk about them, I'll bring them back.

83
00:09:42,540 --> 00:09:48,840
So the only random effect we've seen so far is this random intercept term, gamma AI.

84
00:09:49,380 --> 00:09:56,430
So in the very first handout, we showed how to do a paired T test where you had a random intercept that affected the height of the line.

85
00:09:57,080 --> 00:10:02,340
Right. Compared to the population mean. But it can be much more complex.

86
00:10:02,820 --> 00:10:12,240
And so the you could have a model with not only all of these fixed effects but a lot of random effects that depend on covariance as well.

87
00:10:12,810 --> 00:10:21,930
In my own personal experience, the most common random effects are a random intercept and a random slope that depends on time.

88
00:10:22,860 --> 00:10:28,740
That's most of what I have seen in practice. But you could have times blinds.

89
00:10:30,250 --> 00:10:37,530
In the random affect part of the model, and you could have random effects that depend on site.

90
00:10:38,170 --> 00:10:41,590
If there are lots of sites in a clinical trial that you want to take into account.

91
00:10:42,340 --> 00:10:50,560
It can get very complicated. The more variables you have in the individual regression line, the more you're straining the dataset,

92
00:10:50,980 --> 00:10:55,270
because all of these are extra parameters that the data has to be able to estimate.

93
00:10:56,260 --> 00:11:01,780
And so you can't really fill this line out too much unless you have a lot a lot of measures per individual.

94
00:11:06,220 --> 00:11:10,600
So these random effects are typically different for every person and try to

95
00:11:10,600 --> 00:11:14,740
capture that person's individual trend or trajectory with help from covariance.

96
00:11:16,590 --> 00:11:20,640
And there often, but not always, functions of time or interactions with time.

97
00:11:20,760 --> 00:11:26,720
And those same variables can end up being in the fixed effect model as well.

98
00:11:26,730 --> 00:11:31,980
So you can have some overlap of the predictors between the two models, but they need not be the same.

99
00:11:32,430 --> 00:11:39,330
So for instance, if Sight was something that you considered a random effect that you wanted to account for the variability for,

100
00:11:40,410 --> 00:11:44,729
but otherwise you you didn't want to really analyze as part of the data.

101
00:11:44,730 --> 00:11:48,990
You might have cite as one of these variables, but not in the main model.

102
00:11:52,580 --> 00:11:59,719
So one of the things that's going to be happening behind the scene is that the random effects are

103
00:11:59,720 --> 00:12:06,890
called random effects because they're assumed in general to have a normal distribution with mean zero.

104
00:12:08,410 --> 00:12:11,129
There. In the output.

105
00:12:11,130 --> 00:12:17,040
They will try to estimate if you ask for it, they'll show you what the estimates of the random attacks were for these individuals.

106
00:12:17,670 --> 00:12:22,740
And so these are not quite observed data points.

107
00:12:23,890 --> 00:12:30,880
Right. Because we you know, there's some pretend trajectory line that each individual's assumed to have.

108
00:12:31,150 --> 00:12:43,300
Where their intercept is is mean zero compared to the population mean you know so on average zero different from the population mean and so.

109
00:12:46,470 --> 00:12:53,200
Hmm. I think the main thing I want to kind of emphasize here is that there there mean is zero.

110
00:12:53,290 --> 00:12:58,990
So when we talk about an individual's regression line, on average, they're similar to the population regression line.

111
00:13:00,140 --> 00:13:03,830
You know, I'll. Talk about a little bit later.

112
00:13:04,070 --> 00:13:06,890
So all of those parts of the model called the random effects.

113
00:13:11,480 --> 00:13:19,100
So we're going to kind of learn about this model in slow steps, just like we always do, so that we can tie to things we already know.

114
00:13:20,090 --> 00:13:27,800
And so we've talked about the random intercept model before, but not in the context of linear trend data.

115
00:13:28,700 --> 00:13:36,980
So the random intercept term, Gamma I for all these people is assumed to have a normal distribution with mean zero.

116
00:13:37,220 --> 00:13:43,970
And then there's some variability term that will be estimated from the data of the variability of the random intercept in the people.

117
00:13:44,510 --> 00:13:54,169
And then we have our usual random error term, but now it has two subscripts, because for each person I we have several measurements.

118
00:13:54,170 --> 00:14:03,020
J equals one to something. And so for each outcome that you're measuring on each individual over time, it's not perfectly measured, right?

119
00:14:03,020 --> 00:14:04,760
There's just random variability.

120
00:14:05,120 --> 00:14:13,370
So this is what keeps the individual trajectory from looking like a straight line that perfectly goes through the dots for that person.

121
00:14:14,090 --> 00:14:20,030
There's some residuals remaining even after the fact that you've accounted for a random intercept.

122
00:14:22,680 --> 00:14:28,990
So the random intercept model, assuming a linear trend for the outcome data over time becomes something like this.

123
00:14:29,040 --> 00:14:34,330
So the only covariate I have here so far, just to keep it simple, is time.

124
00:14:35,160 --> 00:14:40,680
So I haven't even included treatment group yet. We'll get to that level of complexity soon.

125
00:14:41,340 --> 00:14:47,399
So if I'm only using time as the only covariate, that means for my population model,

126
00:14:47,400 --> 00:14:55,760
everybody has the same shape of the line of what their data points, their measurements look like over time.

127
00:14:58,100 --> 00:15:01,580
And in the city for data set. I think it's it's like a variable called a week.

128
00:15:03,900 --> 00:15:09,270
So if this is the model and here's my random intercept and here's my random error.

129
00:15:09,720 --> 00:15:13,980
The population mean regression line is everything. That doesn't depend on it.

130
00:15:14,340 --> 00:15:23,130
I over here. So it's going to look like the average outcome for the wise is beta, not plus beta one times time.

131
00:15:28,370 --> 00:15:35,440
And the individual level regression lines for each person are going to look like this.

132
00:15:35,480 --> 00:15:46,100
So the debate or not. And for that individual, they have this random intercept term that's going to be the line for person I.

133
00:15:46,580 --> 00:15:52,460
Their height is going to be different for their line than the population line.

134
00:15:54,570 --> 00:16:01,680
But the trajectory so far with the random intercept model, they're assuming the same trajectory over time as the population.

135
00:16:02,590 --> 00:16:07,510
Only the height of their line is different for this model, with the random intercept only.

136
00:16:08,690 --> 00:16:17,599
So that's the intercept for personalized line. And the mean model doesn't care if time is measured at regular intervals at all.

137
00:16:17,600 --> 00:16:20,870
The variability is covered by the random intercept in the error terms.

138
00:16:20,870 --> 00:16:25,909
So there this model is saying all the individuals have a trajectory that's parallel to the

139
00:16:25,910 --> 00:16:35,330
population line over time and there's still measurement error so that each individual's line,

140
00:16:35,330 --> 00:16:39,980
whatever it is, can still be a little bouncy, not exactly on their individual trajectory.

141
00:16:45,700 --> 00:16:48,750
This is just copying the same model so we can look at a little bit more.

142
00:16:49,370 --> 00:17:00,110
And since the means for the random intercepts are assumed to be zero and the means for these residual errors are also assumed to be zero,

143
00:17:00,560 --> 00:17:05,630
the population mean regression line is just this beta, not plus beta one time.

144
00:17:05,660 --> 00:17:07,700
All the things that don't depend on I.

145
00:17:11,560 --> 00:17:18,640
And when we talk about person eyes regression line, we're kind of conditioning on whatever their random intercept is.

146
00:17:19,300 --> 00:17:26,830
And so conditional on that, the average outcome per person I is paid in a plus game I plus beta one time.

147
00:17:28,530 --> 00:17:34,710
And we can this is kind of a plot that's very that I used and recycled from our pair

148
00:17:34,820 --> 00:17:45,630
test data where they start off on at 50 on average and they end up on at 60 on average.

149
00:17:46,110 --> 00:17:55,110
And then here are two assumed trajectories for individual IV that this is the average outcome for person-I being plotted here.

150
00:17:57,070 --> 00:18:01,630
So that's the population mean trend. It's going to be better not as 50.

151
00:18:01,810 --> 00:18:07,330
And beta one is ten. So that on average, when they go to one, they're a 60 over here.

152
00:18:08,690 --> 00:18:11,900
So that's the bait and not plus bait a one time.

153
00:18:13,870 --> 00:18:23,740
And then for the first pair member sets with subscript I one this is the plot of what that individual mean.

154
00:18:23,860 --> 00:18:28,270
Trend is conditional on them having a random intercept of ten.

155
00:18:28,870 --> 00:18:36,550
So it just shifts the whole line up for the individual, you know, regression line.

156
00:18:38,280 --> 00:18:43,799
And. This other pair member with Subscript.

157
00:18:43,800 --> 00:18:47,790
I too has a random intercept that's -15.

158
00:18:47,790 --> 00:18:50,940
So relative to the population line, they're 15 less.

159
00:18:51,900 --> 00:18:55,830
And they're 15 less. No matter what time point you're looking at.

160
00:18:57,770 --> 00:19:02,690
So when you get an actual data set, you'll have different little measurements.

161
00:19:02,720 --> 00:19:06,500
I think this was just a pure data set, so you'll have measurements that are a little bit varied.

162
00:19:08,100 --> 00:19:15,510
At the beginning. In the end, because of this measurement error and the behind the scenes, your software is trying to estimate these things.

163
00:19:17,080 --> 00:19:19,809
It's not. It's close, but not quite the same.

164
00:19:19,810 --> 00:19:27,070
If you have more dots over time measured, it's it's not quite the same as running a separate individual regression line, but it's close.

165
00:19:27,580 --> 00:19:33,280
One of the features of this model is that when you don't have very many dots compared to other people in the dataset,

166
00:19:34,180 --> 00:19:37,690
your estimated line gets closer and closer to the population.

167
00:19:37,690 --> 00:19:42,250
Mean it's trying to borrow information from other people in the dataset to kind of fill in your pattern.

168
00:19:42,610 --> 00:19:45,790
If you don't have all your dots over time measured.

169
00:19:50,090 --> 00:19:54,600
So that is the that's the intercept model. Okay.

170
00:19:54,830 --> 00:19:58,900
The extra source of variability is how high your trajectory is relative to the population.

171
00:19:59,610 --> 00:20:04,900
So most of the time we don't think that everybody in the dataset has the same trajectory over time.

172
00:20:04,920 --> 00:20:13,020
We think they also have their own slopes. And so we're not going to look at the model that allows different trajectories for individuals.

173
00:20:13,410 --> 00:20:16,500
Oh, actually, we're I don't think I've got it in the rhythm slopes yet.

174
00:20:16,500 --> 00:20:19,740
I think I'm just adding in the treatment covariate as a fixed effect.

175
00:20:20,580 --> 00:20:28,050
We'll get there, though. So for the population model, the population, meanwhile, would like to have trends over time vary by treatment.

176
00:20:28,170 --> 00:20:31,980
We're getting closer and closer to being able to look at the city for data set.

177
00:20:32,520 --> 00:20:39,600
So just starting it off small, the average outcome beta not plus beta one times time plus beta two treatment.

178
00:20:41,620 --> 00:20:48,170
And this is the the plot of what we're assuming for the population mean, Miles,

179
00:20:48,190 --> 00:20:53,440
this seems very much like linear regression from your first linear regression class, right?

180
00:20:53,920 --> 00:20:57,700
So if treatment is equal to one, you have a different.

181
00:20:58,760 --> 00:21:02,240
A regression line that if you have treatment zero.

182
00:21:06,160 --> 00:21:11,020
So when you have just this one extra cover in the model.

183
00:21:12,100 --> 00:21:21,819
It's assuming that the treatment groups have parallel trajectories, so better to hire for treatment one versus treatment zero.

184
00:21:21,820 --> 00:21:26,830
So the difference between these types is this beta to term for treatment.

185
00:21:28,900 --> 00:21:37,840
And this is going to. So this is a bit of a perk up moment whenever you add any covariate to these mixed effects models.

186
00:21:39,750 --> 00:21:42,930
It's going to affect the height of the line.

187
00:21:44,900 --> 00:21:51,080
And effect it similarly over time. So if I if my main interest is in treatment.

188
00:21:52,860 --> 00:21:56,160
And I know I'm dealing with a randomized clinical trial.

189
00:21:57,020 --> 00:22:03,110
I this is probably not the way I would. Expect the treatment effect to manifest.

190
00:22:03,590 --> 00:22:09,950
For one thing, if you're in a clinical trial on average at time zero, the treatment hasn't.

191
00:22:11,050 --> 00:22:16,510
Her time to take effect. And so on average at time zero, the outcomes tend to be the same.

192
00:22:17,050 --> 00:22:22,290
So in almost any clinical trial. When you're looking at trajectories over time,

193
00:22:22,380 --> 00:22:31,590
you you're expecting to see that they start off at more or less the same value and that the trajectories sort of if there's a treatment effect,

194
00:22:31,590 --> 00:22:38,700
that they sort of diverge from there. And if you just put treatment in the model by itself, you don't allow that pattern.

195
00:22:40,220 --> 00:22:47,510
You're assuming that whatever treatment effect there is, it's immediately apparent and that it stays steady over time.

196
00:22:48,290 --> 00:22:56,870
So whenever we have repeated measures over time to really get a sense of treatment effect, we need to add an interaction with time.

197
00:22:58,150 --> 00:23:00,900
And that's going to be true for all variables in these models.

198
00:23:00,910 --> 00:23:09,760
Anything interesting that you want to examine, you need to think about its interaction with time to get there.

199
00:23:10,390 --> 00:23:14,020
Otherwise, if you just have a parameter that's added.

200
00:23:15,390 --> 00:23:25,650
It's going to say, well, I think the heights of people on average is affected by that variable, but it won't be talking about change over time.

201
00:23:27,250 --> 00:23:35,680
So this is something. So this perk up moment is don't just add in variables by themself without considering interactions with time.

202
00:23:38,810 --> 00:23:44,389
So. So just adding treatment by itself is assuming this, which we will want to change.

203
00:23:44,390 --> 00:23:52,440
But let's just look at get some practice. You know, thinking about these models.

204
00:23:54,860 --> 00:24:05,780
So what I'd much rather assume is that that the trajectory will change for the treatment groups over time.

205
00:24:05,780 --> 00:24:12,269
So this spate of three term allows for that to happen. So that I could have different slopes over time.

206
00:24:12,270 --> 00:24:16,950
And in the randomized trial, probably they'll start at the same point, at the same city for value.

207
00:24:19,870 --> 00:24:24,970
So if you have the interaction with treatment time, that assumes that different treatment groups have different trajectories.

208
00:24:27,070 --> 00:24:33,400
And I. If you want to look at the slope for the treatment equals zero group.

209
00:24:34,620 --> 00:24:39,040
Then you would. Be plugging.

210
00:24:39,060 --> 00:24:43,500
Treatment equals zero. Here and here.

211
00:24:43,830 --> 00:24:52,900
And it would just be dependent on beta one. And if you're looking for the true the slope for treatment equals one,

212
00:24:52,990 --> 00:24:59,920
you would plug in a one here and a one here, and you would have two terms that depend on time.

213
00:24:59,920 --> 00:25:08,180
Beta one plus beta three. So this feels like independent linear regression still.

214
00:25:09,090 --> 00:25:19,090
Right. But behind the scenes, the variabilities can be fixed up from the the random effects part.

215
00:25:19,480 --> 00:25:24,940
So question one Do we believe individual trajectories are always parallel to the population mean trend?

216
00:25:26,490 --> 00:25:33,720
And if yes to the above, then you can stay with the random intercept model, you know.

217
00:25:33,840 --> 00:25:38,220
So you could you could assume that everybody on treatment one,

218
00:25:38,850 --> 00:25:45,360
they might have different heights relative to the population mean for treatment one but they're otherwise parallel.

219
00:25:45,840 --> 00:25:56,280
And for treatment zero, whatever that treatment zero line is, people on treatment zero have similar parallel trajectories to that treatment group.

220
00:25:57,240 --> 00:26:01,770
And if you really believe that, then the random intercept model is still likely OC.

221
00:26:03,550 --> 00:26:11,680
But an alternative. And the most common alternative is to allow both the random intercept and have a random slope term for each individual so that.

222
00:26:14,030 --> 00:26:20,569
Over time, they can do anything they want to relative to their population.

223
00:26:20,570 --> 00:26:23,700
Mean. Okay.

224
00:26:23,800 --> 00:26:27,670
So once you have two random effects in the model,

225
00:26:28,990 --> 00:26:35,410
you move from just having a simple normal li distributed single intercept to

226
00:26:36,400 --> 00:26:43,720
a kind of a bell shaped curve distribution for these two random variables.

227
00:26:44,290 --> 00:26:48,280
So they're both individually normally distributed with zero mean.

228
00:26:49,910 --> 00:26:53,620
But they could potentially be correlated with one another as well.

229
00:26:53,630 --> 00:26:57,440
So you have a variance of the random intercept, you have a variance of the slope,

230
00:26:57,770 --> 00:27:05,000
and potentially you have correlation between those two that the package can estimate for you.

231
00:27:06,670 --> 00:27:10,090
And in incest. They call it the G matrix.

232
00:27:10,240 --> 00:27:12,610
Ah, I think R does the same.

233
00:27:15,270 --> 00:27:22,260
And I think that for simplicity, because there's so many ideas being pushed out at your once and they're hard to all unpack.

234
00:27:22,680 --> 00:27:30,450
For the most part, I just I fit models where I'm assuming the covariance between these two is zero, just so you don't have one more thing to model.

235
00:27:31,350 --> 00:27:37,710
But if you had a whole course on longitudinal data, you'd probably play with modeling the correlation between these as well.

236
00:27:38,040 --> 00:27:45,750
Or for this course, I'm just going to stick to thinking about the variability for the intercept and the slope and not adding this.

237
00:27:46,620 --> 00:27:50,549
You can sort of imagine you might because this is a correlation matrix.

238
00:27:50,550 --> 00:27:54,660
We could get very complicated with, you know,

239
00:27:55,350 --> 00:28:06,450
lots of unstructured matrix assumptions or r one or that whole deal we did with G e if you have a lot of these random effects,

240
00:28:06,450 --> 00:28:09,700
you could go to town with that. Okay.

241
00:28:09,820 --> 00:28:13,120
So for just simplicity to learn the material as simply as we can.

242
00:28:14,050 --> 00:28:19,060
I'm going to mostly be asking it to fit models, assuming that this correlation between the rhythm effects is zero.

243
00:28:22,550 --> 00:28:31,250
All right. So the overall model and I still don't have treatment in this particular version of it is just depending on time.

244
00:28:31,250 --> 00:28:36,590
You know, that the overall model is better, not plus better one times time,

245
00:28:37,700 --> 00:28:43,719
and then the random effects part of it is this random intercept per person ie random slope for person.

246
00:28:43,720 --> 00:28:48,590
I times time and then we still have the measurement error for every individual in the dataset.

247
00:28:50,470 --> 00:28:56,990
Every individual and every measure. So again, he's not putting in treatment yet.

248
00:28:57,410 --> 00:29:02,310
If I look at this model that we just saw on the last slide, you know,

249
00:29:02,930 --> 00:29:10,580
the typical questions are what's the population regression line that we're assuming and what's the individual regression line that we're assuming?

250
00:29:11,520 --> 00:29:16,600
And so the population mean regression line is all the things that don't depend on person eyes.

251
00:29:16,610 --> 00:29:24,040
So it's this part. So the average outcome is better up plus beta one time for the population regression line.

252
00:29:24,370 --> 00:29:27,970
This is the one that you're going to write about in your papers, right?

253
00:29:29,300 --> 00:29:41,100
The individual level regression lines for each person involve these random effects, but it's averaging across these individual measurement errors.

254
00:29:41,110 --> 00:29:47,060
So these are mean zeros. It only has the stuff, the random effects range intercept and slope for person-I.

255
00:29:47,570 --> 00:29:51,980
So we talk about individual regression lines conditional on their intercept and slope.

256
00:29:53,940 --> 00:30:05,670
So if I group together the intercepts person, I has an intercept beta not plus gamma zero I and if I put together everything that involves time,

257
00:30:05,910 --> 00:30:09,240
that person's slope is beta one plus gamma when I times time.

258
00:30:12,070 --> 00:30:16,270
So that's the intercept for personalized line and that's the slope for personalized line.

259
00:30:16,630 --> 00:30:19,930
This will be estimated for you by the packages.

260
00:30:20,110 --> 00:30:25,630
If you put in both random interceptions and a random slope for time.

261
00:30:30,230 --> 00:30:31,940
Okay. So now let's add in treatment.

262
00:30:35,720 --> 00:30:44,250
So the random intercept and slope model allowing different linear trends for the outcome data over time according to treatment looks like like this.

263
00:30:44,270 --> 00:30:47,540
So this is one of the simplest models, right?

264
00:30:47,570 --> 00:30:56,059
I just have treatment and time and here I haven't put in any other covariates and I have this interaction between

265
00:30:56,060 --> 00:31:04,880
treatment and time just because I don't necessarily believe that those two lines are parallel over time in the population.

266
00:31:06,080 --> 00:31:11,240
So the population main regression line is everything in here that doesn't involve AI.

267
00:31:11,250 --> 00:31:14,600
So it has Baidu, not plus Baidu to treatment.

268
00:31:15,110 --> 00:31:20,300
And then I've put everything that involves time together.

269
00:31:20,720 --> 00:31:24,830
So it's Baidu one plus Baidu three times treatment times time.

270
00:31:24,830 --> 00:31:34,340
So there's a little. Times here. All right, so that's the population mien regression line.

271
00:31:34,340 --> 00:31:37,430
And so there's one line for each treatment group.

272
00:31:39,960 --> 00:31:44,960
It's the population intercept. Better not.

273
00:31:44,970 --> 00:31:48,560
And then the intercept really depends on what treatment group you're in.

274
00:31:53,500 --> 00:31:58,420
And then this is the population slope. And again, the population slope depends on which treatment you're in.

275
00:32:02,200 --> 00:32:07,210
And if you're looking at the individual level regression lines conditional on their random effects for person

276
00:32:07,220 --> 00:32:13,660
I that's going to look like they do not plus beta two times treatment plus the random intercept part.

277
00:32:13,900 --> 00:32:16,690
So these are all the parts that don't depend on time here.

278
00:32:18,610 --> 00:32:27,640
And then for their the slope for individual I it's beta one plus beta three times treatment plus this other term that depends on time.

279
00:32:28,970 --> 00:32:32,990
And the only thing that's kind of left out is the random error, the means here, a random error part.

280
00:32:38,680 --> 00:32:42,209
So that's this whole first line is the intercept for personnel.

281
00:32:42,210 --> 00:32:48,570
I depends on treatment. And that's the slope for personally that also depends on treatment.

282
00:32:48,580 --> 00:32:52,110
So it's always kind of relative to what the the slope is.

283
00:32:53,130 --> 00:32:58,500
For that particular treatment group. All right.

284
00:32:58,500 --> 00:33:09,770
So we're finally ready to look at the the data set, because we've got at least one model that we've kind of seen how it works in general.

285
00:33:09,780 --> 00:33:20,610
So let's look at the model that assumes a linear trend in this large city for I think it's log C four plus one count over time my treatment.

286
00:33:21,810 --> 00:33:30,510
So the treatment group is triple versus, not triple and the time variable is weak here.

287
00:33:30,510 --> 00:33:38,370
So we want this population mean model that allows the mean log city for count to change linearly over time.

288
00:33:38,790 --> 00:33:43,500
And I'm submerging subscripts because it's just so much easier to read with all the i's in the js.

289
00:33:47,190 --> 00:33:54,090
And so if I look at people who are not taking the triple therapy, that means triple is equal to zero.

290
00:33:55,280 --> 00:34:04,520
All that's left is beating up us beater one week. So this is the trajectory we assume for the people not taking triple therapy over time.

291
00:34:04,970 --> 00:34:08,520
So just a straight line. With the slope beta one.

292
00:34:10,150 --> 00:34:15,540
And for patients taking triple therapy, we plug in one over here.

293
00:34:16,580 --> 00:34:24,770
And so the intercept for those people are going to be better not post the beta two because beta choose just times a one here.

294
00:34:26,450 --> 00:34:29,509
And the slope is going to be everything that's left that involves weeks.

295
00:34:29,510 --> 00:34:34,310
So that's beta one and then triples is going to be one here. So we have made it three times a week as well.

296
00:34:36,660 --> 00:34:41,280
So each treatment groups longitudinal trajectories is said to be linear over time.

297
00:34:47,120 --> 00:34:51,519
Right. And so this still feels like when you're doing work with the population model,

298
00:34:51,520 --> 00:34:59,140
it still feels like you're doing similar exercises to what you did with linear regression in your first linear regression course.

299
00:35:02,340 --> 00:35:04,230
So for the random effects part of the model,

300
00:35:04,950 --> 00:35:13,170
we're going to look at the model with a random intercept and then compare that to the model with both random intercept and slope.

301
00:35:13,410 --> 00:35:18,720
Just to get a sense of what that does to the the data, the output, all that stuff.

302
00:35:21,610 --> 00:35:33,310
So here we're going to do both sets and our code, I have to say and warn you that stealth wins the game here for mixed models, in my opinion.

303
00:35:33,700 --> 00:35:38,319
So I'm going to be showing you both of these pieces of code.

304
00:35:38,320 --> 00:35:44,020
But there are real advantages to learning how to do this in SAS if you've been in our user to date.

305
00:35:45,340 --> 00:35:53,020
So the most trusted estimation method when you're fitting these models is called restricted maximum likelihood estimation,

306
00:35:53,020 --> 00:36:01,360
and you really only need to know that term because you are putting the method equals R.E.M. here.

307
00:36:01,570 --> 00:36:11,590
When you want to report your final model, there's another choice here that you will occasionally have to use if you're doing likelihood ratio tests,

308
00:36:11,590 --> 00:36:20,920
comparing models, if you're doing like the two ratio tests, comparing models, you have to change this yellow term to ML for maximum likelihood.

309
00:36:21,400 --> 00:36:28,360
And so this is kind of a perk up detail that when you report your final results, you want to be using R.E.M. here.

310
00:36:28,720 --> 00:36:34,480
But if you're on your way trying to figure out what the model is and comparing likelihoods,

311
00:36:34,480 --> 00:36:40,780
you have to switch it to EML to be able to use those likely two ratio tests and P values to make those decisions.

312
00:36:43,730 --> 00:36:50,930
So proc mixed method equals R.E.M. And there's a few options here.

313
00:36:50,940 --> 00:36:59,840
There's no CEO print equals one is helpful because it requests to not print class levels unless there's only one outcome in that idea.

314
00:36:59,870 --> 00:37:03,139
And there's this is a huge data set.

315
00:37:03,140 --> 00:37:06,290
So this is going to save you a lot of scrolling to find the output.

316
00:37:07,820 --> 00:37:14,030
And Cove test requests, information on variants and covenants terms and if they are useful in the model.

317
00:37:14,210 --> 00:37:19,280
So we'll see what that looks like and see, you know, if those terms seem useful in the model.

318
00:37:22,040 --> 00:37:25,969
The class I.D. is always going to be needed.

319
00:37:25,970 --> 00:37:32,480
Whatever the variable is that identifies that that the outcomes are from the same person or cluster.

320
00:37:32,720 --> 00:37:36,000
You need that variable here and you also need that variable here.

321
00:37:36,020 --> 00:37:45,810
Subject equals ID. So it needs to be both in the class statement and the random statement and and really this random statement.

322
00:37:46,840 --> 00:37:50,709
Uh, is the one where you're putting in your random effects.

323
00:37:50,710 --> 00:37:55,530
And for now, we're just doing a random intercept. So you just put intercept here.

324
00:37:56,050 --> 00:38:03,280
It knows what that is. So we have kind of two model statements, really.

325
00:38:03,280 --> 00:38:11,739
We have the model statement and the random statement. The model statement is putting in the population mean model with the fixed effects.

326
00:38:11,740 --> 00:38:18,250
So these are the covariates you want to eventually interpret. In your paper, the population model.

327
00:38:18,270 --> 00:38:21,510
So we have a weak triple and triple times a week.

328
00:38:22,560 --> 00:38:27,660
To get just a first peek at what this model can do for looking at treatment effects in this study.

329
00:38:29,570 --> 00:38:39,050
And the second, this random statement is all about the variability of the trajectories over time for the individual.

330
00:38:39,070 --> 00:38:47,780
So random intercept here will eventually look at random intercept and also have we here to get a random slope in there.

331
00:38:53,450 --> 00:38:59,690
And I have this option as after the slash in both the model statement and the random statement

332
00:38:59,700 --> 00:39:03,860
and in the model statement as requests the parameter estimates for the fixed effects.

333
00:39:04,370 --> 00:39:12,080
It's surprising, but you have to actually ask for it with S or it won't print the model you want to interpret the most.

334
00:39:12,180 --> 00:39:20,260
Not sure why they do it that way. In the random statement, the s request estimates for the random effects for personal.

335
00:39:20,330 --> 00:39:25,870
So you can actually look at these and they have a name. They're called blowups for best linear, unbiased predictors.

336
00:39:25,880 --> 00:39:31,590
It's a technicality that you don't need to really. No much about or anything about, really.

337
00:39:31,950 --> 00:39:36,600
But you can ask for personalized random effects to be printed out if you want to look at them.

338
00:39:39,770 --> 00:39:43,940
And then the G and the G core are estimates of the G matrix.

339
00:39:43,940 --> 00:39:48,260
That is the cover is in correlation related to the distribution of random effects.

340
00:39:49,040 --> 00:39:55,669
So we know we thought the most complicated version of this we saw was for the random intercept.

341
00:39:55,670 --> 00:40:01,670
And Slope was a two by two matrix. But for this model, there's only going to be one term.

342
00:40:01,680 --> 00:40:07,700
So it's going to be kind of a boring little number. Nothing matrices about it whatsoever, the random intercept model.

343
00:40:08,180 --> 00:40:12,050
But you're going to look at that there because everybody has different.

344
00:40:14,870 --> 00:40:18,290
Well, actually, that's not important for this G matrix.

345
00:40:18,560 --> 00:40:25,430
For The Matrix. You might have different numbers of measures over time.

346
00:40:25,430 --> 00:40:34,070
And so they tend to report this information for just the first person than the data set if it varies across people in the data set.

347
00:40:35,570 --> 00:40:42,080
So this is a request for estimates of the overall variances and covariance is that the current model is assuming

348
00:40:42,800 --> 00:40:47,690
including the random effect and error related variability and it's reported for the first person and then the dataset.

349
00:40:47,690 --> 00:40:54,440
So this is for the first person in the dataset. It's trying to estimate the correlation between their first measure and their second measure.

350
00:40:54,440 --> 00:41:02,060
The first measure and the third measure, you know, kind of like as if you had this nice model for the covariance,

351
00:41:02,750 --> 00:41:09,620
but since everybody has potentially their own measurement times, this could be different for each individual in your dataset.

352
00:41:09,620 --> 00:41:19,510
It's doing its best to estimate that for each person. So this is finally what some of your output looks like for the random intercept model.

353
00:41:19,540 --> 00:41:28,000
First, you want to make sure that the model converges because you're asking for a lot of things to be estimated.

354
00:41:28,030 --> 00:41:32,230
This is a big data set, so I didn't anticipate any problems with convergence.

355
00:41:32,560 --> 00:41:35,950
But for smaller datasets, we've seen what happens.

356
00:41:35,960 --> 00:41:36,340
You know,

357
00:41:36,340 --> 00:41:45,970
we had that example of the pancreatic enzyme study where we attempted to do something unstructured for the covariance and it it crashed and burned.

358
00:41:46,540 --> 00:41:50,020
So you always want to double check that you've got convergence criteria met.

359
00:41:50,920 --> 00:42:04,060
This is the G matrix. So this is the matrix of the correlation between the different random effects or the covariance of the random effects, rather.

360
00:42:04,660 --> 00:42:09,520
And so we only had one term in the random effects model, just the intercept.

361
00:42:09,520 --> 00:42:13,000
So this is the variability term for the random intercept.

362
00:42:13,480 --> 00:42:15,040
And we talked a little bit about this.

363
00:42:15,040 --> 00:42:23,560
If you want to just compare notes in an earlier handout, we talked about the variability of random intercepts in handout 14, Slide 47.

364
00:42:23,890 --> 00:42:29,350
And one of the things that we did in Handout 14 was show that if you have a random intercept assumption,

365
00:42:29,980 --> 00:42:38,770
it's doing the same thing as assuming that all all pairs of observations are equally correlated.

366
00:42:39,490 --> 00:42:47,799
So when we were doing G, we called that assumption exchange ability or compound symmetry.

367
00:42:47,800 --> 00:42:52,840
I can just see that the jargon is just there's so much of it, like almost every other word is jargon.

368
00:42:52,840 --> 00:42:59,079
I apologize if I'm going to expose it. I'm trying to expose it to you slowly so that you can absorb it.

369
00:42:59,080 --> 00:43:06,610
But I hear myself talking like, Oh my God, jargon, jargon, jargon. So the main point I'm trying to make here is that when we looked at this earlier,

370
00:43:07,180 --> 00:43:13,030
if you use a random intercept assumption for the variability or you assume compound

371
00:43:13,030 --> 00:43:17,860
symmetry or exchange ability when you're doing an assumption about your variability,

372
00:43:18,130 --> 00:43:25,480
they're trying to model the same things. And you're and the reason I'm bringing it up is that you can see that assumption happening right here.

373
00:43:25,480 --> 00:43:31,240
Here's the estimated V matrix just for the first person in the data set.

374
00:43:31,240 --> 00:43:34,870
So I don't know the measurement times for the person.

375
00:43:35,200 --> 00:43:38,950
That's the first person in this data set because that can be different for every person.

376
00:43:39,550 --> 00:43:52,060
But this is sort of estimating the vary the variance covariance matrix for the person and one's first measurement time with itself,

377
00:43:52,810 --> 00:43:54,670
the the measurement,

378
00:43:55,420 --> 00:44:03,190
the covariance between their first measure and their second measure, the covariance between their first measure and their third measure.

379
00:44:03,190 --> 00:44:04,210
And so one.

380
00:44:05,490 --> 00:44:13,500
And when we did, gee, this was easy to imagine in your head, because you could just visualize in your head a column of numbers for everybody's person.

381
00:44:14,010 --> 00:44:17,450
Measurement one A column of numbers for everybody's. Measurement two.

382
00:44:17,460 --> 00:44:24,660
And you can sort of in your head see that you could just do a correlation between those columns of numbers to get a number.

383
00:44:25,560 --> 00:44:32,520
Something more complex is happening behind the scenes here because not everybody has all the measurements done at the same times.

384
00:44:32,970 --> 00:44:40,140
So the model is recreating their best estimate of what this correlation is for person one.

385
00:44:41,250 --> 00:44:48,560
And all of our diagonal terms are the same. And so that is something that you will always see for a random intercept model.

386
00:44:48,570 --> 00:44:52,470
You'll always see all of the off diagonal elements are going to be the same.

387
00:44:59,410 --> 00:45:02,709
And so, again, if you want to kind of tie that back to handout 14,

388
00:45:02,710 --> 00:45:10,630
we sort of had an argument for why you would always see the off diagonal elements the same as we

389
00:45:10,630 --> 00:45:17,470
compared it to a G approach where the exchangeable correlation or compound symmetry correlation.

390
00:45:18,710 --> 00:45:30,290
So you can refresh your memory on that. But this is kind of the only time a random effects model will do the same assumption as a model.

391
00:45:30,320 --> 00:45:32,050
It's the only time that this works.

392
00:45:32,060 --> 00:45:38,840
If it's a random intercept model every other time, it's totally different assumptions for the variability matrices.

393
00:45:42,590 --> 00:45:50,060
All right. And then this is over here. This 0.3843.

394
00:45:50,480 --> 00:45:53,750
That's the residual random error.

395
00:45:54,980 --> 00:46:02,690
We also mention that in handout 14. So this is this is like that sigma squared e that we've gotten the model.

396
00:46:04,260 --> 00:46:10,139
And so just looking at this again, what are the main points of the compound symmetry pattern as mentioned in handout

397
00:46:10,140 --> 00:46:14,040
four is happening with the random intercept model and that will always be the case.

398
00:46:15,690 --> 00:46:20,519
And by looking here, this is the random intercept variability again.

399
00:46:20,520 --> 00:46:27,600
So this .7601, they're just repeating all of these .76 ones that we saw over here.

400
00:46:28,970 --> 00:46:31,160
So that's the same number.

401
00:46:31,640 --> 00:46:39,350
And this line over here with this small P value is saying, boy, that was a very important source of variability to put in your model.

402
00:46:39,380 --> 00:46:42,980
The random intercept variability is saying that, yes,

403
00:46:42,980 --> 00:46:48,350
each individual trajectory is really starting off at a different height compared to the population mean.

404
00:46:48,590 --> 00:46:50,930
And it's capturing a lot of the variability.

405
00:46:51,530 --> 00:46:59,899
And in fact, if you look at it, this random intercept variability is roughly twice that of the random error you see so well in terms of scale,

406
00:46:59,900 --> 00:47:06,530
that's explaining a large percentage of the variability, the outcomes that people start off at different C four counts, you know,

407
00:47:07,040 --> 00:47:15,950
and and that is one of the most important ways to think about the variability in this dataset is that they all start off at different heights.

408
00:47:18,170 --> 00:47:28,800
Different city, four counts. So I want you to just appreciate that this whole slide is about variability of the data.

409
00:47:30,290 --> 00:47:33,530
How have you been gotten to the estimates that we would put in our paper?

410
00:47:33,540 --> 00:47:39,200
This is all about the variability in the data and the assumptions that the random intercept model is using.

411
00:47:39,740 --> 00:47:43,870
So I suspect, you know, this is something that we spend time trying to figure out.

412
00:47:43,880 --> 00:47:49,790
Did we model the variability of the data correctly because this is important to get our P values correct.

413
00:47:54,870 --> 00:47:59,279
So here now finally is some output from our data set.

414
00:47:59,280 --> 00:48:02,729
So I've got some fit statistics that we can use.

415
00:48:02,730 --> 00:48:07,200
So we're going to have likelihoods that we can do likelihood ratio tests with.

416
00:48:07,200 --> 00:48:10,859
But we remember that we we need to have method equals.

417
00:48:10,860 --> 00:48:19,320
ML So I can't actually use this value for the liquid ratio test to to make decisions about the model.

418
00:48:19,620 --> 00:48:22,860
I would have to change the method to ML for that to be correct.

419
00:48:23,310 --> 00:48:28,290
But you can still look at the AIC to get an impression of model fit.

420
00:48:30,660 --> 00:48:36,030
The world tests are going to be your most useful statistic because the world tests are valid.

421
00:48:36,030 --> 00:48:40,350
Whether you've got method equals ML or method equals REMO.

422
00:48:41,460 --> 00:48:47,640
So if you can do all of your hypothesis test based on this column of P values, you're in good shape.

423
00:48:48,030 --> 00:48:52,080
You can just without having to refit the model, you can look at those and they're going to be valid.

424
00:48:53,400 --> 00:48:58,500
But when you have categorical variables where you want to test more than one parameter can be removed at the model.

425
00:48:58,500 --> 00:49:02,819
At the same time, it's going to be a hassle because you're going to have to go back and change

426
00:49:02,820 --> 00:49:07,920
the method equals to eml to do the likelihood ratio tests that you want to do.

427
00:49:09,960 --> 00:49:17,580
If you want to get PE values, but for choosing between variability assumptions, as long as you have the same fixed effects in the model,

428
00:49:18,450 --> 00:49:27,230
AIC is recommended so it performs okay and as good as anything else provided by software, it's not perfect, it's not great, but it's decent.

429
00:49:27,690 --> 00:49:35,679
So if I wanted to. Keep these same fixed effects in the model and figure out whether the random intercept

430
00:49:35,680 --> 00:49:41,230
model is good enough or if I should be using the random intercept and slope model.

431
00:49:41,500 --> 00:49:46,630
I can look at the AIC where the only thing I've changed is the random statement.

432
00:49:49,010 --> 00:49:52,190
And that would be that would be a good way to choose between them.

433
00:49:52,190 --> 00:49:56,750
So we'll do that. So this AIC value would be okay to use.

434
00:49:57,720 --> 00:50:04,800
As long as I don't change any of the fixed effects when I'm trying to make that one change to adding the random slope in for week.

435
00:50:07,380 --> 00:50:13,250
So sharing common assumptions for outcome variability you can.

436
00:50:13,870 --> 00:50:18,419
So if you always assume random intercept and that you're happy with that assumption

437
00:50:18,420 --> 00:50:23,010
for variability and you want to look at the fixed effects part of the model two

438
00:50:23,010 --> 00:50:27,809
population mean models with nested fixed effects can be compared using likelihood

439
00:50:27,810 --> 00:50:32,100
ratio test methods or these while p values if they're okay in the usual way.

440
00:50:32,100 --> 00:50:39,390
But you should use method equals ML instead of method equals remo for this calculation, if you have to do the likelihood ratio test,

441
00:50:39,750 --> 00:50:48,150
so it's usually easier to deal with covariates where you already just have the numbers given to you in the output in this table.

442
00:50:49,990 --> 00:50:55,660
So that's the strategy really. When you're building these models, you're building two models at the same time.

443
00:50:55,660 --> 00:50:57,700
You're building the part of the model.

444
00:50:58,120 --> 00:51:04,450
That's the solution for the fixed effects model, the one you want to interpret and put in many script worthy sentences.

445
00:51:04,870 --> 00:51:13,990
But you're also building the model for the random statement, the random variability, and you can only work with changes to one of them at a time.

446
00:51:14,230 --> 00:51:17,470
So if you're working on the fix effects part of your model,

447
00:51:17,830 --> 00:51:26,080
you want to be using the same variability assumptions about your random effects while you play with the fixed effects and.

448
00:51:27,820 --> 00:51:30,850
If you're trying to figure out the correct variability,

449
00:51:31,480 --> 00:51:36,650
then you need to keep the fixed effects part of your model the same and just play with the the random effects.

450
00:51:36,970 --> 00:51:43,450
So only tweak one at a time and there's a little bit of back and forth because you're,

451
00:51:43,540 --> 00:51:46,659
you know, you have to have both correct to really know what you're doing.

452
00:51:46,660 --> 00:51:55,569
So there's usually a process where people try to figure out the best variability, assumptions that they need and then work on fixed effects.

453
00:51:55,570 --> 00:52:03,450
But you can go back and forth a little bit. Just so you see what they look like.

454
00:52:05,000 --> 00:52:14,629
These are the estimate that the best linear, unbiased predictor estimates or the estimates for the random intercept terms for each individual.

455
00:52:14,630 --> 00:52:16,910
And I'm just showing the first six people here.

456
00:52:17,970 --> 00:52:33,930
And so each of these is the difference that the model is using behind the scenes to say person eyes regression line is .2586 units higher than

457
00:52:33,930 --> 00:52:46,680
the population mean regression line and person six is individual regression line is mine .4267 lower than the population regression line.

458
00:52:46,680 --> 00:52:53,860
So this is behind the scenes being estimated. And I want this to be another perk up moment.

459
00:52:54,010 --> 00:53:03,710
Gosh, there are so many perk moments here. But I've seen people attempt to use this for these estimates for each individuals.

460
00:53:03,730 --> 00:53:11,650
That is, if there were actual data for that person. And I want to tell you, in no uncertain terms, never do that.

461
00:53:12,550 --> 00:53:15,550
This is not individual level data.

462
00:53:16,030 --> 00:53:20,370
These estimates use information from all the data in the dataset.

463
00:53:20,380 --> 00:53:27,010
So you could never, ever come up with 0.2586 if you only had person ones data.

464
00:53:28,240 --> 00:53:31,930
That number is using information from everybody in the data set.

465
00:53:32,500 --> 00:53:42,820
And if person one had fewer measures than other people, it's going to kind of be leaning on the population mean to fill in this intercept.

466
00:53:43,480 --> 00:53:52,720
So you cannot assume that these are values that would be available from person one's data alone.

467
00:53:52,960 --> 00:53:57,460
So you can't they're not going to represent the variability in that person correctly.

468
00:53:58,120 --> 00:54:03,909
So something that I've seen and squash as a reviewer is people trying to treat

469
00:54:03,910 --> 00:54:07,750
these random effects as if they're real data for that person in that person alone.

470
00:54:09,340 --> 00:54:12,790
No, never do that. Okay.

471
00:54:13,750 --> 00:54:17,020
So this is sort of a curiosity that you can look at in your output.

472
00:54:17,020 --> 00:54:25,360
But I, I actually don't ever really use these for anything other than trying to think about the variability of the data.

473
00:54:26,620 --> 00:54:33,880
And that reason alone, it's just a convenient way to talk about how people's data over time can be different from each other.

474
00:54:34,030 --> 00:54:38,540
That's it. All right.

475
00:54:38,540 --> 00:54:43,460
Here's so that I felt like I was on a soapbox there, but that's such an important thing to know.

476
00:54:44,780 --> 00:54:48,910
So here's our code for the same. Mixed effects model.

477
00:54:48,970 --> 00:55:01,450
And so the way the the packages and LME that I'm using and the function is LME and it has the fixed part of the model here with the fixed equals part.

478
00:55:01,450 --> 00:55:05,319
And so here's the outcome. This is like the Equal Week Plus, Triple Plus,

479
00:55:05,320 --> 00:55:11,440
triple times week and then for the random part of the model you go random equals and if it's only the intercept,

480
00:55:11,440 --> 00:55:14,799
you go tilde one But otherwise you can add in.

481
00:55:14,800 --> 00:55:25,150
We'll eventually have a plus. We care if we want a random slope after the pipe is the variable that is saying, you know, measures for the same person.

482
00:55:25,900 --> 00:55:34,600
So this is the variable with all the ID numbers saying, you know, which measures go with which person method equals remote goes here.

483
00:55:34,840 --> 00:55:40,420
The other choices, Mel, you know, if you need to do like the two ratio tests and then the here's just,

484
00:55:41,080 --> 00:55:45,820
you know, some code where I kind of pasted it together. Interesting things from the data set.

485
00:55:48,140 --> 00:55:54,710
That, you know, I printed out here as I showed SAS people the random intercepts of is how you can get it in R.

486
00:55:56,950 --> 00:56:08,439
So so what is the output look like an R? So here is the the standard deviation actually of the random intercept.

487
00:56:08,440 --> 00:56:14,200
So it's present on the standard deviation scale and here is the standard deviation for the members treatment error.

488
00:56:14,200 --> 00:56:19,960
So it's given on the standard deviation scale. So in SAS we got squared versions of these numbers.

489
00:56:22,070 --> 00:56:24,139
And here is our fixed effects.

490
00:56:24,140 --> 00:56:32,540
This is the population mean parameter estimates, the betas that we've been talking about and the world test that we use.

491
00:56:34,380 --> 00:56:38,540
Okay. All right.

492
00:56:38,540 --> 00:56:43,410
So let's. Let's take a break. Meet at 907.

493
00:56:43,440 --> 00:56:47,909
I just want you to have a moment to let your brain unpack a little bit of the jargon.

494
00:56:47,910 --> 00:56:53,640
And the next thing we're going to do is add in a random slope to the random part of the random part of the model.

495
00:56:54,450 --> 00:56:59,890
So. So much information being pushed on you so quickly.

496
00:56:59,900 --> 00:57:07,129
So just take a break and and prepare, you know, stretch the muscle, do your stretches like drinking coffee can be a stretch.

497
00:57:07,130 --> 00:58:46,850
I don't know. I guess.

498
00:58:51,390 --> 00:59:03,150
Yeah. About.

499
00:59:13,070 --> 00:59:21,980
I do. I just. I don't want to do that. You know, I was like.

500
00:59:22,050 --> 00:59:49,020
I was. Okay.

501
00:59:49,740 --> 00:59:55,900
That's. Yeah.

502
00:59:57,610 --> 01:00:01,170
What? With this area.

503
01:00:03,810 --> 01:00:07,140
I think it has something to do with the levels of this part.

504
01:00:07,240 --> 01:00:11,340
Okay. Could you turn it a little closer to me and put my mask on some?

505
01:00:13,590 --> 01:00:20,460
You want to keep getting a couple of different things, but I keep getting like an error saying with these levels on.

506
01:00:23,710 --> 01:00:34,130
Okay. So. Can you just type in my career so we see what that looks like.

507
01:00:36,040 --> 01:00:41,349
Just summary. Yeah. Now, down here, if you want to see what a variable looks like, you just type it in.

508
01:00:41,350 --> 01:00:47,920
It'll show you. All right. So you've got 00100111.

509
01:00:48,120 --> 01:00:52,320
And so you're looking for each of those profiles, you know, if devastating step.

510
01:00:55,070 --> 01:00:58,670
Okay. Um. Why are they? Where are they?

511
01:01:00,760 --> 01:01:13,180
As they expand. So if you see this will try to prove the fact that you want to do or you just want to have.

512
01:01:18,160 --> 01:01:26,739
What the does doesn't. So it's taking it takes all the levels of first cover, all levels of the second degree, and it sort of just.

513
01:01:26,740 --> 01:01:30,890
Oh, okay. So if you if I add them.

514
01:01:31,310 --> 01:01:34,850
Yeah. So I don't know which one you're saying your title says.

515
01:01:36,770 --> 01:01:41,660
Survival by gender or status.

516
01:01:42,790 --> 01:01:46,270
Is that what you want to do? Okay.

517
01:01:46,630 --> 01:01:55,940
So this will give you four. It's it's doing all those for looking for each situation.

518
01:01:55,960 --> 01:02:00,520
So if it equals zero for okay, that actually helps.

519
01:02:01,180 --> 01:02:05,720
Thank you. But we didn't find your error. I think it should add the.

520
01:02:07,000 --> 01:02:17,800
Into this day. So that's going to give you some duplicates.

521
01:02:19,590 --> 01:02:27,190
So. Serve that serve stated that email.

522
01:02:33,760 --> 01:02:37,480
Preserve date about mail is you've already said that part where.

523
01:02:38,960 --> 01:02:42,770
Is that right? Yes.

524
01:02:42,780 --> 01:03:04,340
Yes. All right. So I don't think you want to add in the.

525
01:03:04,930 --> 01:03:08,500
I don't think you want to add in more stuff because that's not going to give you.

526
01:03:09,250 --> 01:03:13,750
Like if you run that in my career, if you like a lot, a lot of stuff,

527
01:03:14,050 --> 01:03:17,240
I don't think you want to do that, but you could want to see what it's doing to learn.

528
01:03:26,920 --> 01:03:31,690
So can we just can you type serve dated that mail and copy and paste it so we can see what it looks like.

529
01:03:57,420 --> 01:04:02,820
Oh. So help me.

530
01:04:02,980 --> 01:04:10,630
It's so you didn't do one to choose one for me.

531
01:04:11,810 --> 01:04:21,310
Yeah. So when you're doing. When you're doing these variables over here, try typing as factor parentheses for a variable like right here.

532
01:04:21,910 --> 01:04:30,050
Yeah. It used to be as high as we have for the institutional master.

533
01:04:34,070 --> 01:04:39,320
Okay. I'm ready for that now. And now try the other stuff and see if it works.

534
01:04:51,950 --> 01:04:57,780
Yeah. I don't know if you're you.

535
01:04:58,760 --> 01:05:08,060
Generous enough to show stuff, by the way. But you have you're going to have four lines, but you only have one.

536
01:05:09,080 --> 01:05:12,130
Label for the merger. And so.

537
01:05:15,570 --> 01:05:20,000
So you're asking it to do four lines. Oh, yeah.

538
01:05:20,240 --> 01:05:23,420
So you've got long time, but you've got two choices here.

539
01:05:23,420 --> 01:05:25,430
You need to have four choices.

540
01:05:26,400 --> 01:05:31,890
So I don't know if you want to go from a solo comedy or how you want to do that, or if you want to just do different line type.

541
01:05:33,060 --> 01:05:38,670
I need to have. More than one legend title if you're going to have a title.

542
01:05:40,140 --> 01:05:49,170
So to do that, you would see to parentheses, quote, gender, quote unquote, next thing quote.

543
01:05:49,590 --> 01:05:55,290
And you're probably going to want to put it in the same order as your my superior, my prettier student.

544
01:05:56,160 --> 01:06:00,450
So remember how when we typed up my phobia, we had to use something.

545
01:06:00,520 --> 01:06:08,900
Each of those is the profile. And so it's one of those magic titles you're going to put in that order is okay.

546
01:06:09,390 --> 01:06:13,200
But I bet that's what it's going to make sure I get.

547
01:06:20,240 --> 01:06:38,920
Okay. Okay.

548
01:06:40,170 --> 01:06:45,060
You feel stretched out brain wise. Feel like you're ready for the next lap.

549
01:06:46,620 --> 01:06:58,729
Okay. So let's get back to work then. So I'm going to repeat myself often just because I know that it's going to help for a while to repeat myself.

550
01:06:58,730 --> 01:07:07,400
So we just fit the random intercept model, which again is assuming that all of the individuals in this study have the same.

551
01:07:08,680 --> 01:07:13,110
Slope. As the population means.

552
01:07:13,200 --> 01:07:19,500
So their slope just depends on. We can triple right now, whichever.

553
01:07:20,940 --> 01:07:28,260
Treatment there on your random intercept model was assuming that their trajectory was just,

554
01:07:28,560 --> 01:07:35,300
you know, parallel but higher or lower than other people from their same treatment group.

555
01:07:36,770 --> 01:07:43,100
And so we want we know that the we looked at the hairball and we couldn't really see what was going on in our hairball plot.

556
01:07:43,250 --> 01:07:47,600
The spaghetti plot is no one really calls it hairball plot, but it's the spaghetti plot.

557
01:07:47,960 --> 01:07:53,420
And it was really hard to see, but most of the time people have their own slopes.

558
01:07:53,420 --> 01:07:58,670
Some people are getting better over time, some people are getting worse, and some people get better faster than the population average.

559
01:07:58,670 --> 01:08:01,670
Some people get worse faster than the population average.

560
01:08:02,120 --> 01:08:09,470
And so accounting for that in the data, the most common thing we put in is this random week.

561
01:08:09,620 --> 01:08:12,700
So this is the name of whatever your time variable is in this dataset.

562
01:08:13,040 --> 01:08:18,120
It's week. And you can keep on adding stuff to this line.

563
01:08:18,480 --> 01:08:25,620
In this handout, where we don't go full through the whole possible analysis of every possible random effect.

564
01:08:26,190 --> 01:08:36,299
But you could keep adding terms here if you thought that everybody had a random bend after 16 weeks, you could put in a spline term here as well.

565
01:08:36,300 --> 01:08:39,310
And the random statement. That would allow for that.

566
01:08:39,670 --> 01:08:44,280
And so you can get very complicated here very quickly.

567
01:08:44,290 --> 01:08:50,830
And at some point you might get so complicated, your analysis breaks because you're asking it to estimate more and more terms.

568
01:08:51,280 --> 01:08:55,540
So people tend to be as conservative as they can while representing the data well.

569
01:08:55,930 --> 01:09:00,250
So usually random intercept and and slope is pretty decent.

570
01:09:02,050 --> 01:09:08,890
That's the only thing that's changed. The main effect part, the fixed effect part of the model is still weak.

571
01:09:08,920 --> 01:09:14,680
Triple, triple by week. We haven't added any covariates at all, like age and gender and so on.

572
01:09:15,850 --> 01:09:19,899
And this is this slide is all about the variability.

573
01:09:19,900 --> 01:09:24,970
Again, there's nothing about the parameter estimates that we want to interpret yet.

574
01:09:25,180 --> 01:09:28,570
So first convergence criteria met. We need to keep checking for that.

575
01:09:30,800 --> 01:09:37,700
And the way I put in the the random effects actually let me pull that up again.

576
01:09:39,230 --> 01:09:47,130
The way I put in these random effects, I didn't assume any variability between these two random intercept, random slope terms.

577
01:09:47,150 --> 01:09:56,930
There's a way to do that with a type equals. So if this was a class that was solely devoted to modeling repeated measures over time,

578
01:09:57,170 --> 01:10:02,059
I might take you through some choices for the type of correlation you could assume.

579
01:10:02,060 --> 01:10:11,300
But if you don't put type equals with some matrix like you went for unstructured, for example, you're assuming these are independent of one another.

580
01:10:13,880 --> 01:10:18,800
And so that's what this output is saying. They don't have any estimates along the off diagonal.

581
01:10:18,800 --> 01:10:22,790
So they're assuming that this is the variability for the random intercept and this

582
01:10:22,790 --> 01:10:28,190
is the variability for the random slope over time and that they're uncorrelated.

583
01:10:28,520 --> 01:10:36,410
So when would you question that assumption? Well, so if if the slope that people had, depending on what height they started at.

584
01:10:37,370 --> 01:10:46,280
Then those are correlated, right? So if you could conceivably imagine a scenario where if they're really, really healthy,

585
01:10:47,150 --> 01:10:50,959
there's slopes tend to be pretty shallow because they're not really changing much.

586
01:10:50,960 --> 01:10:58,240
But if their three four counts were really, really bad to start with, maybe they're decreasing really fast.

587
01:10:58,250 --> 01:11:02,990
So you could imagine scenarios where, depending on where their height of their C4 count started,

588
01:11:03,860 --> 01:11:10,010
they might tend to have individual slopes that behaved differently and in a correlated way to where they started.

589
01:11:11,530 --> 01:11:21,900
And so that's that's a simplification. That says now I'm not going to assume that's the case, but you could change that assumption.

590
01:11:23,490 --> 01:11:33,690
And here is the estimated the matrix for the first person and the estimated V correlation matrix.

591
01:11:33,690 --> 01:11:36,840
And so I think that what we had earlier.

592
01:11:38,310 --> 01:11:45,720
Was, you know, all of these off diagonal elements were the same when we just had random intercept.

593
01:11:46,530 --> 01:11:51,480
Everything that wasn't on the diagonal kind of looked the same and.

594
01:11:54,130 --> 01:11:57,490
And now because of the random slope, they're not the same.

595
01:11:58,540 --> 01:12:04,180
There's still an assumption that's kind of behind the scenes that it's a slope that's driving these correlations.

596
01:12:04,780 --> 01:12:07,960
So this is actually the covariance matrix. This is the correlation matrix.

597
01:12:08,410 --> 01:12:15,520
This is I always think correlations are easier to interpret because they're between zero and one for positively correlated data.

598
01:12:15,850 --> 01:12:23,650
And so it looks like the random slope is letting the correlation between measures over time be weaker, the further apart the measures are.

599
01:12:24,310 --> 01:12:29,080
So it's it it's not quite the r one assumption.

600
01:12:29,200 --> 01:12:32,980
These numbers aren't going to be some correlation to some power.

601
01:12:33,610 --> 01:12:37,030
That's not. You'll never have that matchup exactly.

602
01:12:37,030 --> 01:12:41,920
Between G assumptions and these ever again for random effects.

603
01:12:42,400 --> 01:12:47,050
But it's kind of getting at the same idea. Right.

604
01:12:47,650 --> 01:12:55,660
So in the air in the air one, we would have had all of the diagonal elements that were one away from each other have the same value.

605
01:12:55,990 --> 01:12:59,830
And then all the ones that were two away from each other have the same value.

606
01:13:00,070 --> 01:13:04,570
And so when you're not seeing that, but you are seeing that the correlation over time is getting weaker.

607
01:13:04,720 --> 01:13:13,990
The further part measures are it's also not quite unstructured because you only assumed random intercept and random slope,

608
01:13:13,990 --> 01:13:17,590
and unstructured assumes that you could have different correlation estimated.

609
01:13:17,860 --> 01:13:24,729
And it's using a lot more parameters to accomplish that. So it's somewhere in between an A or one and an unstructured.

610
01:13:24,730 --> 01:13:27,370
It's using many fewer parameters to estimate this thing.

611
01:13:28,360 --> 01:13:36,490
But you can look at what your data is saying that when given the chance, it didn't give you numbers that looked like they were all the same.

612
01:13:37,420 --> 01:13:42,910
Right. So when given the chance to estimate something with a random slope in your model,

613
01:13:43,180 --> 01:13:48,190
it did look quite different from the version of this where we just did Random Intercept

614
01:13:48,190 --> 01:13:52,300
and everybody had the same value and I think it was 6.6905 all the way through.

615
01:13:54,910 --> 01:14:00,309
Okay. So so that's one way to sort of peek at that.

616
01:14:00,310 --> 01:14:04,030
And then it starts gives you this nice table as well.

617
01:14:05,020 --> 01:14:10,840
That shows you for the random week parameter.

618
01:14:11,770 --> 01:14:19,870
There's an estimate here. It's very small relative to the the variability for the random intercept.

619
01:14:20,710 --> 01:14:23,710
But it's this P value says it's quite important.

620
01:14:25,380 --> 01:14:33,060
So when you're interpreting these random slopes, it's helpful to know what the beta is for.

621
01:14:33,060 --> 01:14:36,810
Weaken them in the population mean to see if that's an important scale.

622
01:14:36,810 --> 01:14:43,770
It's kind of hard to tell otherwise, but this p value is saying it's very important to have this in the model.

623
01:14:43,770 --> 01:14:47,820
So even without looking at aces or anything else,

624
01:14:48,300 --> 01:14:57,720
I've already got my answer because of this very small term that the random week was really helping model the variability compared to random intercept.

625
01:14:59,870 --> 01:15:04,609
So I think that this is just all about the variability.

626
01:15:04,610 --> 01:15:11,930
And the overall message here seems to be keep random week, it's helping a lot.

627
01:15:12,740 --> 01:15:17,390
And you can look at the AIC values between the models to see that as well.

628
01:15:19,900 --> 01:15:26,060
So this if you flip back and I don't remember for sure.

629
01:15:26,080 --> 01:15:29,710
So you can tell me if you flip back to the model, which is the random intercept.

630
01:15:30,070 --> 01:15:35,889
I bet this is smaller. This AIC here is smaller when you add in the random slope.

631
01:15:35,890 --> 01:15:40,360
So you could flip back a few slides and tell me what the AIC was.

632
01:15:40,360 --> 01:15:45,100
And we just had the random intercept. But I bet that this is smaller just from what we saw on the previous slide.

633
01:15:47,960 --> 01:15:51,110
And the fixed affects part of the model has shifted.

634
01:15:52,490 --> 01:15:57,350
P Values will shift, especially because you're better accounting for the variability in the data.

635
01:15:57,350 --> 01:16:01,249
So these P values are going to be more correct than the P values from the random intercept model

636
01:16:01,250 --> 01:16:05,810
because we've done a little bit better at modeling the variability in the people in the dataset.

637
01:16:10,230 --> 01:16:14,070
Oh, and I guess I did. Look at that. AIG is smaller with the Adam random slope term.

638
01:16:14,070 --> 01:16:19,560
It stays. I just didn't copy and paste it. So the the population me model.

639
01:16:21,390 --> 01:16:28,440
Four. The triple therapy group is taken from this solution, a fixed effects table.

640
01:16:29,250 --> 01:16:33,510
And so for the triple therapy group, we're plugging in one whenever we see triple.

641
01:16:34,770 --> 01:16:41,760
So what's that going to look like? So the intercept is going to be this intercept term plus the term for triple.

642
01:16:42,790 --> 01:16:47,639
So that's here. Right. Because triples just equal to one.

643
01:16:47,640 --> 01:16:49,190
So this parameter in this parameter,

644
01:16:49,230 --> 01:16:57,480
like the intercept for that group and then all the terms that involve weak are going to be helping us with the slope for triple.

645
01:16:57,930 --> 01:17:04,240
So we'll have the weak term. Which I guess I put second because it had a minus.

646
01:17:04,660 --> 01:17:10,300
And we'll have the week times one over here for that interaction.

647
01:17:12,650 --> 01:17:19,469
So. When you kind of put those add those terms together.

648
01:17:19,470 --> 01:17:28,240
This is the population meat model for triple. And so what kind of hypothesis tests can we look at when we're looking at this model?

649
01:17:28,270 --> 01:17:33,390
I mean, we've got the way the intercept changes when you go to the triple therapy, right?

650
01:17:33,400 --> 01:17:40,910
That's this term here. So this parameter changed the intercept.

651
01:17:42,030 --> 01:17:45,810
Compared to the dual therapy group. This is the intercept for the dual therapy groups.

652
01:17:46,260 --> 01:17:50,010
This is how the intercept changes when you go to triple.

653
01:17:51,400 --> 01:18:00,150
And it's not significant, right? So at times zero when we can zero.

654
01:18:03,090 --> 01:18:07,860
There's no difference in the outcome between treatment groups.

655
01:18:09,260 --> 01:18:18,680
Does that make sense for this context? I need to have you kind of perk up just asking questions just to make sure you're processing this live.

656
01:18:19,710 --> 01:18:26,190
So the fact that this is not significant means that at week equals zero.

657
01:18:27,850 --> 01:18:32,860
There's no difference in the outcome depending on this triple variable.

658
01:18:33,980 --> 01:18:37,070
That whatever this is, it wasn't statistically significant.

659
01:18:37,640 --> 01:18:42,590
So what is that? So. The week at week zero, the logs three four counts were comparable.

660
01:18:43,470 --> 01:18:48,570
On average between groups. So this is a clinical trial.

661
01:18:48,580 --> 01:18:54,670
They were randomized, triple versus dual, so that this is a sign that randomization worked on average.

662
01:18:54,670 --> 01:18:59,800
They started off at the same level of the outcome before treatment had a chance to take effect.

663
01:19:00,250 --> 01:19:11,140
This is something we want to see in clinical trial data that at time zero there weren't imbalances in the groups of what the city for count was.

664
01:19:11,380 --> 01:19:16,120
They had a fair shot on average. On average, they started around the same place.

665
01:19:16,480 --> 01:19:22,520
Then they got randomized to treatment and could do change after. So everything.

666
01:19:22,760 --> 01:19:27,290
So what's the. What's that? The slope. The slope difference.

667
01:19:28,600 --> 01:19:35,770
Between the dual and the triple. Groups are going to be these terms that involve time.

668
01:19:36,720 --> 01:19:40,650
So if you're on dual therapy, you're slope is just this term.

669
01:19:40,860 --> 01:19:45,360
So on dual therapy, they're sitting for count over time is going down.

670
01:19:45,360 --> 01:19:46,440
So that negative sign.

671
01:19:48,240 --> 01:19:56,880
So for every week longer in the study this says they're going down by their like three four counts going down by this much and it's significant.

672
01:19:58,020 --> 01:20:04,049
So they are not stable. They are decreasing over time significantly in their CD4 count.

673
01:20:04,050 --> 01:20:08,130
That's disease progression in the dual therapy group. That's where you see it.

674
01:20:08,760 --> 01:20:13,160
And in the triple therapy group, it's these two terms together.

675
01:20:13,170 --> 01:20:16,850
That's their trajectory over time. And they almost break even almost.

676
01:20:16,890 --> 01:20:21,150
Look at that point minus .01364.

677
01:20:21,510 --> 01:20:26,730
And then it just changes here. It's positive .01312.

678
01:20:26,940 --> 01:20:32,700
And so it almost breaks even. So triple is keeping them much more stable according to this output.

679
01:20:34,540 --> 01:20:41,530
Right there. Slope over time is actually just barely changing.

680
01:20:44,230 --> 01:20:45,310
Compared to.

681
01:20:46,980 --> 01:20:54,840
The people who were on the dual therapy and this probably look a lot more exciting if we had something like a month as our time scale instead of week.

682
01:20:55,290 --> 01:21:01,380
So I always think it's a good idea to plot these over time to sort of see I mean, this seems like is that clinically meaningful?

683
01:21:01,710 --> 01:21:12,390
But over many weeks it will be. And so just for comparison, the population model for the dual therapy groups, it triples equal to zero.

684
01:21:12,400 --> 01:21:15,690
So you're just looking at these first two things for the intercept and the slope.

685
01:21:17,940 --> 01:21:26,349
Yes. So could you just be a little louder?

686
01:21:26,350 --> 01:21:30,790
Because I'm I'm hard of hearing, honestly. So. Oh, so you want to.

687
01:21:41,330 --> 01:21:46,010
Okay. So the question was or there was a statement and a question.

688
01:21:46,010 --> 01:21:54,260
So the statement was just saying once again correctly that this triple parameter we're hoping

689
01:21:54,260 --> 01:21:59,960
isn't significant because the triple parameter is what the outcome is doing at week zero.

690
01:22:00,470 --> 01:22:07,640
Comparing dual to triple therapy, this is how much that's changing at week zero.

691
01:22:08,000 --> 01:22:11,000
And if randomization work, those should be similar on average.

692
01:22:11,690 --> 01:22:15,550
And so we kind of want this to be not statistically significant.

693
01:22:15,560 --> 01:22:19,139
Now this is a weird thing for your brain to look at because usually if we see treatment,

694
01:22:19,140 --> 01:22:24,650
the model, we're like, I want all treatment everywhere to be important and significant.

695
01:22:25,640 --> 01:22:30,140
But this is because we've got trajectories over time.

696
01:22:30,530 --> 01:22:34,849
We've got the starting place and we've got what happens afterwards.

697
01:22:34,850 --> 01:22:41,930
And in a randomized clinical trial, the starting place you're hoping is the same on average before the treatment's been given.

698
01:22:42,050 --> 01:22:52,440
That means you had a fair shot. That both treatment groups were had similar health at the beginning of the study before they were given treatment.

699
01:22:53,310 --> 01:22:58,020
So the thing that you want to see significance for is the interaction over time.

700
01:23:00,400 --> 01:23:08,620
So this is saying as they progressed to week one to week two week three this groups city four count.

701
01:23:09,810 --> 01:23:13,410
Is doing much better because this is a positive value.

702
01:23:13,530 --> 01:23:16,890
This is doing much better over time and significantly so.

703
01:23:18,930 --> 01:23:25,829
That over time the city for count is actually getting a little bit, you know, compared to the other group,

704
01:23:25,830 --> 01:23:30,330
they're actually maintaining their ground better because of this positive coefficient.

705
01:23:30,930 --> 01:23:40,470
So again, sometimes a plot can help. And so if you plot the population mean models for the two groups that we had on the last slide,

706
01:23:40,770 --> 01:23:45,749
you know, so those parameter estimates were like for every week what happens?

707
01:23:45,750 --> 01:23:51,030
And it seemed really teeny tiny but over the 40 weeks shown here in this plot.

708
01:23:52,350 --> 01:23:59,350
The dual therapy group is this one that is getting progressively worse over time.

709
01:24:00,060 --> 01:24:10,150
So this and this slope. Is really just the slope for time on the previous slide.

710
01:24:10,870 --> 01:24:15,340
And then the slope for triple was the slope for the week.

711
01:24:16,340 --> 01:24:21,310
And the week by triple parameter and it almost cancel each other out.

712
01:24:21,320 --> 01:24:28,850
Right? It was only like there was like 0.0005 decrease I think per week and it almost looks flat.

713
01:24:30,030 --> 01:24:33,240
And since city for count, the higher you are, the better.

714
01:24:33,780 --> 01:24:37,860
It really looks like the Triple Therapy's flattening out progression.

715
01:24:39,150 --> 01:24:46,380
With, at least to the CD4 variable. But the dual therapy group is still kind of losing ground on their city for count over time.

716
01:24:47,820 --> 01:24:55,940
And so the and at times zero, their intercepts were so close, they're more or less on top of each other.

717
01:24:55,950 --> 01:25:00,660
They weren't statistically different at time. Zero. That was the parameter for triple.

718
01:25:03,550 --> 01:25:08,890
And so the treatment effect is all about the interaction with treatment and time.

719
01:25:18,360 --> 01:25:23,520
I'm glad you're asking these questions because again, this is the dataset you'll be playing with on your homework as well.

720
01:25:25,780 --> 01:25:29,170
So we want to know the working parts of this dataset, you know?

721
01:25:35,780 --> 01:25:41,440
So. First manuscript worthy sentence based on this very simple model.

722
01:25:41,800 --> 01:25:49,629
Trajectories over time are statistically worse for the dual therapy studied when compared to the triple therapy group.

723
01:25:49,630 --> 01:25:52,750
Less than 0.0001. How did I.

724
01:25:52,780 --> 01:25:59,250
Where did this P-value come from? Which P-value did I pick when I was writing this sentence?

725
01:25:59,790 --> 01:26:09,500
When I said that, the trajectories over time. We're statistically worse for the dual therapy study when compared to the triple therapy group.

726
01:26:10,220 --> 01:26:17,780
So which P-value was I pulling here? The interaction.

727
01:26:17,790 --> 01:26:23,440
Yes, I was this one. This is all about the trajectories over time being different according to treatment group.

728
01:26:23,450 --> 01:26:32,160
This one here. So that was not a trivial question, actually, because you had more than one people tell you that looked small.

729
01:26:32,180 --> 01:26:35,450
Right. So what is this one saying? This one over here. That's by week.

730
01:26:40,880 --> 01:26:46,730
So this is the slope for the dual therapy group, and it depends on time.

731
01:26:48,790 --> 01:26:52,570
And this one is how different was that slope?

732
01:26:53,530 --> 01:26:57,010
When you run the triple therapy. That's this one. This is the one we put in the sentence.

733
01:27:00,880 --> 01:27:07,790
Yes. Okay.

734
01:27:07,800 --> 01:27:12,160
I just lost the volume. I'm sorry. Don't go down. So it's.

735
01:27:17,330 --> 01:27:24,590
Yeah. So over time in the dual therapy group, the decrease is statistically significant.

736
01:27:24,600 --> 01:27:36,250
So when we saw that plot. And that slope that was going down that was statistically significant, that it was going down rather than just saying flat.

737
01:27:40,660 --> 01:27:48,970
So you could write another sentence based on this term that that the dual therapy arms were statistically progressing.

738
01:27:50,730 --> 01:27:57,000
Over time, and you could put like some number per week or per month if you wanted to with the confidence interval.

739
01:27:57,000 --> 01:28:00,810
But this and this would be the p value for that if you wanted to do that.

740
01:28:08,680 --> 01:28:12,340
So the p value is taken from the triple by week interaction term p value.

741
01:28:13,880 --> 01:28:21,640
And so what we haven't done yet is look at the blip at around 16 weeks with a linear supply.

742
01:28:21,680 --> 01:28:25,970
So remember in the treatment group, we thought that it might be going even up a little bit.

743
01:28:26,540 --> 01:28:29,210
Maybe we have to fit it and ask.

744
01:28:29,630 --> 01:28:33,950
And there might be other ways to model this, but we're going to do it with this line just to kind of show you how that goes.

745
01:28:36,210 --> 01:28:41,100
Oh, and here's. Sorry, here is the R code for putting in that random.

746
01:28:42,460 --> 01:28:45,690
Time to into the random statements.

747
01:28:45,690 --> 01:28:56,080
So the only thing that's changed from the code before is that now it says Random Intercept plus random week and otherwise the outputs the same.

748
01:28:56,950 --> 01:29:05,529
Although here I showed you the aces so that you could see that it went down with the intercept plus slope.

749
01:29:05,530 --> 01:29:08,969
I just copied that over from our. All right.

750
01:29:08,970 --> 01:29:14,580
So now we want to add in. To the fixed effects part of the model.

751
01:29:15,700 --> 01:29:23,710
The spline term that allows a bend at week 16 and I haven't even done anything like put an interaction.

752
01:29:24,370 --> 01:29:28,930
Uh, or actually I do here's an interaction with, with the treatment group.

753
01:29:29,290 --> 01:29:35,680
So here's this pipeline for week, week -16 times, the indicator that week is greater than 16.

754
01:29:36,760 --> 01:29:43,930
And I'm putting it in here. So this is going to allow the dual therapy group to have a band at 16 potentially.

755
01:29:44,410 --> 01:29:51,010
And hear this triple by week 16 is going to allow the band to be different if you're in the triple therapy group.

756
01:29:52,440 --> 01:29:59,070
And I'm saving some of the predicted values and estimated random intercepts and slopes just so that we can look at them.

757
01:30:01,110 --> 01:30:04,740
So I'll show that to you in the output. But otherwise the code is the same.

758
01:30:04,860 --> 01:30:11,310
So I'm just kind of keeping random intercept and slope the same as I play with this fixed effect and see if it helps.

759
01:30:13,780 --> 01:30:18,700
And so this is the variability of that's estimated from the data.

760
01:30:18,970 --> 01:30:28,680
Things are going to be just shifting slightly because I changed the population mean model, but the overall patterns are similar.

761
01:30:28,690 --> 01:30:32,080
But every change to one model is going to change.

762
01:30:32,320 --> 01:30:36,340
Every change to the fix effects model's going to slightly affect the random effects part of the model.

763
01:30:39,770 --> 01:30:43,849
And so here is the solution for the fixed effects.

764
01:30:43,850 --> 01:30:47,500
These are the parameters, the population mean parameters that you want to interpret.

765
01:30:47,990 --> 01:30:59,030
And so this is, I think, something that I have you do for your first homework problem so that you exercise this muscle about how to interpret things.

766
01:31:00,380 --> 01:31:03,770
So if you are wise.

767
01:31:05,020 --> 01:31:10,700
Because you want to. This is all about exercising that muscle that can do these things if you were wise.

768
01:31:10,720 --> 01:31:14,680
I've stretched you out perfectly for this exercise.

769
01:31:15,130 --> 01:31:24,430
If you do it today, when your muscles are already kind of started, it'll be easier than if you come back to this in three days or a week or two weeks.

770
01:31:25,090 --> 01:31:29,590
Right. You're you're stretched out and ready to go now.

771
01:31:30,940 --> 01:31:37,690
So if your wise believe in yourself, you're ready to do this problem.

772
01:31:37,690 --> 01:31:42,070
And you should probably do it today so that your that muscle will strengthen.

773
01:31:43,900 --> 01:31:50,830
Okay so this supplying terms that we added in our statistically significant.

774
01:31:51,130 --> 01:31:58,360
And now we need to do the work to figure out what this means for our population model.

775
01:31:58,510 --> 01:32:01,540
How do we interpret this? So here's the exercise.

776
01:32:01,540 --> 01:32:05,730
It's showing up. It's your first homework problem. So this is not to be answered.

777
01:32:05,740 --> 01:32:08,500
Now, this is actually something you're going to work on from one.

778
01:32:08,800 --> 01:32:15,310
So what's the population model for the average city for trajectory if you're on the triple therapy group?

779
01:32:15,910 --> 01:32:17,590
So what does this model look like?

780
01:32:17,590 --> 01:32:26,380
If you plug in triple equals one and you combine terms, so you should have an overall intercept and slope and a spline term.

781
01:32:27,630 --> 01:32:32,160
You should be able to describe this in three parameters when you combine things that should be combined.

782
01:32:32,930 --> 01:32:40,220
Right. And what's the population model for the average C4 trajectory if you're on the dual therapy group?

783
01:32:42,110 --> 01:32:44,030
And can you plot these over time?

784
01:32:44,060 --> 01:32:51,800
I don't remember if I asked for the plot in the homework, but if you're trying to learn how these models work, I believe plots can help teach.

785
01:32:53,080 --> 01:32:59,860
So if you want to see visually what your population models look like, plot them.

786
01:33:00,310 --> 01:33:05,469
I can't remember if I made it a requirement or not for the homework problem I might have,

787
01:33:05,470 --> 01:33:09,880
because it would be it would be a good exercise if I didn't ask. It would would have been a good thing to ask.

788
01:33:10,690 --> 01:33:16,150
And here is our code for doing the same thing where you're just changing the fix to fix part of the model.

789
01:33:16,160 --> 01:33:17,470
So the code is there.

790
01:33:17,980 --> 01:33:25,660
So you're going to this is going to you don't have to run the code to answer this problem because you've got this table looking right at you.

791
01:33:25,660 --> 01:33:31,120
This doesn't require you to run any code problem. One is just about interpreting this thing.

792
01:33:35,450 --> 01:33:42,740
All right. Our lab this week is going to help you set up the dataset to do problem two.

793
01:33:44,470 --> 01:33:48,520
And so I'm going to allude a little bit to that as I go through here.

794
01:33:48,520 --> 01:33:53,700
But this is the last I'll talk about these mixed effects models in the course.

795
01:33:53,710 --> 01:34:01,420
So I wanted to have some summary remarks and just give you some ideas of related topics that we won't have time to address in 523,

796
01:34:02,080 --> 01:34:08,860
but that you you can work on in another follow up course that spends more time on longitudinal methods.

797
01:34:08,860 --> 01:34:13,750
Or you can sort of read there's a great book that I can point you to, to read if you want to do self-study.

798
01:34:14,630 --> 01:34:20,959
But I want to at least put these ideas in your head so there's kind of a treatment trajectory analysis checklist,

799
01:34:20,960 --> 01:34:25,460
and we've done a little bit of it together. So question one are the trajectories.

800
01:34:25,460 --> 01:34:30,770
It means similar or similar or parallel in the treatment groups.

801
01:34:31,520 --> 01:34:39,290
So if you've got treatment one or I've got Group one and Group two over time, if you think that those are parallel over time,

802
01:34:40,610 --> 01:34:47,740
you can assess that statistically with a test of the treatment by time interaction.

803
01:34:47,750 --> 01:34:53,150
If there's no significant interaction, you're saying that these trajectories over time are parallel.

804
01:34:53,870 --> 01:35:02,230
That's what that would look like. If they are indeed parallel, are they also identical?

805
01:35:02,250 --> 01:35:06,809
So if you want if they if you've already decided that their trajectories are parallel over time,

806
01:35:06,810 --> 01:35:10,590
then the next thing is, are they on top of each other for the two groups?

807
01:35:10,590 --> 01:35:16,410
And that to look at that you look at the significance for the main effect of treatment or group here.

808
01:35:18,280 --> 01:35:22,360
This is like the intercepts are the same and the slopes are the same.

809
01:35:25,030 --> 01:35:28,570
If the population means trajectories for treatment or parallel.

810
01:35:29,520 --> 01:35:33,140
Are they all so constant over time? Right.

811
01:35:33,150 --> 01:35:38,520
So one of the questions for the city for analysis is, are they are the patterns stable?

812
01:35:39,120 --> 01:35:45,660
And so you can check for the main effects of time to see if if they're stable.

813
01:35:45,780 --> 01:35:49,230
And here we've started with from the assumption that they were parallel.

814
01:35:49,260 --> 01:35:58,970
So are they also stable over time? And in our data set it looked like they were maybe stable for the triple therapy, but not the dual therapy.

815
01:36:01,130 --> 01:36:04,400
So there's more time to search for these related topics.

816
01:36:04,400 --> 01:36:11,930
So your in the homework problem I'm going I'm having you work through how to use

817
01:36:11,930 --> 01:36:17,150
baseline CD4 count as a variable in your analysis and what that does for you.

818
01:36:17,210 --> 01:36:23,780
How to think about that. So thoughtful use of baseline covariates is something I wanted to at least address a little bit.

819
01:36:23,780 --> 01:36:29,990
You'll see one example of it in your homework, but I want to give you a few thoughts that are a little bit more general.

820
01:36:31,710 --> 01:36:38,310
And a lot of this depends on if your study design is observational versus a randomized group comparison.

821
01:36:41,410 --> 01:36:44,170
So I'll, I'll bring that up in a few slides.

822
01:36:44,170 --> 01:36:49,959
And then another topic I want to just say a few things about was, you know, we've looked at the city for count over time.

823
01:36:49,960 --> 01:36:55,840
Some people like to model relative change from baseline as an outcome itself.

824
01:36:55,840 --> 01:37:02,739
So they changed the outcome from instead of the actual city for count over time relative change from baseline.

825
01:37:02,740 --> 01:37:09,760
So everybody starts off at you know, the relative change from baseline is nothing and then they kind of model that from there.

826
01:37:09,970 --> 01:37:16,750
So I wanted to talk about a little bit, I generally don't like modeling relative change over time unless there's a sentence

827
01:37:16,750 --> 01:37:20,110
in the manuscript that someone really wants to write about relative change over time.

828
01:37:21,710 --> 01:37:24,620
I'll mention I'll talk about that a little bit and then checking for baseline treatment

829
01:37:24,620 --> 01:37:28,190
effects that occur at time zero and are maintained throughout the trial at that level.

830
01:37:28,670 --> 01:37:35,390
So what kind of time dependent covers can you add to test that kind of hypothesis?

831
01:37:35,720 --> 01:37:38,420
All of these things are things I'm going to touch on just very lightly.

832
01:37:39,410 --> 01:37:45,440
And the first one, the one that's most relative to your homework, is the thoughtful use of baseline outcomes.

833
01:37:46,460 --> 01:37:52,460
So in your homework, the CD4 count at times a year before treatments taking effect is the variable I'm talking about.

834
01:37:53,240 --> 01:38:00,500
And so when is it appropriate and not appropriate to use that as a cover in your model?

835
01:38:00,650 --> 01:38:08,360
So in observational studies, let's start there. And observational studies, the mean outcome at baseline often has its own intrinsic value.

836
01:38:09,260 --> 01:38:17,090
And the example that I'm going to show you, at least hypothetically, is this research question.

837
01:38:18,530 --> 01:38:24,800
That I've had to face in my own research over with the pulmonary division on how pulmonary function

838
01:38:24,890 --> 01:38:29,840
FEV1 behaves after they're diagnosed with something called bronchiolitis alliterative syndrome.

839
01:38:31,060 --> 01:38:41,650
Now call cloud, actually. And so they want to know how this trajectory of lung function over time changes once you get your body diagnosis.

840
01:38:42,190 --> 01:38:50,649
But there's different severities of that diagnosis, and there's a grading system that defines the severity when they're diagnosed.

841
01:38:50,650 --> 01:38:56,530
And it's based on how far everyone has fallen from the patient's best post-transplant measure of FEV1.

842
01:38:56,950 --> 01:39:07,300
So the outcome of view over time is very much tied to how badly they their pulmonary function was at the time they were diagnosed with both.

843
01:39:08,510 --> 01:39:13,129
And so question one is at the time of both diagnosis at baseline time zero.

844
01:39:13,130 --> 01:39:24,709
What's the mean FEV1 value according to both grade and that outcome just because both grade is defined in terms of FEV1 involved above one,

845
01:39:24,710 --> 01:39:28,610
we expect it to be different at time zero for each of the grades.

846
01:39:30,070 --> 01:39:36,460
And so the expectation before you've even fit in the data is that there may be differences at time zero.

847
01:39:36,880 --> 01:39:45,510
And so here kind of little made up potential trajectories based on how well they were doing at the beginning when they were diagnosed.

848
01:39:45,880 --> 01:39:52,390
This is like all made up data just to prove the point here, what it looks like if you have grade zero or one B or worse,

849
01:39:52,390 --> 01:39:58,180
what it looks like if you have grade two below was grade three below us and at times zero we're expecting

850
01:39:58,180 --> 01:40:03,850
those to be different since the grade depends on how well they're breathing at the diagnosis time.

851
01:40:06,500 --> 01:40:10,490
All right. So describing that's part of the research question.

852
01:40:11,270 --> 01:40:17,739
All right. They want to know how well their pulmonary function is doing according to the grade.

853
01:40:17,740 --> 01:40:24,459
And the grade is not just their current FEV1, but it's also the grade has to do with how well,

854
01:40:24,460 --> 01:40:29,860
you know, they did right after transplant, and that varies by each person.

855
01:40:31,330 --> 01:40:43,690
So if you were to try to adjust for baseline FEV1, you would be trying to compare trajectories as if they all started off at the same place.

856
01:40:43,690 --> 01:40:49,270
And that just wouldn't match the research question of how these people do over time.

857
01:40:50,410 --> 01:40:58,139
So. So when some observational studies adjusting for baseline values makes little sense,

858
01:40:58,140 --> 01:41:02,700
and this is an example where adjusting for baseline values would not make any sense in your homework.

859
01:41:02,700 --> 01:41:12,850
It will make great sense. This example? No. And so let's just think about what it would do if we did put in.

860
01:41:15,030 --> 01:41:22,020
The baseline FEV1 value. So the same setting we want to study FEV1 trends post both diagnosis by biopsy grade.

861
01:41:23,420 --> 01:41:30,200
Adjusting for that baseline and FEV1 forces a comparison of trends for a common baseline F1 value.

862
01:41:32,380 --> 01:41:40,080
However, it's artificial and unlikely that those with different views grades will share a common FEV1 value at both diagnosis in real life.

863
01:41:40,090 --> 01:41:45,399
I mean, it's possible, but depending on what their best post-transplant pulmonary function was.

864
01:41:45,400 --> 01:41:54,880
But it's very unlikely that people with different both grades will have the same FEV1 function at diagnosis.

865
01:41:54,890 --> 01:42:00,490
So this is what you would be assuming is going on behind the scenes.

866
01:42:00,490 --> 01:42:07,480
If you adjusted for FEV1, you would. This is like a plot for a particular baseline FEV1.

867
01:42:07,750 --> 01:42:10,720
What is happening for the different both grades?

868
01:42:11,320 --> 01:42:19,030
So in this particular situation, the plot, if you if you try to fit this model and produce such a plot.

869
01:42:20,190 --> 01:42:24,180
It would not pass the sniff test if submitted to a journal.

870
01:42:24,990 --> 01:42:27,320
People would see this and go, No, no, no.

871
01:42:27,330 --> 01:42:34,049
Just because I know what these grades are and that they depend on FEV1, this should not be the same at baseline.

872
01:42:34,050 --> 01:42:36,810
They they have to be different. Something went wrong year.

873
01:42:38,070 --> 01:42:44,370
And so it would be it's better to compare trends based on the previous plot unadjusted for baseline.

874
01:42:46,660 --> 01:42:49,840
Because it fits the research question more closely.

875
01:42:51,690 --> 01:43:00,629
All right. So I would not adjust for based on an observational studies where you're trying to describe what these people look like over time,

876
01:43:00,630 --> 01:43:05,820
according to various covariates in randomized clinical trials, though adjusting for baselines.

877
01:43:05,850 --> 01:43:13,069
Awesome. And I think that's what I have you doing your homework.

878
01:43:13,070 --> 01:43:19,610
And the idea is that if you adjust for where they start off CD4 wise in your model,

879
01:43:20,780 --> 01:43:26,060
you can maybe get some more precision in the treatment effect over time.

880
01:43:26,570 --> 01:43:33,200
And you expect people on average to start off at the same value, in fact, when when we plotted our fitted model.

881
01:43:35,700 --> 01:43:43,250
For City for when we had a term that interacted over time, they did just they started off at a very similar place.

882
01:43:43,260 --> 01:43:46,110
We didn't force them to start up a similar place, but they did.

883
01:43:46,710 --> 01:43:56,040
And so, you know, if you put in a baseline city for count, you would just be strengthening that estimate that it's common at times zero.

884
01:43:58,480 --> 01:44:02,650
And you get some efficiency there. All right.

885
01:44:02,650 --> 01:44:07,070
So the next topic I want to cover, I really honestly, I thought I would get to the next part, but we won't.

886
01:44:07,090 --> 01:44:14,100
Not quite. So the next thing that I wanted to look at is whether you you know,

887
01:44:14,110 --> 01:44:20,769
what what happens when you want to look at relative change over time and whether that addresses the research question of interest.

888
01:44:20,770 --> 01:44:29,620
So it's easier to interpret relative changes over time if the baseline values are similar on average as in a randomized treatment comparison.

889
01:44:31,180 --> 01:44:34,329
But you are changing the distribution of your outcome quite a bit.

890
01:44:34,330 --> 01:44:42,100
So relative changes are going to be bigger for smaller baseline values and sometimes that's a good predictor of future prognosis.

891
01:44:42,100 --> 01:44:47,050
You know, certainly it can capture the stability of the disease versus change.

892
01:44:48,400 --> 01:44:58,360
But if you started off with parallel trends before you looked at relative change, they won't remain parallel when viewed via relative changes.

893
01:44:58,360 --> 01:45:05,890
And so it can change your interpretation of what's going on when you change your outcome variable to a relative change.

894
01:45:06,940 --> 01:45:11,620
That's too many words that say change. All right.

895
01:45:12,010 --> 01:45:14,799
Use of baseline outcomes and randomized treatment comparisons.

896
01:45:14,800 --> 01:45:22,240
So in randomized treatment comparisons, it's best to adjust for baseline outcome levels to gain the most efficiency from the data.

897
01:45:22,240 --> 01:45:23,709
So that's what you're gonna be doing in your homework.

898
01:45:23,710 --> 01:45:32,470
So for the city, for count example, we would need to create a new baseline variable for city for count and add it to the model as a fixed effects.

899
01:45:32,470 --> 01:45:35,800
And that lab is going to help you code that.

900
01:45:37,150 --> 01:45:44,020
You'll be enormously helpful. And then it would be interesting to test for treatment effects that vary according to their baseline C.T. four count.

901
01:45:44,530 --> 01:45:52,599
So when you look at this interaction, you can ask questions like, you know, was the treatment more effective over time if you had very,

902
01:45:52,600 --> 01:46:00,970
very high levels of the city for or was it more effective over time if you start off with very low levels of city for it's trying to help you answer,

903
01:46:01,330 --> 01:46:05,470
you know, potentially different treatment effects depending on how sick you were when you entered the study.

904
01:46:05,620 --> 01:46:11,740
And that can be enormously helpful when you're talking to patients later about what to expect on treatment.

905
01:46:13,280 --> 01:46:23,750
So when adjusting for baseline 34 models of changes in outcome since baseline are very attractive in this randomized setting time zero outcome,

906
01:46:23,930 --> 01:46:30,739
the time zero outcome becomes unnecessary as a dependent outcome when analyzing changes from baseline,

907
01:46:30,740 --> 01:46:34,310
since this changes zero by definition at baseline.

908
01:46:35,510 --> 01:46:46,309
So you know, thinking about differences between a model that uses the time zero as an outcome versus a model where you just

909
01:46:46,310 --> 01:46:52,250
take out the time zero as an outcome are interesting to think about in your homework as you think about that.

910
01:46:52,580 --> 01:46:58,790
So you'll be creating a version of the dataset where the outcome doesn't include the time zero fitting that,

911
01:46:59,030 --> 01:47:05,930
and you're going to be comparing that to fitting the model where you keep in the time zero outcome as well as the baseline covariate.

912
01:47:06,200 --> 01:47:14,600
All of this is to help you think through step by step what these different assumptions have put in terms in your models doing to your interpretation.

913
01:47:15,200 --> 01:47:16,309
It's going to take a lot of time.

914
01:47:16,310 --> 01:47:26,060
This is a long homework, but it's very useful for understanding what these small changes to the way you model your data due to your interpretation.

915
01:47:27,230 --> 01:47:32,930
So I highly recommend you. You you are started as soon as you can.

916
01:47:32,930 --> 01:47:36,530
I know you're really busy, so to gain more intuition,

917
01:47:36,530 --> 01:47:42,140
reviewing internal level notes on an COBR is useful for the case of the single baseline and a single post-treatment outcome for each treatment group.

918
01:47:42,530 --> 01:47:49,190
If you have material like that, I always think that thinking back to a simpler case is helpful.

919
01:47:49,550 --> 01:47:57,950
And and COBR is one example where you might be modeling some changes, your outcome and you have your baseline values, a predictor.

920
01:47:58,160 --> 01:48:04,820
And we're doing something kind of similar here by looking at outcomes over time with baseline in there as a predictor.

921
01:48:05,570 --> 01:48:13,460
We saw something similar were when we're looking at handout 14, which is two dependent outcomes and comparing it to paired T test.

922
01:48:13,750 --> 01:48:20,960
You know, we also had both outcomes time one and time to as well as that baseline covariate in there.

923
01:48:21,200 --> 01:48:26,120
So we've kind of done a baby version of this with just two dependent outcomes in that handout as well,

924
01:48:26,360 --> 01:48:29,090
all to give you intuition about how to think about these things.

925
01:48:32,260 --> 01:48:39,489
The last thing I think I'm going to talk about and it's just about the last slide, too, is that in some studies,

926
01:48:39,490 --> 01:48:46,810
the exposure treatment causes a shift in the mean response at or near baseline that's maintained at a constant level across measured medications.

927
01:48:47,470 --> 01:48:52,420
So what we've done with the three four count is sort of to say, well, there's this gradual change over time,

928
01:48:52,750 --> 01:48:58,180
but sometimes like if it was a surgery, you'd expect like a big shift that was that would stay.

929
01:48:58,570 --> 01:49:06,730
And so in such a setting, you can create a new time dependent covariate that would look like post the time of the intervention,

930
01:49:07,030 --> 01:49:12,970
and that variable would be one after the time of the shift, maybe the time of surgery, but zero otherwise.

931
01:49:13,300 --> 01:49:20,590
And so you could sort of measure, you know, that whatever the intervention is, is expected to have a big shift that stays the same afterwards.

932
01:49:20,590 --> 01:49:23,950
So you could model it like this with that kind of a time dependent covariate.

933
01:49:24,580 --> 01:49:31,389
And that variable for time would replace earlier versions that we used for continuous time in our models for this handout.

934
01:49:31,390 --> 01:49:39,490
So you wouldn't necessarily have week in your model if you thought there was just one one treatment shift and then it stayed the same after that.

935
01:49:41,490 --> 01:49:50,190
And if you decide that you need to self teach more before you take another course or even as part of taking another course.

936
01:49:50,430 --> 01:49:59,180
This is my favorite reference. A lot of my examples are taken from this textbook written by Garrett Fitzmaurice, Nan Laird and James Aware.

937
01:49:59,660 --> 01:50:06,840
And you know, there's some there's some technical stuff in there you can just skip through.

938
01:50:07,860 --> 01:50:14,160
They have wonderful practical advice and useful examples. And as long as you're okay skipping over complex equations,

939
01:50:14,490 --> 01:50:21,330
you can gain a lot of intuition from this textbook because they do lean on the practical in the interpretation, but they'll all,

940
01:50:21,330 --> 01:50:28,290
for people who are more technically minded, like calculus and all that stuff, they have things more complex,

941
01:50:28,290 --> 01:50:33,670
like how do they get these numbers, technical stuff in there as well that you probably don't need to.

942
01:50:33,930 --> 01:50:37,990
I know you don't need it to actually use this stuff. So that's it for today.

943
01:50:38,010 --> 01:50:45,919
I guess I'm starting the next train out next time. But you have everything now you need to do.

944
01:50:45,920 --> 01:50:51,350
Problem one on homework six and after lab this week you'll be able to tackle homework.

945
01:50:51,650 --> 01:50:55,130
Problem two. On your final homework.

946
01:50:59,010 --> 01:51:13,310
All right, that's it. Bye. Okay.

