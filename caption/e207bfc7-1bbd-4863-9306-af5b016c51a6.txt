1
00:00:10,550 --> 00:00:18,390
All right. So welcome to our last laugh of 523.

2
00:00:18,410 --> 00:00:26,209
That means that next week we don't have but we do have office hours, but we don't have any more laughs today when you think about what's been lost.

3
00:00:26,210 --> 00:00:30,200
Homework. Homework number six. I remember those days.

4
00:00:30,230 --> 00:00:34,460
It is due December seven. No exception, please.

5
00:00:35,900 --> 00:00:41,630
Okay. Very important before we start up. So it's about the same thing.

6
00:00:42,110 --> 00:00:51,350
Longitudinal analysis. However, we are going to talk about G, which is our special part of longitudinal analysis for count data.

7
00:00:52,340 --> 00:00:58,700
These kind of not negative binomial g cannot be performed using r.

8
00:00:59,060 --> 00:01:06,980
So today is going to be only on SAS and in F in the homework you need to do a negative binomial g.

9
00:01:07,130 --> 00:01:15,260
You will need to use SAS. Our unfortunately doesn't have any name package that we can use to perform the same thing.

10
00:01:15,620 --> 00:01:19,640
Okay. Okay.

11
00:01:19,850 --> 00:01:25,100
So just a brief summary of what we have done so far.

12
00:01:26,180 --> 00:01:29,780
So when we have columns, columns, data as the outcome.

13
00:01:29,780 --> 00:01:35,810
So for example, number of something, number of people, number of infected people, so on and so forth.

14
00:01:36,110 --> 00:01:39,620
We can either use Poisson or a negative binomial model.

15
00:01:39,980 --> 00:01:46,330
The choice between the two of them is going to be dependent on whether there is over this version.

16
00:01:49,710 --> 00:01:53,910
So what we would do in general is first feed our normal person.

17
00:01:54,300 --> 00:01:58,510
The normal person is going to already give you a test for the already special powers.

18
00:01:59,100 --> 00:02:06,900
If we were that test, he's like the variance is equal to the mean or whether the variant is larger than the mean.

19
00:02:07,470 --> 00:02:12,750
Thus the negative binomial. Over the spectrum or in spoken climate there.

20
00:02:14,330 --> 00:02:19,280
Hyperx. So what we would do in real life is to start by setting up some model.

21
00:02:19,670 --> 00:02:24,470
The persona model is immediately going to tell you about that test of the overview special plan.

22
00:02:25,280 --> 00:02:29,810
If we reject the null hypothesis, then we vote with the persona model.

23
00:02:30,110 --> 00:02:34,370
If we do not reject the role, we develop these binomial models.

24
00:02:34,800 --> 00:02:43,790
That's how we choose between the two of them. Depending on that overexpression plant and now within each other we can find either a

25
00:02:43,790 --> 00:02:49,970
black soul or a negative polynomial or a series of inflated plus or ac related negative.

26
00:02:50,000 --> 00:02:58,970
Why don't you? What we will do in real life is, let's say I just had a freedom lesson model because my test told me to.

27
00:02:59,720 --> 00:03:02,780
Now that I know that I have two feet up. So model.

28
00:03:02,780 --> 00:03:06,770
How do I choose between just a fossil or as a conservative fossil?

29
00:03:07,190 --> 00:03:11,480
We will feed both of them and then use the long test.

30
00:03:11,900 --> 00:03:16,370
And then if we fail to reject, we go without also model.

31
00:03:16,700 --> 00:03:20,240
If we fail to reject, we go with a sewer inflated plus.

32
00:03:21,080 --> 00:03:28,190
And the same applies for the negative binomial model. So we see both of them.

33
00:03:28,210 --> 00:03:34,280
We tell you we use the blank test, and then that should tell us which is like our final model.

34
00:03:35,660 --> 00:03:43,940
Okay. So now are dependent outcomes, which means longitudinal data.

35
00:03:44,390 --> 00:03:52,400
So so far what we have been doing so far is we have in the left hand side of the equation,

36
00:03:52,400 --> 00:03:59,120
y i j which is the outcome for the first time in the j in the j operations.

37
00:03:59,450 --> 00:04:01,610
And then here we have all of the covariance.

38
00:04:01,940 --> 00:04:13,130
However, until last week, well, until less than two weeks from today, that was enough because my outcome was a normal distributive outcome, right?

39
00:04:13,730 --> 00:04:16,850
My outcome was like any numbers could be positive or negative.

40
00:04:16,940 --> 00:04:21,390
In zero decimal decimal. We didn't care. We data.

41
00:04:21,410 --> 00:04:30,710
The thing is that we have the same setup. So we have i j i for the individual and j for the setup, for the operation.

42
00:04:31,040 --> 00:04:36,860
The only difference is that here, instead of having y i j I have a function underneath.

43
00:04:37,370 --> 00:04:41,720
In this case, I'm trying to estimate them in with some data.

44
00:04:41,810 --> 00:04:47,390
Specifically, I'm trying to estimate the natural logarithm of the mean of the outcome.

45
00:04:47,750 --> 00:04:53,570
Okay. But other than that, it follows the same logic.

46
00:04:53,600 --> 00:05:01,760
The only difference between what we did last time and today is that last time my outcome was a normal distributed random variable.

47
00:05:01,850 --> 00:05:04,940
Right. It could be it could take any number.

48
00:05:04,940 --> 00:05:09,620
It could be a decimal, it could be 5.3. It could be like negative, positive we didn't get.

49
00:05:10,160 --> 00:05:19,310
Now we want to change that. And our outcome is not enough is not things is not necessarily going to be normally this random variable,

50
00:05:19,670 --> 00:05:24,950
but it's going to be some sort of like comparable, you know,

51
00:05:25,280 --> 00:05:29,479
and that already imposes some restrictions in my outcome because for example,

52
00:05:29,480 --> 00:05:34,969
this kind of thing, I got this or well, it has to be like a discrete five, right?

53
00:05:34,970 --> 00:05:41,120
It can be 5.3. So that's when we were we are going to try to fit today.

54
00:05:45,620 --> 00:05:55,520
Okay. So for these kind of models for home data, so we have either one or negative binomial,

55
00:05:55,790 --> 00:06:02,330
which means when I say was all negative binomial in this setup in my equations that I'm trying to face,

56
00:06:02,600 --> 00:06:10,100
it means whether these but this thing that I have here follows ideal plus or I'm negative binomial distribution.

57
00:06:12,290 --> 00:06:22,460
The very big difference between poisson anything but a binomial is has to be with the variance with the force of variable we have.

58
00:06:23,210 --> 00:06:30,950
It's not an assumption, it's just something that is happens that the mean of the random variable, it's exactly the same as the variance.

59
00:06:31,670 --> 00:06:35,660
However, with the negative binomial, the variance is larger than for me.

60
00:06:36,110 --> 00:06:39,980
That's why we to the negative binomial we call it.

61
00:06:42,850 --> 00:06:49,120
Over dispersion of biometrics because the greater the mean should also increase like the variance,

62
00:06:49,450 --> 00:06:54,310
but that in crowding the variance should be greater than what we see in the question.

63
00:06:55,870 --> 00:07:01,090
So notice that if I can describe violence of the for someone else up on their fee,

64
00:07:01,450 --> 00:07:05,330
that means that me in a negative environment I have the same problem.

65
00:07:05,430 --> 00:07:14,409
The fee cleanser me lost something else, something that is already positive because pay is going to be a number that is positive.

66
00:07:14,410 --> 00:07:17,800
And these mean a square is always my positive number.

67
00:07:18,220 --> 00:07:20,020
So I need to negative my normal setup.

68
00:07:20,020 --> 00:07:30,550
We are assuming or what we try to assume here is that the variance in these negative binomial is greater than the variance of the plot somewhat.

69
00:07:32,710 --> 00:07:37,240
Any question so far? So it makes sense. Okay.

70
00:07:37,690 --> 00:07:48,280
So we are going to try to fit these models within these models using a negative binomial to fit these kind of models.

71
00:07:49,660 --> 00:07:53,050
Last week we did our linear mix effect model.

72
00:07:53,170 --> 00:08:01,380
Right? So we have like. Fix effects and random effects.

73
00:08:01,560 --> 00:08:07,830
Right. And my outcome was a normal variable in this case, since my outcome is going to be a negative binomial,

74
00:08:08,160 --> 00:08:18,660
the model that enables me to feed that equation is called a g, which is it stands for generalized estimated equations.

75
00:08:18,990 --> 00:08:28,020
So that is what we are going to do today. Let's say we are going to we are working with the following data.

76
00:08:28,290 --> 00:08:41,670
So consider that if you look at epilepsy, epileptic seizure data said when patients were randomized to progestin for something therapy or placebo,

77
00:08:42,150 --> 00:08:49,710
the goal of the study was to see there was no benefit, the benefit of the therapy in reducing the number of epileptic seizures.

78
00:08:50,310 --> 00:08:54,420
Okay. And our dataset looks like this.

79
00:08:54,600 --> 00:09:01,770
I have a variable for the treatment. Whether that person was assigned to this therapy or it was like a placebo.

80
00:09:02,310 --> 00:09:13,110
I also have free seizure, which is the number of seizures before the randomization, before eight weeks of the randomization.

81
00:09:13,380 --> 00:09:21,870
And then you have one, one, two, three and four, which stands for the number of seizure seizures that person had in weeks zero two,

82
00:09:21,870 --> 00:09:25,470
two, four, four and six and six and four after the randomization.

83
00:09:26,160 --> 00:09:31,800
And then I computed a new variable which is forced seizure, which is just adding these three things.

84
00:09:32,010 --> 00:09:41,490
Well, in general that will tell me the number of seizures the patient has wait eight weeks post randomization

85
00:09:42,270 --> 00:09:49,980
and then I also have the 18 years of the patient and I try to see my goal will be to fit that model.

86
00:09:51,330 --> 00:09:55,260
Okay, we will see three different models.

87
00:09:55,440 --> 00:09:59,190
The first one is a negative binomial colored model.

88
00:09:59,550 --> 00:10:03,090
So that means I'm going to ignore any type of correlation.

89
00:10:03,090 --> 00:10:06,270
I'm going to ignore that I follow people throughout time.

90
00:10:06,570 --> 00:10:11,700
I'm going to ignore that I have one variable that's it's indicating the number of seizures

91
00:10:11,700 --> 00:10:16,319
between zero and two weeks and then another one between two or eight four weeks.

92
00:10:16,320 --> 00:10:19,530
I'm just going to treat every everything as if it is the same.

93
00:10:20,160 --> 00:10:28,530
So I'm just going to see negative nine on their model, ignoring all of that, like correlation and then outcomes that I have.

94
00:10:29,760 --> 00:10:36,149
The second approach is going to feed again, which means I'm not going to ignore that correlation.

95
00:10:36,150 --> 00:10:44,130
I'm going to take into account that I follow people throughout time and I have the information for

96
00:10:44,730 --> 00:10:53,160
the outcome in given like not a certain number of weeks after the randomization for that model.

97
00:10:53,460 --> 00:10:59,220
Like under that one hour I'm going to see two different models.

98
00:10:59,700 --> 00:11:09,660
The first model includes only the predictors treatment, so treatment assignment, the number of seizures before randomization.

99
00:11:10,110 --> 00:11:17,880
And then the second model, it's going to have treatment, the number of seizures before randomization.

100
00:11:18,270 --> 00:11:21,930
One week I'm going to the action between week and treatment. Okay.

101
00:11:22,350 --> 00:11:29,550
So in this first model, you know, you are taking into account the dependency on the outcome.

102
00:11:30,180 --> 00:11:34,440
I'm not taking into account the treatment effect.

103
00:11:34,710 --> 00:11:43,800
I'm sorry. The week of the week treatment effect indicates in the model to be I'm taking into account those things by adding more covariance.

104
00:11:46,030 --> 00:11:50,499
Okay. So for Moto A, it's exactly what we do.

105
00:11:50,500 --> 00:12:00,370
A couple of weeks ago when I use proc gen mod, my data is the data that I have like.

106
00:12:02,410 --> 00:12:05,770
They already showed you. And then I have.

107
00:12:06,810 --> 00:12:10,240
To a statement. The first statement is tomorrow.

108
00:12:10,290 --> 00:12:24,480
So in this case, my y variable, my outcome is going to be post seizure and then I'm all including to covariance treatment and pretty procedure under.

109
00:12:25,140 --> 00:12:35,920
I need to specify two cells that I want a negative binomial model so I use backslash this and b which stands for negative by number.

110
00:12:36,690 --> 00:12:44,009
And then this last statement here is Q Equal deviance is just so SAS will give us

111
00:12:44,010 --> 00:12:49,530
results that are more compatible with the other two models that we will run later.

112
00:12:51,120 --> 00:12:58,470
Okay. So if we run, these are my outcome or my primary outcome will look something like this.

113
00:12:58,950 --> 00:13:04,919
I have a table for the standard estimates, so I have an estimate for The Intercept.

114
00:13:04,920 --> 00:13:07,709
I also have an estimate for the treatment effect.

115
00:13:07,710 --> 00:13:16,980
I also have an estimate for the procedure and I have an estimate for the dispersion dispersion parameter that this version with here.

116
00:13:16,980 --> 00:13:20,460
Is that the case that we were talking about before? Okay.

117
00:13:22,800 --> 00:13:26,840
What else? And that's it.

118
00:13:27,140 --> 00:13:36,380
So basically this will be our first model which ignores the dependency India.

119
00:13:37,160 --> 00:13:41,300
So as you may like, imagine this is not the best model we can fit.

120
00:13:42,260 --> 00:13:54,270
Okay. No. Of course, we would like to interpret our model even though it ignores the dependency and the outcomes.

121
00:13:54,690 --> 00:13:58,830
So remember that because we are using a negative binomial,

122
00:13:59,130 --> 00:14:05,700
we need to exponentially aid our estimates to be able to talk about the mean of the negative binomial right.

123
00:14:06,570 --> 00:14:10,050
So that's what I'm doing here. So I.

124
00:14:11,310 --> 00:14:17,820
Estimating estimating the e to power of those parameters.

125
00:14:18,180 --> 00:14:26,580
Right. So my parameters are the intercept, the treatment effect and the procedure effect and the over dispersion band.

126
00:14:27,060 --> 00:14:33,010
However, among all of these four things, I actually only need to the treatment effect on the procedure.

127
00:14:34,750 --> 00:14:45,010
Bye. So that's why here I have two statements, one for the treatment effect and then another one for the seizure decreases in effect.

128
00:14:45,550 --> 00:14:51,100
And then if you run this, you will have on top of the previous table,

129
00:14:51,400 --> 00:14:57,910
you will also have the following table, which has the E to the power of those parameters.

130
00:14:58,210 --> 00:15:05,580
So you have the E do the power of, in this case, treatment effect, and then you also have E to the power of in this case, pre-season effect.

131
00:15:06,040 --> 00:15:12,040
Right. And this is something that you can interpret in terms of the mean of the directly binomial,

132
00:15:12,280 --> 00:15:17,019
which in this case it's indicator of the mean of the number of seizures operation is going

133
00:15:17,020 --> 00:15:22,320
to have given that they were assigned either I left to the treatment or to the placebo.

134
00:15:22,600 --> 00:15:30,159
Right. And then this will be like related to the mean of the numbers seizures operation is going

135
00:15:30,160 --> 00:15:36,610
to have depending on the number of procedures that Verizon had before the randomization.

136
00:15:38,090 --> 00:15:42,639
Any questions so far? Okay.

137
00:15:42,640 --> 00:15:50,560
Good. Okay. So, for example, we would like to interpret this model in the following way.

138
00:15:51,100 --> 00:15:57,850
Let's do it for the three. Except, first of all, we notice that it is not significant because this is greater than 5%.

139
00:15:58,150 --> 00:16:07,940
So is that like a non statistically significant reduction in the number of seizures which was observed in the treatment group with

140
00:16:07,960 --> 00:16:18,190
0.81 times the number of seizures seen as in the placebo group during the eight weeks of follow up and then of course adjusted for.

141
00:16:20,100 --> 00:16:28,050
My other covariates in this case, adjusted for the crazy stories, number of procedures and the same way we got.

142
00:16:31,530 --> 00:16:35,150
The same way we can interpret our other parameter.

143
00:16:36,680 --> 00:16:42,200
Just remember. Very important because we are talking about negative binomial.

144
00:16:43,930 --> 00:16:47,460
First we are using the exponents. Oops.

145
00:16:48,100 --> 00:16:54,670
First we are using the variables that are already e to the power off so the exponents yet it by this.

146
00:16:54,970 --> 00:17:02,230
And then very important note is that here I say with 0.81 times because it's a

147
00:17:02,240 --> 00:17:10,740
multiplicative effect is not more is not less is times 0.81 times than the other groups.

148
00:17:12,040 --> 00:17:18,460
Okay. Um.

149
00:17:18,710 --> 00:17:22,700
Okay. Based on the model, we could also ask the following question.

150
00:17:23,120 --> 00:17:28,219
What's the estimated number of seizures in the eight weeks of randomization for

151
00:17:28,220 --> 00:17:36,130
a participant in the treatment group with 15 seizures in the eight weeks prior?

152
00:17:36,170 --> 00:17:42,290
The randomization. So I want to have estimates, particularly for one type of people.

153
00:17:42,710 --> 00:17:49,100
So in this case, it's just asking me to do the same estimate statement.

154
00:17:49,460 --> 00:17:54,590
The other thing is I need to change to make sure I'm like including all of the information.

155
00:17:55,400 --> 00:18:06,650
So first first of all, I want. So I would want to estimate it to the power of Benazir Europe, one plus one times better treatment.

156
00:18:06,980 --> 00:18:13,550
Why? Because in this case, it's telling me that I want it for the treatment group.

157
00:18:13,790 --> 00:18:15,520
So that's data treatment.

158
00:18:16,700 --> 00:18:26,359
And then I know it has that based on that group of people, would have had 15 numbers of number of seizures before randomization.

159
00:18:26,360 --> 00:18:31,490
So that will be 15 times the data associated to that variable.

160
00:18:31,790 --> 00:18:35,660
And then here in the estimate statement, I do exactly the same.

161
00:18:35,930 --> 00:18:40,120
So intercept one, treatment one, and then procedures 15.

162
00:18:40,790 --> 00:18:44,600
I run the whole thing and then I have my contrast matrix.

163
00:18:44,960 --> 00:18:50,540
And then this poly right here is agreed to the power of all of those things.

164
00:18:50,750 --> 00:18:55,440
Okay. So something that.

165
00:18:58,390 --> 00:19:10,840
If I were to ever say I'm sorry to interrupt, because the question is asking, what is the estimated number of seizures?

166
00:19:11,290 --> 00:19:18,550
So if we go back to the information we are trying to offer this model.

167
00:19:20,920 --> 00:19:34,140
Where? Oops. Where am I? So we are trying to fit this model where these new YJ will be like the number of seizures.

168
00:19:34,920 --> 00:19:41,130
Right. So it's the first thing I do is that I don't want no natural logarithm of that.

169
00:19:41,160 --> 00:19:46,360
I want just that. So to get rid of the natural logarithm, I initiate the whole thing.

170
00:19:46,590 --> 00:19:53,579
Right. So I do e to the power of everything that takes off, takes out these natural logarithm.

171
00:19:53,580 --> 00:19:56,760
And then it's like e to the power of all of these things.

172
00:19:57,000 --> 00:20:03,020
Right. And then notice that those things also include virus you.

173
00:20:03,690 --> 00:20:10,320
So my model in this case looks something like natural logarithm of the number of seizures in sequel to

174
00:20:11,190 --> 00:20:24,630
the zero plus beta treatment times treatment plus made up pre seizures times number of procedures.

175
00:20:25,590 --> 00:20:31,160
So if I want just number of seizures I exponential initiate that whole thing.

176
00:20:33,600 --> 00:20:41,320
Does that make sense? Yeah. Okay.

177
00:20:42,350 --> 00:20:46,780
All right. That would be everything we have to do for model number one.

178
00:20:47,530 --> 00:20:55,680
Now, what happens if I want to include or take into account the correlation that there is already in my outcome?

179
00:20:55,690 --> 00:20:58,770
Right. If I want to account that there are dependent outcomes.

180
00:20:59,950 --> 00:21:08,260
So a couple of weeks ago we talked about the different data formats for the previous.

181
00:21:11,560 --> 00:21:19,210
For the previous variable. For the previous model, we were using the Y for OC Formula one, we were using the right format.

182
00:21:19,540 --> 00:21:25,060
The Y format is a one, but for I only have one rule for every person.

183
00:21:25,960 --> 00:21:33,030
However, for model two, which includes like the correlation between the outcome, we will need the long format.

184
00:21:33,430 --> 00:21:39,700
The long format is the one that no one person would like.

185
00:21:39,710 --> 00:21:41,860
It's multiple times in my dataset.

186
00:21:42,280 --> 00:21:53,200
So for example, here I'd be 104, appears four times each time is telling me something different regarding to the number of scenarios.

187
00:21:53,200 --> 00:22:01,209
So for example, here it's always like that person is always assigned to like plus equal the numbers of procedures.

188
00:22:01,210 --> 00:22:05,050
In some ways it's always in limit and the H is always stating one.

189
00:22:05,350 --> 00:22:09,730
The thing that changes is the number of feature it has in that period.

190
00:22:10,060 --> 00:22:13,209
So for the between zero and two weeks,

191
00:22:13,210 --> 00:22:19,240
that person have like five features between two or four weeks it had three between the

192
00:22:19,480 --> 00:22:23,800
five and six with it had three and then between the seven and eight it had three.

193
00:22:24,430 --> 00:22:30,220
So we need the log format for the following models that we will meet.

194
00:22:31,300 --> 00:22:40,000
So in this case, for the second model, we want to acknowledge that there is correlation between the outcome and I want all the two predictors,

195
00:22:40,000 --> 00:22:43,780
treatment and number of creative Pre-treat Sisters.

196
00:22:44,290 --> 00:22:52,840
Okay, so I'm going to use track gen model, which is like generalized model.

197
00:22:55,190 --> 00:23:01,570
I'm going to use data seizure law. So make sure you're using the long format of the data.

198
00:23:02,080 --> 00:23:12,130
And remember that over here, we need the class I.D. The class I.D. is telling SAS that I have repeated observations based on one variable.

199
00:23:12,370 --> 00:23:13,089
In this case,

200
00:23:13,090 --> 00:23:21,490
I have repeated observations based on I.D. so I have multiple observations for the same I.D. That's what I'm telling you here to assess class I.D.

201
00:23:21,850 --> 00:23:26,920
Okay, then I just construct my model the same way I would do it before.

202
00:23:27,970 --> 00:23:36,220
So model is equal to in this case. Notice that if we go back a little bit, if we go back to our long format,

203
00:23:36,670 --> 00:23:45,910
my outcome in this case is this is a number of weeks is a number of I'm sorry, the number of citrus each interval of two weeks.

204
00:23:48,160 --> 00:23:54,459
So my Uncle Arthur Morrow would be my Bible count and then my to predict or treatment.

205
00:23:54,460 --> 00:24:02,610
I'm pretty sure on that. And then I also have to specify that I want the distribution of the outcome to be negative binomial.

206
00:24:04,810 --> 00:24:18,060
Okay. And then remember that after the class I b I also need to specify the type of correlation that I am alleging in the outcome by.

207
00:24:18,970 --> 00:24:25,600
So some equal i b those the same kind of function as the class ID is just telling

208
00:24:25,870 --> 00:24:32,259
us that I have repeated measures based on the look and then here backslash type C.

209
00:24:32,260 --> 00:24:41,079
S stands for compound symmetry. So acknowledging that I have correlation between the outcomes opens that question.

210
00:24:41,080 --> 00:24:45,760
Ask what kind of correlation? Because there can be different types of correlation.

211
00:24:46,120 --> 00:24:57,730
In this case, I'm feeding on the binomial model in which the correlation of the outcome follows a compound symmetry type of correlation.

212
00:24:58,120 --> 00:25:06,430
Okay, we can change these and in fact we should to try different like correlation structures.

213
00:25:06,760 --> 00:25:12,040
For example, you can also change to literally let it be anything.

214
00:25:12,760 --> 00:25:20,620
You can also change to alter aggressive correlation. And then there are many more so like exponential correlation and things like that.

215
00:25:21,580 --> 00:25:29,800
And then model ls ask for the analysis of biomarkers estimates.

216
00:25:30,910 --> 00:25:34,990
So is going to tell us like confidence intervals on those things.

217
00:25:36,100 --> 00:25:41,820
Okay. Any question so far? Good.

218
00:25:46,100 --> 00:25:59,870
Okay. So what we should do in general is grant these models for four times what many times we think that I should change every time.

219
00:26:00,020 --> 00:26:05,660
Is the type of correlation of structure. Right? In this case, I have a compound symmetry.

220
00:26:05,960 --> 00:26:10,260
I should do this same thing. Exactly the same code.

221
00:26:10,310 --> 00:26:17,960
The only thing I change here is instead of compounds going to just ar1 or instead of ar1.

222
00:26:18,920 --> 00:26:24,230
I don't know. C and D, which is like let it be what it is, so on and so forth.

223
00:26:24,470 --> 00:26:27,950
And then compare based on I'm based on a metric.

224
00:26:28,640 --> 00:26:33,260
Which model performs better? So that's what I have here.

225
00:26:33,770 --> 00:26:39,950
So I run that exact same statement four times, changing that C.

226
00:26:39,950 --> 00:26:46,310
S to the different possible values. And then I have the summary of those four models here.

227
00:26:46,910 --> 00:26:54,350
So this is the summary for the independent study of correlation instruction for the final structure,

228
00:26:54,380 --> 00:26:59,360
correlation for the compound symmetry, and then for the ultra aggressive symmetry.

229
00:27:00,170 --> 00:27:04,940
What I should do now is. Select the.

230
00:27:07,420 --> 00:27:17,580
Correlation. A structure that performs better for us better will be based on the QIC criteria.

231
00:27:18,070 --> 00:27:28,080
So the first criteria here. And then I want to truth in general are lower and lower is better.

232
00:27:29,220 --> 00:27:40,110
Nowhere is better. So among all of these four gradations, the one that is better, it could be either in the front lines or complete symmetry.

233
00:27:40,110 --> 00:27:45,930
They have more or less the same right.

234
00:27:46,080 --> 00:27:54,630
Then more negative the better. So I will select the compound symmetry.

235
00:27:55,350 --> 00:28:02,729
If you were wondering how is the compound symmetry a structure?

236
00:28:02,730 --> 00:28:10,560
It looks something like this. So it has one in the diagonal and then the same correlation among the different outcomes.

237
00:28:10,980 --> 00:28:14,520
And these like matrix is telling you the correlation between.

238
00:28:25,860 --> 00:28:32,310
So the way to read this is this would make the correlation.

239
00:28:32,910 --> 00:28:37,200
So like one subject, then it doesn't matter. It's going to be the same for all subjects.

240
00:28:37,530 --> 00:28:48,120
So like one subject, this will be the estimated correlation between the first, you know, and the second outcome.

241
00:28:49,550 --> 00:28:54,680
All right. And then this volume, which is the same because we are using our component symmetry thing,

242
00:28:55,280 --> 00:28:59,390
is the combination between the first outcome and the third outcome.

243
00:29:00,270 --> 00:29:04,070
Okay. That's how you read this matrix. Does that make sense?

244
00:29:05,910 --> 00:29:21,940
Okay. So given that I chose the compound symmetry model, now I'm going to do exactly the same thing before.

245
00:29:22,360 --> 00:29:29,260
I want to explain, initiate the parameters and then interpret those parameters in terms of the models that I'm seeing.

246
00:29:29,800 --> 00:29:35,980
So that is statement of explaining, shading the parameters remains exactly the same.

247
00:29:36,550 --> 00:29:40,120
So is exactly the same as same thing that I'll say paths before,

248
00:29:40,690 --> 00:29:45,879
and then I have a contrast matrix and then I can interpret the confidence matrix in terms

249
00:29:45,880 --> 00:29:51,520
of my new model that accounts for takes into account the dependency within the outcomes.

250
00:29:52,540 --> 00:29:58,120
Okay. Given that, I also want to answer the same question that we had before.

251
00:29:58,600 --> 00:30:03,900
So what's the estimated number of seizures in eight weeks?

252
00:30:03,910 --> 00:30:13,960
Post randomization for a participant of the treatment group that had 15 seizures before randomization.

253
00:30:15,160 --> 00:30:19,990
Okay. So here.

254
00:30:24,500 --> 00:30:30,400
In the first model. What we did is like ignoring the fact that.

255
00:30:31,440 --> 00:30:39,190
You know how they measure the number of seizures between zero and two, then between two and three, then between three and four.

256
00:30:39,330 --> 00:30:43,860
And I just add them all up. I was like, you know, I'm one of them because that's going to be my.

257
00:30:45,310 --> 00:30:55,180
Yeah. What I'm doing in the second photo is exactly the opposite of the opposite, but something different instead of like adding them all.

258
00:30:55,210 --> 00:31:00,070
I'm in full dose as my outcome. What I'm doing is.

259
00:31:02,970 --> 00:31:08,600
Feeding each one of these so far assumed that you I'd have one number for 2 to 4.

260
00:31:08,610 --> 00:31:13,050
I have another number for 3 to 4. I have another number and so forth.

261
00:31:14,310 --> 00:31:20,070
So my outcome here is telling me the number of seizures in two weeks.

262
00:31:20,430 --> 00:31:29,440
Right. However, in the first model it was telling me the number of seizures already in eight weeks because I decided to add this month.

263
00:31:30,110 --> 00:31:34,700
In the second one, my alchemist tells me the number of seizures in two weeks.

264
00:31:35,120 --> 00:31:42,440
So if I want the number of seizures in a few weeks, what I should do in the first column,

265
00:31:42,800 --> 00:31:51,260
absolutely nothing because you outcome is already in the same direction that you needed in the second model.

266
00:31:51,530 --> 00:31:54,650
What I should do is multiply by four, right?

267
00:31:55,280 --> 00:32:02,930
Because there are 4 to 4 times two weeks is equal a equals eight weeks.

268
00:32:03,410 --> 00:32:07,250
So that's what we are going to do. So.

269
00:32:08,390 --> 00:32:13,360
What I do is. Ha.

270
00:32:13,370 --> 00:32:18,200
I don't have it here. I don't have it here.

271
00:32:18,350 --> 00:32:22,780
But you should also do a new estimate here.

272
00:32:24,390 --> 00:32:30,500
Call it however you want. Right. Include the intercept one.

273
00:32:30,510 --> 00:32:34,410
Include treatment one on three seizures.

274
00:32:36,370 --> 00:32:39,370
15 and then run that.

275
00:32:39,850 --> 00:32:43,900
And then if you run that, you're going to have these outcomes.

276
00:32:44,560 --> 00:32:52,020
Okay. And then what we will do is multiply that by four and that's it.

277
00:32:52,450 --> 00:32:58,590
Because this is telling you the outcome for. Two weeks the number of seizures for two weeks.

278
00:32:58,600 --> 00:33:02,409
And I just wanted by for just remembered.

279
00:33:02,410 --> 00:33:13,270
I don't know, I, I don't have it here that you wouldn't need a new estimate, a statement here that has been versus one treatment, one preseason of 15.

280
00:33:13,660 --> 00:33:20,430
Right. As we did in the other one. And that's okay.

281
00:33:21,220 --> 00:33:38,180
Any questions so far? I guess for comparison's purposes, notice that the answer to this question using mine one was close to 13.93.

282
00:33:40,320 --> 00:33:49,530
And then the answer using the second model, it's these multiplied by four, which is 14.3.

283
00:33:49,830 --> 00:33:54,200
So more or less the same. Right. Okay.

284
00:33:54,700 --> 00:33:58,850
Now I want to do exactly the same, but just add more covariates.

285
00:33:58,860 --> 00:34:02,100
Our week on. Week. Time. Time. Street.

286
00:34:03,270 --> 00:34:06,330
Why Dawn? What's the purpose of adding these?

287
00:34:10,360 --> 00:34:14,350
Covariates interim order. So aside of what we are doing.

288
00:34:16,750 --> 00:34:25,270
Look at this. Let's say I have a very easy model that has the following covariance.

289
00:34:25,280 --> 00:34:29,300
It has time. It has treatment effect.

290
00:34:31,580 --> 00:34:35,780
And then it has the interaction between time and treatment effect.

291
00:34:38,130 --> 00:34:41,940
Each time it's measuring something different.

292
00:34:43,220 --> 00:34:51,620
High times weak. So the interaction effect is measuring whether these two lines are parallel or not.

293
00:34:52,340 --> 00:34:56,540
These two lines represents the mean outcome in each group.

294
00:34:56,660 --> 00:35:06,200
So for example, let's say group one is black, group one is the actual treatment and look through is the placebo and it's not the mean outcome.

295
00:35:06,500 --> 00:35:16,360
This is the time and that's the outcome by measuring. If I do, if I perform my hypothesis test on Beda.

296
00:35:17,570 --> 00:35:22,700
Interaction. What I'm testing is whether these two lines are parallel or not.

297
00:35:22,970 --> 00:35:30,440
Right. That's the first thing. So that will be checking the interaction effect.

298
00:35:31,220 --> 00:35:36,650
If I check the significance of.

299
00:35:40,590 --> 00:35:50,960
If I check the significance of treatment. What I'm checking is whether those two lines are on top of each other.

300
00:35:51,530 --> 00:36:00,000
Okay. So in my model, one has three things time lost treatment.

301
00:36:02,880 --> 00:36:07,230
Plus Time Times treatment in this model.

302
00:36:08,550 --> 00:36:19,200
If I check the significance of treatment intrinsically, what I'm doing is just checking whether those two lines are like on top of each other.

303
00:36:19,450 --> 00:36:26,890
I seen these graphs. Okay. I'm not checking whether they change over time or whether they are decrease strain or increase time.

304
00:36:27,180 --> 00:36:30,360
I'm just checking whether those two things are on top of each other.

305
00:36:32,880 --> 00:36:36,210
And then if I check for the time effect.

306
00:36:36,690 --> 00:36:40,290
So remember, I have time. Plus treatment.

307
00:36:41,520 --> 00:36:45,630
Plus treatment. Plus time times treatment.

308
00:36:49,750 --> 00:36:58,030
If I check for the time effect, I'm asking whether those two lines are constant, you know, constant.

309
00:36:59,230 --> 00:37:04,300
Not on top of each other, no parallel. I'm just asking whether they are constant or at times.

310
00:37:05,190 --> 00:37:11,140
So those are the three hypotheses that I come to in the main effect and then in an interaction.

311
00:37:11,220 --> 00:37:15,450
And in general it's like generally speaking when we do like.

312
00:37:16,470 --> 00:37:19,020
Analysis of dependent outcomes.

313
00:37:19,320 --> 00:37:28,950
We are more interested in the three in the interaction effect, whether those two lines are part or not, because if they are not parallel,

314
00:37:28,950 --> 00:37:36,600
it means that even like if you know, time passes, one effect may be more significant than the other effects.

315
00:37:38,300 --> 00:37:43,300
Okay. So morale to me. I do exactly the same thing.

316
00:37:44,030 --> 00:37:48,960
I use the long form of the data. I use my class I.D. I have the same model.

317
00:37:48,980 --> 00:37:53,850
The only difference is that I added to covariate week on with times treatment.

318
00:37:54,290 --> 00:38:02,720
I use remember to include that the distribution is negative binomial and I'm still using the compound symmetry.

319
00:38:05,180 --> 00:38:08,270
The compound symmetry. A structure under.

320
00:38:08,930 --> 00:38:10,990
I'll have something like this.

321
00:38:11,870 --> 00:38:24,019
I have a table that fits their model with their specifications that indicates I have effects for treatment for my baseline variable,

322
00:38:24,020 --> 00:38:27,050
and then for a week and treatment times a week.

323
00:38:27,950 --> 00:38:32,330
And then I just want to check whether those things are significant or not.

324
00:38:33,050 --> 00:38:40,160
So based on what we saw, if I look for the significance of the Treatment Times Week for the interaction effect,

325
00:38:40,520 --> 00:38:47,570
this thing is not significant, which means that the lines are mostly parallel.

326
00:38:47,720 --> 00:38:54,830
Yeah, that's right. That would be treatment. Times of week equals zero versus the alternative.

327
00:38:54,830 --> 00:38:56,030
They are defined months later.

328
00:38:56,510 --> 00:39:06,830
So in this case, I would say something like the linear trend over time of the outcome in this case of the number of users to expose randomization is,

329
00:39:07,220 --> 00:39:13,610
if any, does not depend on the treatment because it's not significant enough.

330
00:39:13,610 --> 00:39:23,129
Statistically significant minor. Which means that trade treatment groups look part of a trend in treatment groups.

331
00:39:23,130 --> 00:39:28,530
No products. Okay, so that will be the first one.

332
00:39:29,490 --> 00:39:38,400
Notice that here we are very cost conscious of the way we specify that it's a linear trend.

333
00:39:38,760 --> 00:39:44,700
And this has to be because for some reason we just decided to include time as linear.

334
00:39:44,730 --> 00:39:52,860
We didn't do time to square old time natural, the beginning of time or something like, you know, different interpretation of time.

335
00:39:54,120 --> 00:39:58,380
So when I analyze these treatment Times Week effect,

336
00:39:58,680 --> 00:40:06,090
I'm also like behind that I have a hypothesis that means that the tide is linear or that the effect of time is near.

337
00:40:07,740 --> 00:40:14,460
Okay. Second one, if treatment is like, it's also insignificant.

338
00:40:14,850 --> 00:40:23,490
So based on the fact that the interaction effect is not significant, I go to the treatment effect.

339
00:40:23,500 --> 00:40:30,750
Remember that treatment effect is looking whether those two lines are exactly the same, one stock of the other.

340
00:40:31,470 --> 00:40:37,260
It's also not significant. So in combination with the insignificance of the.

341
00:40:39,580 --> 00:40:49,210
Interaction. There is no significant difference between the therapy and the placebo group in the number of seizures in two weeks post randomization.

342
00:40:49,720 --> 00:40:53,680
So for me, so far, those two those two groups looks the same.

343
00:40:54,310 --> 00:41:01,900
And then for a week, no, we think that there is an effect of time on the number of seizures is still insignificant.

344
00:41:01,900 --> 00:41:09,640
So on top of the fact that the interaction is not significant, the main effect of week is also not significant.

345
00:41:09,790 --> 00:41:16,059
Okay, so it seems like those things are exactly the same one on top of each other.

346
00:41:16,060 --> 00:41:19,209
And not only that, they are also flat over time.

347
00:41:19,210 --> 00:41:20,920
So it doesn't change over time.

348
00:41:20,920 --> 00:41:31,060
There's no there's no there's no reason for us to believe that there's like an increase or decrease in the numbers of seizures if time increases.

349
00:41:32,140 --> 00:41:35,850
And that's it. Any questions so far?

350
00:41:39,670 --> 00:41:48,960
All right. Well, that's it. That's everything you will need to do.

351
00:41:49,590 --> 00:41:50,580
So how many?

