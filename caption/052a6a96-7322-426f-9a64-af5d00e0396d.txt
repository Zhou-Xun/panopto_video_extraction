1
00:00:00,180 --> 00:00:07,500
Let's continue. So we'll finish the continuous time markup process today.

2
00:00:07,560 --> 00:00:19,580
Last time we talked about the wait time or into arrival time distribution for the birth and death process.

3
00:00:21,150 --> 00:00:25,140
This was not a rigorous derivation,

4
00:00:25,230 --> 00:00:40,230
but the get the intuition that we were trying to calculate the probability by Gregorio P so this is the T of II is the waiting

5
00:00:40,230 --> 00:00:51,600
time when the population size reaches high and then this is the wait time into the next either birth or that event occurs.

6
00:00:51,930 --> 00:00:56,530
So this last time we defined this.

7
00:00:59,310 --> 00:01:04,410
So last time we showed the GDI T plus H.

8
00:01:05,130 --> 00:01:14,370
So essentially this is a tie to t plus h has a relationship in capturing that.

9
00:01:14,370 --> 00:01:21,660
Last year I multiply by II so of H.

10
00:01:21,930 --> 00:01:25,140
So there is no event occurred in this short interval.

11
00:01:25,350 --> 00:01:32,340
H And then that gives us the the differential equation.

12
00:01:32,670 --> 00:01:41,860
G I mean. Q i g i.

13
00:01:42,190 --> 00:01:49,180
So this is the differential equation, what we obtained by using that simple definition,

14
00:01:49,240 --> 00:01:59,410
and then by solving it, we get beyond this exponentially distributed white box.

15
00:02:01,930 --> 00:02:07,330
All right. So again, we you have been a theme here.

16
00:02:07,750 --> 00:02:13,420
So the waiting time for the next event to occur, it's always exponentially distributed.

17
00:02:13,780 --> 00:02:24,000
But in this case, the wait time distribution depends on both the birth rate and the death bursary, the that's rate.

18
00:02:26,290 --> 00:02:38,319
So if you want to be thinking about this the best way in general, the best way to understand or think about continuous time,

19
00:02:38,320 --> 00:02:44,620
stochastic process is actually to think about how you simulated, how you generate it,

20
00:02:46,780 --> 00:02:52,360
how you can generate a simulated dataset or the other way to formulate it.

21
00:02:52,420 --> 00:02:59,260
If you look at the practical situation, you'll want to think about the data.

22
00:02:59,380 --> 00:03:06,070
All the data is to generate a probabilistic model to generate this type of the data.

23
00:03:06,070 --> 00:03:14,080
So well, so far as if you have a wait time distribution for continuous time on stochastic processes,

24
00:03:14,080 --> 00:03:20,560
you kind of think about, you know, you simulate a wait time from this distribution until that event occurs.

25
00:03:21,220 --> 00:03:29,980
So for Persol or pure birth, this type of the process, the event means a unique scenario.

26
00:03:30,040 --> 00:03:36,010
You get a, you know, you're at one, two, you come here because all these are cognitive process.

27
00:03:36,280 --> 00:03:41,470
It's a bit difficult. Well, it's a bit different to be in the situation of a person death.

28
00:03:41,770 --> 00:03:45,310
You need to determine it's either adverse event or death event.

29
00:03:46,120 --> 00:03:55,180
Okay, so there's some randomness to it, but this type of the randomness is actually defined in the definition.

30
00:03:55,300 --> 00:04:00,220
So let's see how this so once the event occurred.

31
00:04:02,790 --> 00:04:23,400
It's a it's a purse or a that that you want to see be able to see the birth process.

32
00:04:26,520 --> 00:04:30,329
So we need to find it. So we know we know the waiting time.

33
00:04:30,330 --> 00:04:33,750
So. So you could start on time. Zero and soon.

34
00:04:33,840 --> 00:04:38,790
So this it will be just a London zero, right?

35
00:04:38,790 --> 00:04:46,490
Where the new zero will be zero. So you're waiting for a certain time and then the first event must be the first event.

36
00:04:46,830 --> 00:04:55,650
Similarly, the population size from zero, you don't have to, but if you start from zero equals zero, then it has to be a first.

37
00:04:55,980 --> 00:04:59,400
The next. The waiting time is a new one plus lambda one.

38
00:04:59,850 --> 00:05:05,250
What I wrote there is I. And then the type of that you.

39
00:05:05,490 --> 00:05:09,660
You need to simulate what an event. So we need to.

40
00:05:09,900 --> 00:05:14,490
So this is the randomness is described by this probability.

41
00:05:15,060 --> 00:05:37,730
It's essentially a birth event conditional on an event occur by type T, right?

42
00:05:37,770 --> 00:05:40,649
So this is really what you want because the simulation,

43
00:05:40,650 --> 00:05:49,020
when we know over the wait time distribution that they can simulate exactly when the event occurs.

44
00:05:49,050 --> 00:05:55,290
So what you need to do now is conditional events occurs at the T that event that is a part of the

45
00:05:55,290 --> 00:06:01,559
word that that you need to determine that that's something you don't think about to flip a coin.

46
00:06:01,560 --> 00:06:06,480
But you need to need you need to know the a chance to get the head or tail.

47
00:06:06,570 --> 00:06:10,440
So in this case, the probability of the birth, the event.

48
00:06:10,980 --> 00:06:20,640
So we don't necessarily know this from the destination, but it's pretty easy to derive this conditional probability from the destination.

49
00:06:20,940 --> 00:06:30,060
So the way you do it, so the destination is not specified at the time t is the don't have information at the particular time point,

50
00:06:30,660 --> 00:06:33,600
but it does give you a time interval.

51
00:06:34,020 --> 00:06:44,940
So if you can tie this and change this problem a little bit higher, this conditioning to a interval and then take the limit of that interval.

52
00:06:45,440 --> 00:06:55,770
And so that will give you the answer. So the way to do it is actually showing this is actually the limit one h goes to zero.

53
00:06:56,430 --> 00:06:59,940
Now you're going to change event a further event.

54
00:07:06,240 --> 00:07:14,610
Now you can condition an event occurs at time instead of a precise time.

55
00:07:14,970 --> 00:07:19,160
The loss of age. Right.

56
00:07:20,250 --> 00:07:23,730
We could always do that. That's not nobody going to follow you.

57
00:07:24,030 --> 00:07:28,470
So you're no event occurred in the interval and now you just take if you do.

58
00:07:29,190 --> 00:07:34,599
Now we know this is going to be a function of age and I would just take the age close to zero.

59
00:07:34,600 --> 00:07:46,680
And that gave us back to this. So the target conditional probability is just we just write that as a limit by changing the problem a little bit.

60
00:07:47,970 --> 00:07:59,250
So this is a easy problem if you have a birth event occur in the in the interval diluted now by the destination,

61
00:07:59,790 --> 00:08:05,310
it must be the birth rate of nine times, age plus old age.

62
00:08:07,020 --> 00:08:12,270
And then you need to divide it by the probability.

63
00:08:12,300 --> 00:08:17,640
In this interval there is a event occurs. So only two types of events can occur.

64
00:08:17,640 --> 00:08:24,530
A birth and the word die. So on the age plus muigai age.

65
00:08:24,870 --> 00:08:33,740
And now you can combine old age here and know we just need to take age close to zero.

66
00:08:33,750 --> 00:08:41,640
So the way to do that is to divide the age on the numerator and denominator, then the limit.

67
00:08:41,940 --> 00:08:53,080
You got old age over age zero and then at the end of the day, we get a lot of the Y, right?

68
00:08:53,790 --> 00:09:02,220
So it's pretty straightforward. The key is connecting the problem at hand to.

69
00:09:02,460 --> 00:09:12,240
What you already know. So if it's not perfectly matched up, create that condition to be connected to the two type of the.

70
00:09:12,780 --> 00:09:18,800
The problem that the questions are are. Are connected and it's just the easy way.

71
00:09:18,810 --> 00:09:30,180
So similar. Similarly, you can show a verse that the event occurs its as new I divided by a longer i plus mean y.

72
00:09:30,360 --> 00:09:47,040
Right. So there are. So is this the the data generating process or this the so called a probabilistic generative model is clear.

73
00:09:47,550 --> 00:09:51,120
So how do you simulate the process of that event? Obviously,

74
00:09:51,120 --> 00:10:03,900
you need to know all of the movies and then Lambda is beforehand and then you just need to simulate the wait time and then one the wait time.

75
00:10:04,950 --> 00:10:08,160
At the end of the wait time, you know, there is that event are going to occur.

76
00:10:08,370 --> 00:10:18,770
You flip a coin, a coin to decide it is either a birth event in the earth that event and just add to the counter, follow the rules.

77
00:10:18,810 --> 00:10:31,200
That's it. If you think of this problem in general, you know,

78
00:10:31,200 --> 00:10:44,170
we have done this four person process races to simulate the exponential waiting time, simulate the unknown for the pure first event.

79
00:10:44,400 --> 00:10:54,660
The same thing we think we show that the wait time is also exponential distributed is not identically exponential.

80
00:10:55,110 --> 00:11:04,590
They don't have to necessarily have the same the parameter for the exponential distribution, but they independent exponential distribution.

81
00:11:06,360 --> 00:11:20,400
And then if you think about what going to happen when the wait is over, you will realize in the person that that event there is invalid Markov chain.

82
00:11:21,360 --> 00:11:24,570
So you have a current population size.

83
00:11:24,870 --> 00:11:30,900
You either have a birth event. That means in that state space you increase your population by one.

84
00:11:31,230 --> 00:11:36,180
According to that transition probability alumni over alumni plus M.I.

85
00:11:36,180 --> 00:11:40,860
UI or according to or a death event.

86
00:11:41,040 --> 00:11:46,430
So that's move the population size from eye to eye minus one, right.

87
00:11:46,440 --> 00:11:52,230
According to the transition probability me y over alumni penis me y.

88
00:11:52,950 --> 00:12:03,780
So you see this type of the thing that's actually naturally connect Markov chain and then continuous time Markov process together.

89
00:12:04,710 --> 00:12:15,960
As a matter of fact, all these Markov process, continuous time can be seen in this general formulation,

90
00:12:16,500 --> 00:12:21,540
especially in a discrete if you squint state space situation.

91
00:12:21,800 --> 00:12:26,070
Right. So you always have a exponential waiting time.

92
00:12:26,340 --> 00:12:31,260
They are independent and then you operate according to a markov chain.

93
00:12:31,860 --> 00:12:38,900
Just change the state from one to the other. According to the transition probability.

94
00:12:42,570 --> 00:13:02,080
That actually specified beforehand a more general extension to this type of the stochastic modeling is actually you keep the Markov chain part.

95
00:13:02,190 --> 00:13:11,160
So you got this discrete. So you set up a markov chain so that Markov chain will governing how the state translate to each other.

96
00:13:11,370 --> 00:13:16,920
Right. So from state I to A-plus one or that I plus from state to state.

97
00:13:18,030 --> 00:13:20,970
Okay. So you set up the Markov change the discrete time.

98
00:13:22,620 --> 00:13:28,920
Well, I'm sure you only need to know the transition probability meaning in the US in that Markov chain.

99
00:13:29,310 --> 00:13:35,310
Okay. So to put that in the continuous time now, you need to simulate the wait time.

100
00:13:36,680 --> 00:13:42,530
You can imagine you simulate any distribution according to any distribution, not necessarily just exponential.

101
00:13:43,190 --> 00:13:47,150
Right. You can simulate, say, uniform distribution.

102
00:13:47,330 --> 00:13:54,200
Obviously, if it's a time you need to simulate according to the correct the support.

103
00:13:54,200 --> 00:13:57,200
So you only simulate positive or negative numbers.

104
00:13:58,010 --> 00:14:07,130
Right. You can simulate according to karma. So in general, if you have independent weight time distribution.

105
00:14:08,270 --> 00:14:15,319
Okay. So the resulting process and then you simulate wait time and then you reference to the Markoff train and to make a

106
00:14:15,320 --> 00:14:23,120
transition and then you simulate next waiting time and then reference the Markoff train to make a next transition.

107
00:14:23,150 --> 00:14:30,410
Right. So that means that well, because now you basically described this type of the transition event.

108
00:14:31,730 --> 00:14:37,040
So in general, this type of the process is known as, say, my Markov process.

109
00:14:37,820 --> 00:14:41,090
In general, they are not Markov process because.

110
00:14:43,850 --> 00:14:51,409
You can see. Well, I mean, you don't need to show, but take my words, those are the mark of property,

111
00:14:51,410 --> 00:14:59,930
not necessarily satisfied if you use their arbitrary independent weight time distribution.

112
00:15:00,380 --> 00:15:03,980
However, if you're using exponential way type distribution,

113
00:15:04,130 --> 00:15:10,520
it's guaranteed the resulting process will be Mark Hall will be continuous time mark of process.

114
00:15:10,850 --> 00:15:16,639
So this is sort of a generalization of continuous time Markov process.

115
00:15:16,640 --> 00:15:28,110
And then there's connections to all the Markov chains that also make what we learn in Markov chain use, right?

116
00:15:28,130 --> 00:15:32,720
So they are easily invited into a continuous time distribution.

117
00:15:32,990 --> 00:15:45,380
It's just different view point over these connectivity diagram and the analysis of aquatic properties will become relevant,

118
00:15:46,310 --> 00:15:49,670
but that's not in the scope of this course anymore.

119
00:15:49,830 --> 00:15:53,270
But you know, you understand this generalization.

120
00:15:54,850 --> 00:15:59,420
It could come in handy later on for your own research.

121
00:16:03,890 --> 00:16:11,620
Right. So the other thing we could talk about this is the limiting behavior.

122
00:16:11,630 --> 00:16:14,800
So so this is first line death process, basically.

123
00:16:14,810 --> 00:16:23,780
Now, you know how this kind of a general reading process of you can simulate it and then you can study the property.

124
00:16:24,080 --> 00:16:34,940
So the third one is in this case, it's a possible the underlying person death process and a limiting behavior.

125
00:16:35,690 --> 00:16:48,440
So in general, if you have pure birth of pure birth or poor song process, those are all counting process to think about invalid Markov chain.

126
00:16:48,800 --> 00:16:53,720
They always go one direction, right? So the population sites always increase.

127
00:16:53,720 --> 00:16:54,860
There's no decrease.

128
00:16:55,160 --> 00:17:04,700
So, you know, if you run up the process forever, there is no limiting behavior because your population size can explode to infinity.

129
00:17:05,100 --> 00:17:14,270
Right. This one is different. There may be an equilibrium point that that your population size got stabilize to the.

130
00:17:16,490 --> 00:17:22,280
So you may have a distribution if you run the chain long enough, you may have a distribution on the population.

131
00:17:22,280 --> 00:17:24,800
So if you run this process long enough.

132
00:17:25,370 --> 00:17:40,520
So the proposition one says if you hypothesize this type of thing, you know, there is a limiting behavior that put into the mathematical language.

133
00:17:40,700 --> 00:17:44,269
What you are essentially saying is transition.

134
00:17:44,270 --> 00:17:57,220
So so this is I to j so initial population size is I and then the transit to J after this long time.

135
00:17:57,240 --> 00:18:05,470
So T goes to infinity kind of be depends only on J, right?

136
00:18:05,580 --> 00:18:14,870
So this is the what we say about in the discrete Markov chain, discrete time Markoff chain.

137
00:18:14,870 --> 00:18:21,080
What is the limiting behavior means? So if there is y, you run the chain long enough.

138
00:18:21,800 --> 00:18:30,770
If you run this process long enough, first of all, the behavior kind of a cop hold the width or initial state distribution,

139
00:18:30,890 --> 00:18:34,850
which is irrelevant in this case, is irrelevant with AI.

140
00:18:35,510 --> 00:18:39,580
And then that becomes only a property of J, right?

141
00:18:39,600 --> 00:18:43,760
So this is the stationary distribution on the population songs.

142
00:18:45,020 --> 00:18:57,670
So this is a proposition one, I think. So what we need is, first of all, that this limit exists.

143
00:18:59,320 --> 00:19:02,860
So this is what we really mean by limiting behavior.

144
00:19:03,340 --> 00:19:09,610
But to ensure this is a distribution on the population size, we also need.

145
00:19:11,270 --> 00:19:25,250
Not only exists and are so obviously this no patient means that independent of the initial state distribution.

146
00:19:34,860 --> 00:19:42,720
So the other thing is to make sure it's a distribution. We need to make sure he WJ equals one.

147
00:19:43,020 --> 00:19:50,550
So they obviously going to be positive and negative because there are elements of net negative quantities, not negative functions.

148
00:19:51,720 --> 00:19:54,900
We just need to have that one right.

149
00:19:55,680 --> 00:20:25,530
So if that's the case, he j is the stationary distribution of the underlying person, that person death process.

150
00:20:28,620 --> 00:20:35,219
All right. So so Paul, I think probably Proposition one, it's not really a proposition.

151
00:20:35,220 --> 00:20:38,800
It's more like the definition of so the two things.

152
00:20:39,540 --> 00:20:45,540
So first of all, not only the person that houses stationary distribution, you can imagine, right?

153
00:20:45,570 --> 00:20:55,560
So if the first rate is, you know, always overwhelming, the mean is much, much larger than the death rate,

154
00:20:55,920 --> 00:21:04,960
then the process behavior is pretty much like a pure birth or can be approximated like you're kind of like the movie all goes to zero.

155
00:21:04,980 --> 00:21:09,870
So the birth and death process, just like a pure birth process.

156
00:21:09,870 --> 00:21:15,630
And then that type of the process, we know we don't have a stationary distribution,

157
00:21:16,830 --> 00:21:25,799
but if they do happen, you need to require some conditions of a on UI.

158
00:21:25,800 --> 00:21:39,150
And then longer than this proposition one tells you there are criteria of a stationary distribution exists.

159
00:21:41,070 --> 00:21:46,290
One is the limit of that function, the transmission kernel exists.

160
00:21:46,290 --> 00:21:50,400
And then the secondly, over this limit has to one.

161
00:21:57,370 --> 00:22:01,270
Well, the Proposition two shows the what ifs.

162
00:22:01,960 --> 00:22:18,730
So first of all, for arbitrary, this is a converges the point of showing this is convergence is you may have finite number of you the

163
00:22:18,730 --> 00:22:29,090
population we can have a state space uncountable madness and take comfortable mathematics and so on.

164
00:22:29,440 --> 00:22:35,080
So it's not guaranteed this one even converges on, let alone converges to one.

165
00:22:37,060 --> 00:22:41,710
But you can show this. This is a pretty simple calculus summation.

166
00:22:42,010 --> 00:22:50,770
Okay, P.J., according to Proposition one, if it does exist, that can be written as the limit of pi.

167
00:22:54,700 --> 00:22:59,830
What have we got to say about this is that he goes to infinity.

168
00:23:00,160 --> 00:23:06,370
And then because in the positive quantities you can of smaller than the summation operation.

169
00:23:06,760 --> 00:23:29,860
So if you switch the the so you see this, the summation here is it's one because that's the only finite time.

170
00:23:29,860 --> 00:23:38,800
T You first do the calculation here. So that sum over all these destinations for any arbitrary starting point I.

171
00:23:39,250 --> 00:23:44,830
So this is less sorry. So this one, let's just say this is the last nine equal to.

172
00:23:49,860 --> 00:23:53,280
This this one in this one.

173
00:23:54,420 --> 00:23:57,540
So there is a convergence of condition.

174
00:24:01,990 --> 00:24:03,170
That is guaranteed.

175
00:24:03,480 --> 00:24:15,760
And so if you have this if you know, the limit exists above the the the sum of all of these probabilities, you're going to be following it.

176
00:24:16,780 --> 00:24:24,550
One, the possibility is, you know, the population.

177
00:24:24,560 --> 00:24:27,910
So the corresponding scenario is one is not one.

178
00:24:28,120 --> 00:24:31,749
Right. So it's it could be. So it's bounded by one.

179
00:24:31,750 --> 00:24:36,010
It cannot be greater than one, but it can be strictly less than one.

180
00:24:36,340 --> 00:24:48,190
So those are the situations you have. A population size grows to infinity in finite time, much like always we said in the pure burst process.

181
00:24:48,370 --> 00:24:57,970
Right. So if you're first rate, it's very, very high. Then you have non-zero probability the population explodes in finite time.

182
00:25:03,400 --> 00:25:06,790
By the way, those are not exactly important to saying,

183
00:25:06,790 --> 00:25:18,880
but I think it's worth mentioning these mathematical properties you could easily derive by doing some simple analysis.

184
00:25:19,720 --> 00:25:36,040
So if you do have always taken one, then you can compute this just like what we did for the discrete Markov chains, because we know that P.J.,

185
00:25:37,210 --> 00:25:53,560
according to the so so the following equation is the same argument for if your Markov

186
00:25:53,560 --> 00:25:59,820
process entering into the stationary distribution will remain the stationary distribution.

187
00:25:59,830 --> 00:26:10,000
Right? So if you are putting you assume or this whole process starting in the stationary distribution will remain in the stationary distribution.

188
00:26:10,390 --> 00:26:24,940
So the point here is if you enumerate all the possible starting point, but they all start in the stationary distribution, I mean, then that should be.

189
00:26:26,730 --> 00:26:34,830
This equation should hold. So this is your initial distribution, which is stationary distribution by assumption.

190
00:26:35,490 --> 00:26:42,490
And then. And then this is a you can prove this.

191
00:26:45,500 --> 00:26:48,670
This is a conclusion. But this is the same thing.

192
00:26:48,680 --> 00:26:59,680
Like as we said in the awarding theory, if a markov process enter into the process, they will make possible.

193
00:27:00,340 --> 00:27:06,410
So the proof is also very similar to what we did before.

194
00:27:13,420 --> 00:27:31,420
Yeah. I cannot get the fox feet on so small.

195
00:27:31,420 --> 00:27:36,760
Small pieces. So.

196
00:27:43,720 --> 00:27:49,840
All right, so let's. Let's show the proof. The P of j according to the definition of this limit,

197
00:27:51,280 --> 00:28:06,930
let's say acyclic infinity p I say K.J. as plus this is all is true for for for any predefined t right?

198
00:28:06,940 --> 00:28:13,190
We just make the t so here this t is given by the problem.

199
00:28:13,360 --> 00:28:16,929
So it's a predefined parameter.

200
00:28:16,930 --> 00:28:20,160
So. So the p object must go to cage.

201
00:28:20,440 --> 00:28:23,649
So we augment another wait time.

202
00:28:23,650 --> 00:28:27,129
Just take that time as goes to infinity. Right. So this is one.

203
00:28:27,130 --> 00:28:31,840
The chain around long enough is according to the definition, the stationary distribution.

204
00:28:31,840 --> 00:28:45,330
These should be equal. And then we're going to apply the macro equation here of the cage as multiplied by P.

205
00:28:47,500 --> 00:29:01,680
So this becomes. But when we do that, we need to do summation all the I's, right?

206
00:29:01,780 --> 00:29:12,549
So this from here to here, we just do Chapman Kolmogorov equation arguments a starting point as a s right.

207
00:29:12,550 --> 00:29:18,910
So this is a enumerate all of the possible starting states at the time.

208
00:29:19,030 --> 00:29:32,330
S So that's I, I here we kind of do another swath and then this limit is essentially nine and then you invoke this limit condition again.

209
00:29:32,350 --> 00:29:40,790
So that's we apply. So that's it.

210
00:29:49,760 --> 00:29:53,450
Right. So so this equation will satisfy.

211
00:29:54,140 --> 00:30:01,340
And this is the implication is again, to say emphasize, just like in the discrete case,

212
00:30:01,670 --> 00:30:12,080
if you're you for your process, you very initial population size distribution, it's only a stationary distribution.

213
00:30:12,380 --> 00:30:14,270
And then the process, right.

214
00:30:14,270 --> 00:30:26,570
Is always in the stationary distribution or the other way to understand it is, is this process enter into the stationary distribution.

215
00:30:26,690 --> 00:30:31,110
Then there will never get out of.

216
00:30:36,580 --> 00:30:42,100
So so this is the way this is the property of this stationary distribution.

217
00:30:42,130 --> 00:30:46,240
This is not the way we calculate a stationary distribution.

218
00:30:46,870 --> 00:30:56,440
The reason is the IGT. We never talk about this transition probability, how to calculate this.

219
00:30:56,980 --> 00:31:09,220
Right. We we talk well, we we did talk about how to there is there some way to calculate this using the differential equation.

220
00:31:09,670 --> 00:31:12,670
But there is actually a simple way.

221
00:31:13,000 --> 00:31:16,380
You don't need to figure out the 82.

222
00:31:17,440 --> 00:31:26,230
Well, I should say this way. So you can either first figure out the other equation to solve for that stationary distribution.

223
00:31:26,740 --> 00:31:28,960
And in the lecture notes,

224
00:31:29,500 --> 00:31:39,400
there is alternative way to do it is actually direct application of the chairman for a member of the equation for the the forward equation.

225
00:31:43,170 --> 00:31:46,810
So last time we show the backwards the equation.

226
00:31:47,320 --> 00:31:58,440
All right, the backwards equation still works here. But just for the fun of it, this time, we use the we use the fourth option.

227
00:31:59,560 --> 00:32:10,750
So if you have the four equation, you're going to get this 1pi.

228
00:32:23,130 --> 00:32:27,400
Know. I think it's one zero.

229
00:32:29,750 --> 00:32:34,110
i0t. I won.

230
00:32:34,140 --> 00:32:40,770
Okay. Sorry. All right, so this is the first equation.

231
00:33:00,290 --> 00:33:02,990
Yes, I think so. Is I want.

232
00:33:08,900 --> 00:33:20,930
So if you look at this equation, so this is a general equation for any births and death process, and that is the stationary distribution that exists.

233
00:33:22,100 --> 00:33:32,930
What the implication to this equation? So we need to go to the fundamentals, the p g t.

234
00:33:37,820 --> 00:33:40,940
So if the limit one t goes to infinity.

235
00:33:41,330 --> 00:33:48,500
This the IGP, although it's written as a function of t, it's becomes actually a constant.

236
00:33:49,820 --> 00:33:54,650
Right. If you take a derivative to a constant.

237
00:33:55,710 --> 00:33:58,260
You always get zero or so the limit.

238
00:33:58,650 --> 00:34:08,260
Well, the implication for this is implies if you have a stationary distribution, the left hand side has to be zero.

239
00:34:08,910 --> 00:34:15,050
The right hand side, if t goes to infinity, then that's on the 0p0.

240
00:34:15,360 --> 00:34:22,050
Right. So that's a come from the stationary distribution plus meal one, p one.

241
00:34:23,880 --> 00:34:28,250
So that's the first equation you're going to solve. This gave you the p0p1.

242
00:34:28,260 --> 00:34:33,720
So you have two unknowns. One, and why are you creation?

243
00:34:33,730 --> 00:34:47,190
That's not enough. But you have a bunch of those. The forward equations, for example, the p i I'm trying to solve the j minus one.

244
00:34:48,460 --> 00:34:51,510
You don't need to write this out. I don't want to write this off.

245
00:34:51,780 --> 00:34:59,250
But you have a bunch of all these. Chatman For my group of equations for different j values.

246
00:34:59,520 --> 00:35:08,610
So here the eye is fixed. You have a different j values for actually for checking which one to all the way up to infinity.

247
00:35:09,480 --> 00:35:18,990
I know those gave you the similar. So using the similar thinking API JP prime in that case, also the left hand side will be zero.

248
00:35:19,440 --> 00:35:24,570
And on the right hand side it can be simplified according to this limit.

249
00:35:26,460 --> 00:35:32,160
And now you actually get a bunch of linear equations to solve that you can.

250
00:35:32,220 --> 00:35:36,330
That way it gives you a stationary distribution.

251
00:35:36,570 --> 00:35:41,760
So, so in a way you don't have to solve for PI.

252
00:35:41,760 --> 00:35:51,719
JP Right, because usually it's more difficult because that involves solving this type of the general differential equation,

253
00:35:51,720 --> 00:35:59,100
which is hard buss for stationary distribution, you just take the derivative becomes zero.

254
00:35:59,100 --> 00:36:05,940
So there is no differential equation involved anymore to solve for the linear system and then you can get it.

255
00:36:07,770 --> 00:36:19,550
So in the way the limiting distribution that's easier than solve than this, the general pitch, this transition occur,

256
00:36:19,660 --> 00:36:31,510
the transition probability over what we call this transition probability function for the pi that clear of.

257
00:36:31,590 --> 00:36:38,310
Yeah. So for that statement is j equal to zero here.

258
00:36:41,280 --> 00:36:44,370
Is this formula? Yes.

259
00:36:44,710 --> 00:36:51,180
Equal to zero. And then in the lecture notes, the last term is p i of t.

260
00:36:51,750 --> 00:36:56,250
Yeah, I should be i. One of the lecture on those is wrong.

261
00:36:56,400 --> 00:36:59,430
Okay. Perfect. Yeah.

262
00:36:59,430 --> 00:37:02,870
So. Well, I think you could start from scratch.

263
00:37:02,880 --> 00:37:07,450
I think it's just a a good way to think about t.

264
00:37:07,510 --> 00:37:11,100
Yes. So this is the bit window, small window.

265
00:37:11,430 --> 00:37:23,520
And then you have a transition from I to J j c plus h and the AI is arbitrary.

266
00:37:23,520 --> 00:37:28,020
So. So in this for this particular equation you're thinking about.

267
00:37:30,210 --> 00:37:34,850
Transition to zero. Right.

268
00:37:34,880 --> 00:37:40,310
So so here, it's pretty obvious that at the time you have a one population size one.

269
00:37:40,310 --> 00:37:41,340
I think because of that,

270
00:37:41,360 --> 00:37:52,220
the other part of this small window and then in this equation is that at the time t you have actually population size is zero.

271
00:37:56,780 --> 00:38:04,240
Yes, that's right.

272
00:38:04,250 --> 00:38:10,430
I think the most important point is connecting this statement to the directive statement.

273
00:38:19,520 --> 00:38:31,009
All right. So now the most important thing, the birth and death process tells us is, you know,

274
00:38:31,010 --> 00:38:40,520
not only of this specific set up is really is kind of a general view of continuous time Markov process.

275
00:38:40,670 --> 00:38:51,530
Like I said, there is exponential waiting and that imbalance and Markoff train, if you want to say so.

276
00:38:51,530 --> 00:38:58,490
So someone called this type of the process of jump process from state one jump to a different state.

277
00:39:01,100 --> 00:39:11,930
So these few without me now we could discuss here's some more general class of continuous time Markov process.

278
00:39:12,590 --> 00:39:15,830
This is called a finite state.

279
00:39:18,710 --> 00:39:29,960
Continuous time of ma called the process.

280
00:39:35,270 --> 00:39:47,300
All right. So well so there is some constraints of this type of the Markov process would only consider a finite state space.

281
00:39:47,330 --> 00:39:52,370
Now, I spoke there of what we cannot discuss.

282
00:39:54,720 --> 00:40:06,629
Today. It's rather general. There is only constraint is going to be either the finance states face so the finite

283
00:40:06,630 --> 00:40:12,390
state space continuous high mark off process is defined by the following properties.

284
00:40:13,290 --> 00:40:26,640
So we have a stochastic process that you find on finite space space.

285
00:40:32,330 --> 00:40:38,550
You can call it as zero one.

286
00:40:39,400 --> 00:40:42,780
It's just some labeling. All right.

287
00:40:43,050 --> 00:40:52,590
So we're going to specify the Markov property for this for this process in the sense.

288
00:40:56,590 --> 00:41:07,270
Is a mark. It's a continuous mark of process.

289
00:41:14,590 --> 00:41:33,100
If the following satisfied and a key IGT rate are equal to zero, so you're going to have a transition probability from state to state.

290
00:41:33,130 --> 00:41:36,250
JAY And then pi. JP Greater or equal to zero.

291
00:41:36,250 --> 00:41:43,690
So those are required. There's nothing out of ordinary, but this is a required for a valid transition kernel.

292
00:41:45,650 --> 00:42:00,400
B We're going to have the same thing for zero two and PJ T equals one for arbitrary identity.

293
00:42:02,290 --> 00:42:12,760
So this is also not. So this is a like sum over the role some in the transition matrix for finite state Markov chain.

294
00:42:12,840 --> 00:42:18,700
Right. So starting from state I have to end up in any of the states,

295
00:42:19,270 --> 00:42:29,350
the valid states in the in the in the in the pre defining the state space, therefore the constraint.

296
00:42:29,620 --> 00:42:32,830
So the third one is actually defining the market problem.

297
00:42:34,310 --> 00:42:38,320
This time we're going to find the Markov property through the Cabinet.

298
00:42:38,330 --> 00:42:41,710
For my whole equation, it's.

299
00:42:44,490 --> 00:43:00,180
All right. So. And.

300
00:43:02,360 --> 00:43:07,520
And. So see your p.

301
00:43:07,860 --> 00:43:11,360
I. K. He says so.

302
00:43:11,660 --> 00:43:15,620
Well, I think this is, you know, this is the first time you see this.

303
00:43:15,620 --> 00:43:25,700
But I mean, I get the sense that the the chairman from abroad, the equation is just channelized the Markov property, right?

304
00:43:26,030 --> 00:43:33,800
So you have the Markov property, but you can always derive the Chapman from across the equation.

305
00:43:34,580 --> 00:43:40,670
Conversely, if you have chairman from a group equation, you can always get to, you know,

306
00:43:41,120 --> 00:43:48,560
if you have unstuck in the Markov chain case, you find steps you can always get back to the ones that transition of use.

307
00:43:49,820 --> 00:43:55,760
So, so this is a kind of a valid way to define the, the mark of property.

308
00:43:55,940 --> 00:44:14,470
And then in this case, again, we get the. Separate the the the t plus size into two consecutive time segment one this t one is s and savings j02

309
00:44:14,560 --> 00:44:33,630
and argument i intermediate stop j i j k j k s so is ten month for my growth equation set is five.

310
00:44:34,260 --> 00:44:40,080
That's the same thing to the to say the underlying process is a markov process.

311
00:44:42,750 --> 00:44:55,950
So with as which sterile for the last one is a bit strange, but it's necessary.

312
00:44:56,800 --> 00:45:22,770
C p i j limit t goes to 0tl So, so this is a pure mathematical definition on the transition kernel that they all the function again.

313
00:45:22,770 --> 00:45:29,130
So if I go through J that's one of if I not equals J has zero.

314
00:45:29,160 --> 00:45:40,370
And so this is just trying to ensure the, the smoothness of pi j right.

315
00:45:40,620 --> 00:45:50,669
You kind of think about. So it's not so much why this has to be there, but if it's not this then you don't have a smooth function.

316
00:45:50,670 --> 00:45:58,930
Pi. JP Okay, so that means literally if you're trying to interpret this in the context,

317
00:45:58,970 --> 00:46:04,830
so that means there is no transition one, but the time interval is very, very small.

318
00:46:04,830 --> 00:46:12,600
There is no sudden jump. Okay. Every of transition, I suppose, not occurred.

319
00:46:14,560 --> 00:46:20,670
So the smoothness of this pie that you have to think of all the pie as a function of t.

320
00:46:20,880 --> 00:46:31,300
All right. So there's a small function to ensure that's a smooth function that you need this kind of strength, which is an important one.

321
00:46:33,220 --> 00:46:50,500
Okay. So with this for conditions you define, we define a general finite state, continuous time Markov process, just like the.

322
00:46:54,130 --> 00:47:09,280
Markov chain case. We have a general representation of Chapman core my growth equation that's the C and then in the in the Markov chain case,

323
00:47:09,430 --> 00:47:13,710
we kind of make a matrix formulation for that.

324
00:47:13,720 --> 00:47:28,730
So in this case, we still have this we could we could again correspond that Chapman Kolmogorov equation to matrix multiplication, right?

325
00:47:28,750 --> 00:47:45,790
So everything follows. We have this P so you can actually represent the transition kernel in matrix 459 status space Markov process just t times.

326
00:47:45,940 --> 00:47:55,149
E Now we are talking about matrix function, nothing fancy.

327
00:47:55,150 --> 00:48:04,480
It's just this matrix. Every entry is every entry is actually a function of 50 plus s.

328
00:48:05,140 --> 00:48:08,230
So in this case every entry is a function of three.

329
00:48:08,590 --> 00:48:15,549
Every entry in that matrix is kind of organizing all these functions together in the matrix form.

330
00:48:15,550 --> 00:48:22,540
So you got the matrix functions with these.

331
00:48:22,540 --> 00:48:28,930
We can also prove this proposition by fields.

332
00:48:28,960 --> 00:48:33,310
T now is a matrix function.

333
00:48:33,490 --> 00:48:37,840
And not only that, some matrix function is also a continuous matrix function.

334
00:48:45,070 --> 00:48:49,600
There is a bit gap what we mean by a matrix function this continuous.

335
00:48:49,660 --> 00:48:50,860
But what we got there,

336
00:48:50,860 --> 00:49:06,610
you see there's actually pretty straightforward a concept so so what we can do here is trying to show is continuous from above or below.

337
00:49:07,150 --> 00:49:08,530
So I found the above.

338
00:49:09,670 --> 00:49:25,150
The way to show is you can show you evaluated the matrix function inside of t evaluate the team plus h and maybe h smaller and smaller.

339
00:49:25,520 --> 00:49:30,190
If you take the limit, h goes to zero than this one.

340
00:49:30,340 --> 00:49:40,390
So possible since P obviously. So in terms of matrix, so this is actually entry wise.

341
00:49:40,410 --> 00:49:49,260
So thinking about what I said is the four is the matrix function is just functions arranged in the matrix.

342
00:49:49,270 --> 00:49:57,680
The multiple functions are arranged in the matrix. So that actually required all the functions have this problem, right?

343
00:49:57,700 --> 00:50:01,120
All the entries have this limiting behavior.

344
00:50:01,810 --> 00:50:05,200
And then this is actually not hard to prove at all.

345
00:50:08,170 --> 00:50:22,329
I imagine we can calculate the left hand side using the so using the macro of the equation,

346
00:50:22,330 --> 00:50:28,570
but taking the the limit form sorry, taking the matrix form.

347
00:50:29,140 --> 00:50:33,190
So this is a p of t times p of h.

348
00:50:35,440 --> 00:50:43,180
All right. So P of key is nothing to do with h, so the limit is only what's on the edge.

349
00:50:45,190 --> 00:50:55,239
P t All right.

350
00:50:55,240 --> 00:51:03,400
So this is where that continuous continuity of property, the property d comes in.

351
00:51:04,030 --> 00:51:08,890
So if you have all of these functions, PJ equals Delta J,

352
00:51:09,370 --> 00:51:15,670
now you can imagine what is function the limit of this function look like the matrix function look like right?

353
00:51:16,180 --> 00:51:27,280
So in the off diagonal I is not j rule number is not the column number earlier o zero at a diagonal is a j here o one.

354
00:51:27,760 --> 00:51:35,370
So this becomes identity matrix. So this one is just ids identity.

355
00:51:35,470 --> 00:51:40,540
Arbitrary function multiplied by identity matrix equals itself.

356
00:51:40,540 --> 00:51:44,430
So just which I suppose is equal to the you.

357
00:51:46,660 --> 00:51:51,390
You could do the same for continuous from the old.

358
00:51:54,200 --> 00:52:14,270
Essentially the same argument, and I'll skip that part.

359
00:52:14,600 --> 00:52:21,110
It's all on the lecture notes. So this is a continuous from above continues from below.

360
00:52:21,470 --> 00:52:30,470
When you show we show that the key is actually with this type of the destination is going to be a continuous function.

361
00:52:33,560 --> 00:53:05,130
So the Proposition six. So again, this is usually there is a little bit of the the notation.

362
00:53:11,580 --> 00:53:22,400
We need to assume each goes zero one minus the ice age oh.

363
00:53:22,410 --> 00:53:40,350
Work H equals Q of h goes to be 0phhhhsq right.

364
00:53:42,990 --> 00:53:46,170
All right. So the reason to define this limit.

365
00:53:50,170 --> 00:53:57,070
It's very similar on how we actually define course on all sides.

366
00:53:57,520 --> 00:54:03,430
Always define the pure basketball sides person that process, right.

367
00:54:03,880 --> 00:54:11,170
So we need to define this transition kernel at least in a limiting sense.

368
00:54:11,410 --> 00:54:17,380
One anxious small. And in this case, you can see the pitch.

369
00:54:18,100 --> 00:54:24,040
So there is no transition to itself. It's going to be a linear function with respect to Iron Age.

370
00:54:24,520 --> 00:54:31,330
And then the pie change is also a linear function of age.

371
00:54:31,600 --> 00:54:37,810
So those things are completely in line with what we have before.

372
00:54:38,200 --> 00:54:48,480
And it's kind of necessary to define the of the behavior, to understand the behavior of continuous time, finite state Markov chains.

373
00:54:49,000 --> 00:54:59,050
So, again, this is not all of ordinary. The point of defining this is you can think of all the pie that the common heights of a pie.

374
00:55:01,530 --> 00:55:11,040
I should say the analogous scenario in the person death process, although that process is not a finite state process,

375
00:55:11,430 --> 00:55:16,410
but they all have the same failure, this type of the limit exists.

376
00:55:16,980 --> 00:55:24,000
Therefore you can write the pi h as a linear function, one minus function plus oh h.

377
00:55:24,060 --> 00:55:27,110
So those things are need to be there.

378
00:55:27,630 --> 00:55:35,850
AQI and the line H And then we get a matrix this case by just a range, these things.

379
00:55:37,110 --> 00:55:42,450
MAXINE McKEW. Zero all the way up to negative one.

380
00:55:43,710 --> 00:55:51,990
All right. On the diagonal, it's coming from this element. The state spaces from 0 to 1 and now the all star.

381
00:55:52,680 --> 00:55:57,030
Q zero. One way to zero.

382
00:56:00,120 --> 00:56:08,620
This one that's. And so this is called a infinitesimal generator.

383
00:56:19,240 --> 00:56:28,180
That's. We'll talk in a minute.

384
00:56:28,180 --> 00:56:37,240
Why this is a comedy and what where this comes in. But this is a necessary notation on beforehand the behavior.

385
00:56:37,420 --> 00:56:46,660
If you're trying to capture the behavior of a finite space, space, Markov process, they would require this two elements exist.

386
00:56:47,110 --> 00:56:51,580
And then this one is just an occasional one. They've got the jargon attached to it.

387
00:56:51,700 --> 00:56:56,210
It's called a infinitesimal. We'll see how this comes up.

388
00:56:59,080 --> 00:57:27,880
So this comes off in the sense of if you want to think the behavior of your transition colonel in a very short time, then this is true.

389
00:57:28,060 --> 00:57:45,450
The speed limit, which goes to the 0ph minus divided by H equals your right.

390
00:57:46,450 --> 00:57:56,109
Again, this statement is just a matrix statement of what I have written in the above lines.

391
00:57:56,110 --> 00:58:02,020
The two lines, right? It's just a way to summarize all the things into a simple.

392
00:58:05,440 --> 00:58:12,720
Into a simple matrix representation. So this a page minus I think P of h you know,

393
00:58:12,730 --> 00:58:22,330
if you have a H and then you subtract the identity matrix and then divided by H and take the limit that you get matrix a there.

394
00:58:22,780 --> 00:58:34,510
Okay. So A is actually represented the infinite as the whole generator is actually this matrix is the limit of this matrix function.

395
00:58:36,970 --> 00:58:37,540
Okay.

396
00:58:40,210 --> 00:58:54,280
So the key point of introducing all of these is trying to establish the differential equation so we can figure out in general what is the P of T,

397
00:58:54,490 --> 00:59:03,130
right? So how do we calculate PLB in general, given it's in the estimate generator?

398
00:59:04,210 --> 00:59:12,940
Right. So this has been the common practice or it's been the consistent practice in a continuous time, stochastic process.

399
00:59:12,940 --> 00:59:16,720
We defined the behavior in small windows.

400
00:59:17,380 --> 00:59:23,530
You find that behavior in short term intervals, that interval, and then that probability,

401
00:59:23,530 --> 00:59:28,060
the transition probability is usually time your and that's those type of the linear function.

402
00:59:28,450 --> 00:59:35,319
But we should have a way to deduce all of these properties.

403
00:59:35,320 --> 00:59:39,340
One, the window size becomes large, arbitrary.

404
00:59:39,910 --> 00:59:44,889
So the way to do it is set up some sort of differential equation.

405
00:59:44,890 --> 00:59:49,120
And in this case we could easily do the following.

406
00:59:49,660 --> 01:00:00,819
We can take an H goes to zero P, P, plus H minus p, t divided by H.

407
01:00:00,820 --> 01:00:09,399
So this is a very straightforward or direct attempt trying to calculate the derivative of a matrix, right?

408
01:00:09,400 --> 01:00:16,960
So the derivative of the matrix function is defined as you take derivative for each element function.

409
01:00:17,440 --> 01:00:22,980
Right. So this is what happens here and.

410
01:00:40,360 --> 01:00:54,770
We've. So this one.

411
01:00:55,280 --> 01:01:08,870
So to see the connections we write T plus h because it's continuous as PTT Times page.

412
01:01:09,050 --> 01:01:13,010
So this is one is based on the chipmunk from across the equation, right.

413
01:01:13,160 --> 01:01:23,390
So the t plus h can be factor into the p t times p h minus p divided by age.

414
01:01:23,900 --> 01:01:29,750
And that would do another factorization each closer to zero.

415
01:01:31,670 --> 01:01:44,240
We should do take that t out and you get this h minus I dividing by H, which is the infinite estimate.

416
01:01:44,430 --> 01:01:48,830
January 3rd, 28.

417
01:01:49,910 --> 01:01:56,239
All right. So what we got here is actually this is what we set up ourselves to get is this

418
01:01:56,240 --> 01:02:07,400
general differential equation in the matrix for prime t equals p t times a,

419
01:02:08,330 --> 01:02:16,730
and you can also prove this one equals eight times p, right?

420
01:02:16,850 --> 01:02:22,370
Instead of calculating this way, you need to operate on p, t minus h.

421
01:02:22,850 --> 01:02:27,290
But these are the equations.

422
01:02:27,290 --> 01:02:30,350
So differential equations you're going to get. So.

423
01:02:31,470 --> 01:02:41,310
If you defined this strong probability in a small window with a linear function or approximate the strong probability of in your functions,

424
01:02:41,730 --> 01:02:56,640
then you could look at this analytically, at least in theory, to calculate for arbitrary transition matrix of T using this differential equation.

425
01:02:57,300 --> 01:03:03,380
Okay. So p prime t equals two times a equals eight times of t.

426
01:03:05,310 --> 01:03:13,280
Another note is this generalization so that basically solve a wish.

427
01:03:13,830 --> 01:03:17,110
How do you define the translation? Right.

428
01:03:17,130 --> 01:03:21,510
So you need that this type of the limit to define the translation.

429
01:03:21,940 --> 01:03:22,250
Right.

430
01:03:22,560 --> 01:03:33,030
So those transitions that you can think of in the context, while in the analogy of the poor song, in that analogy off the obverse of that process,

431
01:03:33,330 --> 01:03:41,360
it's really that in the small window translation to each of the probability and then combined with the problem,

432
01:03:41,940 --> 01:03:45,510
the generalization of continuous time Markov process.

433
01:03:46,110 --> 01:03:54,300
This is basically describe how you can simulate a continuous time, finite state distribution.

434
01:03:54,600 --> 01:04:01,580
So you could have that prime people solve. In this case independent of anything, you know.

435
01:04:01,590 --> 01:04:11,460
Right. Just mathematically, all you need is that definition of these four definitions for the finite space mark process.

436
01:04:11,940 --> 01:04:21,150
Or if you're thinking about how do you simulate those things, again, you're going to simulate according to exponential distribution.

437
01:04:21,360 --> 01:04:26,700
That's the wait time distribution. One, the wait time is trigger.

438
01:04:26,880 --> 01:04:32,430
So if a rival is triggered, then you need to reference this.

439
01:04:34,440 --> 01:04:42,389
There's a matrix, so that tells you how to do the transition and then you can actually update your

440
01:04:42,390 --> 01:04:47,700
Markov chain accordingly and then you simulate next exponential wait time.

441
01:04:48,300 --> 01:04:58,670
So the spun out. So we did talk about the details of these exponential distributions.

442
01:04:58,690 --> 01:05:10,450
You can't figure that out yourself. But but that usually, you know, I think for the purpose of this course is understand, you know what?

443
01:05:10,450 --> 01:05:16,240
We have this mathematical definition about a finite state space, Markov process.

444
01:05:16,600 --> 01:05:22,610
Then we can generally using that equation to calculate the transition kernel for arbitrary.

445
01:05:22,630 --> 01:05:31,690
So transition function in this case poverty for arbitrary value of T by solving that differential equation.

446
01:05:32,410 --> 01:05:35,140
In reality, nobody asks you to solve that equation.

447
01:05:36,370 --> 01:05:47,300
And furthermore, if you try to think about stationary distribution again, so that means the P of t can converge to a constant right.

448
01:05:47,620 --> 01:05:55,690
A constant matrix, just like in the finite state, is just like the Markov chain case for transit times.

449
01:05:56,050 --> 01:06:04,170
If Gornick in the reporting theory that basically the transition P so in there we call this type of

450
01:06:04,180 --> 01:06:13,120
a situation compared to the end right until stream then it means that converge to a single matrix.

451
01:06:13,450 --> 01:06:21,040
So in continuous time you can set up that P prime T equal to zero and then actually solve for.

452
01:06:24,910 --> 01:06:35,650
Yeah. The poverty. So so actually the air here has a very specific meaning that tells you about stationary distribution if it does exist.

453
01:06:36,010 --> 01:06:41,730
So existence of the stationary distribution is connected to the Gornick theorem.

454
01:06:41,740 --> 01:06:51,630
So now you only need to study the connectivity between the states and understand the connectivity diagram.

455
01:06:51,710 --> 01:06:55,030
Understand the pure, pure city.

456
01:06:55,030 --> 01:06:58,840
And understand that the theory current states.

457
01:06:59,380 --> 01:07:02,860
Positive, recurrent, recurrent. Okay.

458
01:07:03,730 --> 01:07:12,760
So this is the last bit of the continuous time state space, a continuous time Markov process.

459
01:07:13,150 --> 01:07:22,420
Next time I think we have one lecture, a little bit of we'll talk about the Brownian motion, but I'm going to skip the irrigation part.

460
01:07:22,690 --> 01:07:30,850
Focus on the definition part. All this derivation is actually on the lecture notes.

461
01:07:31,300 --> 01:07:39,610
So the point of this derivation is over this continuous time, continuous space, space.

462
01:07:41,050 --> 01:07:49,210
All right. I should say that we're going to move on to discuss the last the category of the stochastic process,

463
01:07:49,330 --> 01:07:54,850
which is continuous time, continuous state space, stochastic process.

464
01:07:55,450 --> 01:08:04,840
And we just need to discuss a one off. It's such a process known as Brownian motion, also a special case of Gaussian process.

465
01:08:06,250 --> 01:08:13,540
So in the case of the lecture, those actually spend a lot of time to motivate.

466
01:08:13,780 --> 01:08:21,700
What we have already learned can lead to the derivation or to understand the property.

467
01:08:22,120 --> 01:08:27,100
So this is actually where the the calculus comes in, right?

468
01:08:27,120 --> 01:08:36,430
So all these algebra we learned and you know, I think the coupled with the idea of limiting.

469
01:08:36,820 --> 01:08:42,670
Right. So you're just if you think about continuous states, continuous time,

470
01:08:42,670 --> 01:08:48,760
continuous state space, you can always start with a discrete time as discrete space space.

471
01:08:49,210 --> 01:08:57,010
But in the process of studying those discrete time, discrete states space process, you just make.

472
01:09:03,610 --> 01:09:09,070
And the grade, so to speak.

473
01:09:10,810 --> 01:09:14,830
As fine as possible. So you see, you have.

474
01:09:18,490 --> 01:09:25,840
So the motivations usually for goals in process or the continuous time.

475
01:09:28,300 --> 01:09:35,800
So for Brownian motion, it's actually the the simple run the walk.

476
01:09:38,320 --> 01:09:45,940
So it's a one dimensional, simple, random walk with the interval being the delta T.

477
01:09:46,420 --> 01:09:56,140
So this is actually we don't really care about the what the length of the duration of Delta T is just one second move third.

478
01:09:57,070 --> 01:10:02,580
So this is just the index, the index set and the one dimension.

479
01:10:02,590 --> 01:10:15,130
So this is the way the mechanism, the movement of that particle, you want to think it's on the Y axis every time you move one unit to unit ten three.

480
01:10:15,520 --> 01:10:21,400
So this is what we have been studying for, one dimensional random walk, right?

481
01:10:21,730 --> 01:10:31,120
So you do have a interval for the time that is Delta T and then you do have a interval for this movement or that displacement.

482
01:10:31,420 --> 01:10:44,740
It's called a Delta U. So the idea is, if you you could well, I mean, it's not too hard to connect using the Markov property to connect the plane.

483
01:10:44,800 --> 01:10:52,480
The general placement of you as a function of the Delta U and then Delta, right.

484
01:10:52,490 --> 01:10:56,960
So this is the movement of that particle. So we can simulate this is a rather easily.

485
01:10:57,070 --> 01:11:07,780
So every single for every single time unit, you either move up or down by unit up or in what unit.

486
01:11:07,870 --> 01:11:11,320
So you cannot stay on the origin of that.

487
01:11:11,680 --> 01:11:16,540
But this gives you the general representation of a you know, that you and you have t.

488
01:11:16,870 --> 01:11:25,300
So the idea it's really simple just take the that you on a delta T to zero and then calculate that limiting behavior.

489
01:11:25,450 --> 01:11:33,339
So those results in in this case because you taking limit to two different parameters instead

490
01:11:33,340 --> 01:11:39,280
of ordinary differential equation you're going to get a partial differential equation,

491
01:11:39,640 --> 01:11:43,420
right? One is you one and the other is the T.

492
01:11:44,950 --> 01:11:49,989
So that's the most the part of the the discussion on the first half of the lecture.

493
01:11:49,990 --> 01:11:53,260
No, I'll quickly summarize that next time.

494
01:11:53,260 --> 01:11:59,090
But I'm not going through the the details of the derivation because it's a really simple.

495
01:12:00,000 --> 01:12:03,309
I mean, that's maybe. All right.

496
01:12:03,310 --> 01:12:06,340
Help the differential equation. Just skip the.

497
01:12:06,490 --> 01:12:14,380
Yes, I was just contrast. So with Brownian motion, everything is the focus of the exam, more of the derivations or more?

498
01:12:14,410 --> 01:12:20,469
No, it's not that for the applications of how you use the method, how you calculate different things.

499
01:12:20,470 --> 01:12:25,120
Yes. The property of. So the emotion is not defined through this.

500
01:12:25,150 --> 01:12:32,139
So the derivation will tell you they can give you a description of the Brownian motion,

501
01:12:32,140 --> 01:12:38,630
but that's not how n defined, because usually you come from this world, a discrete world,

502
01:12:40,500 --> 01:12:43,870
they give you an intuition how these two different things connected,

503
01:12:43,870 --> 01:12:51,540
or how some of the terms in that differential equation becomes a natural comes is unexpected expected,

504
01:12:51,550 --> 01:12:56,140
but it's usually not rigorous enough to define a process that way.

505
01:12:56,530 --> 01:13:06,370
So we're going to focus more on the destination of understanding the Brownian motion, which is defined completely independent of this derivation.

506
01:13:07,180 --> 01:13:17,170
It's a bit strange. It's probably just connected to all the things we have studied, but with that definition,

507
01:13:17,530 --> 01:13:23,680
you can already study a lot of different, interesting properties for the price.

508
01:13:23,890 --> 01:13:29,310
Let's talk about that next Monday. Right.

509
01:13:29,320 --> 01:13:33,220
I think you could put more, so.

510
01:13:33,910 --> 01:13:41,680
Oh, so I think I don't need the whole so the remaining lecture doesn't need as a whole class time.

511
01:13:41,800 --> 01:13:48,160
So probably a review towards the end of next Monday.

512
01:13:48,820 --> 01:13:53,629
So we will have. Okay.

513
01:13:53,630 --> 01:13:57,860
So we will have the office hour this Friday.

514
01:13:59,690 --> 01:14:02,750
Next Monday, possible next Tuesday for sure.

515
01:14:03,110 --> 01:14:08,570
Opposed to the time on it. But focus on the you know, just the the lecture notes.

516
01:14:08,580 --> 01:14:14,720
You don't need to rereview anything before class on process.

517
01:14:15,200 --> 01:14:26,060
But if you feel fuzzy about like total expectation law, why that's justified, I still recommend you at least take a look of the material.

518
01:14:26,080 --> 01:14:30,770
What amount of problem particular you designed before that sort of thing.

519
01:14:30,980 --> 01:14:39,140
Um, problems are going to be focused and it's also kind of the exam is also kind of a learning process.

520
01:14:40,760 --> 01:14:46,490
I would maybe some of the things you have never seen, but you know, it's not all the lecture and those,

521
01:14:46,520 --> 01:14:53,900
but I expect that you could as a learning process through the questions, you could learn something new.

522
01:14:54,590 --> 01:15:01,870
So that type of thing you don't need to prepare, but. You have to have a thinking, have a thought process.

523
01:15:03,100 --> 01:15:07,600
That's hard to for me to say, but, you know, just don't need to prepare.

524
01:15:07,810 --> 01:15:13,420
I think it's more trying to mimic what you're going to encounter in research.

525
01:15:13,430 --> 01:15:18,070
You've probably never seen those things, but you're given the task to solve the problem.

526
01:15:19,000 --> 01:15:21,870
Okay. All right. So.

