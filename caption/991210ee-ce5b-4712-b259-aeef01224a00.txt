1
00:00:02,380 --> 00:00:12,890
I think we are only going to see.

2
00:00:26,280 --> 00:00:31,830
It's a weird smile and it know.

3
00:00:37,260 --> 00:00:49,050
I thought that I had my driver's license.

4
00:00:50,570 --> 00:00:59,310
I didn't know that I had spoken so easily.

5
00:01:22,270 --> 00:01:43,040
In Northern California, the state has practically one.

6
00:01:52,610 --> 00:02:25,510
So I think it's.

7
00:02:32,510 --> 00:02:47,920
Yeah. Them.

8
00:03:21,060 --> 00:03:49,690
She's. Two different.

9
00:03:52,410 --> 00:03:57,340
Different. Yeah.

10
00:03:58,890 --> 00:04:32,840
Okay. March 18, 2017.

11
00:04:38,520 --> 00:04:59,940
I'm drinking today. The Constitution.

12
00:05:08,760 --> 00:05:15,030
All right. All right.

13
00:05:15,030 --> 00:05:28,650
A couple of minutes later, today is your time talking about this argument dilution effect and how it shows up in a variety of contexts.

14
00:05:29,610 --> 00:05:34,170
We spend most of the time talking about the drug facts, box table.

15
00:05:34,950 --> 00:05:40,290
And the reason we're going to talk about it is it's probably one of the best templates out there

16
00:05:40,290 --> 00:05:46,830
for communicating medical risk in a decision context where somebody is choosing between A and B,

17
00:05:47,220 --> 00:05:57,720
and I want to take you through it line by line and every little piece of it and show you why it's designed the way it is, what its features are, etc.

18
00:05:58,410 --> 00:06:01,980
This class is most directly related to your next assignment.

19
00:06:02,580 --> 00:06:09,750
So what I am asking you to do for your next assignment is essentially to make a summary table that looks.

20
00:06:11,150 --> 00:06:18,470
As reasonably similar as it makes sense to do for whatever your decision is for another topic.

21
00:06:19,100 --> 00:06:26,870
So let's talk a little bit. I've said some of this before and reinforces now I want you to thinking about as you start working on your next life,

22
00:06:28,070 --> 00:06:31,820
pick a choice that is two options.

23
00:06:32,690 --> 00:06:38,540
Do not go to the oh well, there are three things or four think like no, this gets way more complicated.

24
00:06:38,540 --> 00:06:44,390
It is a nightmare to try and manage designing side by side comparison tables for four or five options.

25
00:06:44,720 --> 00:06:50,060
Don't do it. Pick. Something to have A versus B.

26
00:06:50,270 --> 00:06:58,849
Now, if you are in a situation in which there are often many options in the real world, what usually happens is that the scope gets narrowed,

27
00:06:58,850 --> 00:07:05,180
like you're only a candidate for certain types of things, or you've already decided to do certain types of things.

28
00:07:05,660 --> 00:07:11,510
So, for example, if you're in the birth control context, you can say, so you've decided to get an IUD.

29
00:07:13,040 --> 00:07:17,689
Well, that now narrows the scope dramatically, but you still have choices and tradeoffs,

30
00:07:17,690 --> 00:07:24,170
etc. Within that space or so you've decided to get a knee replacement.

31
00:07:24,800 --> 00:07:33,410
There's this type of that type, etc. Like I give you a complete license to narrow the scope so that you're looking at two choices,

32
00:07:36,110 --> 00:07:40,070
which is the second major requirement.

33
00:07:41,140 --> 00:07:49,030
It must have quantitative risk and benefit information in both of those pieces.

34
00:07:49,030 --> 00:07:53,680
It was important. Like, it just this can't just be this risk exists.

35
00:07:55,140 --> 00:08:02,170
You need to have quantitative information like numbers. And there needs to be.

36
00:08:03,750 --> 00:08:07,680
A trade off. So this is not a situation I don't want you doing this for.

37
00:08:07,680 --> 00:08:13,480
In a situation in which one option dominates the other one, it's better, less risky, everything.

38
00:08:13,680 --> 00:08:19,430
There's got to be some degree of tradeoff. Now, most real world decisions are trade offs.

39
00:08:19,440 --> 00:08:24,090
Like we rarely get such a completely obvious this is way better than that period.

40
00:08:25,440 --> 00:08:34,960
So that still gives you a lot of license. But what I don't want and doesn't mean there are not risk dimensions that are going to be relevant.

41
00:08:35,410 --> 00:08:43,030
So for example, if you're doing a decision related to some kind of treatment and one of them is a pill and one of them is a rejection,

42
00:08:43,900 --> 00:08:47,590
of course that's going to be relevant and that's going to be something you're very cooperative to your table.

43
00:08:48,490 --> 00:08:57,310
If one of the surgeries you're talking about has a six week recovery time and the other one has a six month recovery time, of course that's relevant.

44
00:08:57,880 --> 00:09:05,200
Like if you're doing radiation therapy rather than surgery and you're going to have to come back to the hospital every week for three months.

45
00:09:05,620 --> 00:09:09,250
Of course that's wrong. But all of that stuff also goes in the table.

46
00:09:12,400 --> 00:09:16,180
As well as. Estimates of risk involved.

47
00:09:17,920 --> 00:09:24,000
Related to this. I want you to make the table you wish would exist.

48
00:09:24,900 --> 00:09:36,130
I do not necessarily expect that you will be able to find real world studies that have quantified every last thing that you would want for this.

49
00:09:36,210 --> 00:09:40,860
Clearly, I give you license to make up the numbers if you have to.

50
00:09:43,510 --> 00:09:47,980
If you can't find if you're doing a quick search and you can't find something,

51
00:09:49,390 --> 00:09:53,200
but you have a plausible understanding of what the numbers look like, that's fine.

52
00:09:53,200 --> 00:10:01,600
But please don't tell me that just your headache pill has a 30% chance of killing you like no real world.

53
00:10:01,750 --> 00:10:06,100
Plausible numbers make sense, but I don't want you going.

54
00:10:06,100 --> 00:10:11,230
Well, I can't do this because I can't get the data. But this class is about communication.

55
00:10:11,500 --> 00:10:17,920
You are not experts in all of the underlying medical details or risk details of a particular context.

56
00:10:19,430 --> 00:10:24,829
What I hope is going to happen is that you're going to do a little bit of searching and you're going to get many of the sources of

57
00:10:24,830 --> 00:10:31,970
information very plausibly from existing studies or from other kinds of health and education materials on whatever your topic is.

58
00:10:32,750 --> 00:10:39,229
But you're also going to say, I really wish I had information about X, and my answer to you is,

59
00:10:39,230 --> 00:10:44,030
if you really wish you had more information about that, you have license to make it up.

60
00:10:46,800 --> 00:10:54,840
That's a key piece here. I don't I'm not going to accept the excuse of well, I couldn't find this number, so that's why I didn't include it.

61
00:10:56,160 --> 00:11:00,480
But I want you to be saying, is this is this is the best way we could present the truth.

62
00:11:02,070 --> 00:11:07,440
This is the best way somebody could understand the choice you have.

63
00:11:08,720 --> 00:11:12,770
Pretty broad license in terms of what the choice is in terms of health.

64
00:11:13,220 --> 00:11:16,940
Like almost any medical decision would fall cleanly into this.

65
00:11:17,600 --> 00:11:20,780
There are I have had students in the past do, for example,

66
00:11:21,530 --> 00:11:26,359
different types of personal protective equipment for environmental protection and workplaces.

67
00:11:26,360 --> 00:11:36,720
That five. In theory, you could do this at the community level rather than at the individual level.

68
00:11:36,740 --> 00:11:41,570
I would want to probably talk to you about how to navigate that and how you would provide it.

69
00:11:41,870 --> 00:11:49,940
It's a little bit more complicated when we're talking about risks to, say, community, if we're talking about two different waste treatment plants.

70
00:11:51,200 --> 00:11:54,110
But I could imagine doing that if that's what your passion is.

71
00:11:55,070 --> 00:12:05,160
But for the most part, certainly if you stick in any kind of medical choice, you're going to be likely dealing with some sort of benefit.

72
00:12:05,180 --> 00:12:09,730
Why are you doing this? By the way, all all is fair in this.

73
00:12:09,980 --> 00:12:14,660
I'm not saying this has to be about mortality. Like, if you want to talk about mental health, that's great.

74
00:12:14,660 --> 00:12:20,480
If you want to talk about other kinds of benefits.

75
00:12:20,960 --> 00:12:24,350
Even to the extent of, you know, peace of mind.

76
00:12:24,740 --> 00:12:32,240
Sure. Anything that would fall into the domain of a benefit or a risk could be part of your conversation.

77
00:12:32,960 --> 00:12:41,760
I am more than happy that I encourage you if you're saying, hey, would this be a good decision to work with me now?

78
00:12:41,810 --> 00:12:44,930
It will take me 30 seconds to tell you whether that's a good idea or not.

79
00:12:46,160 --> 00:12:50,969
I'm more than happy to have that conversation and help you sort of navigate what we're doing.

80
00:12:50,970 --> 00:12:54,770
I notice if you look forward in the syllabus.

81
00:12:55,970 --> 00:12:59,180
We believe this assignment is due.

82
00:12:59,330 --> 00:13:07,610
I want to say November seven, whatever that Monday is, of the class before that is a workshop class.

83
00:13:08,420 --> 00:13:11,750
Bring your drafts to that class.

84
00:13:12,020 --> 00:13:21,800
So and I'm expecting maybe not a final like every last piece is there, but a well fleshed out example by the time of that class.

85
00:13:23,080 --> 00:13:26,620
We'll share them across the table.

86
00:13:26,620 --> 00:13:35,080
We'll give each other feedback. I will circulate, I will be available for questions, etc. This is not a so last major piece of advice.

87
00:13:36,130 --> 00:13:41,200
This is not a creativity assignment. The next assignment after this one has some creativity.

88
00:13:41,290 --> 00:13:44,920
This is not. This is a take a good template.

89
00:13:45,430 --> 00:13:49,900
Take the things we've seen in this class and apply it in a new note domain.

90
00:13:50,380 --> 00:13:54,640
Don't reinvent the wheel. There's plenty of work.

91
00:13:54,640 --> 00:13:58,390
There's plenty of challenges involved in applying this idea to a new context.

92
00:13:58,930 --> 00:14:09,640
That's what this assignment is about. I am more than happy to help you navigate that process, but this really is a skill set you want to have.

93
00:14:10,580 --> 00:14:16,549
Comfort with because it's something you will you end up using in a variety of different contexts.

94
00:14:16,550 --> 00:14:21,500
Even if you don't imagine yourself doing patient education at some point somewhere,

95
00:14:21,500 --> 00:14:24,380
you're going to be in some meeting in which you've got a choice to make.

96
00:14:25,160 --> 00:14:28,250
And lo and behold, the techniques we're going to talk about today are going to be relevant.

97
00:14:29,890 --> 00:14:36,790
So that's where we're going with this. And I'm going to I'm going to put up the drug tax example after we spend a little time talking about

98
00:14:36,790 --> 00:14:43,869
argument dilution and really go through it in detail and please ask me questions like if you're like,

99
00:14:43,870 --> 00:14:47,230
Well, we could do it that way. Let's have that conversation.

100
00:14:50,200 --> 00:14:59,630
So. Our last administrative thing before the forward is to remind us no class on Tuesday.

101
00:15:01,060 --> 00:15:06,879
I will be in Seattle. I will not be here. So you can come, you can hang out, whatever.

102
00:15:06,880 --> 00:15:14,560
But I will not be here. This is the Society for Medical Decision Making Annual meeting, and this is my major conference.

103
00:15:15,400 --> 00:15:22,390
And I will I will be there. Any questions on the assignment before we move on?

104
00:15:25,290 --> 00:15:32,110
All right. So. Let's talk about this argument dilution idea and.

105
00:15:34,930 --> 00:15:37,780
I don't want to oversell it, but I also don't want to undersell it.

106
00:15:37,780 --> 00:15:44,330
I think it's a good way into trying to think about what do you do when you're not talking about one risk?

107
00:15:44,350 --> 00:15:49,509
You're talking about many risks. And these things are not all the same.

108
00:15:49,510 --> 00:15:53,530
And so how do we think about communicating effectively in those kinds of contexts?

109
00:15:54,430 --> 00:15:59,250
And to start us off. I want to go to Sarah.

110
00:16:01,570 --> 00:16:04,950
It runs in the ice of.

111
00:16:06,010 --> 00:16:09,850
We were talking and I was I had to pull out little bits and pieces.

112
00:16:09,940 --> 00:16:15,160
You were talking at one point about somebody and I don't remember who was helping you.

113
00:16:15,730 --> 00:16:20,920
Oh, yes. Yeah. Let's talk about that, because this is a relevant thing we want to talk about when we talk about race.

114
00:16:21,430 --> 00:16:25,580
So this past summer, I went to Hawaii with my older sister.

115
00:16:28,050 --> 00:16:37,410
My friend's mom and I went grocery shopping and she was very particular on the things that she had wanted to purchase.

116
00:16:37,410 --> 00:16:43,860
And she was particular because she read every nutrition facts of every single item that had one,

117
00:16:44,700 --> 00:16:50,940
and she would count the number of ingredients on the list. And as she went, she opted for one that had the least amount.

118
00:16:51,450 --> 00:16:57,060
And that's what we always find. It was very exhausting, but that's what she did.

119
00:16:57,140 --> 00:17:01,530
She thought she associate that less number of ingredients means healthier.

120
00:17:03,120 --> 00:17:09,330
And then even as she didn't even understood most like the ingredients list, she opted for the obese number.

121
00:17:10,710 --> 00:17:18,410
So I wanted to raise this for two reasons. One. I talk about this in my other class.

122
00:17:18,800 --> 00:17:24,170
Let's pause for the moment that what defining what is an ingredient means that even

123
00:17:24,170 --> 00:17:28,070
something like a banana can have an ingredient list that's 30 or 40 things law.

124
00:17:28,640 --> 00:17:34,340
If you were to break it down into its all of its chemical components. So there's an arbitrariness.

125
00:17:35,790 --> 00:17:40,920
To how we define and group things that is really important to recognize.

126
00:17:41,550 --> 00:17:46,770
I could say here is a banana or I could give you a list of all the chemical compounds included in the banana.

127
00:17:47,160 --> 00:17:54,300
And my guess is the person talking about would react really differently to one versus the other.

128
00:17:56,920 --> 00:18:00,660
So when we're talking about. Risks.

129
00:18:02,370 --> 00:18:12,060
How do we group that? So this could be everything from all all headaches, group number one.

130
00:18:12,510 --> 00:18:16,460
Then we separate mild headaches from severe headaches.

131
00:18:16,510 --> 00:18:30,530
We separate migraines. And I agree. You can unpack that one thing of chance of headaches into many for you chose to do so and you look

132
00:18:30,540 --> 00:18:37,709
at the same kind of impact we're talking about you when we get into the argument dilution thing.

133
00:18:37,710 --> 00:18:45,690
Notice that part of what that article talks about is the idea of grouping things under categories like severe side effects,

134
00:18:45,690 --> 00:18:52,140
mild side effects, common side effects, etc. as a way of mitigating some of that dilution effect.

135
00:18:52,860 --> 00:18:55,980
But that also means we get some choice in terms of how we group them.

136
00:18:57,510 --> 00:19:00,299
Is this life threatening versus not life threatening?

137
00:19:00,300 --> 00:19:06,870
Okay, so losing a limb is in life threatening, but is that really the way we want to be categorizing things?

138
00:19:09,420 --> 00:19:14,909
It gets complicated. And part of my point here is I want you to be recognizing that whatever

139
00:19:14,910 --> 00:19:20,220
categorizations you see when you're researching the topic for your next assignment.

140
00:19:21,310 --> 00:19:30,640
Is a choice which you can change. Be intentional about thinking about what are the sets of words that are relevant to somebody?

141
00:19:30,880 --> 00:19:31,390
How do you want to?

142
00:19:31,630 --> 00:19:38,560
Sometimes it's really important to group things like you don't actually want people being lost in the 47 million values that sometimes it's

143
00:19:38,560 --> 00:19:56,600
really important to recognize that that is going to tie up with the choice we're going to make and as an intentional group connected with that.

144
00:19:56,770 --> 00:20:06,429
Cindy, you were talking about listening to like drug ads and certain what I remember specifically was certain side effects pop up and you're like,

145
00:20:06,430 --> 00:20:09,630
yeah, what everything has that. Yeah. Yeah.

146
00:20:09,670 --> 00:20:14,320
There was so like with like the ads were they like read the side effects like really quickly at the end.

147
00:20:14,350 --> 00:20:19,329
Like I feel like I do pick up on some of them, like depression, anxiety or like severe ones.

148
00:20:19,330 --> 00:20:23,319
But I hear for like every single like medication basically.

149
00:20:23,320 --> 00:20:29,650
So now like I go to the doctor, I really talk about medication. I'm like, well, like everything has these side effects, so it doesn't even matter.

150
00:20:30,040 --> 00:20:36,400
Yeah. Let's at least pause for a moment. I mean, not that we want to escalate the way in which side effects are being talked about in drug ads.

151
00:20:37,120 --> 00:20:40,510
But let's at least talk about the characteristics of that presentation.

152
00:20:41,500 --> 00:20:50,950
So what a long list without either groupings by severity or type or probability information.

153
00:20:51,160 --> 00:20:55,060
And it's pure possibility communication to use the terminology of the previous hour.

154
00:20:56,950 --> 00:21:03,010
Two, they're all run together, so it's designed to make you group them.

155
00:21:04,360 --> 00:21:08,439
Things might happen. That's a powerful level at which they want you thinking about.

156
00:21:08,440 --> 00:21:13,870
There's not a lot of distinctions between out how this kind of severe stuff not really

157
00:21:13,870 --> 00:21:21,400
processing how likely this is 3040 something else several of you mused about drug ads and.

158
00:21:23,160 --> 00:21:31,800
What's usually happening on screen with that list of things is being read, you know, like happy, like a benefit of the medication.

159
00:21:31,840 --> 00:21:39,980
Yes. So the imagery and the sound as they are going through this is always how I feel.

160
00:21:39,990 --> 00:21:45,719
If you're talking about an allergy medication, for example, you have the person running through the field with flowers,

161
00:21:45,720 --> 00:21:49,050
smiling and all of a sudden super happy or playing with their kids.

162
00:21:49,290 --> 00:21:59,160
In order to do that, they focus on the benefits and not on like side effects.

163
00:21:59,700 --> 00:22:04,260
Yeah, but there's a specific thing we brought up earlier in this class. It's very specific, relevant to your.

164
00:22:05,700 --> 00:22:13,110
Way back at the beginning, we talked about the affect heuristic, where that emotion serves as a common currency.

165
00:22:14,190 --> 00:22:20,190
What are they doing? They're putting positive emotions into your brain as you're listening to the side effects.

166
00:22:20,820 --> 00:22:27,780
What do you automatically makes those side effects feel less relevant, less salient, less severe, etc.?

167
00:22:28,320 --> 00:22:32,190
So the juxtaposition of that positive imagery is a risk.

168
00:22:32,190 --> 00:22:36,630
Communication as well is modifying your response to the information that you're getting.

169
00:22:39,870 --> 00:22:46,080
So. If we sort of frame that as this is, if you want it to be super persuasive, what would you do?

170
00:22:46,110 --> 00:22:54,750
You would run all your side effects together. You want so that you have don't have ability to figure out whether there's a draft likely or not likely.

171
00:22:56,340 --> 00:23:01,770
So push out emotions of the associated negative emotion with this information.

172
00:23:02,580 --> 00:23:07,110
That's the extreme persuasive that I want to push us in the opposite way.

173
00:23:07,440 --> 00:23:14,849
How would you do this in a way that would be balanced with how somebody consider what the side effects are and what the benefits are,

174
00:23:14,850 --> 00:23:20,100
and whether or not this is something they want to do. So in order to accomplish that, what do we need?

175
00:23:20,580 --> 00:23:28,470
We need unpacking, which is really served breaking up into categories that so many likelihood information.

176
00:23:31,350 --> 00:23:36,120
That we can talk about and we will talk about how precise that needs to be.

177
00:23:38,360 --> 00:23:39,950
In order to accomplish our goals,

178
00:23:40,850 --> 00:23:47,600
we need something to distinguish between something that's going to be a 70% chance versus a seven in a million chance.

179
00:23:50,020 --> 00:23:56,400
And we need to enable you to recognize the trade off recognized.

180
00:23:56,470 --> 00:23:59,920
Yes. There might be benefits. Yes, there might be risks.

181
00:24:00,080 --> 00:24:07,520
But think about these separately. All of these features are critical for the design of what we're talking about.

182
00:24:10,190 --> 00:24:23,390
So think about this. The fella you were talking about, like for common over-the-counter beds, like every program and Tylenol.

183
00:24:24,720 --> 00:24:30,780
Let's talk about how we ought to be thinking about those drugs which are very familiar to us.

184
00:24:32,970 --> 00:24:37,560
Well, they lose the side effects on that little label attached to the bottle.

185
00:24:37,560 --> 00:24:40,050
And it's like not many of us pay attention to that.

186
00:24:40,380 --> 00:24:47,520
But if you do and often actually cause a lot of severe side effects, like bleeding or just like liver problems,

187
00:24:48,180 --> 00:24:55,290
but we don't pay attention to that because obviously we still take them. So I guess people just ignore the fact that they're there sometimes.

188
00:24:55,830 --> 00:25:03,600
So let's talk about the three most common over-the-counter pain relievers Tylenol, ibuprofen and aspirin.

189
00:25:06,780 --> 00:25:13,480
Which one was first? You think, Tyler?

190
00:25:15,580 --> 00:25:20,230
Aspirin was first. If you lay out the risks.

191
00:25:21,440 --> 00:25:26,300
Of these medications and the likelihood of severe complications. You know, which one is the worst?

192
00:25:27,390 --> 00:25:32,850
The aspirin. Aspirin by a mile as pretty dangerous.

193
00:25:33,020 --> 00:25:39,560
Well, aspirin, isn't it? You know, people take baby aspirin as an anticoagulant to prevent heart attacks.

194
00:25:40,160 --> 00:25:47,610
I. It is not at all clear to me that if aspirin was brought to the market right next to the Food Drug Administration right now,

195
00:25:47,610 --> 00:25:51,240
that it would be approved for over-the-counter use. My my guess is it would not.

196
00:25:51,270 --> 00:25:53,400
My guess is it would be prescription only. It's too dangerous.

197
00:25:54,670 --> 00:26:00,850
But because it was first and we got familiar with it, you don't feel that risk in the same way.

198
00:26:02,050 --> 00:26:08,850
What was Second Island? Well the major risks that are not because.

199
00:26:08,850 --> 00:26:15,150
No, yeah, it's not liver damage because your body can't process it as to the kidneys.

200
00:26:15,360 --> 00:26:21,030
What else? It's kidneys for Tylenol. Something like the kidneys for Tylenol was kidney.

201
00:26:21,030 --> 00:26:24,320
I never remember which one is, like you said, very specifically or.

202
00:26:24,330 --> 00:26:31,530
No, I'm sorry. You're right. I'm getting things confused. Don't take more than this amount in 24 hours because it's very, very dangerous.

203
00:26:31,530 --> 00:26:36,420
So the trick with Tylenol or anything else. So I'm going to put now put on my patient hat for a moment.

204
00:26:36,750 --> 00:26:41,220
The trick is I'm always this last issue. It's not the fact the risk exists.

205
00:26:41,520 --> 00:26:48,060
It's that it's heavily dose dependent at the point at which it gets dangerous can be achieved relatively quickly.

206
00:26:49,610 --> 00:26:54,020
You can overdose on Tylenol in a day and not that hard.

207
00:26:54,320 --> 00:26:59,440
And when did this really get scary? Is that right?

208
00:26:59,510 --> 00:27:03,620
We see Tylenol poisoning in infants not infrequently.

209
00:27:04,640 --> 00:27:15,260
And I will now tell you one of the horror stories of medical risk communication, which is one of the things that does as far as I know.

210
00:27:15,440 --> 00:27:18,680
Maybe I'm wrong, but I think they have now pulled this off the shelves for this reason.

211
00:27:19,460 --> 00:27:24,980
It is extremely difficult to find what used to be called infants Tylenol.

212
00:27:27,970 --> 00:27:32,160
My children's time. Why am I saying infants versus children?

213
00:27:32,730 --> 00:27:39,090
Well, the infant stuff was the stuff that came as a little drop after 20 minutes and the children's Tylenol and subsequent cups and the liquid,

214
00:27:39,090 --> 00:27:42,580
etc., they look really similar.

215
00:27:42,600 --> 00:27:45,240
They're both flavored, so kids will take them, etc.

216
00:27:45,570 --> 00:27:51,570
Except the infant's Tylenol, I believe, was four times more concentrated than the kids Tylenol was.

217
00:27:52,110 --> 00:28:00,410
And you're dosing it for an infant. It's not very hard to imagine how somebody could breathe and think, Oh, I need to give this cup worth of liquid.

218
00:28:01,410 --> 00:28:08,340
But they happened to have the infant stuff. And so they then overdose by a factor of four their infant.

219
00:28:10,150 --> 00:28:16,400
And so we saw infants with kidney failure from Tylenol on a not infrequent basis because the

220
00:28:16,400 --> 00:28:22,550
dosing was so complicated and because the risk is that quick of showing up in terms of how,

221
00:28:23,750 --> 00:28:29,610
you know, it's not a long term risk. It's a short term risk. Ibuprofen has a variety of risks.

222
00:28:29,610 --> 00:28:34,049
I don't want to minimize that. Most of them tend to be longer term things.

223
00:28:34,050 --> 00:28:38,220
So if you think about the stomach issues, etc.,

224
00:28:39,120 --> 00:28:46,050
intermittent use doesn't tend to be a problem for people taking on a chronic basis for chronic pain management.

225
00:28:46,290 --> 00:28:51,149
That's what happens. So that's why it's over.

226
00:28:51,150 --> 00:28:56,760
The counter is there's been an acceptance of that ideal, but the point you're raising is really important.

227
00:28:57,120 --> 00:29:01,859
You know, when we're familiar with something, we don't tend to get into the weeds.

228
00:29:01,860 --> 00:29:06,060
We're like, Oh yeah, I took it last time. It didn't matter, right? That experiential learning.

229
00:29:06,690 --> 00:29:19,770
So the potential for danger problems in getting people to pay attention to how their behavioral patterns are going to either put them at risk or not.

230
00:29:22,370 --> 00:29:26,400
What I've taken away is basically with Tylenol. Worry about your dose.

231
00:29:26,660 --> 00:29:30,200
Really worry about your dose with ibuprofen.

232
00:29:30,200 --> 00:29:34,790
Worry about duration. Like you want to take it for one day because you've got it.

233
00:29:34,790 --> 00:29:39,830
You wrenched your back. Not much is going to happen to you unless you take a ridiculous number of pills.

234
00:29:40,610 --> 00:29:46,040
You're taking it week after week after week for something. There's some real risk there.

235
00:29:48,320 --> 00:29:54,050
So this is the kind of stuff that we what is our purpose when we're communicating about side effects?

236
00:29:54,410 --> 00:29:59,270
What is it that we want somebody to do or not do? And that's not always a probability thing.

237
00:30:00,870 --> 00:30:05,250
Sometimes it's a you shouldn't touch this drug because you have X kind of thing.

238
00:30:06,630 --> 00:30:11,850
Sometimes it's a does this actually meet your need kind of thing and all of that.

239
00:30:11,860 --> 00:30:21,580
I'm going to come back to in a moment. But before we go there, I want to touch upon actually, let me go there first.

240
00:30:21,670 --> 00:30:28,510
So. Sydney and Alex in different ways, talked about the question of, okay,

241
00:30:28,510 --> 00:30:32,110
so maybe we're supposed to be grouping side effects by some of your side effects.

242
00:30:32,770 --> 00:30:38,440
Mild side effects, that's mild. So why don't you start and then.

243
00:30:39,730 --> 00:30:46,450
Oh yeah, that's kind of like one of my big questions after reading the article on The Ocean Effect and.

244
00:30:47,240 --> 00:30:50,850
And he just seems like such a relative term. Well.

245
00:30:51,240 --> 00:30:59,729
So let's ask this question. Yes, I'll make this easy out of what it is.

246
00:30:59,730 --> 00:31:16,130
Dry skin. I'll. But if it's so correct that you're bleeding and you're at higher risk for getting infections, then probably not.

247
00:31:16,490 --> 00:31:23,010
But but sort of our quick reaction is this feels like if it's just that I've got some flaky skin, I can put some moisturizer on it.

248
00:31:23,030 --> 00:31:28,850
We're not talking life threatening, maybe. But as you say, there could be more extreme versions of this.

249
00:31:30,500 --> 00:31:40,430
Yeah, I think in cases like eczema and stuff, it can range from like very severe, even if it's like not dry.

250
00:31:40,880 --> 00:31:46,070
And notice the key question here is, do you consider diagnosed eczema?

251
00:31:47,280 --> 00:31:52,980
As in the same bucket or different bucket as dry skin.

252
00:31:53,820 --> 00:31:58,740
You could lump those two things together and say these are both part of the same condition, once more severe than the other.

253
00:31:59,100 --> 00:32:01,350
Or you could say, no, no, no, no, no, I need to separate them.

254
00:32:01,350 --> 00:32:07,049
I need to have somebody thinking about risk of eczema in severe enough that we would diagnosis and treat it,

255
00:32:07,050 --> 00:32:11,420
etc. as distinct from a hanging you might want to wear. You know it's a moisturizer.

256
00:32:11,430 --> 00:32:15,790
You're your hands. To make a more concrete example.

257
00:32:17,710 --> 00:32:26,740
There are a variety of issues that caught the eye of the right eyes.

258
00:32:27,160 --> 00:32:35,860
Put some drops in your eyes beginning of the morning and your fine dry eyes that you're in pain blinking every time you blink for the entire day.

259
00:32:36,670 --> 00:32:43,120
But those are different experiences. And so, you know,

260
00:32:43,930 --> 00:32:53,560
how far down the pathway of disruption of daily life do we go and still feel comfortable lumping something into a mild side effect space?

261
00:32:55,170 --> 00:33:02,550
We can talk about headaches to talk about, and then we get into sort of intermediate things like so pain is always a complicated one.

262
00:33:03,210 --> 00:33:07,440
I think it's just pain as in I'll take an ibuprofen and I'll be functional.

263
00:33:07,440 --> 00:33:15,470
Or is this pain as in I can barely move. There's no simple answers to this.

264
00:33:16,040 --> 00:33:22,850
Part of my point here today is to drive home the complexity and also to drive home that we've.

265
00:33:24,020 --> 00:33:27,290
Sort of individual circumstances. That's not the right way to put it.

266
00:33:27,770 --> 00:33:30,230
You do this differently depending upon what the situation is.

267
00:33:32,810 --> 00:33:38,270
There's no sort of like, well, of course, headaches will always be in this bucket and of course drives can always be in that bucket,

268
00:33:38,270 --> 00:33:43,520
etc., because it matters how severe we're talking about and matters what the context is.

269
00:33:47,940 --> 00:33:54,510
Anything more you want to add? You were talking about like metformin and the side effects of that as an example of this.

270
00:33:55,110 --> 00:34:01,259
Right. I think I would just add to that, what you mentioned was that that area I was talking about,

271
00:34:01,260 --> 00:34:09,870
metformin causing diarrhea that are people have different diarrhea, meaning that there are some of those, in fact, as well.

272
00:34:10,410 --> 00:34:17,489
And so that could be different for everyone. And also some people would find it debilitating or socially embarrassing compared to other people.

273
00:34:17,490 --> 00:34:21,370
They find it's not as pretty kind of professional.

274
00:34:21,510 --> 00:34:25,120
Yeah. There is an interesting example. This is an ugly class that we talk about,

275
00:34:27,690 --> 00:34:36,960
but there is an interesting example to think about because you could imagine everything in a spectrum from one episode,

276
00:34:37,470 --> 00:34:40,840
you know, short term discomfort, not a big deal.

277
00:34:41,280 --> 00:34:52,829
Something that we've all experienced in day to day life is a side effect, but it's not a debilitating one to something which is enormously disruptive,

278
00:34:52,830 --> 00:34:56,300
not just in terms of the function, but potentially, you know,

279
00:34:56,550 --> 00:35:00,780
might lead to dehydration and might lead to medical problems that would require treatment.

280
00:35:01,860 --> 00:35:07,350
And the world does not tell you where you fall on that spectrum.

281
00:35:10,100 --> 00:35:14,090
So when we're communicating and we're listening to that long list of side effects and they say,

282
00:35:14,420 --> 00:35:19,820
you know, may cause diarrhea, is the right question to ask is what is this person going to think?

283
00:35:20,240 --> 00:35:30,860
What are they imagining in that moment? And is that well aligned or not to what is actually intended to be communicated in terms of the likelihood and

284
00:35:30,890 --> 00:35:39,260
severity of what I'm pushing you towards here is paying really close attention as you're working in your assignment,

285
00:35:39,530 --> 00:35:44,540
thinking about what are the dimensions or the risks, what are the benefits, etc.

286
00:35:44,930 --> 00:35:50,509
How are you going to label those? How many of them are you going to have and how are you going to label this?

287
00:35:50,510 --> 00:35:57,950
Because that is where a lot of the stuff happens, how somebody is going to pay attention or not attention to what we give them.

288
00:36:01,350 --> 00:36:07,210
And. Oh, yeah.

289
00:36:07,230 --> 00:36:16,690
I don't want to skip over this one any. You brought up a pattern of behavior that I think is important to acknowledge the what

290
00:36:16,690 --> 00:36:21,190
happens when you tell somebody a side effect and they just don't seem to engage with it?

291
00:36:22,690 --> 00:36:28,240
Yeah. So I think I was talking about a patient who came in requesting to start Eliquis,

292
00:36:28,690 --> 00:36:36,730
which is a pretty powerful letter because it's in an ad that it would reduce the risk of stroke,

293
00:36:37,840 --> 00:36:41,139
which it is used for in people who have other conditions.

294
00:36:41,140 --> 00:36:48,370
But she didn't have any predisposing conditions, so we were talking about side effects of the bleeding and eventually we got around to it

295
00:36:48,370 --> 00:36:54,280
can cause strokes and that kind of ended up being a selling point for not starting it.

296
00:36:54,700 --> 00:37:04,030
But the side effects, it seemed, were not concerning to her, given her genuine fear and very understandable fear of strokes because of family history.

297
00:37:04,600 --> 00:37:06,879
So for the reason I wanted to bring I mean,

298
00:37:06,880 --> 00:37:13,930
this is a separate conversation about how people can zoom in and focus only on one risk when they're really being facing many.

299
00:37:14,530 --> 00:37:19,089
And whether that's a a benefit in this case like this will prevent stroke.

300
00:37:19,090 --> 00:37:22,999
And I'm really worried about strokes. I don't care what else is is always a risk.

301
00:37:23,000 --> 00:37:33,280
And then I will never put myself at risk for this, despite the fact that my benefit in many ways we see both part of what we're aiming for.

302
00:37:33,400 --> 00:37:39,160
And the reason I wanted to bring this up is part of what we're aiming for when we design these types of communications is to prevent that reaction.

303
00:37:40,300 --> 00:37:47,290
How do we ensure that somebody will recognize that there are many things that they need to consider, not just one?

304
00:37:48,680 --> 00:37:56,050
So my version of your story is in the cancer treatment context in which you see people who basically say,

305
00:37:57,010 --> 00:38:00,460
What's going to give me the best chance of being alive here?

306
00:38:00,580 --> 00:38:02,799
I don't care how bad it's going to be.

307
00:38:02,800 --> 00:38:10,630
What's going to give me that best chance of being alive is going to reduce my cancer mortality the most, and there is an answer to that.

308
00:38:10,750 --> 00:38:16,060
But there is often a huge tradeoff, and sometimes the answer is, wow,

309
00:38:16,750 --> 00:38:21,700
undergoing treatment or any of the therapies or chemotherapy or whatever is going to make

310
00:38:21,700 --> 00:38:25,990
a big difference in your survival and are sometimes in which you are putting yourself.

311
00:38:26,410 --> 00:38:29,560
And I will use inflammatory language intentionally.

312
00:38:29,950 --> 00:38:32,530
You're putting yourself through [INAUDIBLE] for almost nothing.

313
00:38:36,910 --> 00:38:42,310
And we ought to make sure that the people we're communicating with know the difference.

314
00:38:43,870 --> 00:38:47,450
How much benefit are they getting and what is the trade offs that they are facing?

315
00:38:48,310 --> 00:38:58,000
And so one of the other major pieces of advice I give you for this assignment upcoming, if you will,

316
00:38:58,060 --> 00:39:05,860
I hope to have noticed from the contacts by the table, which again, I'll put up in just a minute, it's not organized, has pros and cons.

317
00:39:06,700 --> 00:39:09,730
It's organized as risks and benefits.

318
00:39:13,680 --> 00:39:21,060
So that when you look at it, you're forced to look at the idea that there are things that will be better if you do this.

319
00:39:21,300 --> 00:39:23,100
And there are things that will be worse.

320
00:39:26,350 --> 00:39:32,860
Pros and cons is a very dangerous way to frame things because then you can just but, you know, probably lower your risk of this.

321
00:39:32,860 --> 00:39:39,960
Your card is like you can you can flip the frame and the way you describe is benefits and call almost anything, a pro or anything.

322
00:39:39,970 --> 00:39:48,190
I can't I don't want to organize it that way. I want you organizing around each individual benefit, in each individual risk.

323
00:39:49,360 --> 00:39:52,920
That's where this grouping becomes such a big deal. Like, well, what do you mean by individual thing?

324
00:39:52,930 --> 00:39:58,300
Well, that's why we're talking about this. So let me call this up.

325
00:40:15,190 --> 00:40:19,480
Figure. I have not about.

326
00:40:29,250 --> 00:40:32,610
I want to make it bigger and scroll. And it's not related to this area.

327
00:40:33,780 --> 00:40:40,680
All right. Before we even get down here.

328
00:40:41,100 --> 00:40:49,040
Just ignore this for the moment. Just look up at the top. Notice.

329
00:40:49,040 --> 00:40:52,790
What's there? What is this drug for?

330
00:40:53,690 --> 00:40:59,870
Why are you doing this? Like, what is its purpose and ensuring that that comes across first?

331
00:41:00,770 --> 00:41:05,530
Is this for pain relief? No. Is this to prevent headaches?

332
00:41:05,650 --> 00:41:09,160
No, this is to prevent heart attacks. Okay.

333
00:41:10,280 --> 00:41:14,130
It does so by reducing blood clotting. Fine.

334
00:41:15,320 --> 00:41:20,149
But that's important. Like there are. I can't count the number of times I have conversations with pharmacists who are like,

335
00:41:20,150 --> 00:41:25,130
you know, people talk to me about the fact they're taking this medication. They have no idea why they are taking it and what it does.

336
00:41:26,330 --> 00:41:31,520
So putting that first is important. Who might consider taking it.

337
00:41:31,590 --> 00:41:36,850
If you think about this from a study design, boost inclusion criteria. Who is the target audience?

338
00:41:41,170 --> 00:41:44,960
I notice the third one. You should not take it.

339
00:41:45,800 --> 00:41:51,830
What is the one of the most critical things that we often tried to do as risk mitigation is to

340
00:41:51,830 --> 00:41:57,980
make sure that people who have specific exclusion reasons like don't take this if you have X.

341
00:41:59,570 --> 00:42:06,750
I know that. And I forget who was talking about rent controls that were associated with.

342
00:42:06,800 --> 00:42:17,260
With. I can't remember what, but the birth control methods that are have particular associations like caused migraines or cause seizures,

343
00:42:17,260 --> 00:42:25,840
etc. Like if you have a particular history of those conditions, like their absolute exclusions, you should not be on this.

344
00:42:26,530 --> 00:42:29,830
And that's not probabilistic. That's absolute.

345
00:42:31,180 --> 00:42:37,509
So defining that, who should who might take this and who doesn't take this?

346
00:42:37,510 --> 00:42:43,270
Who is this for? And who this not for is just as important in terms of the risk communication as anything else we're talking about here.

347
00:42:48,160 --> 00:42:54,700
Sometimes your need to take as if you're doing a therapy or this is an opportunity to communicate that.

348
00:42:57,620 --> 00:43:01,640
I also like the fact that this talks about what else can you do?

349
00:43:01,830 --> 00:43:08,360
Like there are other options here. I mean, acknowledges that this is not the only thing one can do to address it.

350
00:43:09,890 --> 00:43:14,180
The main thing I want you to focus on out there is this idea of the exclusion.

351
00:43:16,090 --> 00:43:21,010
It is actually one of the more important pieces here if you know who should not do this.

352
00:43:22,560 --> 00:43:29,410
Right. I'm just thinking about there's all kinds of medications for which children should not take it.

353
00:43:30,190 --> 00:43:35,440
People who might become pregnant should not take it, etc. like those exclusion criteria are really critical.

354
00:43:41,360 --> 00:43:47,890
All right, the next. Again.

355
00:43:48,130 --> 00:43:54,270
Don't skip down to the numbers. Notice what comes next. Tell us.

356
00:43:56,890 --> 00:44:07,630
What this body of data is from. In this case, 20,000 adults with cardiovascular disease who took it for two years.

357
00:44:09,000 --> 00:44:13,830
Time is important. These numbers are going to be different if that was ten years.

358
00:44:17,320 --> 00:44:20,380
For lifetime for six months or six weeks.

359
00:44:21,220 --> 00:44:28,300
I matters because a calibrates you is important. Or How likely are these events to occur for the change based on the time interval?

360
00:44:28,870 --> 00:44:33,730
This is one thing that most people forget about. Oh well, this, you know, you've got a 6% chance of this happening.

361
00:44:34,000 --> 00:44:37,360
Is that how it is that over the next ten years?

362
00:44:37,870 --> 00:44:48,080
What? Then we get into the table and I've got to point out several critical features here.

363
00:44:48,110 --> 00:44:54,830
First, what's the organizational structure? That's a big block of benefits.

364
00:44:55,190 --> 00:44:59,460
Did it help? And you have a separate block of side effects.

365
00:45:01,290 --> 00:45:08,810
So those are separated very clearly and consciously. Roughly.

366
00:45:08,820 --> 00:45:12,630
Well, in each of those plot, things are ordered in by severity.

367
00:45:15,330 --> 00:45:19,640
You got to stop their heart attacks. That's it. That's why you're taking this in the first place.

368
00:45:19,650 --> 00:45:26,730
So you care about that post. But even when we get down on the side effects, notice this is the argument dilution idea.

369
00:45:27,600 --> 00:45:31,170
Life threatening side effects versus symptoms. Side effects.

370
00:45:31,740 --> 00:45:40,290
Notice this isn't severe versus mild. Life threatening is very it's a more concrete of representing know these are things that might kill you.

371
00:45:41,040 --> 00:45:44,310
Symptoms could vary like we've got diarrhea here.

372
00:45:44,970 --> 00:45:55,710
We also had rash. So and again, there's all the problems we have about variability in those outcomes, but at least we're itemizing them one by one.

373
00:46:00,460 --> 00:46:07,610
You got a nice side by side table. What do you notice about the way the numbers are being presented here?

374
00:46:08,580 --> 00:46:11,810
I'll let you guys call out things you notice you.

375
00:46:25,840 --> 00:46:34,420
It's not that complicated. What do you know? Yeah, if they had, like, similar, like, percentages of people they put in the middle instead of, like.

376
00:46:35,590 --> 00:46:40,440
Yeah, they put it in the middle. What could they have done?

377
00:46:40,470 --> 00:46:43,680
They could have put 1.4 and 1.4.

378
00:46:44,340 --> 00:46:53,280
They didn't. Why didn't they do that? Because it was more important to communicate the equivalence than it was what the exact percentage was.

379
00:46:54,150 --> 00:46:59,250
If the point here is to guide people through choices, and it doesn't matter what you choose, you're going to get the same thing.

380
00:47:00,360 --> 00:47:06,660
Making that really transparent is important. So they put it in the middle and they said it's about the same in both groups.

381
00:47:07,500 --> 00:47:10,740
Good at one big thing. That's good of you. What else?

382
00:47:16,240 --> 00:47:30,120
What formats do they use? For their percentages, they gave a percentage and then also the fraction just in case it was easier for.

383
00:47:30,640 --> 00:47:32,460
Yeah. So there's a story here.

384
00:47:33,040 --> 00:47:41,790
Um, this is from the original Dropbox, but it was nice that they did for every number they presented both as a percentage.

385
00:47:41,790 --> 00:47:45,300
And let's be really clear as a percentage with one decimal point.

386
00:47:46,840 --> 00:47:53,380
And they have presented everything also as a frequency number out of 1000.

387
00:47:53,680 --> 00:47:58,030
So the most critical thing let me reinforce this most critical thing.

388
00:47:58,030 --> 00:48:02,150
What's the denominator? Yeah. A thousand.

389
00:48:02,780 --> 00:48:10,270
Everywhere. It does not change. You will not get a faster way to get me to mark something off.

390
00:48:10,280 --> 00:48:14,000
That's a very good denominator in what you give me. Don't do it.

391
00:48:14,210 --> 00:48:20,580
Ever. That doesn't mean it always has to be a thousand or whatever it is.

392
00:48:20,580 --> 00:48:26,250
Pick it and run with it. They paid the thousands that allowed them to get down to the 10th of a percent.

393
00:48:27,030 --> 00:48:33,210
It's sense, given the frequencies that we're talking about here. Quite consistent.

394
00:48:33,460 --> 00:48:37,710
And that's really important. Yeah, but they also have it written out verbally on the left.

395
00:48:38,070 --> 00:48:41,720
Like more people had this with the percentage there.

396
00:48:41,730 --> 00:48:45,330
So yeah. Not asking people to do the math themselves.

397
00:48:45,360 --> 00:48:52,200
Yeah. So let me actually let me respond to that and I'll come back to the point I was moving towards with the percentages on the frequencies.

398
00:48:52,500 --> 00:48:56,940
So yes, so this is another major feature of this table that you need to pay attention to.

399
00:48:57,450 --> 00:49:01,739
They did the math. Not only did they do the math, but they did.

400
00:49:01,740 --> 00:49:09,190
The qualitative distinction of saying, is the drug better or worse on this dimension?

401
00:49:09,210 --> 00:49:13,170
So this is both a relative possibility communication.

402
00:49:13,200 --> 00:49:22,409
You are more likely or less likely to have something occur if you take the drug and what I would call a comparative probability.

403
00:49:22,410 --> 00:49:26,990
What is the difference? The incremental difference between these.

404
00:49:27,350 --> 00:49:30,950
So if we look here at bruising, more people had bruising.

405
00:49:30,950 --> 00:49:35,210
If they took the drugs, how many more? 1.6% more.

406
00:49:36,200 --> 00:49:41,330
That's the absolute risk increase, not a relative percentage absolute difference.

407
00:49:42,740 --> 00:49:48,260
It is the fact that I have seen other tables physically organize the information differently.

408
00:49:48,260 --> 00:49:56,060
So, for example, I've seen tables that have absolutely compared our absolute rate and then the difference as a third column.

409
00:49:57,230 --> 00:50:01,160
I have seen ones in which it's, you know, people highlighted, okay,

410
00:50:01,160 --> 00:50:08,719
so this one is green because it has better and it would be green here because this is lower.

411
00:50:08,720 --> 00:50:13,160
Like they use color coding or other kinds of symbols to show which one is better on each dimension.

412
00:50:13,670 --> 00:50:22,550
All of that is it's fair game. But the key things to point out here is they're showing you each and every dimension.

413
00:50:23,450 --> 00:50:29,190
Is it better or worse? But the difference is that you're not stuck doing the math.

414
00:50:31,750 --> 00:50:38,500
That's really powerful because if we didn't do that, this requires you to have the numeracy skills.

415
00:50:39,560 --> 00:50:43,990
You may know that what you're supposed to be doing is to compare this versus this,

416
00:50:44,320 --> 00:50:47,590
figure out which one is bigger and figure out that math for yourself.

417
00:50:47,590 --> 00:50:50,770
And that's hard. Like doing that math for them.

418
00:50:51,340 --> 00:50:57,730
We make it like you could ignore this whole table and just give this and you get a lot.

419
00:50:59,600 --> 00:51:02,750
It's not quite as precise, but you still get that incremental difference.

420
00:51:03,260 --> 00:51:11,840
We still get to know, okay, if I have if I do the drug, I've got a 1% more chance of having diarrhea, but I have 4% chance of getting a heart attack.

421
00:51:11,990 --> 00:51:15,080
Okay. Like, there's a lot of information that.

422
00:51:16,640 --> 00:51:24,410
So each one of those features is important. Now, I wanted to go back to this percentage frequency thing.

423
00:51:24,950 --> 00:51:33,710
So a couple of years after this paper was published, this same research group and Dartmouth published a follow up paper.

424
00:51:34,910 --> 00:51:38,480
Comparing this format.

425
00:51:39,700 --> 00:51:45,730
With tables that only had percentages. Eutelsat only had frequencies.

426
00:51:47,440 --> 00:51:53,230
One of the few times I have thought, I know not what a paper was going to say and was absolutely wrong.

427
00:51:54,240 --> 00:51:58,260
I thought that paper came out. This format was going to win.

428
00:51:59,450 --> 00:52:05,749
More understood because it had both the percentages and the frequencies right next to each other.

429
00:52:05,750 --> 00:52:11,350
So that was obvious of the equivalence. The difference isn't big, but it's not.

430
00:52:11,710 --> 00:52:16,870
The thing that wins is percentages. All the. Really surprised me.

431
00:52:17,120 --> 00:52:22,880
And the best explanation I have is this. If you're only communicating one number.

432
00:52:24,320 --> 00:52:27,410
Having multiple formats is probably helpful because you have one.

433
00:52:27,500 --> 00:52:29,120
Each person has only one task,

434
00:52:29,120 --> 00:52:35,240
they have one number to process and helping them think through what does that in a frequency term or a probability sense is helpful?

435
00:52:36,910 --> 00:52:39,910
We got, I don't know, 25 numbers here.

436
00:52:41,130 --> 00:52:51,180
The volume of effort, the amount of quantity, the numerical data, the process makes this much more harder, especially for less numerate people.

437
00:52:52,100 --> 00:52:59,360
So here, if you only go to percentages for cutting out half the numbers, more than half the numbers because the frequencies are two numbers,

438
00:53:00,560 --> 00:53:05,090
and that that simplicity makes it easier, somewhat easier for them to process.

439
00:53:05,630 --> 00:53:11,720
So I am not going to push too hard on the question of percentages versus frequencies.

440
00:53:12,800 --> 00:53:16,370
You make your choices there. Sometimes frequencies make more sense.

441
00:53:17,540 --> 00:53:25,820
Sometimes percentages make more sense. But again, I attention to, you know, what level of decimal point do you need to be processing?

442
00:53:26,580 --> 00:53:31,400
Some of the relevant risks are going to be really small. Maybe working with out of a thousand denominator makes sense.

443
00:53:32,150 --> 00:53:35,480
But again so very your denominator.

444
00:53:38,010 --> 00:53:43,970
Okay. So this is just a curious question because their sample size was 20,000.

445
00:53:44,040 --> 00:53:49,320
Yes. Do you think the frequencies out of 20,000?

446
00:53:50,820 --> 00:53:58,270
So my short answer is no, because 20,000 is not a natural reference point.

447
00:53:58,320 --> 00:54:04,500
Like, I can't imagine 20,000 very easily. And it's not it's even worse because it's not round.

448
00:54:04,500 --> 00:54:13,020
So now. Now, like if I have 12,000 out of 20,000, I have to do the math in my head to recognize that that 60% like,

449
00:54:13,020 --> 00:54:16,200
no, we want to minimize that kind of work that somebody is doing.

450
00:54:17,040 --> 00:54:20,790
I like the fact that they talk about how from where are these data derived?

451
00:54:20,890 --> 00:54:29,250
Like, I will have more trust in these data because they say it came from a study of 20,000 people than if it came from a study of 300 people.

452
00:54:30,960 --> 00:54:36,300
But it's really at the trust level, not in terms of I don't need the absolute rates as a patient.

453
00:54:36,660 --> 00:54:42,209
What I need is to understand the relative magnitudes of these different things and the absolute

454
00:54:42,210 --> 00:54:46,350
magnitude of the benefit or the risk that I would be getting from taking this action.

455
00:54:48,940 --> 00:54:54,360
Let me repeat here. If it's similar by this notice,

456
00:54:54,360 --> 00:55:03,629
they might say it has to be perfectly exact that they use this about here to give them some wiggle room like this could be .46 versus .52.

457
00:55:03,630 --> 00:55:08,610
And they waved their hands and said, okay, it's about point five and we're going to treat them like we want thumbs up.

458
00:55:08,640 --> 00:55:11,760
Please put it in the middle.

459
00:55:11,910 --> 00:55:19,440
They make it really obvious that somebody should treat it as the same. Each separate thing.

460
00:55:22,080 --> 00:55:31,980
The broken apart. And by the way, let me point out this one. Notice that they separate having a heart attack from dying from a heart attack.

461
00:55:34,180 --> 00:55:39,430
Which is actually kind of important here because you have a difference in the likelihood of a heart attack,

462
00:55:39,430 --> 00:55:42,890
but you don't have a difference in the likelihood of dying for heart attacks.

463
00:55:43,090 --> 00:55:46,810
And that's kind of relevant for whether or not you want to take this medication.

464
00:55:50,270 --> 00:55:51,679
Just to make that even more concrete.

465
00:55:51,680 --> 00:56:03,140
Think about this in the context of cancer treatment in which treatments may differ in terms of their likelihood of cancer recurrence.

466
00:56:05,560 --> 00:56:09,879
But if the cancer is relatively highly treated, highly treatable,

467
00:56:09,880 --> 00:56:14,080
especially if you're if your recurrence is going to be caught through monitoring tests,

468
00:56:14,590 --> 00:56:19,930
there are multiple situations in which you may have a higher chance of cancer recurrence,

469
00:56:19,930 --> 00:56:24,040
but no difference in mortality depending upon which choice, which path you take.

470
00:56:24,890 --> 00:56:30,330
And that's kind of important to make sure somebody understands, yes, your kids are right, come back, but it's not necessarily more likely to kill you.

471
00:56:31,080 --> 00:56:35,840
This is an important piece of this. All right.

472
00:56:38,540 --> 00:56:43,430
I'll just mention this last piece down here at the bottom. How long has this been in use?

473
00:56:44,080 --> 00:56:52,280
Oh, it's probably going to use this quantitatively. No, but is it relevant as we think about how much do I trust?

474
00:56:53,390 --> 00:57:01,430
This information, whether or not this is something that used for decades versus whether or not this medication was just created six months ago.

475
00:57:01,880 --> 00:57:05,300
Yes, that's also a relevant feature.

476
00:57:07,090 --> 00:57:08,840
Yeah. I just have a question, right,

477
00:57:08,860 --> 00:57:17,920
because there's a couple of examples there where they talk about a difference of point 7% more points, 6% fewer points, 7% more.

478
00:57:18,730 --> 00:57:21,870
And you talked about how there's this hand-waving going on, you know,

479
00:57:21,910 --> 00:57:32,380
potentially what what defined significance like is point five significant is point for significant and a very fair question.

480
00:57:32,720 --> 00:57:38,590
And the answer is different groups do it different ways and I think appropriately so in the sense that.

481
00:57:40,900 --> 00:57:47,420
In some contexts, the amount of precision you have on these data may be variable.

482
00:57:47,460 --> 00:57:51,400
Like maybe you can. In this case, they are saying the data come from 20,000 people.

483
00:57:51,400 --> 00:57:59,800
They're estimating down to a 10th of a percentage point. In other contexts, it may be that you get down to a whole percentage point.

484
00:58:01,090 --> 00:58:04,930
So you're going to make different choices for what you're going to treat as equivalent or not.

485
00:58:07,270 --> 00:58:15,190
In this case we're talking about. A medication where the all of the numbers are relatively small.

486
00:58:15,190 --> 00:58:19,820
I mean, north of here is the highest number on this page is the 7.1 here.

487
00:58:20,170 --> 00:58:23,800
And there's not 60% here. There's no 20% here.

488
00:58:24,310 --> 00:58:29,380
So to the extent that you're highlighting the fact that differences exist, you've got to get down to fairly small differences.

489
00:58:30,430 --> 00:58:38,890
But in the next class of the following one, I'm going to give you a couple examples of decision aids.

490
00:58:39,910 --> 00:58:46,540
These types of tables, you would see at least one in which anything that's less than 1% is labeled rare.

491
00:58:50,400 --> 00:58:56,280
Just less than 1%. And you might say, well, wait a second, there are some differences there.

492
00:58:56,790 --> 00:59:04,900
Yeah, there are. But they're so small that in that context, they chose to say, you know, anything that's that small, we're not going to worry about.

493
00:59:07,260 --> 00:59:10,319
Sometimes that's fair. Sometimes I think that's ethically problematic.

494
00:59:10,320 --> 00:59:18,450
I don't think there's a there's a hard and fast rule. Remember what I said previously.

495
00:59:18,670 --> 00:59:23,110
There is no such thing as an unbiased communication like this.

496
00:59:23,860 --> 00:59:31,480
You're making choices, and the choice about whether or not you call something similar or different is one of those choices.

497
00:59:32,500 --> 00:59:36,280
You're going to either highlight or minimize. Differences.

498
00:59:38,110 --> 00:59:41,980
And you're going to have to take a stand as to what you think is ethically appropriate given the data that you have.

499
00:59:45,530 --> 00:59:53,780
But I might be more comfortable not worrying about the point 7% on the rash that I am about 2.6% on heart attack.

500
00:59:56,900 --> 01:00:06,680
There are different outcomes. So the question I want to ask you, first of all, so any questions more about this particular format?

501
01:00:07,520 --> 01:00:09,650
And then related to that is I just want to.

502
01:00:11,470 --> 01:00:18,460
Are the questions that are coming up for you as you're thinking about, okay, so how would I do this for a different domain?

503
01:00:18,550 --> 01:00:22,629
Because let's talk about that. Like I could bring in all kinds of other examples.

504
01:00:22,630 --> 01:00:27,600
I've got one other example I will show you in a moment, but I want to make sure we have that conversation.

505
01:00:27,810 --> 01:00:30,430
Yeah, this is a little bit off topic,

506
01:00:30,430 --> 01:00:37,240
but I've got a couple blocks to go and it's also like the assignment and then topic assignment for the next assignment.

507
01:00:38,470 --> 01:00:41,860
We talked earlier, just because we have numbers, we can always use them.

508
01:00:41,950 --> 01:00:45,050
Yeah. In terms of like this examples or something.

509
01:00:45,070 --> 01:00:50,860
Is there like is there an example you can give us on that of.

510
01:00:52,620 --> 01:00:58,470
So let me tell you a story as an answer to that.

511
01:00:59,700 --> 01:01:09,410
But ten or 15 years ago, I was approached I had a couple colleagues approached by an obstetrics professor who had asked about his.

512
01:01:10,430 --> 01:01:18,920
Who was really interested in helping pregnant women who had already given birth once through a cesarean section.

513
01:01:20,110 --> 01:01:29,480
To make a decision about the pros and cons of pursuing a second cesarean next year versus trying to attempt vaginal birth.

514
01:01:30,530 --> 01:01:43,460
And there are. A long list of potential different risks, both to the mother and to the fetus based upon that choice.

515
01:01:45,950 --> 01:01:51,260
We have we sat down with them and we ended up with a list of like 15 different things.

516
01:01:53,640 --> 01:01:59,010
And I met her at the time and I sort of looked at each other and went,

517
01:01:59,460 --> 01:02:08,250
we made a table with like 15 things that ran down onto the second page of all of this quantitative, like precise levels.

518
01:02:09,420 --> 01:02:13,860
Basically we're going to scare every pregnant woman to say, you know, I can't give birth.

519
01:02:13,870 --> 01:02:20,980
This is not too dangerous. It's not actually going to be beneficial for them to have so many different things.

520
01:02:23,660 --> 01:02:28,770
But then through we're back in the we'll wait a second. What gives us the right to say that this is the thing?

521
01:02:28,770 --> 01:02:35,150
We're going to talk about it. This is the thing that we're not going to talk about. There's no easy answer here.

522
01:02:39,350 --> 01:02:47,089
Where I stand and this is my personal sort of ethical line is we want to sort of say, knowing what I know,

523
01:02:47,090 --> 01:02:54,830
what are the things that are likely to have the most important impact on somebody's choice?

524
01:02:56,740 --> 01:03:04,420
One of the things that brought together babies, not because they're medically similar, but because they're emotionally similar.

525
01:03:05,290 --> 01:03:15,060
So it wouldn't be a crazy idea to have to group a variety of things like severe side effects on the fetus.

526
01:03:15,080 --> 01:03:23,950
It's a bunch of really random things, but a group of under severe and the total probability of that is like 1% or something like that.

527
01:03:23,950 --> 01:03:26,740
So it's not like separating them out is going to be that helpful.

528
01:03:28,490 --> 01:03:32,830
Can somebody come back to me and say, Yeah, but this is very different than that in my mind.

529
01:03:32,840 --> 01:03:36,620
Of course they could. So my answer to you is, I don't have an answer.

530
01:03:36,890 --> 01:03:38,870
You're raising a really important question,

531
01:03:39,500 --> 01:03:45,560
which is how many things when when do you need to have that detailed quantitative information and when do you not?

532
01:03:47,480 --> 01:03:59,080
It depends. In fact, I think one of our roles as communicators is in some sense the translator between the expert and the patient or the user.

533
01:03:59,770 --> 01:04:02,560
We listen to the expert and we say that, you know, the expert says, you know,

534
01:04:02,650 --> 01:04:06,460
I pay attention to this one and this one and this one of these are really important for patients.

535
01:04:07,210 --> 01:04:19,380
Well, and then we reflect that it is not. Other thoughts on this while you're thinking about domains that you might use again.

536
01:04:23,280 --> 01:04:26,370
Well, there's one thing. Please interrupt me if something comes to you.

537
01:04:26,380 --> 01:04:33,330
But there's one full conversation I want to have that is related to this, but not in this example.

538
01:04:33,990 --> 01:04:40,890
So they have set this up as a really simple.

539
01:04:42,950 --> 01:04:47,030
Family risk and benefit tradeoff. What's the same here?

540
01:04:48,260 --> 01:04:55,280
You're taking a pill that the experiential elements are the same.

541
01:04:55,820 --> 01:05:08,060
And they say that type being held in combination with many of the choices that you made, have other dimensions like is it a pill versus an injection?

542
01:05:08,210 --> 01:05:12,680
How often do I have to take this? How big of a deal is it if I'm late taking it?

543
01:05:12,690 --> 01:05:16,460
Like, where do I have to do it?

544
01:05:16,600 --> 01:05:20,800
Like, do I have to travel someplace? Cost.

545
01:05:21,220 --> 01:05:32,100
Cost often enters into these kinds of conversations. Other non consider non probability type dimensions.

546
01:05:33,240 --> 01:05:37,890
You can and should incorporate those into your table.

547
01:05:40,250 --> 01:05:43,480
Don't sort of like reserve those away from the probabilities.

548
01:05:43,490 --> 01:05:55,149
They are just as important to the trade off as anything else is. And I will show you an example when we get to the decision aid part that incorporates

549
01:05:55,150 --> 01:05:58,330
different kinds of dimensions so that you'll have something to work with on this.

550
01:05:58,330 --> 01:06:06,600
But. As good as this is, it's it's very much designed for a prescription medication comparison.

551
01:06:06,900 --> 01:06:12,000
And I want to recognize that as you generalize it to other kinds of decisions, other kinds of dimensions.

552
01:06:12,000 --> 01:06:19,190
Come in here to. Now I want to show you one more example.

553
01:06:20,420 --> 01:06:25,340
And unfortunately, Carolyn can't be here because this came from her.

554
01:06:27,690 --> 01:06:35,069
This is just a one page document that was created to talk about the difference in

555
01:06:35,070 --> 01:06:42,350
pre-natal surgical repair versus postnatal surgical repair for spinal defects in fetuses.

556
01:06:43,530 --> 01:06:51,180
And this is reporting of the study. But it's in some sense a kind of decision support table and notice the way it's designed.

557
01:06:55,480 --> 01:07:02,380
They don't do everything that we just saw. But there's a lot of things here. So each individual thing is broken down differently.

558
01:07:04,290 --> 01:07:08,670
Including a non probability dimension which is really important, like birth weight.

559
01:07:08,700 --> 01:07:14,670
Okay, so that's going to be an important thing to pay attention to too, and that's quantitatively represented here.

560
01:07:17,340 --> 01:07:23,700
I would have liked for them to have done the math in some way so that we could see how big of a difference it is.

561
01:07:25,810 --> 01:07:33,770
But you have a clear trade off in the sense that. This is worse on this dimension.

562
01:07:33,780 --> 01:07:36,920
Clearly better on this, does it worse on this dimension?

563
01:07:36,930 --> 01:07:40,690
It. Walking. Walking independently.

564
01:07:40,690 --> 01:07:42,849
We want this. Oh, that's another thing I want to highlight for.

565
01:07:42,850 --> 01:07:48,309
The difference is you notice that I'm having to work to sort of look at each of these dimensions and say, is that a good or a bad thing?

566
01:07:48,310 --> 01:07:53,410
I shouldn't have to do that. It should be really obvious what's a good thing and what's a bad thing.

567
01:07:56,220 --> 01:08:01,320
So I have to sit here. Okay, that's bad. That's bad, you know, etc.

568
01:08:01,710 --> 01:08:08,820
But we've got clear that trade off in the sense that some things are higher and some things are higher on the other, so that you're starting to help.

569
01:08:08,820 --> 01:08:16,920
Somebody thinks like if a mom was facing this decision, do we want to do pre-birth surgery or do we want to wait until after birth?

570
01:08:17,520 --> 01:08:23,460
This would start to help them think about the tradeoffs. What is it that they care more about?

571
01:08:29,310 --> 01:08:37,110
So. I'm just showing you this as another example of like applying this kind of tabular format to a different decision.

572
01:08:41,180 --> 01:08:48,509
So let me just pause there. I hope you're feeling like.

573
01:08:48,510 --> 01:08:56,290
Okay, I can see how I might do this. Still feeling uncertain, like I don't know what to do about that.

574
01:08:56,320 --> 01:08:59,700
First question was great. That's that's exactly what I want the rest of today to be about.

575
01:09:03,390 --> 01:09:10,860
Yes. I think I do have a question like in terms of like the time frame and like giving context for the risks within a certain time.

576
01:09:11,160 --> 01:09:14,910
Yeah. You're going to be like extrapolating a lot of data from different sources that have like.

577
01:09:16,560 --> 01:09:21,570
Potentially this sort of. That's one more context.

578
01:09:21,580 --> 01:09:28,860
So there are certain types of decisions in which the risks or benefits all manifest in a very narrow time frame,

579
01:09:28,870 --> 01:09:36,960
like it either happens to you or it doesn't happen to you and you're done. There are other kinds of risks where time really matters.

580
01:09:37,710 --> 01:09:43,790
So if we're talking about heart attacks. It's like, well, is that heart attacks over one year or ten years?

581
01:09:43,800 --> 01:09:47,910
I'm going to have more likely to have an attack in a ten year time span than a two year time span.

582
01:09:50,610 --> 01:09:55,800
In question about which time frame is really another one of these judgment calls,

583
01:09:58,680 --> 01:10:04,560
like if I do a if we go to Congress or actually, here's another good example.

584
01:10:06,210 --> 01:10:17,220
I was on a project on developing a decision support tool for women at high risk of developing their first breast cancer.

585
01:10:18,960 --> 01:10:22,200
It could be because of genetic factors. It's because of family history or whatever.

586
01:10:24,690 --> 01:10:30,720
Many breast cancers are essentially driven by estrogen.

587
01:10:32,390 --> 01:10:41,060
And so one way to prevent breast cancer in women who have a high enough risk is to suppress estrogen, their medications that are available to do that.

588
01:10:42,350 --> 01:10:50,690
And so at the time, there were two medications that were approved to be prescribed to women who had not yet had breast cancer.

589
01:10:51,080 --> 01:10:54,530
To reduce the chance that they develop breast cancer, Tamoxifen and Raloxifene.

590
01:10:55,910 --> 01:11:00,700
And our task was to develop a table like this, a decision made.

591
01:11:01,200 --> 01:11:04,160
Okay. So here's what happens if you take if you don't take either one of these,

592
01:11:04,460 --> 01:11:07,970
because what happens if you take Tamoxifen is what happens if you take the likes of you.

593
01:11:07,970 --> 01:11:11,410
And by the way, notice that's three, not two.

594
01:11:11,780 --> 01:11:15,350
That made it a much more complicated and nightmarish thing to decide.

595
01:11:15,650 --> 01:11:20,090
I will at some point try and remember to show you the table we came up with, but it was not easy.

596
01:11:20,900 --> 01:11:28,250
But to go to your point. Breast cancer risks are usually measured, both calculated and thus.

597
01:11:28,250 --> 01:11:35,240
The original trial data that showed the effectiveness of reducing breast cancer risk was all in a five year time frame.

598
01:11:35,960 --> 01:11:40,730
So we organize the information in that five year window because that's the data we had.

599
01:11:41,760 --> 01:11:46,020
One of the things that we got back from many people who used it was they didn't want to know five year.

600
01:11:46,020 --> 01:11:50,579
They wanted to look at a lifetime risk. I appreciate that.

601
01:11:50,580 --> 01:11:55,770
We didn't have the data. Nobody had the data like he was going to be 30 years before we had that data.

602
01:11:56,550 --> 01:12:05,700
So there's this sense of, you know, what you have and then there's what the audience wants and you have to make a choice somehow and talk about that.

603
01:12:07,230 --> 01:12:12,060
But very salient issue. If you look at the breast cancer risk calculators out there, for example,

604
01:12:12,600 --> 01:12:15,899
most of them will give you five year risk and then they'll give you a lifetime risk.

605
01:12:15,900 --> 01:12:19,020
And the five year risk looks really small. The lifetime risk looks really big.

606
01:12:19,560 --> 01:12:27,180
And that's part of the tension, because people's emotional reactions to essentially the same output is very different depending on the time frame.

607
01:12:29,420 --> 01:12:36,470
Yep. Um, so you mentioned that like for certain medical decisions that we like to use,

608
01:12:36,620 --> 01:12:45,199
the decision might be like other dimensions to it and I guess like so the other one

609
01:12:45,200 --> 01:12:51,109
was like comparing to drugs and this is comparing like pre and post surgical care.

610
01:12:51,110 --> 01:13:01,069
But like if you had two treatment options that were significantly different and like we experience, like you were saying,

611
01:13:01,070 --> 01:13:08,060
like, are there certain things that should still be like standardized, um, when comparing the two, yeah.

612
01:13:08,660 --> 01:13:12,560
So it, and what it just like depend on what those are.

613
01:13:12,890 --> 01:13:16,850
So let me give you a concrete example. I don't know.

614
01:13:18,140 --> 01:13:27,740
Somebody has a major shoulder injury and has a choice between surgery or followed

615
01:13:27,740 --> 01:13:32,090
by physical therapy versus just doing physical therapy only for recovery.

616
01:13:32,870 --> 01:13:36,120
Obviously, radically different, like you're either having surgery or not.

617
01:13:37,340 --> 01:13:46,810
But there are dimensions which are common. Like so you could look at, you know, percentage of people reporting normal function in six months.

618
01:13:48,200 --> 01:13:49,410
That would be a common thing.

619
01:13:51,020 --> 01:13:59,900
You could talk about likelihood of needing to have a second, a follow up surgery or other additional procedures later on.

620
01:14:01,040 --> 01:14:06,450
You could talk about, frankly, how quickly you would start physical therapy because, you know,

621
01:14:06,560 --> 01:14:12,110
the surgery essentially allows you to start something earlier because you have to if you don't have surgery,

622
01:14:12,110 --> 01:14:16,460
you have to wait for a longer period of time before your body's healed enough to actually start physical therapy.

623
01:14:16,940 --> 01:14:21,230
All of those are not probability dimensions, but they're all super important in that context.

624
01:14:21,800 --> 01:14:25,520
So yeah, there's some judgment, but that's the kind of thing that I'm talking about.

625
01:14:29,840 --> 01:14:37,219
All right. We will obviously come back to this, but let's spend the last 5 minutes of today's class talking about the question of the day,

626
01:14:37,220 --> 01:14:42,380
which I put up here, which is having now looked at this summary table kind of format.

627
01:14:43,400 --> 01:14:48,710
I want you to think about what your objectives are. What defines success?

628
01:14:49,910 --> 01:14:53,810
What do you want to make sure somebody is getting from a summary table?

629
01:14:58,130 --> 01:15:01,190
Is it something simple?

630
01:15:01,250 --> 01:15:08,510
Is it a member of a specific number or is like guided for a moment and talk sort of like, how would you know whether or not you're succeeding?

631
01:15:09,820 --> 01:15:13,690
When you give somebody something like this and how does that get feedback into what do

632
01:15:13,690 --> 01:15:18,760
you think the important design elements are like the historical reception for that?

633
01:15:18,860 --> 01:15:24,080
Yeah, we're going to keep coming back to this for the next couple of weeks, but I really watch it as a soup.

634
01:15:24,130 --> 01:15:29,020
Like it's not just did you stick the numbers there, but are you achieving the communications outcome?

635
01:15:30,220 --> 01:15:44,870
Like. You can make that person so successful.

636
01:15:45,920 --> 01:15:54,770
You know, it's a lot easier.

637
01:15:55,180 --> 01:16:19,800
Yeah. Yeah. You talk like you don't like what I'm going to talk about this week of animation for that.

638
01:16:19,830 --> 01:16:36,360
But I think people think I was like the driver of this, you know, like, you know what?

639
01:16:38,940 --> 01:17:17,670
Like the early, you know, like given how they treat this, like, you know, how it was, put it on the and everything is legendary for those things.

640
01:17:23,270 --> 01:17:38,830
And also going back to the fact that you didn't like it, not so much.

641
01:17:39,740 --> 01:17:49,639
I don't I don't get especially like, you know, translate.

642
01:17:49,640 --> 01:18:03,110
Yeah. I like I like to make it look like.

643
01:18:03,360 --> 01:18:18,260
Yeah. Yeah. I don't know. Welcome to the third quarter.

644
01:18:18,890 --> 01:18:42,800
I might try to make out with you. I think that's why it's so hard to explain anything.

645
01:18:42,830 --> 01:18:57,100
I do. Yeah, I guess. I think I know there's a bit of a heart attack.

646
01:19:01,870 --> 01:19:07,100
Yeah. Is it, like, just a minor accident?

647
01:19:08,050 --> 01:19:19,720
I heard about it. I haven't heard it before.

648
01:19:22,080 --> 01:19:47,780
Not one. I don't know if I can find anything else I look like I just.

649
01:19:50,140 --> 01:20:04,750
I think I just hand them a thing and, like, on their way out or anything like that.

650
01:20:04,760 --> 01:20:08,840
It's like, oh, look at that. Yeah.

651
01:20:09,310 --> 01:20:27,950
Yeah. It's hard to. To go from person to person.

652
01:20:27,950 --> 01:20:32,000
Would really expect something like that.

653
01:20:32,010 --> 01:20:37,720
Yeah. The short answer.

654
01:20:37,760 --> 01:20:44,520
Yes, it does. Except that I don't know, they're.

655
01:20:48,020 --> 01:21:05,440
You guys like me really don't know anything about that.

656
01:21:05,740 --> 01:21:19,250
But really, it was like the winds were, you know, dying and you're out like, you know, five Buddhist monks like that.

657
01:21:23,400 --> 01:21:44,889
So I thought it was you know, when I went up, you know, I realized like, oh, it's one thing to write about 1000 feet.

658
01:21:44,890 --> 01:22:23,470
And I know how it allows me to find out and make sure everything is all right and.

659
01:22:23,790 --> 01:22:35,109
And you haven't read it. I mean, I answered that like, yeah, you let me do it any time in the last half hour.

660
01:22:35,110 --> 01:22:38,530
That's what made everybody wear a couple of minutes past that.

661
01:22:40,150 --> 01:22:43,750
I will not see you on Tuesday. I will see you on Thursday.

662
01:22:44,610 --> 01:22:51,950
Meantime, the readings I am now remembering, the readings for next class are examples of decision is that you want to get engaged with

663
01:22:51,950 --> 01:22:59,650
this further looking at those to be yeah you put styles but very common approaches today.

664
01:23:03,310 --> 01:23:12,210
Thank you. No question.

665
01:23:13,150 --> 01:23:23,470
Good luck with the next thing you know.

666
01:23:24,910 --> 01:23:48,970
So I was lucky because I know I was like all and I don't know what I.

667
01:23:54,820 --> 01:23:59,080
So I don't know. A lot of people have reason to like.

668
01:23:59,080 --> 01:24:03,960
I just want you to see nothing quite like that.

669
01:24:04,000 --> 01:24:10,120
You know, that you will have a nice meeting.

670
01:24:10,150 --> 01:24:15,490
You are you know, all those are two different.

671
01:24:15,640 --> 01:24:19,870
You have such role models to express themselves.

672
01:24:21,850 --> 01:24:25,950
Yeah. That goes back to the question of like let's narrow it down for common.

673
01:24:25,990 --> 01:24:29,350
I know how to get there. I don't know.

674
01:24:29,950 --> 01:24:37,089
I can't really answer number. You have these actions, it's essentially what you're talking about.

675
01:24:37,090 --> 01:24:46,040
What's a shoulder surgery? Except for others. Yeah, but it's very different outcomes between judges by first thing now.

676
01:24:46,600 --> 01:24:53,560
Okay we didn't find out about like you don't get not even been Sunday night.

677
01:24:54,050 --> 01:24:58,620
Wow that's yeah that's kind of what happened on that.

