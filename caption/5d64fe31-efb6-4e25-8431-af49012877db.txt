1
00:00:29,170 --> 00:00:40,240
I have no with.

2
00:00:49,940 --> 00:01:08,000
I want to know what. Chapter.

3
00:01:19,080 --> 00:01:22,690
Point. Yeah.

4
00:01:25,050 --> 00:01:28,510
Oh, yeah. Yeah. You got any point by which.

5
00:01:28,520 --> 00:01:34,520
Let's be honest, that's the thing. So, like, it's been great.

6
00:01:39,440 --> 00:01:43,790
Okay. Good afternoon. I have your midterm here. I will return it and lecture.

7
00:01:44,270 --> 00:01:55,880
So I already post some of the key points in the form for this solution of return and be happy to discuss the midterm if you have any questions.

8
00:01:57,020 --> 00:01:59,089
Okay. Here's the plan for today.

9
00:01:59,090 --> 00:02:09,110
I want to finish up last a few slides about extend the ESR model and then I will start the new topic of spatial data analysis.

10
00:02:09,200 --> 00:02:11,980
And that's that's it.

11
00:02:11,990 --> 00:02:24,440
So some of you request the meetings with me to discuss the data, capture things I available after the lecture until 330, you know, the seminar time.

12
00:02:24,440 --> 00:02:34,380
And so if you want to stop by and certainly be happy to talk about your data capture issue questions.

13
00:02:36,080 --> 00:02:48,730
Okay. So we have been talking about the how to use this extended use years, our multimode computer model to, you know,

14
00:02:48,860 --> 00:02:57,589
deal with some of this prediction the early stage of, you know, the ah to the outbreak of the pandemic.

15
00:02:57,590 --> 00:03:06,530
And so as I said that the simple modification that you could do here is that to think about

16
00:03:06,530 --> 00:03:13,760
the time varying intervention programs that essentially change the transmission rate.

17
00:03:14,480 --> 00:03:27,550
Of course, you can also introduce some of the improvement of treatment or hospitalization that can change the recovery rate or something like that,

18
00:03:27,560 --> 00:03:34,880
whichever you think that would be useful to modify this constant recover rate or constant

19
00:03:35,690 --> 00:03:41,690
transmission rate in the infectious disease system that could be incorporate into this,

20
00:03:42,740 --> 00:03:55,250
you know, multi compartment model. Then you can use, um, CMC algorithm to really estimate those underlying prevalence using this observed data.

21
00:03:55,460 --> 00:04:03,710
Okay. So what we did in the first sort of the modification of the model is really introduce the

22
00:04:05,150 --> 00:04:16,460
time varying intervention program according to what has happened in the in Hubei province.

23
00:04:16,730 --> 00:04:30,320
So there are a number of this sort of very large scale intervention program implement province wide is national provincial level,

24
00:04:31,190 --> 00:04:43,520
province level or provincial intervention and that so you can have a very sort of sharp sort of the strict program or,

25
00:04:44,180 --> 00:04:44,569
you know,

26
00:04:44,570 --> 00:04:58,600
intervention to ask people are quarantined or isolated in their home or to reduce the transmission neural activity in the community and so on,

27
00:04:58,640 --> 00:05:07,550
so forth. Okay. So this job could be something related to magnitude of the intervention or you can think about general population level,

28
00:05:08,180 --> 00:05:11,569
sort of mild graduate sort of intervention.

29
00:05:11,570 --> 00:05:18,920
When people have more and more knowledge about the infectious disease, they become more aware of the,

30
00:05:19,640 --> 00:05:25,280
you know, the disease and then and know more ways to protect themselves.

31
00:05:25,670 --> 00:05:31,040
So this is happening gradually as a mild intervention program.

32
00:05:31,040 --> 00:05:38,359
So if you incorporate them then into this mode, confirm permanent, you can look at that the, you know, the prediction.

33
00:05:38,360 --> 00:05:51,170
And so that the intervention program does help to sort of reduce the number of infections or even reduce the mentality.

34
00:05:52,430 --> 00:06:01,940
So from government point of view, this, you know, you meet sort of use this kind of thing as a evaluation of policies.

35
00:06:01,940 --> 00:06:08,400
Suppose that you have different types of policy quantified by different like this type.

36
00:06:08,440 --> 00:06:09,710
It's more like a prior.

37
00:06:09,920 --> 00:06:21,320
It's things like you set it up and what the level of this you want to sort of implement in population that you can see what are the the.

38
00:06:21,970 --> 00:06:38,450
Timing of change points in this infectious disease dynamics and how this sort of the average for that pandemic pattern can finally finish up,

39
00:06:38,450 --> 00:06:42,700
would you not, to to to come to the end of this thing?

40
00:06:42,700 --> 00:06:52,470
So so that's one thing we did in the software that you can play with that not everything is about create this, you know,

41
00:06:52,540 --> 00:06:57,669
quarantine compartment because at the time in February, China,

42
00:06:57,670 --> 00:07:05,890
there was a very strong intervention program to put people in the quarantine sort of situation.

43
00:07:06,400 --> 00:07:14,290
And then suddenly that number of susceptible individual has been dramatically reduced.

44
00:07:14,580 --> 00:07:19,809
Okay, so so this fight function is the function that over time,

45
00:07:19,810 --> 00:07:26,799
how many people have been put into quarantine situation and then this quarantine

46
00:07:26,800 --> 00:07:33,430
compartment can really reduce number of people at risk so that it reduce the,

47
00:07:33,910 --> 00:07:42,489
you know, the infection. So this is another way to handle this quarantine, you know,

48
00:07:42,490 --> 00:07:52,990
particularly when you have very strong sort of intervention program now the quarantine sort of option in the control of the infectious disease.

49
00:07:53,290 --> 00:07:59,230
So you can modify this as another compartment, the Q compartment,

50
00:07:59,710 --> 00:08:09,790
and having this sort of fight as this sort of the rate of transmission from this susceptible at risk to quarantine.

51
00:08:10,150 --> 00:08:14,590
So you create another compartment to, you know,

52
00:08:15,070 --> 00:08:26,550
implement this mode compartment so you can have different level of this sort of quarantine policy, right?

53
00:08:26,560 --> 00:08:31,510
So, so, so you can have modified policy, you can have strong policy.

54
00:08:31,510 --> 00:08:36,579
You want put more people into home isolation and so on, so forth.

55
00:08:36,580 --> 00:08:48,940
But this is not a continuous sort of quarantine because you still only reduce number of people under risk in the in the system.

56
00:08:48,940 --> 00:08:59,979
So you sort of forcefully some individuals out of this system rather than the exit from the system through this removal removal remove department

57
00:08:59,980 --> 00:09:10,150
that they are sort of terminated their participation in this infectious disease system in the early stage at susceptible compartment.

58
00:09:11,020 --> 00:09:23,290
So this is what this sort of the jump points happens due to this fight function to, you know, reduce number of the people at risk.

59
00:09:23,620 --> 00:09:30,380
So you can still work on this into this system and then you do the prediction.

60
00:09:30,580 --> 00:09:36,700
Okay. As well. Yeah. So this software can allow you to do that as well.

61
00:09:37,270 --> 00:09:49,060
And the why you show, we notice that in the data when we do the analysis is there is a huge jump from either one particular day.

62
00:09:49,870 --> 00:09:58,330
So we feel that probably there is some kind of issue in the reporting system in the Hubei province, CDC.

63
00:09:59,140 --> 00:10:04,390
It's not possible just one day. There is a huge jump in terms of number of infections.

64
00:10:04,570 --> 00:10:11,709
Okay. So maybe there was some kind of mis reporting issue before this day and suddenly

65
00:10:11,710 --> 00:10:20,030
that there is a kind of a sort of modification or jump out for to come up,

66
00:10:20,050 --> 00:10:23,230
is that right? Number of the infected individuals.

67
00:10:23,950 --> 00:10:35,650
So so that looking at this particular sort of under-reporting issue before this day, we tried to calibrate this using a calibrated function.

68
00:10:36,070 --> 00:10:42,970
So what we did here is really trying to smooth out this jump point.

69
00:10:43,150 --> 00:10:48,940
Okay. So we want to smooth out this jump point using this exponential growth curve.

70
00:10:49,780 --> 00:10:59,270
Okay. So post that this this block the curve suggest the by the black dots or you know,

71
00:10:59,320 --> 00:11:08,440
the curve suggest the by the black dots is governed by this exponential function, exponential increase in function.

72
00:11:09,100 --> 00:11:16,390
So now you are trying to just calibrate the are the actually number of infections using this curve.

73
00:11:17,170 --> 00:11:21,300
So given that at a time T zero then.

74
00:11:21,890 --> 00:11:32,100
A number of the you know at a time t zero then you the the A will you call two minus B rate.

75
00:11:32,100 --> 00:11:35,940
So when t is equal to zero, then you have zero infections.

76
00:11:36,420 --> 00:11:40,680
Then A would you call two minus B or B equals two minus eight.

77
00:11:41,430 --> 00:11:53,370
So you want to minimize A and lambda so that you can have this, you know, the best sort of the prediction calibration error.

78
00:11:53,790 --> 00:11:58,559
So what we're trying to do here is to minimize the one step ahead ex correlation

79
00:11:58,560 --> 00:12:05,610
of error for Feb 13 t time t32 but it means of like a range multiplier.

80
00:12:06,780 --> 00:12:18,900
So we want this curve to be constraint on the date of, you know, February 12, but trying to do one step escalation of this,

81
00:12:20,160 --> 00:12:28,560
you know, minimize the parameters to reach the minimum sort of one step ahead escalation.

82
00:12:29,340 --> 00:12:35,820
So so if you do that, then you can create a calibrated curve for that.

83
00:12:36,990 --> 00:12:46,530
You know, this underreporting issue here, blue dots are actual observed one clearly on February 12th, there was 13 there.

84
00:12:47,310 --> 00:12:50,639
There is a big jump here is not really possible.

85
00:12:50,640 --> 00:12:55,490
Like one day you have tremendous sort of a increase of number of infected.

86
00:12:55,980 --> 00:13:07,860
There must be some underreporting happens cumulated up to this time and suddenly the re correct is so reporting system to to another much much

87
00:13:07,860 --> 00:13:18,659
higher level than there was you know the jump so if you believe that this growth is a continuous growth that this component is not possible.

88
00:13:18,660 --> 00:13:33,300
So so using this growth model we proposed using this calibration sort principle or, you know, formula, or then you can create this red dot.

89
00:13:33,660 --> 00:13:43,410
Okay, so here they do this one supposed to be correct afterwards and anything before that is subject to some kind of underreporting issue.

90
00:13:44,010 --> 00:13:54,809
So you want to make up this gap in the way that you sort of calibrated to the the observed the value by this RET points,

91
00:13:54,810 --> 00:14:06,450
which is created by this curve. Okay. So that after you do this and then you, you, you suddenly feel that the cumulative number of infection,

92
00:14:07,920 --> 00:14:11,100
in fact, that cases becomes a lot smoother and more reasonable.

93
00:14:11,460 --> 00:14:16,590
And if you look at the data increments, the daily number of the changes,

94
00:14:16,800 --> 00:14:23,460
suddenly on the February 12th, there is a huge shown and then going down here.

95
00:14:23,820 --> 00:14:30,180
So there must be some kind of accumulation of this underreporting up to this point.

96
00:14:30,630 --> 00:14:36,930
So what we are trying to do here is to create the kind of calibrated number of infected cases

97
00:14:37,620 --> 00:14:42,570
before the February 12th then so that you can see that those calibrated numbers are very matte,

98
00:14:42,840 --> 00:14:49,020
very much match with the numbers of of actual observed numbers after this February 12th.

99
00:14:49,530 --> 00:14:52,559
So how that calibration affects your prediction?

100
00:14:52,560 --> 00:15:04,020
Well, that has none of it. In fact, in for example, here, this is the case with no calibration, and this one is based on the calibrated data points.

101
00:15:04,020 --> 00:15:08,549
You can see that the predictions are somewhat different.

102
00:15:08,550 --> 00:15:12,300
This has a little bit more uncertainty in the calibration.

103
00:15:12,900 --> 00:15:23,040
And here, if you bring this quarantine numbers into this and then you can see that this has much better the calibrate data

104
00:15:23,040 --> 00:15:33,420
tells you a lot more sort of promising story what's going to happen if the quarantine policy is actually used.

105
00:15:33,870 --> 00:15:43,680
If you do not do the calibration, you can see that there are a lot of uncertainty in this prediction about the policy of quarantine.

106
00:15:44,460 --> 00:15:52,740
But if you have calibrated the data, then the quarantine policy seems to be very effective as indicate in the prediction.

107
00:15:53,280 --> 00:15:59,969
So so that we present the data to the CDC in China.

108
00:15:59,970 --> 00:16:10,260
They really think that the quarantine is the fact that we at that time to really, you know, help mitigate this whole pandemic.

109
00:16:10,830 --> 00:16:14,819
Okay. So that's more and more data points.

110
00:16:14,820 --> 00:16:20,790
What I want to see here is this our package. So, yes, that's the.

111
00:16:21,520 --> 00:16:27,309
How could you create one? And so this is a quite a popular package.

112
00:16:27,310 --> 00:16:33,490
And of course, that Berman is some sort of research team has extend this package a bit.

113
00:16:34,480 --> 00:16:46,420
And anyway, so this was the first the package we we used the we this whole program is based on the our and is still available.

114
00:16:46,420 --> 00:16:53,710
And if you are interested, you can run this and before this package you can only run daily data.

115
00:16:54,070 --> 00:17:06,560
Now we modified this package. Now it can run weekly, take the monthly data and so on so that this package is available in our.

116
00:17:06,910 --> 00:17:15,260
So if you don't have any label applied, then like why should we go for like, uh, like a very difficult, important.

117
00:17:15,550 --> 00:17:23,110
The pi is very subjective, yes. But because I remember like the analysis, like what we're doing for things like you,

118
00:17:23,110 --> 00:17:30,979
something like the time we need production number for like after some family perspective devastating like an issue of time

119
00:17:30,980 --> 00:17:39,520
when you production numbers the maximum one where you go to very subject like it is a subjective yet so you even you

120
00:17:39,520 --> 00:17:48,710
have a data you still have a lot of such subjective choices what you can do or you cannot do right so so so one time that

121
00:17:48,730 --> 00:17:56,629
people think that you could change the parameter as so the question here fundamentally here is how do you extend the,

122
00:17:56,630 --> 00:18:07,030
the Voter.com power model in the way that they become more flexible to the allowed incorporation of more data.

123
00:18:07,060 --> 00:18:14,200
Observe data into this system seems to be quite ideal to you know, do the prediction.

124
00:18:14,200 --> 00:18:18,549
But the actual due reality is very, very complex.

125
00:18:18,550 --> 00:18:26,950
Right. So how do you extend this model to more realistic of these or other variables into like a vaccination time during,

126
00:18:27,610 --> 00:18:30,850
you know, thing, even for the policy part is.

127
00:18:31,570 --> 00:18:44,020
Yes. So yeah, I think this is interesting problem because before in you do this was now that it means stream of

128
00:18:44,020 --> 00:18:50,799
research team boss statistic I mean including myself I never work and seriously in the facility

129
00:18:50,800 --> 00:18:56,140
this small field now be that becomes such an important field that more and more about the decision

130
00:18:56,140 --> 00:19:02,810
was the issue sort of jump into this field trying to figure out the ways to improve the,

131
00:19:02,860 --> 00:19:07,599
you know, modeling and data analysis and some software toolboxes?

132
00:19:07,600 --> 00:19:09,579
And I think there is a long way to go.

133
00:19:09,580 --> 00:19:22,270
And but I h and other funding agencies are really thinking to create some channels for researchers to get a funding to do research in this field.

134
00:19:22,270 --> 00:19:27,159
So I think that in the next few years that will must probably see more.

135
00:19:27,160 --> 00:19:34,690
And now I'm the associate editor of Analysis of a place I receive quite a number of requests by

136
00:19:34,690 --> 00:19:42,459
editor to review the papers for like infectious disease modeling and analysis method and so on.

137
00:19:42,460 --> 00:19:54,190
So I think there is a strong interest in the community how to, you know, do this kind of folder development of this mode,

138
00:19:54,190 --> 00:20:04,780
compare, make it the more sort of useful to more flexible to handle the situation.

139
00:20:04,810 --> 00:20:18,010
Yeah. Anyway so that's something you could, could do and, and you know, this is a very nice story that at the time so we,

140
00:20:18,010 --> 00:20:26,229
you know, in, in late January 2020 I think that so we were just 20.

141
00:20:26,230 --> 00:20:30,040
So we probably need to do something to to help a bit.

142
00:20:30,520 --> 00:20:36,819
And so that's why we started this whole project that the goal was very clear.

143
00:20:36,820 --> 00:20:42,520
We want to create a user friendly software so people can crunch the number of, you know,

144
00:20:42,520 --> 00:20:50,980
and at that stage, in the early stage of the pandemic, data sample size is very small and really simple.

145
00:20:51,460 --> 00:20:57,280
We just want to make a prediction on to see what's going to happen next three months.

146
00:20:57,370 --> 00:21:06,099
And we did not think that this pandemic can continue to even now it's it's more and more complicated

147
00:21:06,100 --> 00:21:11,649
that that's why we need space based or because the model that can take more information,

148
00:21:11,650 --> 00:21:16,570
a more variable analysis. And that time I was quite a naive, very simple way.

149
00:21:16,570 --> 00:21:21,130
We want to use very limited data to see what's going to happen next three months, if we can.

150
00:21:21,970 --> 00:21:28,270
Sort of quantitatively assess certain policy and to make some comparison that whether or

151
00:21:28,270 --> 00:21:35,980
not quarantine is a fact of policy to intervene the process of this infectious disease.

152
00:21:36,070 --> 00:21:41,830
But anyway, it's also a fascinating work and it's in early 2020.

153
00:21:42,420 --> 00:21:45,520
Yeah, it's okay.

154
00:21:45,530 --> 00:21:53,110
So back to the the.

155
00:21:54,140 --> 00:22:02,590
I want to switch the gear to spatial statistics. Now we have a lot more data, a lot of more higher resolution data.

156
00:22:03,100 --> 00:22:07,540
And then so now we need two different two boxes to handle data.

157
00:22:08,110 --> 00:22:19,330
Okay. So suddenly we need to deal with spatial heterogeneity and model the spatial heterogeneity cross country or across the regions and so on.

158
00:22:19,720 --> 00:22:23,620
So we need to really start to work on those spatial statistics.

159
00:22:23,800 --> 00:22:31,510
Okay. So I'll just give you a quick a review of all the spatial statistics.

160
00:22:31,510 --> 00:22:41,260
Right. Basically, spatial statistics maybe categorized in three major areas like areas, statistical point pattern analysis, few statistics.

161
00:22:41,500 --> 00:22:53,340
So so area statistics is, is the one we will focus on because we basically have the data coming from counties.

162
00:22:53,570 --> 00:23:02,360
So and point pattern analysis is more like a sort of sort of the like car accidents.

163
00:23:02,590 --> 00:23:08,880
So when you have car accidents, you basically know exactly the location where the accident happened, right?

164
00:23:08,920 --> 00:23:13,850
So when car accident happens, police will come over and make the record.

165
00:23:14,110 --> 00:23:19,870
Like which special location in the kill space this accident happens.

166
00:23:20,110 --> 00:23:27,220
So essentially, if you so if you think about a region, right, you really just have all the points.

167
00:23:29,510 --> 00:23:36,290
In a space where each point indicates their location, a car accident happens.

168
00:23:36,410 --> 00:23:40,280
You know, exactly, because there are some police reports on that.

169
00:23:41,510 --> 00:23:48,040
But so this data is identical, actually, and so so is public opinion.

170
00:23:49,400 --> 00:23:54,620
But when you talk about universities, you cannot talk about the personal, private information.

171
00:23:54,620 --> 00:23:59,620
You can. Obviously, this hospital has in fact, the person with this hospital has the.

172
00:23:59,930 --> 00:24:07,940
That's due to pandemic. So that private information is not allowed to be produced in the public surveillance system.

173
00:24:08,300 --> 00:24:11,630
You have to have the aggregate information comfortable.

174
00:24:11,810 --> 00:24:22,340
So so that's why we talk about errors and statistics, not our thing that, you know, you look at a lot of this continues to modeling,

175
00:24:23,000 --> 00:24:36,800
which is mostly about news ecology or think about very sort of how to kill distant.

176
00:24:36,920 --> 00:24:45,160
Based on modeling which may not be really related to public health because public health will be it's hard to define distant period.

177
00:24:45,500 --> 00:24:48,860
So you see that this is close. We're not close.

178
00:24:48,870 --> 00:24:53,660
It's hard to really define that geo distance.

179
00:24:54,200 --> 00:25:01,250
There are a lot of factors involved in actually definition of distance in infectious disease modeling.

180
00:25:02,270 --> 00:25:06,810
So which methods to use, in not least depends on actual question and data quality.

181
00:25:07,280 --> 00:25:13,820
And our data quality is clear is on this counter level aggregate data.

182
00:25:14,720 --> 00:25:21,890
So in connection to infectious disease, small in surveillance data and also given degree of data resolution areas that is most

183
00:25:21,890 --> 00:25:26,120
useful for all the other to measure may be that when should have a data available?

184
00:25:26,280 --> 00:25:37,639
Right. So if you have a particular data set that gives you the a household location to address, you could work out this pattern point pattern,

185
00:25:37,640 --> 00:25:48,530
low cost process of sort of cost point process to model this, you know, sort of a spatial process or modeling.

186
00:25:51,560 --> 00:26:00,170
So I plan to introduce each of this spatial analytics and our application in the analysis of if you do this data.

187
00:26:00,470 --> 00:26:03,680
Okay. So I want to focus on Arrow data first. Okay.

188
00:26:04,430 --> 00:26:10,130
So basically, the data are typically collected. Attributes are fixed polygon areas.

189
00:26:10,240 --> 00:26:15,490
Right. So like you have a geometric shape like a polygon, right?

190
00:26:15,530 --> 00:26:18,080
So it's represented by county or central region.

191
00:26:19,310 --> 00:26:30,290
So you regularly spaced such as boundaries of counties or you know or add or small or or same shapes, another center shapes.

192
00:26:30,290 --> 00:26:40,280
Right. So, so you can think about example like census census blocks, voting districts are skewed a tendency boundaries.

193
00:26:40,490 --> 00:26:50,330
Okay. So spatial features attributes such as distribution, corrosion, cluster regression, you know, are of interest in analysis.

194
00:26:50,660 --> 00:27:03,290
So, so the event had couple of the nice, nice sort of books, very elementary spatial data analysis side.

195
00:27:03,890 --> 00:27:07,040
I have a book on my bookshop. If you want, you can have one.

196
00:27:07,070 --> 00:27:16,540
It's quite a useful that place. Spatial data analysis with art I think is I take some of the material from that book putting that lecture.

197
00:27:16,550 --> 00:27:26,240
So it's quite nice book that if you want to learn how to analyze data and think about high level modeling or statistic analysis,

198
00:27:26,240 --> 00:27:34,760
just talking about data analysis, be that paper of the book, it's quite nice, very popular in the spatial data analysis.

199
00:27:36,470 --> 00:27:43,460
So let's pick up this Syracuse leukemia data, which is other data giving that book.

200
00:27:43,650 --> 00:27:47,900
Okay. So you have the outcome variable y, z.

201
00:27:49,520 --> 00:27:58,850
So basically, you're looking at the the I think the the the prevalence of the leukemia disease,

202
00:27:59,270 --> 00:28:06,740
you know, in the region of Syracuse is a city in New York state of New York.

203
00:28:07,430 --> 00:28:12,480
So you can log transform this rate is actually this report.

204
00:28:12,860 --> 00:28:17,130
Okay. And then normalized the by census tract population.

205
00:28:17,150 --> 00:28:25,850
So basically it is calculus log of one, sometimes number of cases plus one divided by population.

206
00:28:26,000 --> 00:28:29,120
Okay. That's basically the the outcome data.

207
00:28:29,200 --> 00:28:32,530
You have. So is the population of the track.

208
00:28:34,760 --> 00:28:40,580
So. So you have some covers, like you have this chemical exposure.

209
00:28:40,610 --> 00:28:54,050
Okay. So this kind of pollution exposure, the potential exposure computed as the logarithm of 100 times the inverse of the distance,

210
00:28:54,720 --> 00:29:00,620
the between the census tract centroid and the nearest TCE.

211
00:29:01,130 --> 00:29:04,280
This is chemical for our producing site. Okay.

212
00:29:04,670 --> 00:29:09,740
So this is basically it measures the potential exposure.

213
00:29:10,490 --> 00:29:24,170
So you have a this chemical producing this toxic toxic sort of the chemical TCE is Sarah because there's a

214
00:29:24,950 --> 00:29:34,999
beside to produce this chemical and then so that you will measure a particular location to that producing site.

215
00:29:35,000 --> 00:29:41,450
And while inverse of that distance will be measured as a potential exposure.

216
00:29:41,690 --> 00:29:49,520
The closer the higher exposure, the more distant less, you know, exposure to this TCE.

217
00:29:49,730 --> 00:29:53,480
Okay. So this is a surrogate way to make exposure.

218
00:29:53,810 --> 00:30:02,140
Of course, the best way is that you have a instrument that can sit in the room and really measured is concentration in the air,

219
00:30:02,240 --> 00:30:06,140
in the water, in the whatever, the living environment.

220
00:30:06,380 --> 00:30:10,760
If you have a device that can directly measure a concentration level, that would be ideal.

221
00:30:11,090 --> 00:30:16,170
But we don't have data here. You create a surrogate exposure variable.

222
00:30:16,690 --> 00:30:30,890
So basically it's inverse of the distance between the the census track, this particle du particle and the of the TCE producing site.

223
00:30:30,980 --> 00:30:40,730
Okay. And also, they looking at the the age population, what's the percentage of individuals older than 65 years old?

224
00:30:41,270 --> 00:30:48,710
And some are younger because the age represents the long term exposure to this factor.

225
00:30:49,320 --> 00:30:57,110
So longer would have more cumulative exposure than of two to this TCE.

226
00:30:57,350 --> 00:31:05,210
And they're also the looking at the percent of homeowners, which indicates lifestyle or economic level.

227
00:31:05,420 --> 00:31:14,180
So this is another series of variable to measure the socioeconomic status to to say their home ownership.

228
00:31:14,450 --> 00:31:16,760
So those are a very well off variables.

229
00:31:17,120 --> 00:31:27,230
But they're, you know, when you have limited data that those are variables that they use to trying to understand the variability of this outcome.

230
00:31:28,730 --> 00:31:33,410
So here is the the the data.

231
00:31:33,440 --> 00:31:37,550
Okay. So here you'll you'll plot this.

232
00:31:37,760 --> 00:31:42,890
Okay. So this is leukemia incidence rate.

233
00:31:43,010 --> 00:31:47,960
Okay. Here is the, you know, your locations, the latitude and longitude.

234
00:31:48,620 --> 00:31:53,870
And then you for each polygon represents a census tract.

235
00:31:53,900 --> 00:32:02,059
Ray So, so then you have all the numbers printed on that and so that you can see what,

236
00:32:02,060 --> 00:32:11,820
what is the, you know, the, the incidence rate and each census tract in this region of Syracuse.

237
00:32:11,900 --> 00:32:16,190
Right. So you can rank them and use some different color to rank them.

238
00:32:16,580 --> 00:32:21,799
Clearly here, this this region, this little area.

239
00:32:21,800 --> 00:32:26,240
Right. Has much, much darker sort of.

240
00:32:26,420 --> 00:32:32,060
This is ranked number one has the highest sort of the incidence rate.

241
00:32:32,840 --> 00:32:39,600
So so, you know, this certainly is a hot spot that captures your attention, really.

242
00:32:39,920 --> 00:32:47,600
So why this area has the highest you know, this leukemia incidence rate, rank number one first.

243
00:32:47,600 --> 00:32:54,830
And also it's much, much darker than only other. So this has much, much higher leukemia incidence for this region.

244
00:32:55,520 --> 00:33:03,630
So the question immediately would be why? Why this is this is because this will be older than the other area.

245
00:33:03,680 --> 00:33:18,020
You can just plot this. Percentage of individuals who are 65 or older is the age really the factor that create this high of darkness for this region?

246
00:33:18,260 --> 00:33:21,500
So so that's one question you're like ask.

247
00:33:21,740 --> 00:33:28,120
But it turns out that this is not the region that has the oldest population here.

248
00:33:28,130 --> 00:33:33,140
You can see that this one has much higher percentage of.

249
00:33:33,490 --> 00:33:36,790
Older people and here you have darker.

250
00:33:36,790 --> 00:33:49,240
And so age is sort of explain this but now certainly a not clear A is not clear the leading factor to explain this high level of the incidence rate.

251
00:33:49,450 --> 00:33:55,089
Okay. So then you ask the question, is the leukemia incidence correlative is age in this sense of track?

252
00:33:55,090 --> 00:34:05,020
That's obviously something you like to ask later is are there any hospital benefits such as some occasion specific risk factors.

253
00:34:05,680 --> 00:34:14,650
So those things that you know, you like to ask besides the age, for example, what additional risk factors you can find to explain this.

254
00:34:14,890 --> 00:34:17,950
Okay. Okay. So.

255
00:34:18,190 --> 00:34:27,430
So what's the reason? Okay. So if you if you rank them, this sort of the incidence rate,

256
00:34:27,880 --> 00:34:37,330
you can see that the the census tract or areas around this tend to have a higher this incidence rate.

257
00:34:37,340 --> 00:34:40,899
It's okay to rank quite high.

258
00:34:40,900 --> 00:34:44,410
And so why why this one is so high?

259
00:34:44,410 --> 00:34:49,480
Because there is a link. Okay. Is called up on a darker link.

260
00:34:50,080 --> 00:34:53,889
So why why this link is is the big, you know,

261
00:34:53,890 --> 00:35:00,430
the the the geographic so sort of the the the cost to this high this because

262
00:35:00,430 --> 00:35:08,530
this t c produce inside it is right under the poor their waste into the lake.

263
00:35:08,950 --> 00:35:14,200
So the water is severely polluted and the people are leaving here.

264
00:35:14,200 --> 00:35:25,330
Around here we basically exposed the water or surrounding soils and some the environment that tends to have higher, you know, deaths.

265
00:35:25,750 --> 00:35:32,900
So so the question here would be really, you know, after you do kind of this out, like,

266
00:35:33,070 --> 00:35:38,320
you know, then you really start to think about what is the pollution level of this water,

267
00:35:38,320 --> 00:35:46,090
is that the lake actually is the, you know, the cheating factor of this high incidence of leukemia in this region.

268
00:35:46,900 --> 00:35:54,330
They tend to help you to identify hotspot identify problems that you know were and how we are going to investigate next.

269
00:35:54,970 --> 00:36:02,200
It is may not be able to answer your question completely, but at least give you a clear direction where you are going to go next.

270
00:36:02,740 --> 00:36:12,790
So that's very important for people to use spatial data and they plot to really identify hotspot that you can really find clear direction where to go.

271
00:36:13,060 --> 00:36:22,750
Okay so why the surrounding areas are really something that concern me according to the ranking on the leukemia incidence rate.

272
00:36:23,080 --> 00:36:32,630
Okay. So of course that we come back to the the US air pollution data, that's the study that I did.

273
00:36:33,430 --> 00:36:41,259
And as you said, I work quite a bit on the kidney disease and so so here is the actually the number of

274
00:36:41,260 --> 00:36:47,770
dialysis outside assets in 2014 because when people reach their end stage renal disease,

275
00:36:47,770 --> 00:36:50,919
they have to go to, um, you know, dialysis.

276
00:36:50,920 --> 00:36:53,350
You were to get, you know, the treatment.

277
00:36:53,560 --> 00:37:07,560
Okay, so, so that the dialysis answer would be a very clear, you know, the different, the marker to define the beginning of this kidney failure.

278
00:37:07,570 --> 00:37:12,280
Right. So so here you can see at the county level, this is really, um, county level.

279
00:37:12,460 --> 00:37:27,070
Okay, so the, the, you look at the number of dialysis onset here and then you look at also the air pollution PM 2.5 for example,

280
00:37:27,340 --> 00:37:36,489
the average PM 2.5 in that year can clear see that here in the Ohio Valley right around Cincinnati.

281
00:37:36,490 --> 00:37:40,140
I'm not trying to Cincinnati. I mean, it is just okay.

282
00:37:40,390 --> 00:37:46,690
Okay. So I was divided by universes in that he as if to attend to to give a similar I

283
00:37:46,690 --> 00:37:53,440
said wow you know I can see that there's a can and and and you know the Ohio

284
00:37:53,440 --> 00:37:58,599
and Indiana there's a lot of traffic in this region and there's a valley so

285
00:37:58,600 --> 00:38:04,450
that you can see that that the air quality was much worse than our for sure.

286
00:38:05,200 --> 00:38:08,710
But anyway here is PM 2.5 concentration.

287
00:38:09,040 --> 00:38:18,190
You can see that there's a lot of concentration here, that the question here is whether or not there is association between,

288
00:38:18,400 --> 00:38:28,300
you know, the number of dialysis or the incidence rate of end stage renal disease associated with air pollution.

289
00:38:28,540 --> 00:38:34,210
Okay. So now we are doing the correlation analysis between two maps, correct.

290
00:38:34,930 --> 00:38:45,700
So in your six 5651, you do a correlation of why column and X column using a one over Pearson correlation or regression.

291
00:38:46,390 --> 00:38:53,080
Now I gave you two maps, right? So here is the the number of dialysis assets.

292
00:38:53,230 --> 00:38:57,730
Right. So there's there are about four, 4700 counties in U.

293
00:38:57,730 --> 00:39:05,380
S then then you have the air pollution, which has another map, you know, heat map here.

294
00:39:05,860 --> 00:39:13,360
So how do you calculate the correlation of this to whether or not the average of 2.5 is correct?

295
00:39:13,690 --> 00:39:20,180
The number of the. Okay. So that's something special data analysis will help you to solve.

296
00:39:20,360 --> 00:39:28,190
Okay. So what a risk as our tribute to the special features, variations, patterns and numbers.

297
00:39:28,610 --> 00:39:31,970
So we come to this error attributes.

298
00:39:32,090 --> 00:39:41,900
Okay, so the error attributes are made here for entire polygon and can now be folder localized.

299
00:39:42,220 --> 00:39:52,040
Okay. So that's the first thing that. So this is basically that this attribute, it's actually the level of resolution you can have.

300
00:39:52,180 --> 00:39:56,130
Okay. So then we use the polygon level covers.

301
00:39:56,330 --> 00:40:05,060
Okay. To improve the resolution to understand the variation of attributes, cross plugins this basically geeking out that this spatial tonality.

302
00:40:05,090 --> 00:40:17,690
Why you know why the here you see a darker more wrapped things higher up PM 2.5 compared to here.

303
00:40:18,290 --> 00:40:24,820
So so people are trying to look at this spatial variation across regions and so on.

304
00:40:24,830 --> 00:40:31,340
So. So that's something people trying to work on as this error data analysis.

305
00:40:31,970 --> 00:40:35,930
So we normalize the attributes by size of polygon,

306
00:40:36,320 --> 00:40:49,550
namely infectious disease is number of the population size and after totality and then the practice in order to get good interpretation.

307
00:40:49,560 --> 00:40:55,970
So basically work on the prevalence or incidence you actually the public health study.

308
00:40:57,650 --> 00:41:10,930
Then many, many times we need to really create a sort of data linkage to link multiple data sources together in order to conduct arms associated.

309
00:41:11,270 --> 00:41:18,770
That's exactly what you're doing for your project. So, so for example, this, this data is from our tests.

310
00:41:18,770 --> 00:41:30,740
This, we have a national database to hold all the dialysis patients because dialysis is paid by U.S. government.

311
00:41:30,900 --> 00:41:42,630
Okay. So United States and the each year will pay like $20 billion for dialysis treatment for people have in this stage disease.

312
00:41:42,640 --> 00:41:49,460
So every dialysis patients is documented in a database called the PA SDR database.

313
00:41:50,090 --> 00:41:53,930
And then, of course, this database is the medical database.

314
00:41:53,930 --> 00:42:01,339
It has nothing to do with this air pollution. Now you have to go to EPA or go to CDC to get air pollution data.

315
00:42:01,340 --> 00:42:05,149
That's what one group is doing to get air pollution data.

316
00:42:05,150 --> 00:42:15,590
Right. To to. And you can look at other unemployment aids or, you know, political segregation or no different data set coming from different sources.

317
00:42:15,590 --> 00:42:21,740
You have to really find a way to link them at the level of particle, namely country level.

318
00:42:21,860 --> 00:42:30,440
Okay. So a lot of time people did the data will be up to the level of the census track or county.

319
00:42:30,650 --> 00:42:40,639
Okay. So that that's the make up data is something like when you work on this spatial data analysis is something that you

320
00:42:40,640 --> 00:42:48,980
really need to work on first to create a link from multiple data sources in order to do some association analysis.

321
00:42:52,010 --> 00:42:59,330
Tessellation is it's another thing that we worry about in, in the sort of spatial data analysis.

322
00:43:00,710 --> 00:43:12,320
So, so when you divide the study area, you should not use the, a, a relevant variable to divide the regions.

323
00:43:13,220 --> 00:43:26,900
So, so, so like county country had is now that a clinical variable is just a geographic variable that, you know, defined for other reasons,

324
00:43:26,990 --> 00:43:37,879
maybe just for an administration for some historical reason, you are not dividing the regions according to the cancer sort of prevalence.

325
00:43:37,880 --> 00:43:44,060
Right, right. So you're not to divide the data using some kind of meaningful biology variable.

326
00:43:44,060 --> 00:43:48,410
So so this tessellation is talking about duration of study area.

327
00:43:48,890 --> 00:43:53,940
This typically settled by reasons irrelevant to data collection data analysis.

328
00:43:53,960 --> 00:43:55,880
Note that that's something. Well,

329
00:43:56,000 --> 00:44:07,310
we don't have this worry since we use country data we county is a more an administration boundary it has no you know biological some relevance there.

330
00:44:08,930 --> 00:44:15,110
And also this division of data and study error can now be modified by a data analysis.

331
00:44:15,200 --> 00:44:17,719
Sometimes people try to, you know,

332
00:44:17,720 --> 00:44:27,470
create some P-value more than .05 and they're trying to massaged the division of the data and trying to do here or there.

333
00:44:27,470 --> 00:44:30,890
And this is not allowed in the spatial analysis.

334
00:44:30,920 --> 00:44:36,980
You decide the division over your study areas fixed it.

335
00:44:37,270 --> 00:44:41,690
Okay. So you can not modify that on during this study.

336
00:44:41,810 --> 00:44:48,810
Okay. Because there's something I will talk about that this modifiable area union problem.

337
00:44:48,950 --> 00:44:55,280
Okay. So this modify the reaching boundaries of the polygons.

338
00:44:55,820 --> 00:45:01,850
You can create some variable statistic results you wish.

339
00:45:04,160 --> 00:45:09,290
But this whole thing can change over time. They come t and the population.

340
00:45:09,290 --> 00:45:17,780
That's why we need to do the national census to understand the migration of population or to,

341
00:45:18,320 --> 00:45:27,610
you know, decide the division of this electoral sort of district or something like that.

342
00:45:27,620 --> 00:45:31,250
So this can be changed over time.

343
00:45:31,760 --> 00:45:36,100
So, for example, one that new squeezed field out for the boundaries of school a tendency.

344
00:45:36,800 --> 00:45:44,410
And then you have to really, you know, revise this sort of the the sort of error.

345
00:45:44,420 --> 00:45:54,740
If you decide that this area is based on the boundary of a school, a tendency when a new school is built up, then this whole thing will be revised.

346
00:45:54,830 --> 00:46:04,100
Okay. So when you have this time varying like definition of this polygons and of course you have

347
00:46:04,100 --> 00:46:13,760
longitudinal data and then your your data will be little bit different pre post of this tessellation,

348
00:46:13,770 --> 00:46:19,730
right. So. So that you have to bring some of the template structure into this data analysis.

349
00:46:20,780 --> 00:46:26,530
So harmonizing data, these varying polygons is important step data pre-processing.

350
00:46:26,540 --> 00:46:30,890
And this is really something we work on in the practice.

351
00:46:31,280 --> 00:46:37,729
When the boundaries changes, the polygons, you know, changes over time.

352
00:46:37,730 --> 00:46:41,330
How we're going to make the data compatible before and after.

353
00:46:41,420 --> 00:46:51,450
Okay. So this is something that people are trying to do. So this is called this modifiable errand unit problem.

354
00:46:51,740 --> 00:46:56,970
This is the problem directly related to this alteration of practice.

355
00:46:57,240 --> 00:47:05,880
People can massage their sort of definition upon Agos or division of study area that can create a different statistical result.

356
00:47:06,010 --> 00:47:14,820
Okay, so pairwise correlation at the more general levels are generally stronger at finer levels.

357
00:47:15,340 --> 00:47:18,840
Okay. This is the fact, right? I mean, it's not always true,

358
00:47:18,840 --> 00:47:26,700
but in general is the case association and regression model and more general levels are generally stronger than a final level.

359
00:47:27,210 --> 00:47:36,720
Okay. So if you if you make your problem more general, then you can get stronger correlation in general.

360
00:47:37,620 --> 00:47:41,969
This is because much noise can be average out in aggregation.

361
00:47:41,970 --> 00:47:51,370
The average is has a smaller of errors. So this comes to a issue called ecological phonics.

362
00:47:51,630 --> 00:48:00,880
So so inferences about individuals or subject global causation regression can be induced from inference from their input.

363
00:48:01,290 --> 00:48:09,810
Okay. So that's why this is also a potential issue in our infectious disease analysis.

364
00:48:10,020 --> 00:48:18,629
Okay. The suggestion here is that we set up a level based on this scientific question, hypothesis and interpretation of policy making.

365
00:48:18,630 --> 00:48:22,640
So so that don't manipulate this.

366
00:48:22,650 --> 00:48:30,770
You could because I should say that if you merge some areas that you could analyze this in a more general level rather than a finer book,

367
00:48:31,230 --> 00:48:37,710
then you can artificially increase your correlation and something like that to get the P values more than .05.

368
00:48:38,280 --> 00:48:46,290
So this issue has to be noticed in the spatial data analysis, particularly, people want to be warned in advance.

369
00:48:46,470 --> 00:48:52,140
So do not do that if you have to have a clear reason why you want to do that.

370
00:48:52,320 --> 00:49:03,410
Okay. So of course, in this spatial data analysis, very, very critical sort of variable is meter distance, right?

371
00:49:03,650 --> 00:49:11,630
So how do you determine the distance between two special locations or two polygons?

372
00:49:12,140 --> 00:49:15,820
So so this is something that you have to really work on.

373
00:49:15,830 --> 00:49:20,370
And there are many, many different solutions in spatial statistics.

374
00:49:20,370 --> 00:49:26,029
It's not just a simple, you know, the Manhattan distance or do you clean distance.

375
00:49:26,030 --> 00:49:30,210
There are many, many different versions of this. People define the spatial statistics.

376
00:49:30,360 --> 00:49:34,940
Okay. So how to determine distance between neighbors?

377
00:49:35,000 --> 00:49:43,970
Okay. That's first, probably the most important question that we need to figure out before we start to do any serious spatial data analysis.

378
00:49:44,750 --> 00:49:55,970
Okay. So what people are trying to do, the first choice is do the distance between two centroid of polygons using latitude and longitude data.

379
00:49:56,300 --> 00:50:01,970
Okay. So that's quite a lot to do for it to be able to answer that.

380
00:50:02,270 --> 00:50:10,120
So. So you have one area and then you have another county.

381
00:50:10,680 --> 00:50:16,110
So you have the central right of this county.

382
00:50:16,110 --> 00:50:26,220
And you know what? That's essentially that nature of this use there of, you know, the lines to the longitude.

383
00:50:26,230 --> 00:50:30,180
But this is very straightforward. You claim use thing to make true.

384
00:50:30,660 --> 00:50:39,180
But the question here is that if there's a river between us, do you think this decision makes sense?

385
00:50:40,020 --> 00:50:48,300
Because you know that this would reflect how easily people can connect to each other, shorter and easier to connect.

386
00:50:48,870 --> 00:50:52,350
But if this says there's a river, well, there's a mountain range.

387
00:50:55,080 --> 00:51:02,399
People don't even understand the language outside of this, given that distance, very, very like ten kilometers.

388
00:51:02,400 --> 00:51:07,889
But there's a mountain, high mountain between this and people and old time people.

389
00:51:07,890 --> 00:51:11,980
Do you sort of come together or visit each other?

390
00:51:12,060 --> 00:51:16,290
They don't even understand language emphasized. That happens in some regions in time.

391
00:51:16,860 --> 00:51:24,300
I don't know how, but in India, one other places where you have high mountains, people are not able to even understand the language of the outside.

392
00:51:24,960 --> 00:51:31,130
So this distance sometimes is not really the ideal, of course is easy.

393
00:51:31,770 --> 00:51:40,240
Sure. So I know this isn't the options given, but for something like that we have like a mountain river with incorporating like Google Maps data,

394
00:51:40,260 --> 00:51:46,920
the time it takes to get from point to point, yet another distance measure than just straight shape, right?

395
00:51:47,110 --> 00:51:51,150
Exactly. That could be one solution to deal with that.

396
00:51:51,160 --> 00:51:56,520
Right. And the old time that maybe takes 5 hours to climb the mountain from one side to the other.

397
00:51:56,850 --> 00:52:01,530
And all the time you don't have the way to, you know, have a boat.

398
00:52:01,680 --> 00:52:05,610
Maybe the rich people would have boat from one side to the other side.

399
00:52:05,790 --> 00:52:17,759
So it's hard to say, like when people think about mosquito is the source of infection, they become, you know, some kind of various carried over.

400
00:52:17,760 --> 00:52:22,510
And if you have high mountain between this two sided basket, cannot use a fly over.

401
00:52:23,500 --> 00:52:27,270
So. So it's more complicated than one would think, right?

402
00:52:27,310 --> 00:52:30,540
But of course, this is assumed that the ideal situation,

403
00:52:30,540 --> 00:52:39,700
you would measure the distance between two polygons by the distance between the centroid or polygons using the Atlantic to land.

404
00:52:39,900 --> 00:52:48,390
But there are more complex situation in practice. So not only people do this using this sort of common borders, right?

405
00:52:48,780 --> 00:52:54,290
So they have neighbors and first order like, look, this is coming from chess, right?

406
00:52:54,420 --> 00:53:01,620
The common lines of Queen two have come points neighbors so they can define neighbors,

407
00:53:01,620 --> 00:53:08,909
first order neighbors and second order of neighbors according to whether or not they share the common boundaries or.

408
00:53:08,910 --> 00:53:12,710
SHUBERT Common points. Just give you example here.

409
00:53:12,720 --> 00:53:16,290
This is the Syracuse map.

410
00:53:16,890 --> 00:53:20,430
You can see that this county right here, right. This one.

411
00:53:21,660 --> 00:53:26,730
Right. And this one, they do not share the common line, but they share a common point.

412
00:53:27,780 --> 00:53:34,920
So they have a common point that right here. Right. Do say that you can I mean, just make it bigger.

413
00:53:38,900 --> 00:53:42,260
So you can see the gray area. So here this one, right.

414
00:53:42,740 --> 00:53:49,370
And this county here, they do not share a common line confounder, but they share common point.

415
00:53:50,060 --> 00:53:54,320
So this is called a queen labor queen type member.

416
00:53:54,620 --> 00:54:03,590
In our we are used to our function, you can specify which neighbors types of neighboring neighbors you want to find the queen or group.

417
00:54:04,310 --> 00:54:10,320
So for example, this one and this one, right? Or this one, this very common border mine.

418
00:54:10,340 --> 00:54:14,170
Then we have the first order root of neighbor.

419
00:54:14,480 --> 00:54:22,879
So you can see rook queen. And they have different choices in our function to define neighbors here did not define

420
00:54:22,880 --> 00:54:33,830
according to this physical numerical distance it defined as a adjacency matrix zero one.

421
00:54:34,460 --> 00:54:42,660
Okay. So, so if you have this county, one country, two counties three.

422
00:54:43,310 --> 00:54:47,209
So and end of day country one, country two, come here.

423
00:54:47,210 --> 00:54:52,190
Three. So of course, county one. The county two can always, always will be share.

424
00:54:52,190 --> 00:54:55,219
Either you know or it depends on how you define here.

425
00:54:55,220 --> 00:54:58,330
Could be zero. I've never seen of your demise or it doesn't matter.

426
00:54:58,340 --> 00:55:01,550
So you think either way you want to find this.

427
00:55:02,150 --> 00:55:05,879
Now for looking at the neighboring. So we're interested in neighboring.

428
00:55:05,880 --> 00:55:10,820
Right? So you can have 001. Okay, this could be zero.

429
00:55:11,060 --> 00:55:15,890
If you use rook, if you use Queen, maybe this becomes one.

430
00:55:16,400 --> 00:55:27,530
Okay, so so basically and you get this zero one matrix zero defines that there is no shared common law or no shared a common point.

431
00:55:27,800 --> 00:55:33,020
Y means there's two neighbor is c1c to have a shared common line.

432
00:55:33,300 --> 00:55:38,030
You know, shared common point is the way to define neighbors, right.

433
00:55:38,030 --> 00:55:51,440
Or to have physical distance defined as, you know, this shared sort of connectivity that you can define the number of steps reach a common border.

434
00:55:52,010 --> 00:55:57,079
You can define first order. Okay. So I'm talking about this the first order.

435
00:55:57,080 --> 00:56:05,530
They have an immediate common point where they need to two points to step to get that common so you can have first order, second or third one.

436
00:56:05,930 --> 00:56:13,669
Typically, people would use in the first order to define this partition matrix, you can do second and third order.

437
00:56:13,670 --> 00:56:17,899
The algorithm can help you to find out your own standard, right?

438
00:56:17,900 --> 00:56:23,810
So for example, this one can go this to the two step to reach this point.

439
00:56:24,020 --> 00:56:35,720
Okay. So than this one. Okay, this county would have a second order, you know, Queens sort of relationship or what is the shortest.

440
00:56:36,500 --> 00:56:43,820
This one here, one point with this one this one shared the border of is this anyway so.

441
00:56:44,240 --> 00:56:48,810
So this is a the way that you you you define this.

442
00:56:49,250 --> 00:57:04,370
Okay. Okay. So choose between distance or steps depends on the purpose of analysis and unorganized spatial process for data generation.

443
00:57:04,910 --> 00:57:10,760
And the former is more frequently used in public health surveillance data analysis.

444
00:57:11,330 --> 00:57:15,770
But in some, you know, spatial analysis,

445
00:57:15,920 --> 00:57:26,209
people use this sort of incidence matrix additions of matrix to define route or define queen as their distance in terms of connectivity.

446
00:57:26,210 --> 00:57:33,560
Connect it or not, connect is a force order connection or sequence of connection and so on, so that you can define this.

447
00:57:33,800 --> 00:57:41,540
Okay. So, so there are as what I'm saying here is the multiple different ways to define distance.

448
00:57:41,630 --> 00:57:49,700
It's not only the the sort of the typical distance we're thinking about using latitude and longitude.

449
00:57:52,100 --> 00:58:04,820
So sometimes we use this, you know, baffle to sort of to define the sort of nearest neighbors.

450
00:58:06,020 --> 00:58:17,090
So in this case, you know, you could define that you give a distance amount the distance this region,

451
00:58:17,240 --> 00:58:22,940
you know, and what are the the nearest neighbors with a given distance.

452
00:58:25,110 --> 00:58:33,720
So. So there are a lot of our packages available for us to do this sort of spatial data analysis for the error data analysis.

453
00:58:34,020 --> 00:58:38,430
Actually, this is very well developed field in statistics already.

454
00:58:38,540 --> 00:58:51,479
Okay. So we can have this this sort of spatial data analysis package and do the causes and methods for spatial analysis.

455
00:58:51,480 --> 00:58:59,219
And you can have a spatial dependance analysis package where you can get the weighting schemes,

456
00:58:59,220 --> 00:59:04,740
statistics and all those and useful to generate some descriptive statistics.

457
00:59:05,190 --> 00:59:09,510
For example, look at how many neighbors to assign paralyzed weight.

458
00:59:09,930 --> 00:59:15,780
You either binary asset define the connectivity you know adjacency.

459
00:59:15,780 --> 00:59:24,330
For example, you specify route that this pairwise weight will give you zero one whether or not this two counties

460
00:59:24,450 --> 00:59:32,279
would have would share a common boundary line where you use queen do whether or not this you know this

461
00:59:32,280 --> 00:59:39,479
pairwise way to a give you whether or not this country would share a common point and also can give

462
00:59:39,480 --> 00:59:48,450
you a continuous one basically defined by the distance between two central centroid of the polygons.

463
00:59:48,840 --> 00:59:57,540
So there are different types of choices to define the weights and generate some sparse descriptive statistics.

464
00:59:59,280 --> 01:00:03,660
And also you can do this spatial NINA regression model.

465
01:00:03,900 --> 01:00:13,620
Okay. So you can regress. For example, you can regress this this map, this is this.

466
01:00:13,860 --> 01:00:21,990
Okay. If you have data, you wonder if a two point pm 2.5 will be associated with the the incidence of dialysis onset.

467
01:00:22,260 --> 01:00:29,310
So you just use you can use as PLM to do the analysis to figure out the correlation or association.

468
01:00:29,350 --> 01:00:33,780
Okay. Running a regression analysis between two spatial maps.

469
01:00:35,340 --> 01:00:37,979
And so there are additional R packages.

470
01:00:37,980 --> 01:00:45,660
I would I will tell you more later on, but I'm just telling you that there are a lot of kind of things that we could do.

471
01:00:45,660 --> 01:00:51,750
Study is quite interesting field to look at that those spatial heterogeneity.

472
01:00:52,070 --> 01:01:07,430
Mm hmm. So, first of all, I think that what people usually do here is, you know, just like a test that is sort of the temporal dependance.

473
01:01:07,590 --> 01:01:14,880
Right. So so the first question people would do in context in the House is you have done quite a bit of time the in house.

474
01:01:15,750 --> 01:01:21,930
So really people want to test whether or not there was an alpha correlation.

475
01:01:22,560 --> 01:01:32,310
So you have a time the right people will see if this data is there some Markov property exist.

476
01:01:33,120 --> 01:01:37,920
So this one in the yesterday will be related data to date.

477
01:01:38,160 --> 01:01:40,730
All right. So you've tested the test.

478
01:01:40,740 --> 01:01:50,370
The only correlation whether or not that this data is independent sample from is a sample independent observation or is temporal dependance.

479
01:01:50,760 --> 01:01:54,690
So the data today is depend on data in history. So that's it.

480
01:01:55,050 --> 01:02:04,020
So here in these spatial statistics, there is a super, which is the data in this region and often come to you would be related to memory counties.

481
01:02:05,000 --> 01:02:13,020
Okay. So there is a similar question. So basically understanding same error or disk, dissimilar polygons.

482
01:02:13,080 --> 01:02:17,190
Okay. So basically test the autocorrelation in the spatial space.

483
01:02:17,520 --> 01:02:24,990
Spatial domain attributes in neighboring particles more similar than expected by chance.

484
01:02:25,890 --> 01:02:34,800
Okay. So interpret that attribute in the neighboring polygons, more similar than is expected by chance.

485
01:02:34,830 --> 01:02:39,060
Basically you test that this spatial type autocorrelation.

486
01:02:39,300 --> 01:02:45,660
Okay. Whether or not the of the variable the one county is related to neighboring countries.

487
01:02:45,720 --> 01:02:54,390
Okay. So there are many useful visualization toolbox available for us to to work on this.

488
01:02:55,020 --> 01:03:01,110
So in time system, we have this famous eight alpha correlation function, right?

489
01:03:03,300 --> 01:03:10,620
So all the formation function is the very, very famous visualization tool box to okay temple.

490
01:03:10,830 --> 01:03:18,870
Although correlation and in spatial statistics there are something very popular is called Morenci.

491
01:03:19,470 --> 01:03:22,920
Okay. I don't know why is called I but maybe.

492
01:03:24,080 --> 01:03:28,430
Independents, probably this is essentially Morenci.

493
01:03:28,520 --> 01:03:34,160
Okay. So you can use the very grams or coral grams of neighboring matrix.

494
01:03:34,400 --> 01:03:40,610
This is more like correlation with the weights based on the central inverse distance.

495
01:03:40,910 --> 01:03:45,860
This is our package will create something like this.

496
01:03:45,860 --> 01:03:51,740
Visual gram. A coral coral grams is more like a safe.

497
01:03:51,980 --> 01:04:02,000
Okay. More like a safe. So called kilograms of neighboring matrix is something similar to operation function.

498
01:04:02,690 --> 01:04:09,770
And so what's popular? I use quite a bit for simple analysis is use more as I.

499
01:04:10,170 --> 01:04:17,630
Okay. So, so basically this is a test statistic to test the independence.

500
01:04:18,080 --> 01:04:21,200
So whether or not the, you know, the neighboring polygon.

501
01:04:21,200 --> 01:04:28,090
So we're really calling this the VCT.

502
01:04:29,480 --> 01:04:32,750
They are the data of the current target country.

503
01:04:32,780 --> 01:04:40,130
Okay. So so there are some assumptions, spatial correlations not induced about common leaf and factors.

504
01:04:40,730 --> 01:04:48,530
And the sign the neighbor weeds are reflective, the meaning of similarity to similarity of problem under investigation.

505
01:04:49,220 --> 01:04:55,580
Okay. So there are two assumptions. So here is a definition of global moray.

506
01:04:55,580 --> 01:05:01,340
I call it fishing is a modification of Pearson correlation, essentially.

507
01:05:01,400 --> 01:05:06,650
Okay. So here. Here is how this coefficient is defined.

508
01:05:07,130 --> 01:05:16,540
So Y is actually the attribute. For example, they are the incidence rate of the leukemia in one census tract.

509
01:05:16,580 --> 01:05:25,040
Right. So so why is the incidence rate of leukemia in one census tract in one park.

510
01:05:25,220 --> 01:05:38,650
Right. So why bar is the overall the average incidence rate, the fall, all the leukemia across all the counties in the Syracuse region.

511
01:05:38,690 --> 01:05:44,150
Right. So so they can calculate overall average. You have the individual incidence rate.

512
01:05:44,450 --> 01:05:48,140
Okay. So you have I come to you and J come to you.

513
01:05:48,170 --> 01:05:52,909
Right. So this is, you know, really just a product that.

514
01:05:52,910 --> 01:06:02,690
Right. This is basically covers if you have no W ag, then this is the sample covariance divided by one over and that is,

515
01:06:02,690 --> 01:06:07,540
you know, and so it's, you know, so so this is basically sample covariance, right?

516
01:06:08,000 --> 01:06:09,470
That we know. Mm hmm.

517
01:06:10,070 --> 01:06:21,500
But now you put the tableau ag into this because you, you'll know that the neighboring counties should receive higher wheat in the consideration.

518
01:06:21,710 --> 01:06:27,140
Right. If if this two counties are very far away, like here is a neighbor, this is saddle.

519
01:06:27,740 --> 01:06:37,080
Why you want to do the products that are so far away in terms of a distance that maybe this product is not very meaningful in the calculation.

520
01:06:37,100 --> 01:06:42,530
Right. So so really what you want to do here is to control the relative contribution

521
01:06:42,530 --> 01:06:47,390
of their product in their calculation of their dependance using the wheat.

522
01:06:47,450 --> 01:06:53,130
How nearly this two counties are adjacent each other.

523
01:06:53,210 --> 01:07:02,630
Right. So this w ag either you use Duke, you use queen, you use geo distance, whichever way you want to with this.

524
01:07:02,630 --> 01:07:07,910
But you have a factor that controls the the level contribution,

525
01:07:08,480 --> 01:07:19,309
the the near the more near appears to receive more contribution to calculation because so you're doing this kind of weighting

526
01:07:19,310 --> 01:07:29,020
that you need to really weight this by the actually way to use to make this I to be of coefficient between -1 to 1.

527
01:07:29,030 --> 01:07:34,460
So you want to normalize this. So this is the variance of everything.

528
01:07:34,730 --> 01:07:40,730
Okay. So here you assume that the variance of the all counties are same, so that you have the constant.

529
01:07:41,240 --> 01:07:46,740
Essentially you should have a square root, but you have this sort of the uh,

530
01:07:47,480 --> 01:07:54,290
homogeneity assumption of same variance so that you basically have IV minus eight bar squared.

531
01:07:54,980 --> 01:08:05,600
So if you do not have the w it year and essentially this is what, what, you know, the, the, the Pearson quotation you have.

532
01:08:05,960 --> 01:08:12,800
Okay. But now you had the weight and then you just justify this, this kind of morenci coefficient,

533
01:08:15,140 --> 01:08:23,590
a large eye coefficient in the case a strong autocorrelation spatial autocorrelation among m polygons and implies there it.

534
01:08:23,660 --> 01:08:31,060
There's certain spatial patterns. Okay. So basically you're testing whether or not this coefficient is zero.

535
01:08:31,070 --> 01:08:33,139
If there is. If this continues zero,

536
01:08:33,140 --> 01:08:42,650
then those poly polygons evolve in this region are basically independent that they are don't they don't relate to each other know spatial.

537
01:08:43,790 --> 01:08:55,880
Okay. Well, maybe they are too geographically neighboring to each other, but for the y variable, this y variable for the y data you're looking at,

538
01:08:56,120 --> 01:09:07,040
maybe this y y variables are not really related to their geographic neighbors structure, their, you know, location wise.

539
01:09:07,040 --> 01:09:10,370
They are near each other. No problem with that.

540
01:09:10,610 --> 01:09:13,999
But the Y itself has nothing to do with.

541
01:09:14,000 --> 01:09:18,500
They are actually the underlying geographic configuration or adjacency.

542
01:09:19,040 --> 01:09:24,169
So this is trying to test the water, not the y sample from this spatial region.

543
01:09:24,170 --> 01:09:27,560
Swiss M polygons are actually correlated or not.

544
01:09:27,920 --> 01:09:31,850
If this one you test this equal to zero, there's no correlation.

545
01:09:32,420 --> 01:09:37,700
I mean, y has no spatial type of condition. You know, I'm still on target, right?

546
01:09:37,700 --> 01:09:41,420
So so that you you you trying to do you test this?

547
01:09:42,770 --> 01:09:52,850
So you basically test whether or not the null hypothesis is that you have random allocation of attribute values into the M particles of the Y,

548
01:09:52,880 --> 01:09:59,370
y could be leukemia incidence and then, you know, you have no patterns and so on.

549
01:09:59,390 --> 01:10:02,690
So you can. So how do you do the haphazard testing?

550
01:10:02,690 --> 01:10:11,450
And Mora has calculated, if you use this test statistic, the meaning of that is minus Y, over n, minus one.

551
01:10:12,290 --> 01:10:21,319
And variance of this coefficient is the complicated is given in the Wikipedia if you want to find it is a very complicated form.

552
01:10:21,320 --> 01:10:29,360
I'm I'm not writing here and but certainly you can program this and then you you use this.

553
01:10:29,360 --> 01:10:38,540
I observed the data given this weighting matrix, you gave the map and you asked the the R packages to give you weight,

554
01:10:38,630 --> 01:10:42,800
specify what type of distant weighting matrix you want.

555
01:10:43,220 --> 01:10:45,950
You should able to get that directly from the map.

556
01:10:46,760 --> 01:10:59,930
And then so you calculate the this either statistic and minus this mean and divide by square root of this then and more improve that this you

557
01:10:59,930 --> 01:11:08,030
know follow a normal distribution and you can transform the observer right to a normal z-score then you calculate the p value under none.

558
01:11:08,750 --> 01:11:13,970
The small p value suggests a onto a high chance that observed is different from

559
01:11:14,180 --> 01:11:19,639
expected value of which random location of attribute value is assigned to the polygon.

560
01:11:19,640 --> 01:11:24,580
Basically small p value indicates there is a spatial correlation.

561
01:11:24,590 --> 01:11:29,270
Spatial pattern evolved. Okay, so it's not independent sample.

562
01:11:29,810 --> 01:11:39,050
So here is actually the the ah package from sp1 was the the package I used.

563
01:11:39,950 --> 01:11:42,950
Okay. The name of it. So many packages.

564
01:11:43,790 --> 01:11:49,540
I think that's the. Okay.

565
01:11:49,930 --> 01:11:58,750
This, this one I think I use oh, it's PDP or it's Sparks.

566
01:12:02,080 --> 01:12:05,920
Either this one or this one. I forgot which one to use.

567
01:12:05,920 --> 01:12:13,570
But anyway, so actually, as a matter of fact, it is more, as I say, that it has been programed by many officers.

568
01:12:13,870 --> 01:12:16,899
There are multiple ah package available.

569
01:12:16,900 --> 01:12:26,770
But the one that I, I think I think is best PDP, the spatial dependance, a package that offers a calculation.

570
01:12:26,770 --> 01:12:41,710
For example, using this data you use rook adjacent a matrix by a common shared common by sharing common line this kind of rook type of neighborhood,

571
01:12:41,740 --> 01:12:50,020
you can calculate the estimates, you can calculate the expected value, we can calculate the variance so that that you can calculate the p value.

572
01:12:50,290 --> 01:12:58,380
Okay. And this no surprise that there is a spatial dependance, as you can see that those are this cluster, right.

573
01:12:58,390 --> 01:13:03,010
So you can clearly see that there is a cluster here.

574
01:13:03,250 --> 01:13:06,640
Right? So they take the census tracts.

575
01:13:06,970 --> 01:13:20,590
Okay. The polygons near the lake, this this link up on a gondola like those are having similar high incidence rate of the leukemia.

576
01:13:21,040 --> 01:13:31,210
So there they are. Now the you can clearly see there's cluster patterns in similar patterns for the census tract near duplex.

577
01:13:31,450 --> 01:13:35,350
Yeah. So that's that's no surprise.

578
01:13:37,240 --> 01:13:44,860
No surprised that the the P value is pretty small then reject the null hypothesis of random allocation of leukemia

579
01:13:44,860 --> 01:13:55,120
incidence across the Syracuse census tracts and so so you know that that that's important to do the first thing.

580
01:13:55,540 --> 01:14:04,689
Okay before you started this spatial modeling you need to know whether or not that there is a special correlation inside the data that you need to.

581
01:14:04,690 --> 01:14:14,650
Baldivis okay. Now next, the y is really thinking that, oh my god, if I have this corp that how this course it looks like,

582
01:14:15,010 --> 01:14:21,130
okay, can I find a way to identify hotspots or to define some clusters, right?

583
01:14:21,160 --> 01:14:24,879
So you really want to know the little bit more details.

584
01:14:24,880 --> 01:14:30,250
How do these sort of spatial dependance looks like rather than having overall test?

585
01:14:30,460 --> 01:14:35,800
Yes, there is a spatial dependance. You want to move me toward more details.

586
01:14:36,400 --> 01:14:48,729
So this local more insight coefficient is really just looking at this more as I at the individual level and here is how it's constructed and how you

587
01:14:48,730 --> 01:15:01,330
project and how do you identify the spatial structure and using this local Morris eye coefficient defined this way we can you know identify hotspot.

588
01:15:01,600 --> 01:15:09,040
Another alternative way is do this get this old local g coefficient that's not a one that we use.

589
01:15:09,940 --> 01:15:16,660
Okay, maybe I just stop here and return your midterms and we can continue to smoke one next week.

590
01:15:17,890 --> 01:15:31,380
But it's something like different type of analysis we do in the two sticks, two volume method.

591
01:15:34,850 --> 01:16:03,810
And. You.

592
01:16:07,880 --> 01:16:23,680
She's not here. Okay.

593
01:16:25,180 --> 01:16:32,900
And you? It's all.

594
01:16:41,690 --> 01:16:47,580
I feel well, you know.

595
01:16:53,200 --> 01:17:02,300
And she said, you can take her exam if you're still comfortable with that, but I don't know if you feel comfortable that she's told you.

596
01:17:22,510 --> 01:17:27,340
You were. What was your.

