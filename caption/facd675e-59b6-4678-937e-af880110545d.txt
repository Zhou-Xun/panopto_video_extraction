1
00:00:00,530 --> 00:00:06,870
We start this recording. So we're going to talk about research ethics today.

2
00:00:06,870 --> 00:00:11,430
Before I get into that, just a couple quick sort of logistic things for the course.

3
00:00:11,730 --> 00:00:15,930
So labs start today. All right. So if you've got a Thursday lab, you'll you'll go today.

4
00:00:15,930 --> 00:00:18,090
If you have a Friday lab, you'll go tomorrow.

5
00:00:19,650 --> 00:00:27,030
Remember, the purpose of labs today is one just to see other students that are in your lab section get a chance to meet the GSI,

6
00:00:27,060 --> 00:00:33,750
who's going to be leading your section for the semester, create your project groups or not project, you know, your lab groups, right?

7
00:00:33,750 --> 00:00:39,780
They're going to be groups of like 4 to 5 students that will be working together on those labs throughout the semester.

8
00:00:41,280 --> 00:00:46,650
If if you're listening to this recording, I guess, and you can't make it, it's okay.

9
00:00:46,770 --> 00:00:52,590
You know, you just email your GSI and you can get added to the list somehow or they'll send you the list.

10
00:00:54,900 --> 00:00:58,049
So we want to keep those groups of like 4 to 5 students. All right.

11
00:00:58,050 --> 00:01:01,020
So that that we can share out the work there on those.

12
00:01:01,980 --> 00:01:06,840
You know, if you've got friends in the lab section, you know, feel free to make groups around that.

13
00:01:07,200 --> 00:01:12,090
If you're not having anybody that you really know and you feel like you need some help joining a group,

14
00:01:12,480 --> 00:01:17,910
that's another thing your GSI can help you with. But there's no actual assignment this week.

15
00:01:17,910 --> 00:01:24,450
Like there's no project this week that will start next week. Homework one got posted yesterday.

16
00:01:24,930 --> 00:01:31,530
I don't know. Maybe I got a chance to look at it. So what homework one is doing is it's got a couple two sets of questions.

17
00:01:31,530 --> 00:01:39,240
One on one side, a question is on some media articles being compared to an actual primary research article.

18
00:01:39,570 --> 00:01:43,170
It's about a recent Alzheimer's drug that's been in the news. You might have seen it.

19
00:01:43,470 --> 00:01:50,490
So just asking you, you know, and some of these are don't worry about correct answers always, especially initially,

20
00:01:50,760 --> 00:01:56,159
like I'm just trying to get you to think about, you know, and recognize the fact and I try to put notes in these questions.

21
00:01:56,160 --> 00:02:01,050
So you see what I mean? You know, when we get to the back end, like the statistics, there are definitely right answers.

22
00:02:01,380 --> 00:02:08,100
You know, here we're looking for you to just just think about the presentation of the research and these different media articles.

23
00:02:09,210 --> 00:02:14,430
So that's due next Thursday by the start of class a week from today.

24
00:02:15,780 --> 00:02:19,739
Again, I encourage you to work together with other people on that.

25
00:02:19,740 --> 00:02:25,830
You feel free to talk to each other about it, but whatever you write up and whatever you submit, that should be your own.

26
00:02:26,100 --> 00:02:31,580
Okay. Any questions or questions about the course?

27
00:02:32,090 --> 00:02:36,959
Things we should. All right, so let's get started.

28
00:02:36,960 --> 00:02:42,990
So we're gonna talk about research ethics today. Um, how many people have heard about research ethics before?

29
00:02:43,570 --> 00:02:45,690
Course, hopefully you've heard about it by this point.

30
00:02:45,990 --> 00:02:55,260
So, just like last time with research articles, you know, it's impossible for me to not, you know, recover some things you've already heard about.

31
00:02:56,340 --> 00:02:59,999
So bear with me while I talk about things you maybe heard about before.

32
00:03:00,000 --> 00:03:03,990
But I do hope that I do hit on some things that that will be new to you.

33
00:03:04,800 --> 00:03:09,240
So what our objectives are today are we're going to work our way towards the Belmont Report,

34
00:03:09,240 --> 00:03:17,969
which was came out after probably the most high profile research misconduct that ever happened in the United States,

35
00:03:17,970 --> 00:03:20,190
which is the Tuskegee syphilis study.

36
00:03:21,510 --> 00:03:29,400
I want to focus on the ethical principles that came out of that and then think about how those can be applied to other studies, or we'll do that here.

37
00:03:31,080 --> 00:03:35,340
Something I want also give you some examples of are ethical gray zones in research.

38
00:03:35,340 --> 00:03:39,669
So, you know, sometimes it's it's easy to look at something and say, well, that was clearly wrong.

39
00:03:39,670 --> 00:03:40,830
This people shouldn't have done that.

40
00:03:41,130 --> 00:03:48,840
But I'm going to give you some examples of places where, you know, it's not so clear that that an ethical misconduct has actually occurred.

41
00:03:49,920 --> 00:03:56,070
And then finally close it out with some some new considerations for the show up in the world of data.

42
00:03:56,610 --> 00:04:00,500
And so, again, this class is primarily about analyzing data. But, you know,

43
00:04:00,510 --> 00:04:06,239
the reason that this ethical research ethics is included in here is because of the

44
00:04:06,240 --> 00:04:09,660
fact that we have to consider where data comes from and what is then done with it.

45
00:04:11,970 --> 00:04:18,330
Joel. I was preparing these slides because. Because I'm not going to tell you this again, reminds us I'm not a bioethicist.

46
00:04:18,480 --> 00:04:25,800
This is not my area of expertise. So it took a lot of reading and looking at stuff that I don't normally look at while I was preparing this lecture.

47
00:04:26,130 --> 00:04:27,750
But this quote really stuck out to me.

48
00:04:27,750 --> 00:04:36,090
And so it's it's from the chief of or at the time, a couple of years ago, I guess, the chief of the NIH Clinical Center, Department of Bioethics.

49
00:04:36,510 --> 00:04:39,780
And she wrote, When people are invited to participate in research,

50
00:04:40,290 --> 00:04:44,729
there is a strong belief that it should be their choice based on their understanding

51
00:04:44,730 --> 00:04:48,630
of what the study is about and what the risks and benefits of the study are.

52
00:04:48,900 --> 00:04:55,500
Right. And those are these these three key points. And we're going to see these key points reflected multiple times throughout this lecture.

53
00:04:57,350 --> 00:05:05,000
So before we we dig into the actual con material for this, I want to get you to just think for a minute,

54
00:05:05,320 --> 00:05:10,129
you know, about where, you know, imagine you were put in a situation, right?

55
00:05:10,130 --> 00:05:11,970
So this is going to be our minute paper for today.

56
00:05:12,920 --> 00:05:20,270
So imagine that you are working on a research study, you know, potentially as a graduate student, a postdoc.

57
00:05:21,230 --> 00:05:27,740
And the goal of that study is to identify genetic variants that increase risk for obesity or type two diabetes.

58
00:05:28,910 --> 00:05:39,740
So the way the study was happened is that that we collected large families and we sequenced them to genome sequencing on them.

59
00:05:40,040 --> 00:05:48,170
And then you're trying to track which genetic variants or mutations are segregating through the family with either high BMI or diabetes.

60
00:05:48,920 --> 00:05:50,659
So while you're doing the analysis,

61
00:05:50,660 --> 00:05:57,889
one of the things that you notice is that a woman in the dataset has two genetic variants predicted to be a complete knockout,

62
00:05:57,890 --> 00:06:02,959
a function for the brackets you gene. There's a BRCA2 gene if you're not familiar.

63
00:06:02,960 --> 00:06:06,590
So it's BRCA right. So that stands for breast cancer.

64
00:06:07,010 --> 00:06:11,840
It's a tumor suppressor gene which is involved in repairing damage, DNA, DNA.

65
00:06:12,770 --> 00:06:19,820
If you have one bracket or two mutation, you're at increased risk for breast cancer, ovarian cancer.

66
00:06:20,000 --> 00:06:29,610
If you have two loss of function bracket mutations, your chance of getting a breast cancer is almost a sure thing.

67
00:06:29,630 --> 00:06:33,050
Okay. And not only that, but you can also pass it down to children.

68
00:06:33,650 --> 00:06:39,860
So my question for you and this is why I want you to think about just take a minute and I want you to just your visceral reaction to this.

69
00:06:40,250 --> 00:06:47,830
If you found this information in your dataset, would you inform the woman about your discovery and then tell me why or why not?

70
00:06:47,900 --> 00:06:59,310
And so go to the minute paper and write that up. Just your honest.

71
00:07:00,120 --> 00:07:19,170
What's the what's the first thing that pops in your head? It makes sense.

72
00:07:19,770 --> 00:07:57,450
I'm asking. Remember, I don't need a full on thing.

73
00:07:57,450 --> 00:08:01,540
I just want I just want you to think about it. I just want to get you primed for what we're going to talk about.

74
00:08:02,930 --> 00:08:06,800
Okay. And then just write down, you know, what you want. Why would you? Why would you or why wouldn't you?

75
00:08:07,880 --> 00:08:15,410
And then once you do that, I'd like you to go to poll everywhere and just tell me yes or no.

76
00:08:16,190 --> 00:08:23,930
Would you tell the woman with two suspected BRCA2 mutations about the this incidental finding?

77
00:09:44,790 --> 00:09:49,650
To try to get try to get your response in here. We're going to we're going to capture these just just to get a sense.

78
00:09:50,460 --> 00:09:55,260
So I'm guesstimating this probably about 60 to 70 people in here.

79
00:09:56,290 --> 00:10:00,370
So let's try to get that number up so that we have as many responses as we can.

80
00:10:15,440 --> 00:10:19,190
All right most people done you submitted them in a paper that that in.

81
00:10:20,290 --> 00:10:23,409
Okay. So I'm going to I'm going to leave. This will stay open. Okay.

82
00:10:23,410 --> 00:10:26,590
So if you need to respond, you know, you haven't quite gotten there yet. That's totally fine.

83
00:10:26,590 --> 00:10:30,969
We're up to 60, so it looks like most people have responded. Well, I can show you the answer.

84
00:10:30,970 --> 00:10:34,030
Yeah. Let's come back to that later. All right.

85
00:10:34,030 --> 00:10:37,599
So a couple of things we want to get out of the way. Let's just do some definitions first before we get started.

86
00:10:37,600 --> 00:10:42,200
So what is ethics that we're going to be talking about? So I'm sure you've you've heard of it.

87
00:10:42,310 --> 00:10:46,330
You've already said you already told me you've seen some research ethics. So I just want to quickly define here.

88
00:10:46,330 --> 00:10:50,410
So ethics is the discipline concerned with deciphering what is morally right and wrong.

89
00:10:51,280 --> 00:10:57,760
And sort of my own sort of definition here is that life involves choices and decisions, and those decisions carry consequences.

90
00:10:58,510 --> 00:11:03,940
And ethics is examining, you know, how we weigh the potential benefits and harms of those decisions.

91
00:11:04,630 --> 00:11:06,400
And so we're going to talk about research ethics today,

92
00:11:06,400 --> 00:11:13,420
but there's all sorts of ethical codes that exist throughout human history and then in all all facets of human life.

93
00:11:13,420 --> 00:11:17,440
Something I feel. Yes.

94
00:11:17,550 --> 00:11:19,030
There are societal laws.

95
00:11:19,060 --> 00:11:25,389
There's there's academic integrity, which you've talked about here, like what's acceptable in terms of what's morally correct.

96
00:11:25,390 --> 00:11:29,110
Right. So certainly cheating on exams, not morally acceptable.

97
00:11:29,110 --> 00:11:30,340
Right, in academic settings.

98
00:11:30,700 --> 00:11:35,470
And even though we get into sports, things like unwritten rules in baseball and these things actually change over time, right?

99
00:11:35,480 --> 00:11:43,040
So if you're like a baseball fan, you might hear people complain about, well, in the old days, you would never be allowed to do that kind of thing.

100
00:11:43,060 --> 00:11:46,420
Right. So even even ethical standards change over time.

101
00:11:47,140 --> 00:11:50,680
So I will again remind you, I am not a bioethicist.

102
00:11:51,070 --> 00:11:55,360
I don't have all the answers. So if you have questions during this lecture, I encourage you to ask them.

103
00:11:55,840 --> 00:11:57,820
But I don't always have the answers to these things. Right.

104
00:11:58,240 --> 00:12:06,610
That said, as a researcher, I still have a responsibility for any work that I do in any scientific work that I encounter,

105
00:12:06,850 --> 00:12:15,520
that I am responsible for some level of ethical standards. And I'll remind you that even though you most of you will probably not be bioethicists,

106
00:12:15,820 --> 00:12:22,540
if you get involved with public health and medicine and research, you will have that same responsibility.

107
00:12:24,610 --> 00:12:30,730
So, you know, I want to distinguish here sort of between a couple types of of ethics that we might have in our mind.

108
00:12:30,740 --> 00:12:35,170
So the first is medical ethics. And this focuses on health care, right?

109
00:12:35,170 --> 00:12:39,069
So making individual level medical decisions, right?

110
00:12:39,070 --> 00:12:44,050
So like a single person. So if you're going to be a clinician or a doctor, I know which many of you want to be.

111
00:12:44,470 --> 00:12:48,490
Right. That that's more of what a medical issue is when you're treating a specific patient.

112
00:12:49,120 --> 00:12:54,030
I'm thinking here more about research ethics, which is when you're conducting research, right.

113
00:12:54,040 --> 00:13:00,520
You're trying to acquire new knowledge. You know, what are the acceptable norms when you're in the process of trying to acquire new knowledge?

114
00:13:01,600 --> 00:13:08,200
So there are actual, you know, professional bioethicists that they spend their time thinking about the ethical,

115
00:13:08,200 --> 00:13:17,109
social and legal issues that arise both in in biomedical biomedicine and in biomedical research they teach.

116
00:13:17,110 --> 00:13:20,380
Is anybody taking a bioethics course here like a full on course?

117
00:13:20,740 --> 00:13:24,360
Yeah. Okay. Okay.

118
00:13:24,570 --> 00:13:28,620
Yeah. So is this like full courses like that you can spend on this? You're doing one lecture, right?

119
00:13:28,860 --> 00:13:32,879
So just to let you know, we have, um, you know, so, you know, why do we need, what do we need?

120
00:13:32,880 --> 00:13:38,340
Bioethicist Right. Can we all just agree we should care for the welfare of humans, right?

121
00:13:38,340 --> 00:13:41,850
And I'm sure you don't need to be told this, but we cannot.

122
00:13:41,860 --> 00:13:44,220
We cannot just trust that everyone will do this.

123
00:13:44,850 --> 00:13:52,829
I pulled this off and now each website they had, you know, quite a bit of information on on bioethics.

124
00:13:52,830 --> 00:13:59,430
And, you know, this one was called like a or a timeline of ethical failures and scientific research.

125
00:13:59,880 --> 00:14:03,030
And, you know, some things, you know, it's like some of these names might sound familiar.

126
00:14:03,030 --> 00:14:10,229
So like Louis Pasteur administering an experimental rabies vaccine to a nine year old without testing it on animals,

127
00:14:10,230 --> 00:14:16,049
first injecting yellow fever bacteria into five patients without their consent.

128
00:14:16,050 --> 00:14:22,379
All patients develop the disease and three die. Here's one about a Nobel Prize winner.

129
00:14:22,380 --> 00:14:24,570
When people went back and looked at his notebooks,

130
00:14:24,570 --> 00:14:33,209
they found out that he did not report some of the data that didn't fit along with with what his his desired observation was.

131
00:14:33,210 --> 00:14:33,570
Right.

132
00:14:35,130 --> 00:14:44,130
The Tuskegee Tuskegee syphilis study, which we'll talk about in a minute, the Nazi experimentation that was done during during the World War Two era.

133
00:14:44,910 --> 00:14:49,739
How about this one, Francis and Crick, certainly you've heard about them. Francis Crick and James Watson.

134
00:14:49,740 --> 00:14:52,860
You've heard about them coming up with the structure for DNA.

135
00:14:53,340 --> 00:14:57,300
But Rosalind Franklin was somebody that contributed quite a bit of work to that,

136
00:14:58,080 --> 00:15:02,510
and they used her work without her permission or even even worse, acknowledging her work.

137
00:15:03,450 --> 00:15:08,009
Another one you've probably heard about is a paper that showed up in 2000.

138
00:15:08,010 --> 00:15:12,270
Well, the paper showed up before 1998, but it took quite a while.

139
00:15:12,600 --> 00:15:20,490
A paper that linked vaccinations to autism had quite a bit of of ethical failures in it.

140
00:15:20,580 --> 00:15:27,389
Right. And then I'm going to talk more recently about something that involves Gene edited babies.

141
00:15:27,390 --> 00:15:32,370
That was an example of ethical misconduct. So you could see, you know, this is this is a problem.

142
00:15:32,400 --> 00:15:38,730
You know, I put an arrow on this side and an arrow on that side because I'm sure there are many more that predate this and unfortunately,

143
00:15:38,740 --> 00:15:43,200
will probably be able to expand this timeline in the future. But they're not all the same thing.

144
00:15:43,200 --> 00:15:46,290
Right. We see some examples of here where like people are.

145
00:15:48,860 --> 00:15:54,680
You know, these are sort of like the classic forms of misconducts, right, where it's actually happening on an individual.

146
00:15:54,920 --> 00:15:57,770
But we also see misconduct here, like as I said. Right.

147
00:15:57,770 --> 00:16:03,920
This is this is not being truthful about one's data, not acknowledging the contributions of others.

148
00:16:04,520 --> 00:16:08,150
So there's there's misconduct and ethical failures can take on a lot of forms.

149
00:16:11,460 --> 00:16:17,580
So why? Why? Why do we need to care about academic misconduct and research misconduct?

150
00:16:18,060 --> 00:16:24,090
Because there are repercussions that come from it. Okay. So the most obvious is harm to individual study participants.

151
00:16:24,390 --> 00:16:27,540
Right. This can be physical harm. Right. We see some of those examples.

152
00:16:27,540 --> 00:16:31,710
There can also be like financial harm, social harms that exist.

153
00:16:32,610 --> 00:16:40,270
There's also the potential to exploit vulnerable individuals or populations and the opportunity to advance false narratives about groups.

154
00:16:40,310 --> 00:16:48,690
Right. So if research is done in an unethical fashion and the results that come out of it are then picked up and taken about a large group of people.

155
00:16:48,690 --> 00:16:50,340
Right. Those can push false narratives.

156
00:16:51,630 --> 00:17:01,770
So sort of less obvious in terms of the repercussions of academic research is that it leads to public distrust, right, as it should.

157
00:17:01,780 --> 00:17:05,010
You know, academic research misconduct gets a lot of attention. Right?

158
00:17:05,310 --> 00:17:09,720
It shows up in the news. Right. It makes people distrust what people are doing in terms of science.

159
00:17:10,500 --> 00:17:16,350
It's important to remember that science relies upon the support and the financial backing from the public.

160
00:17:16,360 --> 00:17:22,500
Right. So the research that's done here at the University of Michigan, most of it is funded by taxpayer dollars.

161
00:17:22,860 --> 00:17:29,370
Right. If people are saying, well, we don't want to fund this kind of research because we don't trust these people, that's a problem.

162
00:17:29,640 --> 00:17:32,640
Right. And you're going to learn this as you go out and become professionals.

163
00:17:33,030 --> 00:17:36,750
You might have a grand plan for what you want to do, but you have to get money for it somehow.

164
00:17:39,920 --> 00:17:44,690
Also the fact that if people don't trust you, they're not going to participate to be involved in your research.

165
00:17:44,690 --> 00:17:48,649
Right. And that's a problem. It also leads to waste of resources.

166
00:17:48,650 --> 00:17:52,610
So we'll see. The example I asked you to watch a video about, it's called Deception at Duke,

167
00:17:52,610 --> 00:17:58,100
and it was about clinical trials that were run based on faulty information.

168
00:17:58,430 --> 00:17:58,720
Right.

169
00:17:58,730 --> 00:18:07,190
And so if you if people believe what you did and then they try to follow up on that by doing new research and they're expending resources on it.

170
00:18:07,370 --> 00:18:12,260
Right. That's a waste, the waste of time, waste of money, and potentially can harm people like that as well.

171
00:18:14,230 --> 00:18:16,200
So just I just want to hit on, you know,

172
00:18:16,290 --> 00:18:24,790
I think you can't really have a research misconduct without going back to the first code of ethics for research on human subjects.

173
00:18:25,090 --> 00:18:26,499
And it's called the Nuremberg Code.

174
00:18:26,500 --> 00:18:32,770
And it came out of the aftermath of World War Two in response to the things that were being done by Nazi scientists.

175
00:18:32,770 --> 00:18:37,930
And so this is, you know, just to put a date on this August 1947.

176
00:18:38,140 --> 00:18:41,110
And so these these Nazi scientists were put on trial.

177
00:18:41,500 --> 00:18:48,990
And in the process of the verdict in the trial, they released ten points for permissible medical experiments.

178
00:18:49,000 --> 00:18:51,680
This became known as the Nuremberg Code. Okay.

179
00:18:51,880 --> 00:18:57,160
And so I've listed out these ten points here and then a link here so you can get more information on them.

180
00:18:58,000 --> 00:19:02,860
But I highlighted the things that I think are the the keys, right.

181
00:19:02,860 --> 00:19:08,530
And the things that we that that we will see again in the Belmont Report.

182
00:19:08,530 --> 00:19:11,650
You know, here's ten of them. I focus on the Belmont foxes, only three.

183
00:19:12,430 --> 00:19:21,910
But things like voluntary consent, right. Fruitful results for the good of society, avoiding all unnecessary physical and mental sufferings.

184
00:19:22,270 --> 00:19:25,420
Right. The problems should be of humanitarian importance.

185
00:19:27,030 --> 00:19:31,049
One of the other ones that that I think is useful here. Right. Scientifically qualified persons.

186
00:19:31,050 --> 00:19:37,740
Right. So that, you know, people who are able to actually, you know, determine if what they're going to do is is valuable.

187
00:19:39,770 --> 00:19:45,110
Another one that anybody, any human subject should be at the liberty to bring the experiment to the end.

188
00:19:49,780 --> 00:19:55,509
So if that was on like an international scale, it turns out that the Nazi scientists, I mean,

189
00:19:55,510 --> 00:20:00,730
the one that that is most well known is the Tuskegee Syphilis Study in the United States.

190
00:20:01,090 --> 00:20:05,770
So I want to take a minute to describe to make sure you have an accurate understanding of what happened there and

191
00:20:05,770 --> 00:20:11,409
then talk about what came out of that in terms of what's called the Belmont Report and then how that Belmont report,

192
00:20:11,410 --> 00:20:16,330
the principles in it, can then be used to evaluate other other research studies.

193
00:20:17,080 --> 00:20:23,170
So I have to go back to the early 20th century here and understand the state of disease at the time.

194
00:20:23,500 --> 00:20:26,470
So there was something called the US Public Health Service.

195
00:20:26,930 --> 00:20:31,809
S Right, and they were tasked with monitoring and developing interventions to treat diseases.

196
00:20:31,810 --> 00:20:38,480
And so specifically one of the things that they were dealing with was venereal disease rates of sexually transmitted diseases.

197
00:20:39,160 --> 00:20:44,200
So syphilis and the earliest 20th century was a venereal disease that was untreatable.

198
00:20:45,490 --> 00:20:53,319
And so, you know, one of the goals of this course was to try to figure out, you know, what is the natural progression of the disease.

199
00:20:53,320 --> 00:20:56,590
Because if you understand the natural progression of the disease,

200
00:20:56,890 --> 00:21:01,870
it might provide valuable information that you could use to come up with treatments or prevention.

201
00:21:02,410 --> 00:21:09,430
So there's some valuable there, some valuable some legit scientific understanding to knowing what that natural progression is.

202
00:21:11,120 --> 00:21:18,630
So the problem was with how they went about doing it. And so the the this the US public health service, you know,

203
00:21:18,650 --> 00:21:22,850
how did they decide they would go about trying to study this natural progression of syphilis?

204
00:21:22,880 --> 00:21:27,560
Well, they recruited 600 African-American men from Macon County, Alabama.

205
00:21:28,160 --> 00:21:31,250
So 399 of these people had syphilis.

206
00:21:31,550 --> 00:21:34,160
201 individuals were disease free. Okay.

207
00:21:34,160 --> 00:21:40,250
So one thing that's important to note, I think, is that, you know, these people were not infected with syphilis.

208
00:21:40,250 --> 00:21:43,010
Right. They naturally had syphilis and they were identified.

209
00:21:44,930 --> 00:21:50,060
But they were not, you know, not a representative group, but these were participants that were racial minorities.

210
00:21:50,600 --> 00:21:54,590
And beyond that, they were poor. Many of them were like farming.

211
00:21:55,100 --> 00:21:59,540
They were illiterate. Didn't know a whole lot about medicine. Did know a lot about what was available.

212
00:22:00,800 --> 00:22:10,459
And they had few options for for medical care. They were told that they were being treated for bad blood, which is, again, I wasn't alive at the time.

213
00:22:10,460 --> 00:22:14,860
But as I've read, you know, this was a pretty common term that was used, sort of a colloquial term.

214
00:22:14,870 --> 00:22:17,959
Right. So not something that was unheard of. Right.

215
00:22:17,960 --> 00:22:22,250
But certainly not specific enough to let people know why they would be involved in this study.

216
00:22:23,670 --> 00:22:28,680
And then they were provided with free medical exams, treatments.

217
00:22:29,070 --> 00:22:31,830
They were given meals and even burial provisions.

218
00:22:32,970 --> 00:22:38,760
So that's one stop and take a minute to ask, sort of like, you know, what do people think about this point?

219
00:22:38,780 --> 00:22:42,600
So remember when this study starts. It's 1932.

220
00:22:42,960 --> 00:22:46,530
This is still remember if you go back to that Nuremberg code, which was 1947,

221
00:22:46,770 --> 00:22:51,180
right where before that there is no treatment for syphilis at this point.

222
00:22:52,050 --> 00:22:55,230
What do you think about the ethical design of this study so far?

223
00:22:55,950 --> 00:23:07,440
Clearly wrong. We think. I look like a lot of.

224
00:23:13,870 --> 00:23:17,430
Sure. So, you know, I'm sure that. So that.

225
00:23:17,590 --> 00:23:31,080
Absolutely. What else is a problematic area? Anybody? So there's no treatment, right?

226
00:23:31,680 --> 00:23:37,080
So on a different note, I mean, if no informed consent whatsoever.

227
00:23:38,400 --> 00:23:41,750
Uh, yeah. Well, we'll get back to that in a second.

228
00:23:41,950 --> 00:23:44,960
They they said they agreed to be in the study.

229
00:23:45,650 --> 00:23:52,910
So they weren't they weren't forced into it or agreed to it. But we'll talk about did they have was there enough information in the informed consent?

230
00:23:52,940 --> 00:24:05,390
Yeah. So that's, that's, that's what I was getting at with the informed consent.

231
00:24:05,440 --> 00:24:08,730
Right. They don't they didn't quite understand why they were being studied. Right.

232
00:24:15,560 --> 00:24:20,660
To provide. Right.

233
00:24:20,660 --> 00:24:25,069
So there was there was no treatment for syphilis, so that was not withheld from them.

234
00:24:25,070 --> 00:24:28,130
But there was something to try to get at this natural progression. What?

235
00:24:29,740 --> 00:24:34,330
Oh, you just dumped two slides ahead. I'm sorry. I'm just trying to talk about where we are when the study starts.

236
00:24:34,960 --> 00:24:42,070
So what's the obvious thing, right? I mean, why go at a racial minority that's poor, that's illiterate, that has no other options, right?

237
00:24:42,880 --> 00:24:46,970
I mean, they thought that they could take advantage of these people, right? You know, that's true.

238
00:24:46,990 --> 00:24:55,090
So, you know, while this probably made some of the researchers feel like, well, you know, we're giving them something that they don't have.

239
00:24:55,120 --> 00:24:59,380
Right. You know, they did. They went at a population that they thought they could exploit.

240
00:25:00,970 --> 00:25:05,740
So what you just mentioned was what changed? You know, again, like 15 years into the study, right.

241
00:25:05,740 --> 00:25:14,260
At some point in 1947, specifically penicillins introducing penicillin becomes a highly effective treatment for syphilis.

242
00:25:14,470 --> 00:25:20,049
And this is where the study completely falls apart in terms of any kind of moral justification.

243
00:25:20,050 --> 00:25:27,280
Right. We no longer have to understand what the natural trajectory of syphilis is if we have a highly effective treatment and care.

244
00:25:27,820 --> 00:25:37,240
And researchers decided to not treat these participants so that they could continue studying the long term progression of untreated syphilis.

245
00:25:38,980 --> 00:25:42,030
Participants died. Went blind.

246
00:25:42,180 --> 00:25:43,530
Develop mental illness.

247
00:25:43,980 --> 00:25:49,710
And one of the things that that I wrote about that I didn't like, it just doesn't even click until you understand more about it.

248
00:25:49,980 --> 00:25:55,740
They allowed these was because they didn't tell them what they were trying to study them or what they were tracking them for.

249
00:25:56,010 --> 00:26:02,500
Right. About syphilis. They allowed the allowed people to continue to spread the disease to their family members.

250
00:26:02,530 --> 00:26:09,360
Right. So it wasn't even like, you know, if they had known a little bit more about why they were being in this study,

251
00:26:09,750 --> 00:26:12,870
potentially, they could have stopped some of the spread of syphilis as well.

252
00:26:13,800 --> 00:26:16,920
And all of these things happened after a cure was discovered.

253
00:26:17,370 --> 00:26:22,440
And even even crazier is that this went on it went on until 1972.

254
00:26:22,650 --> 00:26:27,330
And it wasn't till a journalist with The New York Times actually alerted to the public

255
00:26:27,330 --> 00:26:34,620
to this that there was this 40 year experiment going on where a a a treatment existed.

256
00:26:35,160 --> 00:26:42,389
Right. And so I've actually linked this this is the the original article is worth going back

257
00:26:42,390 --> 00:26:46,740
and reading like an original from the time to understand what people said about it.

258
00:26:48,850 --> 00:26:52,299
So in response to the Tuskegee syphilis study,

259
00:26:52,300 --> 00:26:58,150
as well as other studies that were going on at the time that were also sort of had some ethical failures in them,

260
00:26:58,870 --> 00:27:05,920
a board was brought together called the IRB by the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research.

261
00:27:06,850 --> 00:27:13,870
And they put out something called the Belmont Report. And in this Belmont Report, they emphasize three basic principles.

262
00:27:13,870 --> 00:27:15,399
And for each one of those principles,

263
00:27:15,400 --> 00:27:23,470
they gave an application for how you can achieve that principle within human research for human subjects in research.

264
00:27:24,160 --> 00:27:28,990
And so that's why we're going to talk about now. So those three principles are what I've bolded right here.

265
00:27:29,350 --> 00:27:32,650
So respect for persons, beneficence and justice.

266
00:27:32,860 --> 00:27:37,710
And then each underneath, each one with a little arrow, bullet point thing is the application, right?

267
00:27:37,720 --> 00:27:41,470
So how do you achieve respect for persons? It's by having informed consent.

268
00:27:41,740 --> 00:27:47,300
How do you achieve beneficence? By doing a risk benefit assessment and how do you achieve justice?

269
00:27:47,320 --> 00:27:51,330
It's through your selection of research subjects in court.

270
00:27:51,640 --> 00:27:57,490
So what is respect for persons means? So there's just like big words in here that I think that really get across this idea.

271
00:27:58,150 --> 00:28:02,290
The first is individuals should be treated as autonomous agents or autonomous agent,

272
00:28:02,290 --> 00:28:07,060
meaning somebody that can make decisions for their own good and they can act on those decisions.

273
00:28:07,390 --> 00:28:11,110
Right, that they understand what's best for them and that they can then act on that.

274
00:28:11,380 --> 00:28:14,470
So any research subject needs to be treated like that.

275
00:28:14,530 --> 00:28:17,680
They have those that knowledge and that ability.

276
00:28:18,970 --> 00:28:24,370
So denying an individual the ability or the freedom to act on their own judgments or to withhold

277
00:28:24,370 --> 00:28:28,870
information that allows them to make those judgments is a lack of respect for the person.

278
00:28:29,530 --> 00:28:37,359
We also have to acknowledge that there are there are individuals with diminished autonomy that cannot make these decisions for themselves.

279
00:28:37,360 --> 00:28:43,750
They either can't understand, right, can't make the decision or they don't have the ability to act on those decisions.

280
00:28:44,440 --> 00:28:50,560
So some examples they give are young or advanced age illness or mental illnesses, right.

281
00:28:50,590 --> 00:28:54,140
Or vulnerable? Are populations vulnerable to mistreatment?

282
00:28:54,400 --> 00:28:57,220
So specifically, maybe something like prison populations.

283
00:28:57,580 --> 00:29:05,780
So I think the whole work actually asks you to think about how prison populations were used during the COVID trials.

284
00:29:05,800 --> 00:29:10,900
Okay. And so I give you a paper there and I want you to think about, you know, and it's sort of like an ethical question.

285
00:29:11,230 --> 00:29:18,840
Right? And in fact, some people are so in need of protection that they shouldn't even be included.

286
00:29:18,900 --> 00:29:24,180
Right. They should just automatically be excluded from from any kind of research and again, depending on what the research is.

287
00:29:25,500 --> 00:29:28,040
So, again, how do you achieve this respect for persons?

288
00:29:28,050 --> 00:29:34,350
We've thrown out this this term of informed consent, but this is the way that you can achieve respect for persons.

289
00:29:35,040 --> 00:29:43,169
The subject should be given the opportunity to understand and choose what shall or shall not happen to them through information,

290
00:29:43,170 --> 00:29:47,739
comprehension and voluntariness. And so I emphasize this three points.

291
00:29:47,740 --> 00:29:52,540
So information. Right. So what is the procedure to be done?

292
00:29:52,560 --> 00:29:55,680
What are people getting involved in when they sign up for a research study?

293
00:29:55,920 --> 00:30:05,340
What's the purpose of it? What's the risks? What's the potential benefits that both they can experience, but also that can come from the study itself?

294
00:30:06,870 --> 00:30:12,140
What are alternative procedures? Right. So what already exists that if I don't enter this trial?

295
00:30:12,150 --> 00:30:17,640
Right. Most people that enter a clinical trial. Right. Have some kind of illness or something that they're trying to get treated.

296
00:30:17,850 --> 00:30:24,090
What are the other things that exist already that could I could alternatively do instead of going to this clinical trial.

297
00:30:25,380 --> 00:30:31,470
They need to be given the opportunity to ask questions, and they also have to be given the opportunity to withdraw from a study at any time.

298
00:30:33,120 --> 00:30:37,050
So once they have that information, another important thing is comprehension.

299
00:30:37,320 --> 00:30:42,150
Just because you tell somebody something doesn't mean that it necessarily makes sense to them.

300
00:30:42,660 --> 00:30:52,280
Right. If anybody has looked at the informed consent forms for clinical trials or any kind of research, sometimes they can be fairly dense.

301
00:30:52,290 --> 00:30:56,910
Right. And so just just showing it to somebody doesn't mean.

302
00:30:56,940 --> 00:31:00,120
Right. They're not they're not in college, in public health. Right.

303
00:31:00,120 --> 00:31:03,630
They're not constantly surrounded by medical research. Right.

304
00:31:03,810 --> 00:31:10,530
You need to be able to present that information in a manner in a context that they can actually make an informed decision.

305
00:31:10,540 --> 00:31:15,480
Right. They can be that autonomous agent. And finally, it's voluntariness, right?

306
00:31:15,720 --> 00:31:20,190
We need to be free from coercion or undue influence to join the study.

307
00:31:20,460 --> 00:31:24,130
Or if they want to lead the study, they have to be allowed to leave the study to.

308
00:31:27,070 --> 00:31:32,950
So beneficence is a is a big word. It means an obligation to do good for the research participant.

309
00:31:33,040 --> 00:31:35,530
Okay. So this is the second principle that came out of the Belmont study.

310
00:31:36,070 --> 00:31:44,770
So we have to start with this this this realization that somebody who enters into a research study faces inherent risks and benefits.

311
00:31:45,220 --> 00:31:50,320
Right. And so what what what beneficence says is that we need to ensure.

312
00:31:50,590 --> 00:31:55,750
Right. That that that we maximize the well-being of the patient and minimize any possible harms.

313
00:31:56,140 --> 00:32:02,200
And these things change based on the study, right? You know, what are the potential benefits and what are the potential harms?

314
00:32:02,470 --> 00:32:06,460
Right. In order to really ensure that beneficence has been met.

315
00:32:06,790 --> 00:32:09,460
Right. We need to do an assessment of the risks and benefits.

316
00:32:11,360 --> 00:32:16,820
So here was a systematic and comprehensive review of the benefits on the research and the potential harms.

317
00:32:17,270 --> 00:32:24,260
And so research should only be carried forward if it can be justified on the basis of a favorable risk benefit assessment.

318
00:32:24,770 --> 00:32:33,830
So it's important to remember that, you know, if you go into a clinical trial and the clinical trial is about, you know, how can I improve sleep,

319
00:32:34,430 --> 00:32:41,050
the harms that are going to be acceptable there are very different than if it's a clinical trial for people that are in,

320
00:32:41,060 --> 00:32:44,540
you know, stage four of cancer. What are the potential harms there?

321
00:32:44,780 --> 00:32:47,540
Right. So these all have to happen on a study basis, right?

322
00:32:47,960 --> 00:32:54,650
So they should account for both the chance or the probability of experiencing the harm or the severity or magnitude of the of the harm itself.

323
00:32:55,760 --> 00:33:00,110
And again, harms don't just have to be physical. They just don't have to be like medical kind of situations.

324
00:33:00,350 --> 00:33:06,709
They can be what you do psychologically or mentally to the person if it puts them in legal ramifications,

325
00:33:06,710 --> 00:33:10,640
social or if it affects their ability economically.

326
00:33:10,640 --> 00:33:20,210
Right. And when we do these risks and benefits, the benefits can also be weighed, both for for overall society as well as the individuals.

327
00:33:20,720 --> 00:33:26,960
And so, again, we should be clearly these risks and benefits need to be clearly defined or described in the informed consent.

328
00:33:29,110 --> 00:33:33,850
And then the last topic or the last principle that came out of the Belmont Report is justice.

329
00:33:34,550 --> 00:33:38,530
And so this you know, the way to think about justice is who ought to.

330
00:33:40,170 --> 00:33:43,530
Bear the burdens and who ought to receive the benefits of research.

331
00:33:43,530 --> 00:33:48,120
Right. And so, you know, how is selection of the research participants performed?

332
00:33:48,540 --> 00:33:53,850
So justice demands that the burdens and the benefits of the research should be evenly distributed.

333
00:33:54,390 --> 00:34:03,630
An injustice occurs when some benefit to which a person is entitled is denied without good reason, or when some burden is imposed unduly.

334
00:34:04,110 --> 00:34:09,929
So particularly right and this had was was written especially with Tuskegee syphilis experiment in

335
00:34:09,930 --> 00:34:15,510
mind where a particular group cannot be systematically selected because of their easy availability,

336
00:34:15,870 --> 00:34:23,419
their compromised position, or their manipulative ability. So balanced selection of subjects.

337
00:34:23,420 --> 00:34:26,540
And I think maybe that word balance should go in front of selection of subjects here.

338
00:34:26,550 --> 00:34:35,240
This is how you achieve justice within a research study, and this can apply both at the individual level and the population level.

339
00:34:35,840 --> 00:34:44,090
So researchers, when they're identifying subjects that will be in there that they want to approach for their research study.

340
00:34:44,510 --> 00:34:49,579
Right. They should they should not offer potentially beneficial research only to some patients or

341
00:34:49,580 --> 00:34:57,480
populations who are in favor or select undesirable persons or populations for risky research like.

342
00:34:59,230 --> 00:35:04,780
And especially if you're looking at populations that are already heavily burdened.

343
00:35:04,780 --> 00:35:12,309
Right. So racial minorities, economically disadvantaged people that are very sick, institutionalized, talked about people in prisons.

344
00:35:12,310 --> 00:35:18,070
Right. These are particularly vulnerable and extra care needs to be taken when you are looking at

345
00:35:18,310 --> 00:35:23,980
including focus recruitment specifically on these individuals within a research study.

346
00:35:25,680 --> 00:35:30,090
And so Belmont is not the only sort of guiding principles that have come out for research.

347
00:35:30,540 --> 00:35:34,379
Almost every professional association, you know, government agencies,

348
00:35:34,380 --> 00:35:40,870
universities have their own specific codes and policies, but they always share very similar themes, I think.

349
00:35:40,870 --> 00:35:45,270
Right. And so here I've given you links to things from the National Institute of Health

350
00:35:45,570 --> 00:35:48,780
down to the American Statistical Association and the University of Michigan.

351
00:35:48,780 --> 00:35:55,590
So every one of these links would sort of send you off and give you the ability to see sort of what their policies are for for ethical research.

352
00:36:00,340 --> 00:36:04,360
All right. So hopefully that should have been a lot of review, I think.

353
00:36:04,510 --> 00:36:06,850
Yeah, people have heard a lot of this before. It's okay.

354
00:36:07,060 --> 00:36:10,900
So let's try let's try to apply these things and let's start to think about some newer examples,

355
00:36:11,560 --> 00:36:14,440
more recent examples, maybe a lot of research misconduct.

356
00:36:15,770 --> 00:36:21,169
So now that the Belmont gave respects for persons, beneficence and justice, we can think about,

357
00:36:21,170 --> 00:36:25,260
well, how are these things specifically violated in the Tuskegee Syphilis Study?

358
00:36:25,280 --> 00:36:29,479
So what about respect for persons? So remember here, this deals with informed consent.

359
00:36:29,480 --> 00:36:37,610
So how is respect for persons violated? So we talked a little bit about this, but now that we talk about specifically the informed consent part.

360
00:36:38,000 --> 00:36:45,100
How is it violated the informed consent? Not only did they not.

361
00:36:53,570 --> 00:36:56,840
Is that right? Yeah. Okay. So you see, you took a whole class, you know?

362
00:36:57,380 --> 00:37:04,010
Yeah. So? So they. So if that does that go for respect for a person's right, their autonomy was taken away.

363
00:37:04,100 --> 00:37:08,120
Right. When we talked about treating people as autonomous agents, we took away their decision making.

364
00:37:08,630 --> 00:37:11,660
Right. Exactly. Right. And even without them potentially even knowing.

365
00:37:12,710 --> 00:37:23,700
Good. I thought about that one, so thank you. Yep. They knew.

366
00:37:23,700 --> 00:37:28,850
They knew who had syphilis for sure. That's like what maybe, you know.

367
00:37:29,030 --> 00:37:34,520
I mean, I think I am not it's not clear to me if if they told them exactly what they had, they knew they had some kind of ailment.

368
00:37:41,680 --> 00:37:48,890
My guess is they probably told him as little as they needed to tell them. What about beneficence?

369
00:37:49,040 --> 00:37:53,660
So beneficence is a balancing or a favorable risk, harm benefit.

370
00:37:55,270 --> 00:38:00,880
So what about when this study first started? What what would you say about the balance of risks and harms being in this study?

371
00:38:02,310 --> 00:38:07,320
What would you say about that? That balance was it? Incredibly out of out of balance.

372
00:38:21,420 --> 00:38:25,380
Mm hmm. Hard to say, right? It's not clear.

373
00:38:26,820 --> 00:38:30,560
Sure. But then what? Absolutely right.

374
00:38:30,570 --> 00:38:37,890
The risk benefits they're being enrolled in, this study very clearly went towards more harm than good.

375
00:38:38,640 --> 00:38:46,620
Right. And what about justice? Right. So we talked about the justices who bears the burden and who gets the benefits from the research.

376
00:38:50,460 --> 00:38:53,560
Yeah. Did you have your hands up? Right. Okay.

377
00:38:53,950 --> 00:39:23,929
I try not to pick on people at random. Well, you've got to think about populations, though.

378
00:39:23,930 --> 00:39:31,069
And I would actually say that that that just the injustice was done before I get to beginning the selection of subjects.

379
00:39:31,070 --> 00:39:35,990
So before, you know, the treatment even became available, there was still injustice being done, right?

380
00:39:36,350 --> 00:39:43,240
Because they went at a marginalized group. They went to people that were poor, low education, racial minorities.

381
00:39:43,250 --> 00:39:49,610
Right. So they took on all the burden here. And again, it's not like the actual research result came out of this study.

382
00:39:50,120 --> 00:39:55,730
Right. But other people stood to benefit if if scientific knowledge came out of this.

383
00:39:56,330 --> 00:40:00,200
All right. But yeah, so so, you know, the respect for persons part, you know,

384
00:40:00,200 --> 00:40:04,220
these people did voluntarily consent, but they weren't really told what this study was about.

385
00:40:04,970 --> 00:40:08,510
They certainly were not informed about penicillin when it when it became available.

386
00:40:08,750 --> 00:40:12,860
And as you mentioned up here, they were not given the ability to make decisions for themselves.

387
00:40:12,860 --> 00:40:16,520
Right. So, you know, they should have been able to say, oh, there's a treatment for this.

388
00:40:16,630 --> 00:40:22,910
I'm out of this study. Okay, I'm going to get treated. Right. But again, this 19, 1940s, right.

389
00:40:22,910 --> 00:40:24,889
It's not like this is showing up on Twitter, right?

390
00:40:24,890 --> 00:40:33,290
That there's there's a treatment right now, as we said, this this risk benefit ratio, you know, arguable where it is in the beginning.

391
00:40:33,590 --> 00:40:37,060
Right. But clearly, after a treatment becomes available, you know,

392
00:40:37,070 --> 00:40:46,070
it's it's clearly in the favor of the or the risks far outweigh the benefits look good so let's not try to take

393
00:40:46,550 --> 00:40:52,460
those ideas from the Belmont Report and think about how they translate to other areas of scientific research.

394
00:40:52,970 --> 00:41:00,320
So specifically, I want to talk about, you know, something that popped up in my area of work because I work on in the in the area of genetics.

395
00:41:00,620 --> 00:41:06,290
And so a lot of the ethical considerations that come with scientific technology, so many people are familiar with the CRISPR.

396
00:41:07,810 --> 00:41:11,560
So like everybody. So when I first presented this, like three years ago, nobody put their hands up, right?

397
00:41:11,560 --> 00:41:17,350
So so imagine like how, how, how quickly knowledge become gets more complex, right?

398
00:41:17,350 --> 00:41:22,749
So for for if you don't know much about CRISPR, CRISPR is a a scientific technique,

399
00:41:22,750 --> 00:41:28,720
a lab technique that allows you to make targeted changes to DNA sequence.

400
00:41:29,080 --> 00:41:32,409
Right. So I realized when I was looking back over this, I put the word easily here.

401
00:41:32,410 --> 00:41:35,319
You know, it's not like anybody can just go in and just do this right.

402
00:41:35,320 --> 00:41:39,969
But compared to what was needed before, it was something that could be done at scale.

403
00:41:39,970 --> 00:41:41,379
Right, to make changes to DNA.

404
00:41:41,380 --> 00:41:48,850
And so it offers the potential for sort of like big changes to human health, being able to be very targeted treatments for human health,

405
00:41:49,610 --> 00:41:54,160
already being used extensively in basic science, agriculture and in model systems.

406
00:41:54,490 --> 00:41:57,639
It's use in humans, though, is much more controversial.

407
00:41:57,640 --> 00:42:05,210
Right. And so there are some clinical trials that exist where they try to use CRISPR to treat genetic diseases or.

408
00:42:05,270 --> 00:42:09,879
But we need to we need to distinguish here between treating a static disease on a

409
00:42:09,880 --> 00:42:16,240
single person versus making changes that could then be spread right to to offspring.

410
00:42:16,670 --> 00:42:24,670
Okay. So what I mean by this is like if somebody has like a liver ailment, right, and you try to do targeted treatment on that person's liver, right?

411
00:42:24,670 --> 00:42:27,190
Well, those any changes that you make, right.

412
00:42:27,190 --> 00:42:34,150
Those will be propagated on to other people if you make changes in germline cells and specifically like sperm or egg.

413
00:42:34,420 --> 00:42:37,450
Right. Those are changes that could be passed on to subsequent generations.

414
00:42:37,450 --> 00:42:44,680
Right. People that don't actually need the treatment that the original person's getting it for, but inherit any kind of changes that get made.

415
00:42:45,810 --> 00:42:52,980
And so it brings up the question, you know, should we make changes that could fundamentally affect future generations without their consent?

416
00:42:54,690 --> 00:42:57,000
And so there have been a couple, you know, things people have thought about.

417
00:42:57,030 --> 00:43:04,560
One is this idea of like designer babies, like if you make it so that if you have an embryo and you could say, I want them to have that color hair.

418
00:43:04,570 --> 00:43:08,430
Right. Well, could you make that change? So there is this idea of designer babies.

419
00:43:10,050 --> 00:43:18,570
There's also the issue that even if you you make changes using CRISPR as a perfect right to do every single time gets it exactly right.

420
00:43:18,600 --> 00:43:23,240
There's these issues of off target effects right where I want to change this gene.

421
00:43:23,250 --> 00:43:28,050
But the sequence looks pretty similar to what's over here. And I actually make a change here and here.

422
00:43:28,320 --> 00:43:34,440
Right. And I wasn't expecting this change. So now I maybe don't even haven't thought about what the what the consequence of that is.

423
00:43:35,460 --> 00:43:43,590
And so the National Academy of Sciences, Engineering and Medicine in 2017 put out a report with guidelines and recommendations.

424
00:43:43,870 --> 00:43:50,790
You know, people stayed away from from doing using CRISPR in ways that could could could involve germline editing.

425
00:43:51,180 --> 00:43:56,100
Right. So it says, you know, you know, several criteria that should be met before allowing germline editing,

426
00:43:56,670 --> 00:44:00,960
but that, you know, this is already being used in some sort of non heritable applications.

427
00:44:02,510 --> 00:44:09,010
Just because you can do something doesn't mean that you or you can do something, doesn't mean you should do it right.

428
00:44:09,020 --> 00:44:12,230
And unfortunately, some people went ahead and did this right.

429
00:44:12,240 --> 00:44:17,270
So there's an example of a Chinese scientist named G.

430
00:44:17,420 --> 00:44:21,559
G and who hopefully I got that close to. Right. And so this was in 2000.

431
00:44:21,560 --> 00:44:32,350
He got up at a conference and announced that he had used CRISPR to genetically modify three human embryos to and that

432
00:44:32,390 --> 00:44:39,380
what he specifically did was he modified this gene in the embryos that would be thought to confer resistance to HIV.

433
00:44:41,270 --> 00:44:47,839
I sort of go back to the syphilis study where people can rationalize the actions that they have.

434
00:44:47,840 --> 00:44:52,130
Right. People rationalized. Well, it would be good to know the natural progression of syphilis.

435
00:44:52,400 --> 00:44:54,380
So that justifies what I'm about to do here.

436
00:44:55,100 --> 00:45:06,050
His rationale for why he did this was that if he could, Gene, make this change in a gene that was thought to confer resistance to HIV,

437
00:45:06,620 --> 00:45:15,020
it would it would eliminate sort of especially in places where HIV is still fairly highly prevalent.

438
00:45:15,590 --> 00:45:23,480
He mentioned, you know, places in Africa, especially where children or mothers could potentially hand down HIV to their children,

439
00:45:23,840 --> 00:45:29,570
that it would save them from the health problems and severe discrimination that people with HIV face.

440
00:45:30,200 --> 00:45:38,220
So he rationalized this to himself why this was okay for him to do, even after making that rationalization.

441
00:45:38,240 --> 00:45:47,600
He then went and found couples right that were willing to participate in this work where it was the father that was infected with HIV,

442
00:45:47,600 --> 00:45:54,650
but the mother was not. And so a father is not going to be able to pass down their HIV infection to the child.

443
00:45:54,680 --> 00:45:59,930
Right. So even the individuals that he recruited to be part of the study,

444
00:46:00,260 --> 00:46:04,040
they themselves didn't even stand to benefit from the changes that could have happened.

445
00:46:04,400 --> 00:46:08,240
Right. These children were not at increased risk of HIV infection.

446
00:46:10,750 --> 00:46:16,920
So the research community overwhelmingly deemed this, you know, an ethical failure.

447
00:46:16,930 --> 00:46:21,370
This was a premature experiment, irresponsible, unjustifiable,

448
00:46:21,370 --> 00:46:28,359
in part because of the fact that the people that it was performed on these children, you know, they had no consent to do this.

449
00:46:28,360 --> 00:46:34,780
Right. These children were born being modified in a way that they were not allowed to say something about.

450
00:46:35,260 --> 00:46:40,960
And above that is the fact that the modifications that were made weren't even protecting them from anything.

451
00:46:41,200 --> 00:46:46,270
Right. They were not at risk of inheriting or getting the HIV infection.

452
00:46:46,570 --> 00:46:51,340
So these changes were made to them, but they wouldn't even benefit even if the change worked, which wasn't clear.

453
00:46:51,940 --> 00:46:55,660
Right. So the effectiveness was unclear and it was just unneeded for these children.

454
00:46:56,830 --> 00:47:01,930
And so, you know, these are these are you know, this is this is 2018, 2019.

455
00:47:02,350 --> 00:47:11,380
You know, these are our three children that now have these mutations where the long term consequences are just completely unclear.

456
00:47:11,980 --> 00:47:16,660
So he was sentenced to three years in prison while I did my check up on this this story yesterday.

457
00:47:16,660 --> 00:47:22,750
He was just recently released from prison, actually. And here's if you're if you're interested in this, I put a link here,

458
00:47:23,050 --> 00:47:28,990
a much more detailed story about, you know, the fact that he was not alone in acting on this,

459
00:47:28,990 --> 00:47:32,240
that there were a lot of people involved in this and that, you know,

460
00:47:32,290 --> 00:47:39,910
whether he was a rogue by himself or he was sort of the scapegoat that was used for for a large group of people that were involved in this.

461
00:47:40,360 --> 00:47:42,970
You can get more information at that store or at that link.

462
00:47:44,930 --> 00:47:50,659
But if we take those same ideas of respect for persons, beneficence and justice, you know, we can think about,

463
00:47:50,660 --> 00:47:59,300
you know, how did you know this scientists violate these these principles by the research that he did.

464
00:48:00,050 --> 00:48:04,490
And again, my point here that I want to make so again, this bill one report created these principles,

465
00:48:04,970 --> 00:48:09,790
gave the applications for how you achieve those principles. And they weren't just for one study right there.

466
00:48:09,790 --> 00:48:15,079
There are things that we can take and we can apply to any study. So what about respect for persons?

467
00:48:15,080 --> 00:48:18,680
How do you think that was violated in this specific research study?

468
00:48:21,570 --> 00:48:26,880
Yeah. Right.

469
00:48:26,890 --> 00:48:31,230
So so we said, right, informed consent is is how you achieve respect for persons.

470
00:48:31,800 --> 00:48:34,850
They certainly didn't get the informed consent of the children. Right.

471
00:48:35,040 --> 00:48:40,560
That were embryos at the time this was done to them. They were not.

472
00:48:40,580 --> 00:48:43,800
Yep. Go ahead. Yeah. Go for it.

473
00:48:53,870 --> 00:48:59,389
I would say you look a little bit more here. Like I said, I don't know all the details, but it's a fair question.

474
00:48:59,390 --> 00:49:04,250
Right. You know, it goes at that informed consent because informed consent is giving full information.

475
00:49:04,520 --> 00:49:08,180
Why is this being done? What exactly is being done?

476
00:49:08,420 --> 00:49:11,780
What are the risks to me? What are the benefits?

477
00:49:11,780 --> 00:49:14,510
Potential benefits to me? What are the potential benefits to society?

478
00:49:14,540 --> 00:49:22,100
Maybe he emphasized, you know, potential benefits to society and not the potential benefits to the people who are actually in the study.

479
00:49:22,550 --> 00:49:30,080
Did they know that their child might not have been able to or their child was not at increased risk for HIV infection?

480
00:49:30,110 --> 00:49:34,700
I don't know. It's a good question. But it it gets it all goes into that respect for persons.

481
00:49:34,700 --> 00:49:40,460
Right. Clearly, they were not given the informed consent so that they could act as that an autonomous agent.

482
00:49:41,690 --> 00:49:56,890
What about beneficence? Yeah, I think. Exactly right.

483
00:49:58,500 --> 00:50:03,030
Yeah. What what was their what was their risk if they hadn't been in this this trial?

484
00:50:03,360 --> 00:50:09,440
Nothing. Just as any other person. Right. You know, again, how I said some of these things change, right?

485
00:50:09,450 --> 00:50:15,870
There are times when children are born with conditions where if left untreated.

486
00:50:16,530 --> 00:50:20,150
Right. That that's a problem. Right. You know, this is not that situation.

487
00:50:20,160 --> 00:50:24,960
Right. So the risks and benefits have to be completely different. These are children that are not facing any inherent risks.

488
00:50:25,260 --> 00:50:30,600
What benefit are you even providing for them? Justice is on justice.

489
00:50:33,890 --> 00:50:43,640
Selection of subjects, we think. Mm hmm.

490
00:50:48,820 --> 00:50:52,100
But they kind of have. Sure.

491
00:50:52,850 --> 00:50:55,880
Good. I don't have a good one. So that that's as good as any I could come up with.

492
00:50:56,060 --> 00:51:02,240
There's only three people in it. It's hard to. I don't know. And actually, part of the reason we don't know is because they've been protected.

493
00:51:02,360 --> 00:51:06,020
Right. I mean, you know, we don't want to advertise who these children are.

494
00:51:06,470 --> 00:51:10,400
Right. I mean, that's not fair to the children. Right. That they would have to go through. So actually, they've been protected.

495
00:51:10,400 --> 00:51:15,170
So I couldn't tell you, you know, much about the people that were actually in the study.

496
00:51:15,170 --> 00:51:18,770
But I mean, yeah, I mean, he might have preyed on the fears of people, right?

497
00:51:19,350 --> 00:51:25,910
But yeah, this is what we got here. So again, taking these these these ideas of respect for persons, beneficence and justice,

498
00:51:26,240 --> 00:51:34,280
they are the way in which you can start to analyze the, the, the the ethics of a research study.

499
00:51:34,280 --> 00:51:37,230
And so in your homework, I'm going to ask you to do this, okay?

500
00:51:37,340 --> 00:51:43,760
I'm going to ask you to look at a research study and try to think about how do each of these principles apply.

501
00:51:45,970 --> 00:51:52,870
So the the examples I've given up to now, you know, we've thought about harm done to to an actual human person, right?

502
00:51:52,870 --> 00:51:57,190
That that's where the misconduct existed. This, of course, is a class about data.

503
00:51:57,460 --> 00:52:01,630
Right. So I want to move into this idea of that research misconduct can exist with data.

504
00:52:02,050 --> 00:52:05,020
Right. And so there are there are multiple ways in which this can happen.

505
00:52:05,020 --> 00:52:09,940
You know, so if you look at this, this link from the NIH, they give it fabrication,

506
00:52:09,940 --> 00:52:14,769
falsification and plagiarism, and they give you more examples at their website.

507
00:52:14,770 --> 00:52:19,900
Right. But fabrication is just making up data or making up results and just presenting them as correct.

508
00:52:20,530 --> 00:52:27,999
Falsification is when you have some data and you you sort of change or omit it so you're just not accurately presenting it.

509
00:52:28,000 --> 00:52:35,080
So maybe you choose not to report all your data if it doesn't go align with what the story that you want to tell.

510
00:52:35,740 --> 00:52:38,980
And plagiarism, right. Is the appropriation of other people's work.

511
00:52:39,370 --> 00:52:44,660
Right. Without giving them appropriate credit. So I asked you to watch.

512
00:52:45,050 --> 00:52:49,910
You know, I'm going to give you a couple examples of of research misconduct that that occurs with data.

513
00:52:50,480 --> 00:52:57,820
So I ask you to watch this video deception at Duke. So this story was from, you know, a couple of years ago.

514
00:52:58,000 --> 00:53:03,390
But, you know, it's just this huge example, not only because it was done, it was done at Duke,

515
00:53:03,580 --> 00:53:08,080
which is a well-respected place of higher learning or at a medical school.

516
00:53:08,440 --> 00:53:11,030
And also the ramifications that came out of it.

517
00:53:11,060 --> 00:53:20,110
You know, I talked about wasted resources and how people can still be hurt, even if the the original ethical work was done with data.

518
00:53:21,100 --> 00:53:28,270
So the take home, if you didn't watch the video yet is that two researchers and Neil Party and Joseph Nevins,

519
00:53:28,660 --> 00:53:33,819
they had some publicly available gene expression data that was looking at expression for

520
00:53:33,820 --> 00:53:38,380
tumor cells and expression for normal cells that had been treated with different chemicals.

521
00:53:39,250 --> 00:53:46,420
And what they what they claimed that they were able to do was that they were able to figure out, to predict,

522
00:53:46,840 --> 00:53:57,459
based on looking at individuals genetic profile, they could figure out, you know, who was likely to respond to seven different chemotherapy drugs.

523
00:53:57,460 --> 00:54:03,130
Right. And so, again, if you are, you know, hopefully you're not super familiar with this, but, you know,

524
00:54:03,430 --> 00:54:08,829
chemotherapy to some degree is like a hit or miss kind of thing where you try to treat, see if it works.

525
00:54:08,830 --> 00:54:14,920
If it doesn't work, let's try something else. Right. But there can be time wasted, right, in trying these different treatments.

526
00:54:15,310 --> 00:54:22,690
Imagine how important or how critical it would be from a clinical perspective if you didn't have to do that trial and error treatment.

527
00:54:22,700 --> 00:54:30,460
Right. You just could sort of immediately know what is the most likely chemotherapy that'll work for this specific person.

528
00:54:30,760 --> 00:54:36,400
And so this result, you know, really had the ability or the potential to change treatment for cancer patients.

529
00:54:36,700 --> 00:54:39,879
And it was such an exciting result when it got into nature medicine.

530
00:54:39,880 --> 00:54:47,350
We talk about high profile journals. That's certainly one. But people that actually started clinical trials were initiated based on this results.

531
00:54:47,570 --> 00:54:50,629
Right. So I just included this here.

532
00:54:50,630 --> 00:54:56,709
I'm not going to go through this if you're don't know a whole lot about gene expression, how it's measured up microwaves or arrays.

533
00:54:56,710 --> 00:54:59,740
You know, here's some information here. Not not necessary for this.

534
00:55:00,310 --> 00:55:07,510
Right. But people people's lives were being put at stake by enrolling in these clinical trials where, you know,

535
00:55:07,660 --> 00:55:12,460
it's going to turn out faulty information was being used as the basis for granting these trials.

536
00:55:13,450 --> 00:55:14,739
So, again, if you watched the video,

537
00:55:14,740 --> 00:55:25,510
you heard the story of two biostatistician or actually three biostatistician at MD Anderson that went and looked at the data.

538
00:55:25,520 --> 00:55:31,630
So one of the keys that made this possible was they were able to go look at the data upon which this result was based,

539
00:55:31,930 --> 00:55:36,280
and they tried to reproduce the results that the original researchers showed.

540
00:55:36,850 --> 00:55:42,690
So we're going to talk next Tuesday about reproducibility and replication and the importance of it, right?

541
00:55:42,700 --> 00:55:48,790
This is one of the examples of importance of it, you know, and they didn't go into this thinking, oh, this was definitely a lie.

542
00:55:49,120 --> 00:55:56,019
This is not true. Right. They were probably working in similar areas and they said, oh, let me go see if I can take the next step.

543
00:55:56,020 --> 00:56:02,050
Right, because that's how research works. And one of the ways you take the next step is you say, let me be able to do what that person did,

544
00:56:02,320 --> 00:56:09,610
get to that step and then let me see what I can do further. And in the process of doing that, what they realized is that there were lots of errors,

545
00:56:09,660 --> 00:56:13,900
like things that were sort of inexplicable would be beyond careless to explain.

546
00:56:14,500 --> 00:56:18,820
And they just seemed intentional, right? Things that were beyond enormous innocent mistakes.

547
00:56:19,390 --> 00:56:23,530
And so they published their their analysis in the Annals of Applied Statistics,

548
00:56:24,310 --> 00:56:28,600
and it led to an investigation of of those initial researchers at Duke.

549
00:56:28,600 --> 00:56:30,370
And so hopefully that's what you saw in the video.

550
00:56:31,390 --> 00:56:36,040
So this highlights, again, the role of reproducibility, which we're going to get into more in our next lecture.

551
00:56:37,640 --> 00:56:45,530
Okay. But again, it's what you do to realize the fact that that so many people had already enrolled in clinical trials.

552
00:56:45,530 --> 00:56:50,749
And I talked about waste of resources. Right. So people trying to follow up on this result, right, to the next step.

553
00:56:50,750 --> 00:56:58,159
So that's time and that's money that was wasted, right? The time and money that gets wasted on recruiting patients to be in a clinical trial.

554
00:56:58,160 --> 00:57:03,739
All the paperwork that goes into starting a clinical trial, not to mention the harm to the patients who,

555
00:57:03,740 --> 00:57:09,830
you know, imagine being misled, entering into a clinical trial. You think there's some hope that that you might actually be able to be helped.

556
00:57:10,190 --> 00:57:13,680
Right. And so that, you know, I'd imagine that I don't know.

557
00:57:13,690 --> 00:57:14,690
I don't want to get in people's heads.

558
00:57:14,690 --> 00:57:23,420
But I imagine when that the original people are doing that research and I don't know why they would have done this fabrication of data,

559
00:57:23,840 --> 00:57:26,930
but they probably weren't thinking about it as hurting people.

560
00:57:27,050 --> 00:57:36,230
Right. They're probably the pressure to publish. Right, that the the desire to to get big publications and look like, you know, scientific heroes.

561
00:57:36,230 --> 00:57:40,730
Right. They probably weren't thinking about the downline ramifications that it had on people.

562
00:57:41,720 --> 00:57:48,650
So we said, you know, not a small number of people, 117 patients were already enrolled, lots of retractions that went on.

563
00:57:49,190 --> 00:57:53,090
So those people were barred from funding from the NIH for a certain amount of time.

564
00:57:53,300 --> 00:58:00,650
And actually, that main person, would you believe it still has a professor position in North Dakota?

565
00:58:01,220 --> 00:58:04,310
So, you know, isn't it crazy that you could still be?

566
00:58:05,980 --> 00:58:10,870
You know, at an academic research institute, even after doing something like this.

567
00:58:14,760 --> 00:58:17,820
But this one. This one's. How many people know about Elizabeth Holmes?

568
00:58:18,390 --> 00:58:23,490
Yeah, everybody knows about her now. Again, this is this was like a cool story a couple of years ago that people were learning about.

569
00:58:23,490 --> 00:58:27,360
Now she's in the news all the time. Right. So if you don't know.

570
00:58:30,900 --> 00:58:36,390
So Elizabeth Holmes was a 19 year old Stanford dropout.

571
00:58:36,690 --> 00:58:41,070
You know, one of the people that was, you know, dropped out of school because they had an amazing idea that,

572
00:58:41,520 --> 00:58:43,520
you know, I'm just going to go run with this idea.

573
00:58:43,530 --> 00:58:51,360
And what she did when she started a company called Theranos and her claim was that she created this microfluidic device,

574
00:58:51,540 --> 00:59:01,889
a minuscule miniature device that with a very small amount of blood, could sort of instantaneously run tons of lab tests.

575
00:59:01,890 --> 00:59:06,520
Right. So you could imagine how fantastic an invention this would be.

576
00:59:06,540 --> 00:59:14,490
Right. One of the one of the troubles of, you know, if you talk to people about like if they go to an area and they're trying to help people,

577
00:59:14,760 --> 00:59:21,930
is that you just these places are often not set up with the equipment to be able to do the medical work that you have or that you need.

578
00:59:21,930 --> 00:59:25,829
Right. And so, you know, she had this claim that she could do this.

579
00:59:25,830 --> 00:59:30,000
And it's just this is a pretty remarkable achievement that she had done.

580
00:59:30,270 --> 00:59:35,760
Walgreens, right. As a as a place that runs blood tests, developed a partnership with her.

581
00:59:37,010 --> 00:59:41,930
The FDA. Right. They gave her approval to move forward with these kinds of things.

582
00:59:41,930 --> 00:59:46,459
Right. So you're like, we're trusting the FDA. They're paying attention to what's going on.

583
00:59:46,460 --> 00:59:53,270
Right. She had gotten many investors, the company she personally, I guess, was was valued at over $10 billion.

584
00:59:54,080 --> 00:59:55,640
It had many investors involved.

585
00:59:56,720 --> 01:00:02,629
But what slowly started to happen and this was in part with Walgreens, like they were like, okay, once, when are we going to get this?

586
01:00:02,630 --> 01:00:06,500
When's the first shipment coming in or can we see some results? You know, can you let us know?

587
01:00:06,830 --> 01:00:10,400
And the FDA starts saying, hey, you know, we gave you some approval, but we need you.

588
01:00:10,730 --> 01:00:13,790
We've got to see something, right? Where's the proof that you're actually able to do this?

589
01:00:14,360 --> 01:00:20,150
And the company, Thanos, you know, just repeatedly failed to meet the deadlines or produce any published results.

590
01:00:20,660 --> 01:00:24,049
Right. And so, you know, this is a key. Right. We talked about how important publications are.

591
01:00:24,050 --> 01:00:29,150
Right. You'll notice sometimes that that companies don't publish results.

592
01:00:29,150 --> 01:00:36,139
Right. And it's problematic, you know, from their perspective, they have a financial incentive to not publish their results.

593
01:00:36,140 --> 01:00:39,420
Right. Because their company and the business of their company relies on it.

594
01:00:39,800 --> 01:00:45,200
And yet, if you're not publishing, how the heck do we know that what you're you're selling us actually works and is true?

595
01:00:45,590 --> 01:00:48,770
Right. So they got around, you know, they just kept delaying this.

596
01:00:48,770 --> 01:00:55,310
And so eventually they got caught out by the SCC and the company executives were charged with massive fraud.

597
01:00:56,000 --> 01:01:01,970
And so in January of 2022, she was found guilty of defrauding investors.

598
01:01:02,150 --> 01:01:06,830
And so just very recently, I just added that line, you know, so she was sentenced to 11 years in prison.

599
01:01:07,190 --> 01:01:15,320
And originally I had here Jennifer Lawrence was going to play her in a movie.

600
01:01:15,890 --> 01:01:21,920
And that was my bullet point. But but Jennifer Lawrence just said that Amanda Seyfried did it and she's like, Well, I'm not going to top that.

601
01:01:21,920 --> 01:01:28,280
So she dropped out of it. So if you want to follow up more on the story, you know, there is a Hulu movie on it.

602
01:01:31,140 --> 01:01:37,440
Okay. So in each one of these cases that I've showed you so far, it's clear, right?

603
01:01:37,450 --> 01:01:43,570
It's obvious that the person has done a person or persons or entity has done something wrong.

604
01:01:44,300 --> 01:01:51,270
But that's not always how it is in research. Right. And one of the things I want to talk about now is this ethical gray zones, right.

605
01:01:51,290 --> 01:01:55,270
That they exist and they're much more common than you probably think in research.

606
01:01:55,660 --> 01:02:01,360
And I think until I really got going and involved in research, I didn't realize how frequent and common they were.

607
01:02:02,920 --> 01:02:09,220
And so I asked you to read a paper to people read the paper clinical clinical trials in developing nations.

608
01:02:09,910 --> 01:02:13,180
So I asked you to read that one. It's a short paper if you haven't read it yet.

609
01:02:13,960 --> 01:02:19,600
And then I primed you initially with this with this idea about an incidental or secondary research findings.

610
01:02:19,600 --> 01:02:27,310
We'll get to that in a second. So this, this, this the paper that I asked you to read about was clinical trials in developing nations.

611
01:02:27,520 --> 01:02:34,540
So the most the thing where you have to start when you think about this is the fact that people that are

612
01:02:34,540 --> 01:02:41,530
often most in need of medical aid are also the ones that are the most vulnerable to being exploited.

613
01:02:42,040 --> 01:02:45,549
And this can happen particularly in developing nations where they don't have the

614
01:02:45,550 --> 01:02:52,030
financial resources right to to have high standards of medical care and as a result,

615
01:02:52,030 --> 01:03:01,370
can be exploited. So this paper gives an example of clinical trials that were run in the 1990, in the early 2000.

616
01:03:01,960 --> 01:03:11,350
And the goal of them was to to reduce vertical transmission, again, this idea of mother to child transmission of HIV.

617
01:03:11,710 --> 01:03:18,550
And so the the the twist comes in here is that a lot of research was already been done on this.

618
01:03:18,550 --> 01:03:24,370
And people had already discovered that that through this extensive retroviral regimen,

619
01:03:25,000 --> 01:03:29,180
they were able to drastically reduce this vertical transmission curve.

620
01:03:29,830 --> 01:03:35,810
The issue was this treatment, which was both highly effective, was also highly expensive.

621
01:03:35,830 --> 01:03:46,090
Right. And so the individuals and the poor nations who would definitely stand to benefit from this, you know, would not be able to afford.

622
01:03:46,420 --> 01:03:51,819
Right. This treatment that was available. And so a series of clinical trials were run, as I said,

623
01:03:51,820 --> 01:04:01,600
in the early in the nineties and early 2000s were run with the goal of creating a less expensive regimen of the antiviral drugs.

624
01:04:01,610 --> 01:04:08,140
Right. And so they might not be as effective as what already exists, but they will be less expensive.

625
01:04:08,690 --> 01:04:13,270
Okay. So wrap your mind around that for a second. That's what's different than a lot of clinical trials, right?

626
01:04:13,570 --> 01:04:17,010
When you think of clinical trials, you usually think of, well, there's this, this is what we've got.

627
01:04:17,020 --> 01:04:20,170
This is what the current treatment is. And we're trying to make something better.

628
01:04:20,650 --> 01:04:24,790
Right. Very, very, very standard clinical trial. This one's a little bit different.

629
01:04:24,970 --> 01:04:30,340
We've got this thing that works really well over here, but not everyone's going to be able to get this.

630
01:04:30,520 --> 01:04:34,630
So can we come up with something that works? You know, almost as well.

631
01:04:35,630 --> 01:04:45,470
Right. But less expensive. Okay. So these these trials were were targeted towards benefiting people in developing countries.

632
01:04:45,830 --> 01:04:52,219
But there was also a lot of controversy about whether they were exploiting the individuals in those countries.

633
01:04:52,220 --> 01:04:54,980
And so I assign you this this paper to read,

634
01:04:54,980 --> 01:05:02,870
because it lays out what some of the arguments against doing the trials are and then give some defense of why they might be okay.

635
01:05:03,680 --> 01:05:10,579
So the first argument was that trials used to control arm that received placebos or obviously a

636
01:05:10,580 --> 01:05:17,420
placebo as being no effect treatment to determine the effectiveness of lower cost retroviral regimen.

637
01:05:17,810 --> 01:05:21,139
Right. And so typically when we think about a placebo, right,

638
01:05:21,140 --> 01:05:27,380
it's sort of the best available care that exists that's already gone through approval and clinical trials.

639
01:05:27,650 --> 01:05:33,360
And now I'm going to try something experimental in these cases, the placebo was nothing.

640
01:05:33,380 --> 01:05:38,300
It was no treatment. Right. If you gave the best possible treatment.

641
01:05:38,780 --> 01:05:44,420
Right. Which we know exists in these retroviruses. Well, we know these new things are probably not going to work as well.

642
01:05:44,720 --> 01:05:49,460
Right. But that's not our goal, is to make them effective as as equally as effective.

643
01:05:49,670 --> 01:05:52,740
It's to make them better but affordable.

644
01:05:52,760 --> 01:06:03,010
Right. But it opened up this controversy. Right, about why, you know, is this okay to provide a placebo when when the current, you know, what is that?

645
01:06:03,350 --> 01:06:06,440
How do we define the current standard of care?

646
01:06:06,920 --> 01:06:10,970
Do we define it by what's available in a in another country?

647
01:06:11,690 --> 01:06:17,060
Right. Or do we define it by what's available in this place where the treatment could be used?

648
01:06:17,450 --> 01:06:21,140
Right. And that's what the controversy dealt with.

649
01:06:21,680 --> 01:06:27,889
So one of the proposals that people put forward was to use historical controls from from other clinical trials.

650
01:06:27,890 --> 01:06:31,100
Right. So other trials that were have already been run. Right.

651
01:06:31,100 --> 01:06:34,700
And we know that trials had to be run on those original retroviral.

652
01:06:35,090 --> 01:06:39,709
Right. And there the placebo would have been no treatment. Why not just use those people.

653
01:06:39,710 --> 01:06:46,100
Right, and compare that treatment or that placebo arm, you know, to these these new drugs that are being used.

654
01:06:47,180 --> 01:06:52,220
But the problem in that if you read the paper, he actually shows it gives numbers from trials that,

655
01:06:52,370 --> 01:06:57,350
you know, HIV transmission rates, they vary over time. They vary in different in different settings.

656
01:06:57,630 --> 01:07:04,370
Right. So if you use a historical controls, you know, as I said, he gives an example where the historical trial,

657
01:07:04,700 --> 01:07:09,290
historical controls would have made a treatment feel appear to be effective.

658
01:07:09,730 --> 01:07:17,780
Right. So, okay, so if I take these controls from another study or the placebo arm from another study and I compare these ones over here.

659
01:07:18,200 --> 01:07:21,889
Right. I'm not comparing apples to apples. Right. There's different transmission rates that are going on.

660
01:07:21,890 --> 01:07:24,890
So I can't be sure that I'm actually seeing a difference. Right.

661
01:07:26,560 --> 01:07:30,200
Right. So like what what would have been the ramifications of the implications of this?

662
01:07:30,200 --> 01:07:35,329
Right. We would have thought, oh, this this drug appears effective, so let's start ramping it up.

663
01:07:35,330 --> 01:07:41,930
Let's start mass producing it. Let's start distributing it. Right. But in fact, it failed that that the two arm trial kind.

664
01:07:43,450 --> 01:07:49,959
Another argument that was made, you know, sort of that these are unethical was that the participants were coerced into participation,

665
01:07:49,960 --> 01:07:52,450
did not have had true voluntary consent.

666
01:07:53,080 --> 01:08:01,140
And so the argument here is that these are you know, we're thinking about women who currently were pregnant, right.

667
01:08:01,330 --> 01:08:06,280
Who have no alternative to protect the child that they're going to have from HIV infection.

668
01:08:06,640 --> 01:08:10,870
You know, are you exploiting them or are you giving them an opportunity?

669
01:08:11,010 --> 01:08:13,510
Right. And again, this is where we think about gray zones.

670
01:08:14,410 --> 01:08:23,049
You know, to I think the way the way I think of it was like to, you know, good, good natured people can disagree on the answer to that.

671
01:08:23,050 --> 01:08:26,560
Right. Or if they have no alternatives, their child's getting HIV.

672
01:08:27,010 --> 01:08:30,010
If we run this study, we might be able to prevent that fact.

673
01:08:30,040 --> 01:08:37,720
Right. And so, you know, it says some people, you know, all the time, you know, people are being offered incentives to join clinical trials.

674
01:08:38,150 --> 01:08:42,040
So it's not unheard of. Right. You just can't make that offer too good to refuse.

675
01:08:42,040 --> 01:08:45,070
So that that's when you start to become exploiting people.

676
01:08:45,670 --> 01:08:49,389
And so this this author claims, you know, this is get this is one person's claim.

677
01:08:49,390 --> 01:08:54,820
This is not the the definitive truth. But this author's claim is that, you know, nothing was denied.

678
01:08:54,820 --> 01:08:58,560
The individuals who chose not to participate so they weren't punished.

679
01:08:58,570 --> 01:09:05,260
You know, there's no punishment for for not being involved. And in most cases, the offer to be in this study was not unwelcome.

680
01:09:05,260 --> 01:09:13,030
That that the people who who most would benefit from it actually welcomed the opportunity to be involved in it.

681
01:09:14,260 --> 01:09:17,319
And they were properly informed that they might receive a placebo.

682
01:09:17,320 --> 01:09:26,770
So they weren't being lied to. And then one final argument that that he addresses is whether countries were being exploited,

683
01:09:26,770 --> 01:09:30,150
individuals and countries were potentially being exploited. Right.

684
01:09:30,220 --> 01:09:35,640
And so, again, this is the this goes out to the idea of what are the the balance of risks and benefits,

685
01:09:35,650 --> 01:09:40,210
who's taking on the risk, and then who stands to benefit from the research.

686
01:09:40,630 --> 01:09:48,070
And again, so remember that this is not a this is there's already an existing it's a retroviral very effective treatment.

687
01:09:48,560 --> 01:09:53,860
You know, so who stands to benefit? Well, it's not the people in countries that can afford that right now that stand to benefit.

688
01:09:54,280 --> 01:09:57,280
It's the people, developing nations that stand to benefit from this.

689
01:09:57,280 --> 01:10:02,680
Right. So that's if that's where you're going to run it. But now they're taking on all the risk, too, right.

690
01:10:02,890 --> 01:10:06,730
So there's this this this argument that he goes into.

691
01:10:06,730 --> 01:10:11,680
And one of the things where he says that that this these trials maybe got wrong is

692
01:10:11,680 --> 01:10:17,530
that if they were going to use these populations right to make to run their trials,

693
01:10:17,770 --> 01:10:28,330
perhaps there should be a little bit more of an incentive or to not not incentive the the places that run the trials and make the drugs.

694
01:10:28,600 --> 01:10:30,580
But you have to make these things affordable. Right.

695
01:10:30,760 --> 01:10:34,780
If you're going to use the population, you should ensure that the population can then benefit from them.

696
01:10:35,050 --> 01:10:43,300
Right. But he does argue there's this danger in in requiring, you know what, some people have argued that everybody has to be given the drug.

697
01:10:43,300 --> 01:10:46,720
Right, if it passes the test. But I mean. But that's expensive, right?

698
01:10:47,050 --> 01:10:52,600
And if you if you could you could essentially eliminate the ability to do some

699
01:10:52,600 --> 01:10:56,470
clinical trials by putting in a very strict requirement like that in court.

700
01:10:56,770 --> 01:11:01,690
So I don't I'm not involved in clinical trials, but I remember when when I when I learned about this,

701
01:11:01,990 --> 01:11:06,790
you know, this is a great example, I think, of an ethical gray zone because it's it's just not clear.

702
01:11:06,790 --> 01:11:10,240
Right. It's not obvious that something was being done incorrectly. Right.

703
01:11:10,250 --> 01:11:13,540
And there definitely are some positive things that came from the study.

704
01:11:16,990 --> 01:11:20,170
So now I want to switch now to another ethical gray zone.

705
01:11:20,170 --> 01:11:22,989
And so this is this idea of incidental findings. Right.

706
01:11:22,990 --> 01:11:26,950
So if you're if you're not familiar with the term incidental or secondary findings are surprise

707
01:11:26,950 --> 01:11:32,620
results that fall outside the primary research purpose of a study or a procedure that's being done.

708
01:11:32,950 --> 01:11:40,810
But they can have important health or personal, or can it impact the health or personal lives of the research participants?

709
01:11:41,290 --> 01:11:45,400
So, you know, some examples of what unanticipated research findings might be.

710
01:11:46,730 --> 01:11:56,540
Right is imagine that you a person enters a study, they get an MRI scan, and it's it's for trying to understand something about brain function.

711
01:11:56,810 --> 01:12:00,740
And in the process, you know, a mass is identified in somebody's brain.

712
01:12:00,740 --> 01:12:02,110
Right. But that was not the purpose. Right.

713
01:12:02,110 --> 01:12:08,540
You're not doing some sort of clinical work to try to understand, you know, what some some symptoms were or something.

714
01:12:08,540 --> 01:12:12,080
Right. It was just incidentally discovered in the process of this.

715
01:12:13,010 --> 01:12:18,589
This one should sound familiar. A pathogenic genetic variant discovered during sequencing for an unrelated trait.

716
01:12:18,590 --> 01:12:21,760
That's the one I talked about. What about this one?

717
01:12:22,210 --> 01:12:28,130
Can I give you some genetic ones? Because that's the area in familial inconsistencies in genetic data.

718
01:12:28,150 --> 01:12:34,090
Right. So. So parental surprises. Right. I didn't go into the study trying to find out who the parents were.

719
01:12:34,420 --> 01:12:40,780
Right. But we collected data on families and it's like, well, this person can't be the parent of this child.

720
01:12:40,810 --> 01:12:44,490
Right. Or ancestry. Right. I think this I see this a lot.

721
01:12:44,500 --> 01:12:47,110
I get very I hate to say it,

722
01:12:47,620 --> 01:12:55,000
I've seen people write stories about how disappointed they are when they do 23 in me and learn that they're not what they really thought they were.

723
01:12:55,970 --> 01:13:01,520
Of course you are. What you what you thought you were, right? I mean, you know, so just misunderstandings of stuff like that.

724
01:13:03,460 --> 01:13:05,740
And so why is this an ethical?

725
01:13:05,740 --> 01:13:13,180
GRAYSON Because the obligations and the responsibilities about what should happen when you find something that was not part of your research plan,

726
01:13:13,180 --> 01:13:18,570
it's not always clear. So here's the results of the poll everywhere that I started with earlier.

727
01:13:18,720 --> 01:13:23,070
So not a surprise. Almost all of you, 89% said, yes,

728
01:13:23,070 --> 01:13:30,810
you would tell this person that has this this potentially life altering genetic mutations about this incidental finding.

729
01:13:31,590 --> 01:13:34,440
Now, I'll tell you, I didn't make this up out of nowhere. This actually happened to me.

730
01:13:34,440 --> 01:13:39,930
So when I was a postdoc, I was working on a sequencing study and we were analyzing data.

731
01:13:39,940 --> 01:13:47,430
It was for type two diabetes. And I distinctly remember seeing in office with another post-doc guy named Qiu Jun

732
01:13:47,850 --> 01:13:52,469
and we're looking through this data and we're looking at loss of function variance,

733
01:13:52,470 --> 01:13:55,660
you know, that that could show up because we're hypothesizing that,

734
01:13:55,710 --> 01:14:00,900
that we might be able to see some loss of function variance segregating with type two diabetes.

735
01:14:01,200 --> 01:14:04,200
And we both saw it almost at the same time. We saw it in this woman.

736
01:14:04,410 --> 01:14:09,210
And like instantaneously we're both like [INAUDIBLE]. And we didn't know what to do.

737
01:14:09,220 --> 01:14:14,430
And it's like this sinking feeling that you're looking at information on a screen that you're like,

738
01:14:14,430 --> 01:14:21,930
This is a real human being out here that this information could have huge, you know, impact on her health.

739
01:14:21,930 --> 01:14:27,720
And she had children. Right. And she would potentially you know, she might not have known that she was passing this on to children.

740
01:14:28,110 --> 01:14:34,610
And I will tell you that I was in this. Yes. Group initially. We reached out to the peers of the study because we weren't sure what to do.

741
01:14:34,620 --> 01:14:38,940
Obviously, you know, we don't have a name for this person. But that was my initial inclination, too.

742
01:14:40,140 --> 01:14:45,780
But as it turns out, you know, and we got more involved in this, there's a lot more to consider than what we just saw at that moment.

743
01:14:46,420 --> 01:14:49,640
You know, so for one, we were a team of statistical geneticists. Right.

744
01:14:49,650 --> 01:14:53,310
I'm not a genetic. But they called.

745
01:14:54,310 --> 01:14:59,590
Jake. Councilor, thank you. Well, I'm not a genetic counselor. I'm not trained in how to evaluate or communicate that risk.

746
01:15:00,010 --> 01:15:05,020
Right. If I called this person up, could I do a good job of explaining what I actually found?

747
01:15:05,970 --> 01:15:11,390
Here's another one. How do I know this person can actually do something given this information?

748
01:15:11,400 --> 01:15:18,510
Right. For one, we're using sequencing technology that is good for research but would never be approved for clinical.

749
01:15:18,540 --> 01:15:27,960
Right. So I work with some pharmaco genomic people, you know, and they very much distinguish between research, genotyping and clinical genotyping.

750
01:15:28,290 --> 01:15:32,250
Right. And so, you know, I can't be positive that there wasn't a mistake on these.

751
01:15:32,790 --> 01:15:36,930
And I certainly don't have the means to provide, you know, treatments for this person.

752
01:15:38,600 --> 01:15:45,559
You know, it's also important to say, you know, there's everybody has a chance of a disease versus absolutely getting the disease right.

753
01:15:45,560 --> 01:15:50,730
And these are two different two different things. Right. But this is another one that's important.

754
01:15:50,750 --> 01:15:55,210
Patient regret over learning information. Right. It's one thing you can say upfront.

755
01:15:55,220 --> 01:16:03,080
Yeah, no, I would want to know that. But then if you're handed a piece of information, you know, for an incurable disease, you can't undo that.

756
01:16:03,350 --> 01:16:03,710
Right.

757
01:16:04,280 --> 01:16:13,040
Another one that pops up is what if you have people, two people who are equally affected by the incidental finding, but they have different opinions.

758
01:16:13,310 --> 01:16:16,880
Right. So imagine you've got the children of this woman.

759
01:16:17,210 --> 01:16:20,840
Right. Maybe one of them would want to know. And one of them doesn't want to know.

760
01:16:21,230 --> 01:16:26,570
Right, if their mother has this right. Well, once I tell one, I mean, it's going to get out eventually.

761
01:16:26,570 --> 01:16:32,240
Right. And so the person that didn't want to know is sort of violating, you know, what their wish was.

762
01:16:34,220 --> 01:16:39,110
So there there are risks and benefits of disclosing incidental findings and research.

763
01:16:39,110 --> 01:16:41,440
And, you know, this is this is something that I learn firsthand.

764
01:16:42,110 --> 01:16:48,260
So, yes, there is this opportunity to potentially provide and the chance that somebody can get medical help.

765
01:16:49,790 --> 01:16:52,790
It's something that people can plan ahead, even if it's an untreatable thing.

766
01:16:52,790 --> 01:16:57,529
They can plan ahead, could help their family. But there are risks.

767
01:16:57,530 --> 01:16:59,360
And these are the things that I really learned. Right.

768
01:16:59,630 --> 01:17:06,790
You know how people will take this knowledge that learning about an untreatable disease or changes in social or familial relationships.

769
01:17:06,800 --> 01:17:11,630
Right. This is the one that I that I mentioned of, like, you know, if you sequence a family and it turns out that,

770
01:17:12,180 --> 01:17:17,420
you know, the father maybe is not who you thought the father was, that could change family relationships.

771
01:17:17,630 --> 01:17:21,140
And if that's not what the research was about, have you really done anything good for them?

772
01:17:22,550 --> 01:17:24,700
Again, financial considerations I mentioned.

773
01:17:24,920 --> 01:17:30,260
And then again, there's always the fact that like I wasn't doing this, I wasn't specifically looking at BRCA2.

774
01:17:30,470 --> 01:17:41,520
You know, that information could potentially be wrong. So I will tell you that what wound up happening was we did not return this information.

775
01:17:43,960 --> 01:17:51,430
Getting back at some of the things I talked about before, when these people were were recruited to this study, there was a consent procedure, right.

776
01:17:51,430 --> 01:18:00,250
So they got informed consent. They actually signed off that they did not want to be contacted in the event of incidental findings.

777
01:18:01,060 --> 01:18:07,570
This study. You know, this is you know, this data. These people are consented probably 15 years ago or something.

778
01:18:07,930 --> 01:18:14,920
You know, I think now there's much more nuance in this that people are thinking more about incidental findings and how they might return them.

779
01:18:16,300 --> 01:18:20,920
But it does lay out the importance of having a predetermined protocol that thinks about this.

780
01:18:20,920 --> 01:18:28,210
Right. How important is it thinking about, you know, in advance, what will we do if we find some kind of incidental finding?

781
01:18:28,420 --> 01:18:31,390
So I'm not going be able to finish today's lecture, but I want to touch on this one real quick.

782
01:18:31,720 --> 01:18:37,150
So this this is an interesting one that popped up during COVID. And so it was something called challenge trials.

783
01:18:37,360 --> 01:18:43,809
Anybody hear about these challenge trials? Yes, it's pretty interesting. So they had there was this idea of it takes a long time.

784
01:18:43,810 --> 01:18:49,810
Right. If you say if you give somebody a COVID immunization.

785
01:18:49,810 --> 01:18:53,650
Right. And then you just send them out, well, you've got to wait just to see if they get COVID.

786
01:18:53,650 --> 01:18:57,879
Well, first, you need them to actually come in contact with COVID and that, you know,

787
01:18:57,880 --> 01:19:02,170
it would be a lot quicker if we just just sort of snorts and COVID, right.

788
01:19:02,440 --> 01:19:10,990
And lets then we know they've been, they've been, they've come in contact with it and then that'll speed up our findings.

789
01:19:11,440 --> 01:19:17,200
So these were actually put forward, these human challenge trials to accelerate COVID vaccinations.

790
01:19:17,200 --> 01:19:22,720
Right. And so I'm giving you two back and forth, right? So one is one is arguing why these things could actually work.

791
01:19:23,140 --> 01:19:26,560
And importantly is who do you actually recruit for these things? Right. So it's very clear.

792
01:19:26,560 --> 01:19:31,140
Right. You have to include, you know, where you're recruiting people who are young people who don't have co-morbidities.

793
01:19:31,150 --> 01:19:34,719
Right. And people understand that, yes, you are about to snort COVID.

794
01:19:34,720 --> 01:19:38,650
Okay. And then there were people that were like, no, no, this is crazy.

795
01:19:38,650 --> 01:19:48,730
You cannot do this. You cannot intentionally have somebody be exposed to this this virus that is causing a pandemic.

796
01:19:48,740 --> 01:19:54,430
Right. So it's interesting to read these two. They actually did happen in the U.K. There were actually challenge trials that occur.

797
01:19:54,820 --> 01:19:58,030
So I'll let you go ahead and read this and I'll pick up next time.

