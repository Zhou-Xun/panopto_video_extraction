1
00:00:09,098 --> 00:00:12,573
In this module, I'm going to show you
some examples of how we can apply systems

2
00:00:12,573 --> 00:00:14,680
thinking to environmental health.

3
00:00:14,680 --> 00:00:17,980
I'd like you to reflect for a moment on
the fact that you're likely to face some

4
00:00:17,980 --> 00:00:22,370
big thorny, very complex public
health challenges over your career.

5
00:00:22,370 --> 00:00:24,510
It may seem very difficult
to address these but

6
00:00:24,510 --> 00:00:28,785
actually systems thinking could give you
a potential solution to that problem.

7
00:00:28,785 --> 00:00:31,451
And that application of
some of these theories, and

8
00:00:31,451 --> 00:00:34,177
tools may let you take that
very complex problem, and

9
00:00:34,177 --> 00:00:38,311
distill it down into something that you
can understand, and ultimately act on.

10
00:00:38,311 --> 00:00:44,740
So, I'm going to run you through examples
of a theory and a couple of tools.

11
00:00:44,740 --> 00:00:48,930
The first thing we're going to focus
on here is path dependency theory.

12
00:00:48,930 --> 00:00:52,600
So we talked about path dependency
theory in a previous module, but not so

13
00:00:52,600 --> 00:00:55,190
much how it was applied
to environmental health.

14
00:00:55,190 --> 00:00:57,780
It has however been used in
studies of climate change.

15
00:00:57,780 --> 00:00:59,180
So as an example,

16
00:00:59,180 --> 00:01:03,700
one study evaluated the implementation
of environmental regulations or

17
00:01:03,700 --> 00:01:08,150
other response measures that would help
us adapt or respond to climate change.

18
00:01:08,150 --> 00:01:12,910
So the path dependency here is primarily
created by resource constraints.

19
00:01:12,910 --> 00:01:17,600
We have a finite amount of resources to
dedicate to addressing climate change.

20
00:01:17,600 --> 00:01:20,690
And those resource constraints
end up narrowing the range of

21
00:01:20,690 --> 00:01:24,770
options that we have for
regulations that we could implement.

22
00:01:24,770 --> 00:01:29,000
So in this particular study, they actually
identified some issues in the potential

23
00:01:29,000 --> 00:01:32,530
pathway between us and
solving climate change.

24
00:01:32,530 --> 00:01:37,385
One of the facts was the imperfect
nature of the information available to

25
00:01:37,385 --> 00:01:42,690
regulators, often means that by the time
they get new information, it's too late.

26
00:01:42,690 --> 00:01:46,260
They've already made a decision
that maybe was based on poor or

27
00:01:46,260 --> 00:01:48,290
insufficient information.

28
00:01:48,290 --> 00:01:53,380
And so the idea here is that path
dependency theory lead them to the result

29
00:01:53,380 --> 00:01:57,500
that existing approaches to climate
change are probably not sufficient

30
00:01:57,500 --> 00:02:01,620
to tackle the issues because we have
such imperfect information, and

31
00:02:01,620 --> 00:02:05,000
such narrow resource constraints.

32
00:02:05,000 --> 00:02:08,570
So a second example focuses
on agent-based modeling.

33
00:02:08,570 --> 00:02:09,296
In this case,

34
00:02:09,296 --> 00:02:13,794
there was an environmental health study of
safety performance on a construction site.

35
00:02:13,794 --> 00:02:17,545
And they were looking at the interactions
between a couple of different agents,

36
00:02:17,545 --> 00:02:19,820
the work site itself,
the individual workers.

37
00:02:19,820 --> 00:02:24,480
And then certain safety investments that
the management of the site might make.

38
00:02:24,480 --> 00:02:28,460
As with many systems thinking studies,
this one did not focus on the absolute

39
00:02:28,460 --> 00:02:31,540
relationship between an investment and
safety, and safety performance.

40
00:02:31,540 --> 00:02:35,220
Rather, this set out to set
up a framework to investigate

41
00:02:35,220 --> 00:02:38,550
how these investments in safety
might interact with human and

42
00:02:38,550 --> 00:02:42,330
environmental factors to
influence safety performance.

43
00:02:42,330 --> 00:02:44,890
So if we dive into this
one a little bit more,

44
00:02:44,890 --> 00:02:49,220
this began as many studies do with
an investigation of construction sites to

45
00:02:49,220 --> 00:02:53,640
identify potential factors that
could lead to decrease safety.

46
00:02:53,640 --> 00:02:57,114
Then the researchers created
an agent-based model or ABM, and

47
00:02:57,114 --> 00:02:59,505
that's pictured in the image you see here.

48
00:02:59,505 --> 00:03:03,210
And they looked at two things, one is
safety investments shown in yellow, and

49
00:03:03,210 --> 00:03:06,692
the other is safety performance which
you can see outlined in sort of a blue

50
00:03:06,692 --> 00:03:07,276
plus sign.

51
00:03:07,276 --> 00:03:11,104
And so the workers on the site are all
performing, and performance, and

52
00:03:11,104 --> 00:03:13,130
safety is one of their performances.

53
00:03:13,130 --> 00:03:17,815
So that's captured by all the workers
being within this safety performance area.

54
00:03:17,815 --> 00:03:20,605
And then there were three safety
investments that the researchers

55
00:03:20,605 --> 00:03:21,210
considered.

56
00:03:21,210 --> 00:03:25,736
So one was an increased focus
on co-workers cooperating.

57
00:03:25,736 --> 00:03:30,900
The second was an increased focus on
having supervisors do safety inspections.

58
00:03:30,900 --> 00:03:34,580
And the third was a more technical
assistance to individual workers to design

59
00:03:34,580 --> 00:03:37,670
to allow them to behave more safely.

60
00:03:37,670 --> 00:03:41,850
Pictured here are some additional
factors that they considered.

61
00:03:41,850 --> 00:03:46,350
So they tried to model the potential
influence of safety awareness among

62
00:03:46,350 --> 00:03:49,950
the workers,
as well as the costs of safety.

63
00:03:49,950 --> 00:03:53,670
They also considered things like
accident record and productivity.

64
00:03:53,670 --> 00:03:57,710
And so ultimately, by looking at
the interactions between all these agents

65
00:03:57,710 --> 00:04:03,010
they were able to develop this framework
for predicting safety performance.

66
00:04:03,010 --> 00:04:05,960
The third theory I'll give you
an example of is scenario planning.

67
00:04:05,960 --> 00:04:08,220
And this scenario or

68
00:04:08,220 --> 00:04:12,075
research that I'll describe to you
actually was focused on Korea.

69
00:04:12,075 --> 00:04:15,510
In a study where they were
trying to prepare for

70
00:04:15,510 --> 00:04:19,610
emergencies and
response as a result of nuclear emergency.

71
00:04:19,610 --> 00:04:23,770
So you can see in the center of the image,
they're developing a database here.

72
00:04:23,770 --> 00:04:27,000
And that database actually involves
generating different scenarios

73
00:04:27,000 --> 00:04:28,240
of potential emergencies.

74
00:04:28,240 --> 00:04:32,310
They could do that manually and put in
certain parameters of an emergency, but

75
00:04:32,310 --> 00:04:35,560
the system could also generate
automatically different types of

76
00:04:35,560 --> 00:04:37,000
scenarios.

77
00:04:37,000 --> 00:04:40,130
They also want to evaluate
how well the response goes.

78
00:04:40,130 --> 00:04:44,580
And so the system can suggest
different evaluation methods, but

79
00:04:44,580 --> 00:04:49,320
of course people can also input
manual evaluation methods as well.

80
00:04:49,320 --> 00:04:52,560
Then this allows them to
actually run an exercise,

81
00:04:52,560 --> 00:04:58,630
a modifiable field exercise and
come up with a master scenario list.

82
00:04:58,630 --> 00:05:01,910
And so when you can do a field
exercise with actual people,

83
00:05:01,910 --> 00:05:06,600
you can determine does the field exercise
bear out what the scenario thought would

84
00:05:06,600 --> 00:05:10,390
happen, or were their failure
points that were unexpected?

85
00:05:10,390 --> 00:05:14,486
And if you identify those, that allows
you to go back and further improve and

86
00:05:14,486 --> 00:05:18,885
revise the original scenarios.

87
00:05:18,885 --> 00:05:22,375
So now let's shift into applying tools for
systems thinking, and

88
00:05:22,375 --> 00:05:25,750
the first one we'll focus
on is causal loop diagrams.

89
00:05:25,750 --> 00:05:30,180
And so one study that's used these
actually wanted to look at the impact of

90
00:05:30,180 --> 00:05:35,520
production pressure on safety management
in for example, a work place.

91
00:05:35,520 --> 00:05:40,160
And so they developed a conceptual causal
loop diagram trying to identify and

92
00:05:40,160 --> 00:05:45,290
characterize the relationship between the
schedule of work, and the quality of work.

93
00:05:45,290 --> 00:05:47,400
And also the relationship
between those things,

94
00:05:47,400 --> 00:05:51,110
and the safety of workers including
their perceptions of safety.

95
00:05:51,110 --> 00:05:55,520
Whether or not they got training, and
how strong the safety supervision was.

96
00:05:55,520 --> 00:05:58,470
So here's the causal loop diagram
these researchers came up with.

97
00:05:58,470 --> 00:06:00,018
You can see it's quite complicated.

98
00:06:00,018 --> 00:06:04,310
We're not going to go into the details
here of the entire diagram, but

99
00:06:04,310 --> 00:06:06,400
I want to highlight we have
a number of things here.

100
00:06:06,400 --> 00:06:09,300
So we have arrows in the causal loop, and

101
00:06:09,300 --> 00:06:12,670
those arrows show
the direction of causality.

102
00:06:12,670 --> 00:06:16,930
We also have positive and minus signs
in here, those indicate whether this

103
00:06:16,930 --> 00:06:22,740
particular relationship is increasing
the outcome or likely to decrease it.

104
00:06:22,740 --> 00:06:24,730
And we have a number of
feedback loops here.

105
00:06:24,730 --> 00:06:27,950
We have reinforcing loops
shown in this figure as an R,

106
00:06:27,950 --> 00:06:31,760
but can also be presented as
a plus sign with a semi circle.

107
00:06:31,760 --> 00:06:34,200
And we have balancing loops,
which are shown here is a B but

108
00:06:34,200 --> 00:06:38,300
could also be shown with a minus sign and
a semi circle symbol.

109
00:06:38,300 --> 00:06:42,060
So again, we can look at the relationships
and the loops here, and try to connect

110
00:06:42,060 --> 00:06:47,510
all of these different causal activities,
and understand how they interrelate.

111
00:06:47,510 --> 00:06:51,410
So how would you actually go about
making your own causal loop diagram?

112
00:06:51,410 --> 00:06:52,610
I'm going to walk you through that.

113
00:06:52,610 --> 00:06:55,998
So the first thing we do as you
see is start with the seed model.

114
00:06:55,998 --> 00:06:58,680
And in the seed model,
we have a very basic set of variables.

115
00:06:58,680 --> 00:07:01,700
So in this case,
we're interested in safety climate,

116
00:07:01,700 --> 00:07:05,350
how that's related to incidents rate and
actual metric of safety.

117
00:07:05,350 --> 00:07:06,930
How that relates to production rate, and

118
00:07:06,930 --> 00:07:10,140
how that relates to
supervisor effectiveness.

119
00:07:10,140 --> 00:07:12,990
Next, we start to add
additional variables and

120
00:07:12,990 --> 00:07:16,830
arrows, and again the arrows
showing the direction of causality.

121
00:07:16,830 --> 00:07:20,670
So in this case, you can see we've added
a couple of steps that actually create

122
00:07:20,670 --> 00:07:24,470
a loop back between incidence rate and
safety climate.

123
00:07:24,470 --> 00:07:28,770
So safety training and hazard awareness
are related to incidence rate, but

124
00:07:28,770 --> 00:07:30,670
they also can influence safety climate.

125
00:07:30,670 --> 00:07:34,760
We also see that production rate
can influence schedule delays.

126
00:07:34,760 --> 00:07:36,940
Schedule delays then
increase work pressure,

127
00:07:36,940 --> 00:07:40,650
and that in turn influences
production rate.

128
00:07:40,650 --> 00:07:45,005
So continuing to build our causal loop
diagram, we add in more variables and

129
00:07:45,005 --> 00:07:45,830
more arrows.

130
00:07:45,830 --> 00:07:49,396
We may think of additional
variables we haven't considered.

131
00:07:49,396 --> 00:07:52,714
We may even identify new external
variables that feed in or

132
00:07:52,714 --> 00:07:53,875
impact the system.

133
00:07:53,875 --> 00:07:58,437
And so you can see we've added a number
of different variables to this figure.

134
00:07:58,437 --> 00:07:59,687
And then ultimately,

135
00:07:59,687 --> 00:08:03,116
we start to characterize
the nature of these relationships.

136
00:08:03,116 --> 00:08:05,640
So adding in those plus and
minus signs, and

137
00:08:05,640 --> 00:08:08,380
we can start to formally
label feedback loops.

138
00:08:08,380 --> 00:08:11,545
So if we look at this still
distillation of just a portion of

139
00:08:11,545 --> 00:08:14,910
the overall conceptual diagram
the researchers identified.

140
00:08:14,910 --> 00:08:19,670
You can see we now have one really
reinforcing loop between schedule delays

141
00:08:19,670 --> 00:08:23,110
and rework, basically someone doing
something wrong and having to do it again,

142
00:08:23,110 --> 00:08:25,080
and errors, and work pressure.

143
00:08:25,080 --> 00:08:28,260
And then we have a balancing
loop between incidence rate and

144
00:08:28,260 --> 00:08:33,620
safety climate that's actually mediated
by safety training, and hazard awareness.

145
00:08:33,620 --> 00:08:38,361
So, by creating these causal loops again,
we can illustrate a number of different

146
00:08:38,361 --> 00:08:42,100
variables, and look at the causal
relationship between those.

147
00:08:42,100 --> 00:08:48,130
And ultimately determine how is the system
being influenced by each of these factors?

148
00:08:48,130 --> 00:08:51,340
So we've already actually considered
a very basic process map that we've

149
00:08:51,340 --> 00:08:56,160
been focused on for many previous modules,
which is our exposure disease framework.

150
00:08:56,160 --> 00:09:00,170
We have a source, the movement
of pollutants, human exposure,

151
00:09:00,170 --> 00:09:02,020
dose and adverse health effects.

152
00:09:02,020 --> 00:09:03,790
So we've basically again,

153
00:09:03,790 --> 00:09:08,290
identified a process that results
in potential human health outcomes.

154
00:09:08,290 --> 00:09:11,590
And of course, we can then identify places
that we can potentially intervene and

155
00:09:11,590 --> 00:09:13,818
stop those outcomes.

156
00:09:13,818 --> 00:09:16,950
So I want to give you a little bit
more about how to actually create

157
00:09:16,950 --> 00:09:17,890
a process map.

158
00:09:17,890 --> 00:09:20,432
There are formalized symbols that we use,
and so

159
00:09:20,432 --> 00:09:22,852
those are illustrated
here with some detail.

160
00:09:22,852 --> 00:09:24,292
I'll just go over the highlights.

161
00:09:24,292 --> 00:09:26,564
On the left side we have
an oval terminator.

162
00:09:26,564 --> 00:09:30,220
This is basically the point at
which the process maps starts.

163
00:09:30,220 --> 00:09:32,680
Then you can see we move into
an orange rectangle, and

164
00:09:32,680 --> 00:09:36,463
rectangles indicate an action
step oor some tasks that we do.

165
00:09:36,463 --> 00:09:40,826
Then we move into a green diamond, and
the diamond captures a decision point.

166
00:09:40,826 --> 00:09:43,870
Basically a yes/no branch.

167
00:09:43,870 --> 00:09:47,450
You can see on this diagram,
we've then got a blue and a brown shape.

168
00:09:47,450 --> 00:09:51,420
These indicate basically
some input into the system.

169
00:09:51,420 --> 00:09:54,510
It can be documentation or another step.

170
00:09:54,510 --> 00:09:57,730
And then the right-hand side we
have another oval terminator.

171
00:09:57,730 --> 00:10:01,190
This one indicating
the stop of the process.

172
00:10:01,190 --> 00:10:04,246
So as you'll see here
the arrows all need to connect.

173
00:10:04,246 --> 00:10:07,320
So there shouldn't be any symbols
that are unconnected in the process.

174
00:10:07,320 --> 00:10:11,830
And they all need to move in the same
direction showing the flow of the process.

175
00:10:11,830 --> 00:10:15,435
I'll end here with just a very simple
process map about troubleshooting your

176
00:10:15,435 --> 00:10:16,040
cellphone.

177
00:10:16,040 --> 00:10:17,840
So on the left we have
our oval terminator,

178
00:10:17,840 --> 00:10:20,080
your cellphone doesn't turn on.

179
00:10:20,080 --> 00:10:22,250
Well, what's the first
decision point here?

180
00:10:22,250 --> 00:10:25,350
Let's look and see is the cellphone
on in the first place.

181
00:10:25,350 --> 00:10:27,250
If it's not, well,
let's turn the phone on,

182
00:10:27,250 --> 00:10:29,360
it will probably work once it's turned on.

183
00:10:29,360 --> 00:10:32,560
If you turn it on and it still doesn't
work, then the next decision point,

184
00:10:32,560 --> 00:10:33,840
is the battery dead?

185
00:10:33,840 --> 00:10:35,330
If it's dead then we recharge.

186
00:10:35,330 --> 00:10:39,358
If it's not dead then we need
to contact a cellphone expert.

187
00:10:39,358 --> 00:10:41,860
So that's a simple and
maybe almost comical example.

188
00:10:41,860 --> 00:10:45,740
But you could see with this very
powerful process map approach,

189
00:10:45,740 --> 00:10:49,040
we can actually map out some
very complicated processes.

190
00:10:49,040 --> 00:10:52,340
And that will enable us to look
at them in greater detail, and

191
00:10:52,340 --> 00:10:55,019
yield greater understanding.

