1
00:00:06,120 --> 00:00:10,440
And you can use these arrows to to move if actually.

2
00:00:19,900 --> 00:00:23,020
Okay. Okay. Hello, everyone.

3
00:00:23,230 --> 00:00:31,210
My name is Houghton and this is my team. Today we are going to prison or criminal project, which Mr. James wasn't quite documentation.

4
00:00:32,710 --> 00:00:40,210
So this is the outline of our to this presentation in which I'm going to introduce the background methods and implementation results,

5
00:00:40,870 --> 00:00:46,240
and the mathematician is going to present the efficiency comparison and optimization part.

6
00:00:48,040 --> 00:00:57,100
So our first beginning with our background so Audrey and me so kind of regression models that model a response y function or transformation of X.

7
00:00:57,850 --> 00:01:00,990
So in general, simple linear regression is also a trend,

8
00:01:01,630 --> 00:01:07,900
but it also feels in this situation that the underlying distributions between the outcome and covariance are not linear.

9
00:01:08,350 --> 00:01:11,980
So this brings us to the use of dubious points in regression analysis.

10
00:01:15,030 --> 00:01:18,810
So there are three different methods for using splicing regression analysis.

11
00:01:19,500 --> 00:01:29,070
The first and second method of each are regression. Spice and the Panama Spice are based on a set of basis functions which are given a set of laws.

12
00:01:29,580 --> 00:01:36,330
And for the smoothing spice method, it tends to minimize this optimization functions,

13
00:01:36,630 --> 00:01:41,450
which is the sum of squares, of residuals plus of smoothing parameters.

14
00:01:42,720 --> 00:01:47,129
So our goal is to build in our package to efficiency efficiently.

15
00:01:47,130 --> 00:01:53,130
We implement and smoothing as part of this course validation option to select the smoothing parameters.

16
00:01:55,770 --> 00:02:03,570
So if the method part in the duration from multiple smoothing functions to be estimated and in this case

17
00:02:03,840 --> 00:02:10,560
the smoothing is by as measured six for a combination of smooth functions to minimize the sum of squares.

18
00:02:11,460 --> 00:02:20,010
So we go first of the minimization problem for the univariate case and then we will extend it to the replaced by applying the evacuation algorithm.

19
00:02:22,650 --> 00:02:33,450
So the first we have established the case. The solution to this optimization problem is a natural curious find with not at all the unique values of X.

20
00:02:34,650 --> 00:02:38,400
So in which this energy is so natural to this fine basis function.

21
00:02:38,940 --> 00:02:49,680
And so after we we put this function in this original formula, the optimization problem can be reduced to this matrix representation,

22
00:02:50,040 --> 00:02:54,150
and the solution can be seen to be this, this formula in the bottom.

23
00:02:56,730 --> 00:02:59,880
So for the multivariate case, we will apply the back feeding algorithm.

24
00:03:00,660 --> 00:03:10,590
And the central point of this algorithm is to fit the working residual for each covariate, abide by the univariate, smoothing explains.

25
00:03:11,010 --> 00:03:14,070
So you will repeat until the convergence.

26
00:03:15,480 --> 00:03:21,030
So after we implement the algorithm, we applied our package to the empirical dataset.

27
00:03:21,750 --> 00:03:32,370
So the response variables of interest is two mpg and we have three predictors which are horsepower or number of sliding doors and row after ratio.

28
00:03:34,110 --> 00:03:43,800
So we first begin Mr. Univariate case. So as we can see, as we increase lambda from 1 to 100, the curve smoother than before,

29
00:03:44,460 --> 00:03:53,670
and we also examine our algorithm for the multivariate case and we also try a combination of Lambda to be one and A to be 100.

30
00:03:54,720 --> 00:04:00,750
And lastly, we use two across validation to select a optimum lambda for each covariance.

31
00:04:01,020 --> 00:04:08,550
The results are given in this figure. So that's my teammates is just going to present our efficiency comparison.

32
00:04:09,960 --> 00:04:16,830
Yes. So after we derived our algorithm on, the first question we have is how can we boost the efficiency?

33
00:04:17,160 --> 00:04:22,220
So here, just to give you like a final overview of what we cheat.

34
00:04:23,280 --> 00:04:29,670
So the as we can see here, the first one is the can package implemented by someone.

35
00:04:30,090 --> 00:04:38,460
Basically, it does the same thing. And the second function is our implementation of in our code smoothing, splicing.

36
00:04:39,420 --> 00:04:41,490
And the result is pretty surprising to us.

37
00:04:41,820 --> 00:04:56,700
So as we can see here, the GAM function does give us like a mean of 1500 milliseconds and in our RSVP version it gives us like 148 milliseconds.

38
00:04:57,390 --> 00:05:02,190
So it's roughly like ten times more efficient than their implementation,

39
00:05:02,790 --> 00:05:11,790
which is quite surprising to us because we thought we would be like somewhat similar or or worse than there's about we actually does better.

40
00:05:12,750 --> 00:05:23,850
And also we found that like we did, like our own, our implementation and our, our security implementation is also like roughly ten times faster.

41
00:05:24,330 --> 00:05:28,020
So and then we would later look at their source code.

42
00:05:28,020 --> 00:05:38,280
We found they achieved that function also. So that's probably explained why our functions like more efficient and just give

43
00:05:38,280 --> 00:05:43,890
you like some just to dig into like different functions of what we implemented.

44
00:05:44,140 --> 00:05:51,690
And we did a similar thing, spline univariate, which, which is basically for the univariate case.

45
00:05:52,470 --> 00:05:58,049
And in this case it's like also ten times faster than our, our implementation.

46
00:05:58,050 --> 00:06:02,400
So the first one is the implementation and the second one is our CIP implementation.

47
00:06:03,170 --> 00:06:11,810
And and this another function would be like natural supply, which just basically divides the whole, the whole formula.

48
00:06:11,820 --> 00:06:14,890
And then. Maybe a smaller chunk.

49
00:06:15,220 --> 00:06:25,710
And this one is actually more aggressive. It does like 43 milliseconds versus 800 milliseconds by the hour implementation.

50
00:06:25,720 --> 00:06:29,770
So it's like roughly more than 20 times, if not more efficient.

51
00:06:30,940 --> 00:06:35,200
And the other one is the kernel matrix, which is, again, much more efficient.

52
00:06:35,560 --> 00:06:40,420
And it's actually like 33 times, 70 times faster.

53
00:06:40,900 --> 00:06:45,459
So overall, because we didn't write everything in our system,

54
00:06:45,460 --> 00:06:53,110
because there are some features that are super nice in our such as the matrix like matrix manipulation.

55
00:06:53,410 --> 00:07:02,920
So we keep that part in our but we do that. We are like modules, a lot of code, especially in the loop and in our thinking.

56
00:07:03,220 --> 00:07:05,350
So that's why we sure it is efficiency.

57
00:07:07,120 --> 00:07:17,259
And yeah, like I said, so basically this part is just an analysis why like our RCP implementation is much faster than the our,

58
00:07:17,260 --> 00:07:26,200
our or our implementation. So those codes are just captured from the natural splicing function that I show you above also,

59
00:07:26,260 --> 00:07:33,490
which is like roughly like 20 times faster than our situations and like 20% faster than the, our implementation.

60
00:07:33,880 --> 00:07:39,760
And here, as we can see, it presents like a double double nested for loop.

61
00:07:40,300 --> 00:07:46,600
So basically we interface through like all the element in a matrix called the and and then we do

62
00:07:46,600 --> 00:07:54,940
some update and calculation in the in this loop because our sequences for loop is so much faster.

63
00:07:55,150 --> 00:07:59,380
So we decided to implement this function in our security.

64
00:07:59,770 --> 00:08:03,730
And that's like one reason why, one of the reason why it's faster.

65
00:08:04,000 --> 00:08:12,430
And secondly, we try to modulate those like computationally heavy code into like functions.

66
00:08:13,330 --> 00:08:21,530
So, so down we can modulates this part in the, in the CVP and utilize its efficiency as we did as we did here.

67
00:08:21,550 --> 00:08:28,740
So it's like this function is calculate the coefficient and this function is wrong in the wild loop.

68
00:08:28,750 --> 00:08:35,350
I didn't show here because of this space limit and then this function is like kind of code repetitively.

69
00:08:35,650 --> 00:08:40,150
So it's almost almost like a, like a double list.

70
00:08:40,360 --> 00:08:44,590
And that's the loop, which is like one square time complexity.

71
00:08:44,950 --> 00:08:52,180
And then we can build model like this part of the code into our cookie to reduce its running time.

72
00:08:52,870 --> 00:08:59,710
Yeah. So that's basically what we, what we achieved by boosting the efficiency using our city.

73
00:09:00,100 --> 00:09:07,450
And for the future work which well we are now implementing the logistic regression and hopefully also being really compete.

74
00:09:07,660 --> 00:09:14,830
And then later we will make a package and upload our work into the give them some a GitHub

75
00:09:15,070 --> 00:09:19,720
so probably everyone can like you'd like to do some leverage to achieve what they want.

76
00:09:20,680 --> 00:09:25,960
Yeah. And that's our presentation on. Thank you for listening and we're happy to answer any questions you have.

77
00:09:26,620 --> 00:09:37,260
Thank you. So questions we have about a one minute, one question.

78
00:09:40,350 --> 00:09:50,729
Okay. So this this might be useful because I'm using I noticed the gap is actually slow and I did not notice that they haven't used the RCP.

79
00:09:50,730 --> 00:09:58,920
So, uh, is it, is it something you think you want to consider something doing beyond the line, beyond this?

80
00:09:59,310 --> 00:10:07,110
Uh, I mean, if this is going to be useful to the community, do you think it's worth like publishing something shorter paper or something like that?

81
00:10:07,110 --> 00:10:10,980
I guess that that's definitely something you may want to consider.

82
00:10:11,130 --> 00:10:15,680
I mean, it doesn't have to be high profile people, but so that people can, can,

83
00:10:15,680 --> 00:10:23,270
can use it if if it's a certain, then there's no other packaging that listed anything for sure.

84
00:10:23,280 --> 00:10:32,970
Yeah. If it's, if it's hard to be like and if we implemented not just regression just to make it more like reaching in terms of functionality,

85
00:10:33,120 --> 00:10:37,970
we definitely can consider to make it for everyone to use.

86
00:10:37,980 --> 00:10:38,090
Yeah,

87
00:10:38,130 --> 00:10:48,990
but also you may want to research whether there's absolutely no similar packages about any other question and then keep up with it depending smart.

88
00:10:49,620 --> 00:10:53,580
So next, who's next? Group nine.

89
00:11:12,480 --> 00:11:17,520
A higher rung on your knee. And this is this is dial up.

90
00:11:17,880 --> 00:11:23,760
And our topic is about fractional reserve regression, which is kind of are the ones with regression.

91
00:11:25,920 --> 00:11:30,720
A few everyone over asked. And we basically have two main problems.

92
00:11:31,200 --> 00:11:37,690
First is overfitting and the second is the unstable estimates when predictor are highly correlated.

93
00:11:38,610 --> 00:11:47,970
So this is where regression comes out. It basically introduces panel analyzation of the L to none of the coefficients for a linear regression.

94
00:11:49,440 --> 00:11:59,040
But regression also has some challenges. First, you have low to high efficiency, especially when how to analyze a large scale dataset.

95
00:11:59,520 --> 00:12:03,240
And also when we want to test a range of client parameters.

96
00:12:03,660 --> 00:12:11,160
It may be costly and it is also difficult to interpret the effects of regular vision parameters.

97
00:12:12,600 --> 00:12:22,890
So we think about rational regression is basically use pre calculated relationship that and l to know of the regression.

98
00:12:23,190 --> 00:12:29,370
It's a fraction of the L2 known of the beta of S which may improve the efficiency.

99
00:12:32,850 --> 00:12:38,430
Um, so the fractional reg regression is like they were all draconian, motivated and principled.

100
00:12:38,790 --> 00:12:42,990
And here I would like to show the methodology behind the fractional retrogression.

101
00:12:44,100 --> 00:12:49,589
So let's first consider running a regression problem. Y is equal to X, transfer, greater plus epsilon.

102
00:12:49,590 --> 00:12:56,850
And the first trick this algorithm does is it replaces the input matrix X with a single or relative combination.

103
00:12:57,150 --> 00:12:58,980
And as through these replacement,

104
00:12:59,250 --> 00:13:06,100
we can rewrite the overall solution to read a regression solution in terms of the parallel matrix of all singular values,

105
00:13:06,180 --> 00:13:09,840
I'm double x and also the transform value of Y.

106
00:13:10,140 --> 00:13:18,930
And in this table we presented three kinds of solution and if we focus on last column is the simplified transform solution for each estimate here.

107
00:13:19,170 --> 00:13:27,180
So if we take a look at the both of these two solutions, we actually can find out a relationship between the OAS and the regression solutions.

108
00:13:27,480 --> 00:13:33,240
And this relationship defines the regression solution in terms of the fraction of the OAS solution.

109
00:13:33,480 --> 00:13:41,190
And this is like makes a lot of this do us things review work as it considers the read regression solutions as the scaled down versions of

110
00:13:41,190 --> 00:13:50,100
the OR as coefficients and the shrinkage is different based on its like the for each estimates are based on a corresponding singular value.

111
00:13:52,020 --> 00:13:59,400
So these are relationship layers. The base of the fractional read regression and the core concepts of the fractional read regression is that

112
00:13:59,700 --> 00:14:06,480
it defines the fraction as the ratio of out of the regression solution to the norm of the OAS solution.

113
00:14:06,840 --> 00:14:12,360
So here we presented the mathematical formula for like how we can calculate this friction gamma

114
00:14:12,690 --> 00:14:20,430
and this algorithm or the user's input will be the data matrix X and the vector Y and also the,

115
00:14:20,670 --> 00:14:29,400
the fractions they want to achieve. So because the transformation page has to be a preserves in their product since we is a unitary matrix.

116
00:14:29,640 --> 00:14:32,590
And so that means we this solution will be different.

117
00:14:32,610 --> 00:14:37,230
This fraction will be guaranteed in the original space, even though we are now working in the transform space.

118
00:14:38,400 --> 00:14:45,120
And the core thing like the core part in this algorithm is that we first will evaluate a range

119
00:14:45,120 --> 00:14:50,309
of our four values and then like for each of our value we can based on this relationship,

120
00:14:50,310 --> 00:14:58,980
we can get the corresponding gamma and then we will use the interpolation methods to find the final R4 value that achieves the desired fashion,

121
00:14:58,980 --> 00:15:03,240
which is users input gamma. So once we find the target value,

122
00:15:03,240 --> 00:15:12,000
we will then finally use it to calculate the our final regression solution and eventually transform the solution back to the original space.

123
00:15:12,360 --> 00:15:21,600
And in the algorithm, if we provide the base range of, of our values, a verifying upgrading the interpretation method has been proving very well.

124
00:15:27,330 --> 00:15:32,290
So basically we used that as a medical insurance dataset to test our results.

125
00:15:32,550 --> 00:15:37,890
So this dataset we are using features such as the BMI number of children in a

126
00:15:37,890 --> 00:15:42,330
family or other socio economic status to predict a medical insurance cost.

127
00:15:42,690 --> 00:15:50,790
So we can see the first two people here shows you the R squared value between the standard to reach regression and a fraction of each aggression.

128
00:15:51,060 --> 00:16:01,950
So we can see that with the alpha decrease and the fraction which is our gamma increase, the R squared value actually increase from 0.13 to 0.67.

129
00:16:02,910 --> 00:16:07,799
So these two shows that shows us the R squared value actually consistent with each other.

130
00:16:07,800 --> 00:16:14,370
So which is a good thing. And another thing we can notice, like the RFA is E versus also with the fraction,

131
00:16:14,370 --> 00:16:17,550
which is actually also consistent with our mathematical equation here.

132
00:16:17,700 --> 00:16:21,300
We can see here from the RFA increase though, gamma here actually decrease.

133
00:16:22,110 --> 00:16:23,070
The second part here,

134
00:16:23,400 --> 00:16:32,160
actually this one is the most important results from our project shows the execution timing comparison among three different models,

135
00:16:33,060 --> 00:16:40,440
standard regression. And as we did the composition regression and a fraction of regression and the orange line here shows

136
00:16:40,440 --> 00:16:46,500
you the ratio time e ratio between standard to reach regression versus the fraction regression.

137
00:16:47,430 --> 00:16:53,880
And the blue line here shows you the activity decomposition for our ranger regression versus fractional reserve regression.

138
00:16:54,390 --> 00:16:58,850
We can see the fraction ranger regression has the shortest exclusion timing,

139
00:16:59,070 --> 00:17:05,070
while the standard regression has the longest execution time, which is actually our same as our expectations.

140
00:17:05,810 --> 00:17:11,460
And another interesting, we can see the fractional region regression behaves really good with a sample size increase.

141
00:17:11,700 --> 00:17:19,500
We can see like he's almost like 30 times faster than the standard of retrogression with the sample size increase about to like come top hundred.

142
00:17:19,890 --> 00:17:25,020
And the second results, we can see that generally fractional regression performs even better.

143
00:17:25,020 --> 00:17:30,660
With the sample size increase and our working progress,

144
00:17:30,670 --> 00:17:37,260
we can finalize our our package with a detailed documentation, as we say, like there's no our package as we know.

145
00:17:37,470 --> 00:17:41,370
So we can just upload our package to GitHub and make everybody could use it.

146
00:17:41,730 --> 00:17:44,790
And also we can compare our model with Jim on that.

147
00:17:45,840 --> 00:18:02,870
That's all we got. Thanks. So yeah, these are, these are interesting and I am familiar with some of these concepts.

148
00:18:02,870 --> 00:18:10,700
So, uh, so my question is that when you do the values, so first, the first question,

149
00:18:10,730 --> 00:18:16,210
what is the time complexity because it looks like a, your time completely almost didn't change.

150
00:18:16,220 --> 00:18:20,480
But I think it should be intuitive because we are doing still I the composition.

151
00:18:20,480 --> 00:18:24,620
So are you actually increasing the time to doing that target composition is the right word.

152
00:18:25,190 --> 00:18:28,399
You mean the as we did the conversation. So you need to do this really once.

153
00:18:28,400 --> 00:18:35,000
Is it is it right? Yes. Yes. So that that'll be basically the dominating factor when you increase the sample size.

154
00:18:35,000 --> 00:18:42,799
So so is it really true? So in in this view, looks like it didn't change up almost at all.

155
00:18:42,800 --> 00:18:51,080
Is it because of the simple solemnity of a similar or do you actually expect that it'll get an increase in cubic time complexity?

156
00:18:51,680 --> 00:19:00,049
Um, yeah, because this plot is we plot the fraction of the activities of regression overall fractional regression.

157
00:19:00,050 --> 00:19:06,320
Okay. This is a ratio. Yeah. So would you the same way to do the estimate for both, uh, in both methods.

158
00:19:06,530 --> 00:19:12,830
So we actually expect the like the they are very similar in terms of time efficiency.

159
00:19:13,280 --> 00:19:16,720
Okay. Yeah, I see. So and so.

160
00:19:17,450 --> 00:19:29,550
Okay. So. And so so can you can you go back to the previous slide that you actually show?

161
00:19:30,070 --> 00:19:37,690
So just understand that these are actually same result, but the x axis is killed differently.

162
00:19:37,870 --> 00:19:41,360
Is it right? Or so they did understand the problem correctly.

163
00:19:41,410 --> 00:19:45,360
So these are basically these are the alpha.

164
00:19:45,430 --> 00:19:48,790
There should be 1 to 1 relationship between alpha and friendship. Yes.

165
00:19:48,890 --> 00:19:57,640
Right. Okay. Okay. So yeah, I didn't know that this method has a name, but I make I work in a similar problem.

166
00:19:58,890 --> 00:20:02,560
Okay. So. Okay. Right. Other questions?

167
00:20:03,160 --> 00:20:09,100
Okay. Then we are a little one minute early, but if there's no other question, we can move on.

168
00:20:10,630 --> 00:20:14,020
Okay. Let's move up. Thank you. Yep.

169
00:20:17,770 --> 00:20:31,850
Next is Guru 15. So just to say that the 14 and 15 has a few people.

170
00:20:32,270 --> 00:20:39,800
So you you may want to account for it when you when you try to do your evaluation.

171
00:20:43,820 --> 00:20:46,840
But to allow everybody just go 15.

172
00:20:46,850 --> 00:20:49,160
My name is Jihad. Hi, my name is away.

173
00:20:49,760 --> 00:20:57,260
So our project is focused on the implementation of kernel logistic regression and in for the machine for classification.

174
00:20:59,490 --> 00:21:03,180
You can use it. Yeah. Adam Yeah.

175
00:21:03,180 --> 00:21:05,250
So first of all, what is classification?

176
00:21:05,790 --> 00:21:16,890
Classification is a supervised model approach to characterize data into this tiny number of classes where we can assign label to each class.

177
00:21:17,430 --> 00:21:24,980
And there are a lot of classification hours in right now and those forecasts are just for classification errors in logistic regression terminology.

178
00:21:25,020 --> 00:21:29,790
Regression spoke about the machine and implement machine learning discussing our project.

179
00:21:32,310 --> 00:21:35,490
So here's a table showing you the advantage.

180
00:21:35,490 --> 00:21:39,230
And these these are the values for all those for algorithm for example,

181
00:21:39,240 --> 00:21:47,370
you can see here the regression is very easy to implement, but you cannot use in your summary raw data.

182
00:21:47,910 --> 00:21:52,980
So in order to solve problems, we can add a kernel in front of that regressions.

183
00:21:53,610 --> 00:21:57,479
And for the CEO, it works very well with the binary classification by the can.

184
00:21:57,480 --> 00:22:03,750
Now you actually provide a probability you have to make and it may fail with multiclass inputs.

185
00:22:04,320 --> 00:22:14,250
So we have a solid promise, right? Because we look as good as the as we have in the better, because you can also provide estimate,

186
00:22:14,490 --> 00:22:18,900
probability estimate and the generalize into multiclass cases.

187
00:22:22,680 --> 00:22:27,230
So here's the basic algorithm. We use the alphabet of the key drivers.

188
00:22:28,050 --> 00:22:33,240
So we find out our objective function from the reverse regression,

189
00:22:33,690 --> 00:22:38,670
and we set our gradient as the, first of all, the derivative of the long lasting function.

190
00:22:39,370 --> 00:22:44,160
And for the opposition party, we choose to use BFG as evidence.

191
00:22:47,330 --> 00:22:52,640
Again, this part is the basic algorithm of the IBM hours that we use in our project.

192
00:22:53,360 --> 00:23:00,530
So basically this algorithm is to find a better way to minimize our objective function by

193
00:23:00,530 --> 00:23:06,920
setting the derivative of H with respect to with respect to the vector be equal to zero.

194
00:23:08,420 --> 00:23:15,070
Then we use the new direction methods to immediately saw the score function are conversion.

195
00:23:16,280 --> 00:23:17,990
So in this algorithm we will.

196
00:23:18,260 --> 00:23:26,810
So we would like we will make a with like a subset of the entire observations and only use this subset to make a prediction.

197
00:23:30,450 --> 00:23:35,429
So there are a lot of different kind of terminal functions and the rectilinear kernel,

198
00:23:35,430 --> 00:23:39,900
polynomial kernel, physical mode and also the kernel can be used in different situations.

199
00:23:40,380 --> 00:23:45,180
For example, the Linux kernel is used to mean data is linearly separable.

200
00:23:48,760 --> 00:23:54,010
So what is turning away when you are right now is that there's a packet called category battery.

201
00:23:54,520 --> 00:24:01,600
It has a function to perform a kernel regression for the battery output, but it only has to close.

202
00:24:02,230 --> 00:24:07,060
So one of the goals of our project is to employ more carriers in our.

203
00:24:10,220 --> 00:24:14,629
And another thing to note is there is no implementation of IBM.

204
00:24:14,630 --> 00:24:18,860
So we want to make our own implementation to invite more people to use this method.

205
00:24:19,430 --> 00:24:23,210
And that's why I'm going to introduce the two functions we've implemented,

206
00:24:23,510 --> 00:24:30,409
the TR and its prediction function and the IBM and its prediction function is still under construction but almost finished.

207
00:24:30,410 --> 00:24:39,530
So yeah, and ah, we take in a matrix x which contains all the predictors and the binary vector.

208
00:24:39,530 --> 00:24:44,269
Why that is the response and the user can choose ah which kernels we want to use.

209
00:24:44,270 --> 00:24:44,780
For example,

210
00:24:44,780 --> 00:24:58,820
polynomial rb applies sigmoid or turn and they can also choose the parameter lambda and the convergence criterion and they can also set the their own

211
00:24:59,030 --> 00:25:11,420
hypervisors for each of the kernels and it will return a list that contains the original data and also the parameters and the predictor coefficients.

212
00:25:12,710 --> 00:25:23,390
And we also implement to predict call ah, that can take in the pre-trained ah object and then use the new dataset to predict the response

213
00:25:23,960 --> 00:25:30,650
and it returns the probabilities of the response that you want for all of the new observations.

214
00:25:31,940 --> 00:25:37,790
And we perform our functions on simulation data and also the real world data.

215
00:25:37,790 --> 00:25:45,290
So we there's a benchmark we perform on the benchmark simulation data which are moon circle and blobs.

216
00:25:45,650 --> 00:25:55,400
And also I wrote this, I called Puma Indians Diabetes, where we can predict whether a patient has diabetes based on several diagnostic measurements.

217
00:25:55,940 --> 00:26:03,020
And we split all of the data into our training and the test dataset with a rate of 80% to 20%.

218
00:26:04,310 --> 00:26:07,910
And here is the first simulation data.

219
00:26:08,270 --> 00:26:17,419
We can observe that the original data is like two moons scatter around and we can observe that the linear and logistic regression

220
00:26:17,420 --> 00:26:25,790
fails to capture the nonlinearity in the data because they can only like separate those two clusters into a by straight line.

221
00:26:26,060 --> 00:26:30,650
But the other nonlinear kernels perform pretty well,

222
00:26:32,150 --> 00:26:43,940
and for the circle it has a similar result where the linear kernel just use a straight line and the logistic regression just gave out for some reason.

223
00:26:43,940 --> 00:26:49,400
I don't know why and but the other no but the other kernels perform pretty well.

224
00:26:50,540 --> 00:26:57,350
And for the block or the to the normals we can observe that linear analog doesn't perform much better,

225
00:26:57,800 --> 00:27:03,200
basically because the original data has a more linear separable than the other two.

226
00:27:05,330 --> 00:27:11,030
And we can also compare the test errors of all of those data sets.

227
00:27:12,050 --> 00:27:21,830
And so among all of those kernels, the cubic kernels generally perform much better than the others for the nonlinear

228
00:27:21,830 --> 00:27:28,010
datasets and the linear and the logistic perform pretty well for the block dataset.

229
00:27:28,760 --> 00:27:37,969
And we also observe that for the real data sets, the linear kernel actually produce tells are being zero.

230
00:27:37,970 --> 00:27:41,000
That is absolutely not non normal.

231
00:27:41,450 --> 00:27:51,380
We we think it might be because the real data only has like around 300 observations which is too small and a lot of logistic regression,

232
00:27:51,440 --> 00:27:58,580
basically overfitting. So we want to find a larger dataset in the future to further tests or methods.

233
00:28:00,110 --> 00:28:07,970
And we also compare the times before among all of them and we can see that the logistics are absolutely much faster than the others.

234
00:28:08,180 --> 00:28:11,300
So there is a tradeoff between accuracy and logistic.

235
00:28:13,820 --> 00:28:14,040
Yes.

236
00:28:14,380 --> 00:28:26,120
And beyond what we have done, we want to also first, we want to complete our implementation of IBM and compare its performance with the other methods.

237
00:28:26,360 --> 00:28:32,120
And also a natural extension is to implement a CB function which allows users

238
00:28:32,120 --> 00:28:36,349
to choose the best unit parameter lambda and we want to find a larger deal,

239
00:28:36,350 --> 00:28:40,669
etc. So as I explained and beyond the scope of this project,

240
00:28:40,670 --> 00:28:46,430
we want to another nice thing to do is to extend this binary classification

241
00:28:46,430 --> 00:28:51,499
into a multiclass classification and then apply them to hyperspectral images,

242
00:28:51,500 --> 00:28:56,090
which are most mostly the IBM algorithm we used or applied to.

243
00:28:56,420 --> 00:29:00,500
And also compare the observations selected by SVM and IBM.

244
00:29:01,280 --> 00:29:04,490
And that concludes our presentation and questions. The.

245
00:29:04,550 --> 00:29:18,510
Q Thank you. So really interesting quotation, something that I'm curious about your side of the curve on this, too.

246
00:29:20,120 --> 00:29:26,060
Oh, yeah. You guys, what I am curious about, like, why they didn't.

247
00:29:26,550 --> 00:29:32,790
You were like, are the other girls, like, specific to, you know, like some sort of.

248
00:29:33,330 --> 00:29:41,610
Because if you look I mean, it seems that just changing the colonel didn't really change much for any of the kids.

249
00:29:41,820 --> 00:29:48,180
So for me, like, are they just like we're we're like, what's the difference between what they have?

250
00:29:49,360 --> 00:29:54,920
Oh, I think the currently the most popular colonel is RB of which we have implemented.

251
00:29:54,930 --> 00:29:58,260
I don't know why they only implemented my turn in the law class,

252
00:29:59,040 --> 00:30:05,760
but there is actually like from the graph, there isn't much difference between Gabi and the others,

253
00:30:06,030 --> 00:30:13,820
but there is actually some like difference in that has areas where we can see that our Barracuda

254
00:30:13,830 --> 00:30:20,790
QB polynomial is performs a little bit better than the other like they've implemented.

255
00:30:21,240 --> 00:30:29,790
So I think there is that difference, but we need to pass on more datasets to see if there is actually a significant difference.

256
00:30:29,790 --> 00:30:34,349
But this is beyond the scope of the project over time.

257
00:30:34,350 --> 00:30:39,420
But the key question is, is the result just from K an R, which is from oh, look here.

258
00:30:39,480 --> 00:30:43,320
So, I mean, yeah, we're working on it. Just just to clarify.

259
00:30:43,380 --> 00:30:54,920
Okay, great. Thank you. Okay. Next to the group one.

260
00:31:00,860 --> 00:31:04,660
Okay. But. Yeah.

261
00:31:05,150 --> 00:31:10,380
I accidentally played it, I guess.

262
00:31:12,590 --> 00:31:21,210
But however, on this one today, we're going to talk about a few mortar range, an inference evident and is in pigmentation.

263
00:31:22,380 --> 00:31:28,950
And here is our agenda for today. So let's start with some background motivation.

264
00:31:28,950 --> 00:31:34,500
So nowadays high dimensional data are quite common to be used in practice in many fields.

265
00:31:34,890 --> 00:31:38,250
For example, health care data are known to be high dimensional,

266
00:31:38,550 --> 00:31:45,000
and some biomedical research might be interested in joint effects of high dimensional predictors on the outcomes.

267
00:31:45,630 --> 00:31:55,380
So there are just countless examples of them. While high dimensional data are useful in imposes challenges on statistical analysis to

268
00:31:55,380 --> 00:32:00,510
rescue Bayesian inference as a powerful tool for leveraging the large amounts of data,

269
00:32:00,990 --> 00:32:05,610
including noisy data. Data wasn't to say information and data from different sources,

270
00:32:06,060 --> 00:32:11,340
so to gold standard of patient inference as the work on chain Monte Carlo classes of algorithm.

271
00:32:11,670 --> 00:32:18,960
It guarantees convergence for most of the cases, but might be intractable in high dimensions.

272
00:32:19,440 --> 00:32:28,740
So to account for the limitation of CMC, numerous classes of organizing have since been developed, but few have made, implement and are.

273
00:32:29,070 --> 00:32:32,580
Therefore, we will work to make a package on those new algorithms.

274
00:32:34,610 --> 00:32:44,060
So here is a brief overview of our package. In our package, we will include three main functions with the same format of expected input and output.

275
00:32:44,420 --> 00:32:54,620
Using this function, we can expect to get the samples of beta coefficient and corresponding predictions here and CMC will work as

276
00:32:54,620 --> 00:33:02,960
the baseline of the and then we will compare as we go and we examine CMC to see if the new algorithm works.

277
00:33:03,350 --> 00:33:10,670
In addition to the three main functions, we also included some helper functions which will serve as the input to the main functions.

278
00:33:11,930 --> 00:33:18,110
The user can specify which prior, which log likelihood and which model they want to use.

279
00:33:18,650 --> 00:33:26,690
For our project we used the breast cancer dataset where we use a repository of the main data source for testing to implement the algorithm.

280
00:33:27,200 --> 00:33:32,960
So the outcome is a binary stating whether the cancer is malignant or benign.

281
00:33:33,260 --> 00:33:37,610
So the petition or the predictors are all continuous variables.

282
00:33:37,910 --> 00:33:45,050
Therefore, we primarily use our with logistic model and logistic like like who being part of the main functions.

283
00:33:48,660 --> 00:33:51,900
Okay. So I'd like to describe our two algorithms.

284
00:33:52,100 --> 00:33:55,050
First is Stein variational gradient descent.

285
00:33:55,320 --> 00:34:01,500
So as the name suggests, it's a gradient, descent style approach where you take samples from some sort of reference distribution.

286
00:34:01,500 --> 00:34:08,910
Typically the prior and you sequentially update these samples again and again and again until they start to converge to your posterior distribution.

287
00:34:09,600 --> 00:34:17,009
So in order to get this algorithm to work, you need two components. One is the derivative of your posterior distribution.

288
00:34:17,010 --> 00:34:24,810
And so thankfully with glands and no networks, for example, getting derivative information is quite easy to get.

289
00:34:25,050 --> 00:34:33,780
And then you also need a user defined kernel. This is typically the RPF kernel which allows you to interpolate the evaluation of your posterior.

290
00:34:35,950 --> 00:34:39,429
The second algorithm and this is the projected. STEIN Variational.

291
00:34:39,430 --> 00:34:44,530
NEWTON So it's analogous to the second order, Newton's method. So you get second order conversions.

292
00:34:45,130 --> 00:34:51,160
Furthermore, because that the dimensionality of the problem is typically a challenge for a lot of these algorithms,

293
00:34:52,000 --> 00:34:55,810
the projected aspect is you project this problem into a lower dimension.

294
00:34:56,020 --> 00:35:02,200
So you you get just the dimensions or the components of your problem that are most influenced by your data.

295
00:35:02,440 --> 00:35:07,659
So you infer those and you freeze the rest of your components at the prior and you're able to get substantial

296
00:35:07,660 --> 00:35:13,000
dimension reduction here as well as second order of convergence with your patient inference algorithm.

297
00:35:15,110 --> 00:35:19,120
And for turn around and for the implementation. Now we have for instance,

298
00:35:19,120 --> 00:35:24,819
the and as we did in part and as she has been in South Africa and the around and we put

299
00:35:24,820 --> 00:35:31,360
it and adopted steps so that the the proposals were more suited to the empowerment and

300
00:35:31,360 --> 00:35:39,430
skill that's for improve the American and speed and also our with what virtualization is

301
00:35:39,640 --> 00:35:44,590
already pretty good in all the three algorithms but we will still try and sort them out,

302
00:35:44,590 --> 00:35:48,520
but only to be able to see if it will be faster.

303
00:35:48,880 --> 00:35:55,270
And also like here take the function prior logistic like people that take that as an example,

304
00:35:55,270 --> 00:36:00,970
we plotted other predictions of transport and actually what for here can see the fourth parameter.

305
00:36:01,390 --> 00:36:09,400
And in the other part you can see for a large timeline separations as well it is closer to zero, more stable in and then fancy.

306
00:36:09,820 --> 00:36:18,050
And the first part they look similar. But for the things involved, the CMC part is like more concentrated around the metro browser,

307
00:36:18,250 --> 00:36:23,830
but it is like more spread out around the mean and that can reduce the risk of overfitting.

308
00:36:24,070 --> 00:36:27,610
And this part is also showing our accuracy comparison.

309
00:36:32,120 --> 00:36:38,780
So then showing the final predictive performance of our logistic model with the patient inference on the parameters.

310
00:36:40,070 --> 00:36:50,060
So we will see that the confusion matrices of the two different in our inference algorithms as well as that the accuracy of the balanced accuracy.

311
00:36:50,840 --> 00:37:01,879
And we see that across almost all of these metrics our algorithm, the SPG outperforms and CMC, which is nice and it is also substantially faster.

312
00:37:01,880 --> 00:37:05,580
So doing that, the timed aspect of these algorithms,

313
00:37:06,260 --> 00:37:14,060
we were able to to get SPG to converge in almost an order of magnitude less time compared to that the vanilla CMC.

314
00:37:14,270 --> 00:37:21,080
And we would expect this trend to scale up substantially more in much higher dimensions for future work.

315
00:37:21,330 --> 00:37:24,200
When we implement the projected time Variational Newton's method,

316
00:37:24,200 --> 00:37:31,640
we would expect probably even faster convergence than SPG just because it's second order as opposed to first order.

317
00:37:33,590 --> 00:37:42,410
Great. Thank you so much. Thank you. Time for probably two questions.

318
00:37:44,180 --> 00:37:48,590
When you're running it and you get an equal number of simple things that you've got the convergence of both.

319
00:37:49,280 --> 00:37:52,580
How does the runtime compare when you're sampling from the posterior?

320
00:37:53,600 --> 00:37:59,659
How does the run with equal samples? Yeah, like when you run equal an equal number of samples after convergence for both of them.

321
00:37:59,660 --> 00:38:04,479
How does that work? By comparison. Yeah.

322
00:38:04,480 --> 00:38:12,040
So I guess with CMC you need to generate a very large number of samples in order to get convergence.

323
00:38:12,040 --> 00:38:15,819
So it's not even so much that like you're running it until you've converged.

324
00:38:15,820 --> 00:38:18,940
And then all samples past that are, you know, considered converge.

325
00:38:18,940 --> 00:38:22,600
You just need to like run a ton in order to get that appropriate mixing.

326
00:38:24,010 --> 00:38:30,129
One nice benefit of the SVG method is that how we have it are implemented.

327
00:38:30,130 --> 00:38:37,420
It's all vector. So like when we have say 100 SVG samples, all those 100 samples can be processed in parallel,

328
00:38:37,630 --> 00:38:42,370
whereas with CMC it's kind of a requirement of the algorithm that they all be done sequentially.

329
00:38:42,370 --> 00:38:45,339
So it has to all be done in a a for loop.

330
00:38:45,340 --> 00:38:52,630
So hopefully that kind of gets at the idea of like all around it's going to be faster with this, this other algorithm compared to CMC.

331
00:38:54,940 --> 00:38:59,079
Other questions. I have a question.

332
00:38:59,080 --> 00:39:05,290
So. So this is a very interesting work when you do so.

333
00:39:05,320 --> 00:39:11,379
Mm hmm. See? And so for personal reasons or implementation.

334
00:39:11,380 --> 00:39:14,800
Or do you teach it to some other people? That's a question.

335
00:39:16,040 --> 00:39:20,560
I'm like, well, I mean, what happened to that?

336
00:39:20,980 --> 00:39:25,000
That's sort of beyond our control.

337
00:39:25,240 --> 00:39:32,400
Okay. So you're using to your country or so and your.

338
00:39:32,770 --> 00:39:38,470
So when you do any kind of agent method and there is a there's a part that.

339
00:39:38,680 --> 00:39:42,460
What kind of model are you using and why is that? What is your objective?

340
00:39:42,880 --> 00:39:47,260
So that wasn't clear in the beginning. So I wanted to listen to. You're trying to use the logistic.

341
00:39:47,530 --> 00:39:51,180
Yeah, I would provide the sweep. Yeah.

342
00:39:51,660 --> 00:39:57,460
Right. So you're in the your goal is to estimate the parameter or do you want to.

343
00:39:58,210 --> 00:40:00,450
Are you interested in a point estimation. Why.

344
00:40:00,460 --> 00:40:07,210
I insisted the estimating the distribution of was description of those parameters of what is the success criteria here.

345
00:40:07,210 --> 00:40:18,540
Because I don't understand how you set the creative team and convert them into by a classification problem when it's actually not a class amendment.

346
00:40:18,740 --> 00:40:24,940
Yeah. So I guess we, we had to go. So our, our package eventually puts two things, which is one,

347
00:40:25,150 --> 00:40:30,760
samples from the parameters so we can judge do we have an accurate distribution of our parameters,

348
00:40:30,970 --> 00:40:35,590
but then also the forward pass with the model from those parameters.

349
00:40:35,590 --> 00:40:36,290
And so yeah,

350
00:40:36,310 --> 00:40:45,310
we would have a distribution of predicted samples and so we can use that to get say like a point estimate if we want to take that the mean or median,

351
00:40:45,550 --> 00:40:52,990
we can also use that for say like uncertainty if we wanted uncertainty quantification on our predicted element.

352
00:40:53,000 --> 00:40:59,650
So for our accuracy comparison, we, we did that just with that for the prediction.

353
00:40:59,800 --> 00:41:02,740
Yeah. Then the mean of the predicted samples,

354
00:41:02,950 --> 00:41:10,820
but we also have some nice uncertainty quantification components that wouldn't be there if we didn't use CMC or SPG.

355
00:41:11,230 --> 00:41:14,580
Okay. And this one is not so you.

356
00:41:14,680 --> 00:41:21,310
So are you going to compare with the method that doesn't doesn't does it used as a local update like we have to

357
00:41:21,430 --> 00:41:27,070
be a face or something that probably wouldn't work if you're using estimating or module estimation estimate,

358
00:41:27,370 --> 00:41:31,180
are you going to compare with that or just compare only with this is do that?

359
00:41:32,140 --> 00:41:39,670
Yeah, I think that would be a really interesting thing to to compare against maybe for that both the timing side and also the accuracy.

360
00:41:39,670 --> 00:41:46,630
Because it is true that like with CMC and as mentioned, there's some extra computational effort needed in order to get those, you know,

361
00:41:46,660 --> 00:41:57,850
the recommended because I wasn't clear that how much time so how much complicated problem this with a problem sorry for 30, 30 seconds over.

362
00:41:58,330 --> 00:42:03,430
Yeah. Let's move on. Thank you. And group eight.

363
00:42:21,570 --> 00:42:27,600
Hello everyone. We are a group eight and our project is increment in our.

364
00:42:33,260 --> 00:42:36,770
Let me introduce a background for our project.

365
00:42:37,400 --> 00:42:44,600
The whole genome sequencing, also known as WG s is used significantly in the clinical field and it is

366
00:42:44,600 --> 00:42:49,700
considered the most comprehensive collection of an individual's genetic variation.

367
00:42:50,150 --> 00:42:55,340
And we can extract the variants from the sequence to treat rare disease.

368
00:42:55,700 --> 00:43:00,410
And we can also detect structural variants, also known as species,

369
00:43:00,800 --> 00:43:11,960
and including the copy number variants also known as the and these DB of use then for the distribution based feature extraction.

370
00:43:12,740 --> 00:43:21,800
And we can it is used to extract features from a vector of varying length and

371
00:43:22,010 --> 00:43:28,760
the features can be used to construct predictive models and to classifications.

372
00:43:30,020 --> 00:43:36,470
The features are basically the count of the variants in a given evidence of ranges,

373
00:43:36,860 --> 00:43:49,849
and the ranges can be obtained by the ranges can be the ranges can be obtained by dividing the lengths,

374
00:43:49,850 --> 00:43:59,600
varying lengths into several beings, and each beings contains the count of the very length, and we can use the count as our features.

375
00:44:00,200 --> 00:44:06,860
And there are many methods of the age of do this job. We will, but we will be mainly talk discussing both for methods.

376
00:44:07,550 --> 00:44:12,590
The first one is to divide the thence into equal with beings.

377
00:44:13,460 --> 00:44:18,230
The second method is to generate being beings based on the quality of the lens.

378
00:44:18,740 --> 00:44:23,990
And so a method is to do clustering based on the distributions of the nest.

379
00:44:24,290 --> 00:44:31,700
And the last one is the kernel. This is density estimation could be it is a supervised method.

380
00:44:35,090 --> 00:44:44,600
I want to introduce our data set. And first, so in this dataset, each row means each sample ends a specific number in each sample.

381
00:44:44,870 --> 00:44:58,250
It's a lens of a structural variant. The first column is the name of Queens, and the third column is a is like zero one, the labor of cloth.

382
00:44:59,330 --> 00:45:04,389
So as we mentioned, we have four methods, but the first method has some problems.

383
00:45:04,390 --> 00:45:08,960
So we going to take it as a foundation. The first method is equal ways.

384
00:45:09,200 --> 00:45:14,779
So the goal for this method is separating the range of variant loans into a fixed

385
00:45:14,780 --> 00:45:23,240
number with eco with analysis method cannot reflect the the distribution of the.

386
00:45:25,930 --> 00:45:30,489
Eight. Is that so it may leads to a situation that some bins contents.

387
00:45:30,490 --> 00:45:41,050
No information's there empty. Well other beans compress too much information, but this problem can be fixed with a second method which is quantile.

388
00:45:41,410 --> 00:45:45,130
Suppose we well know for beans then. The separation.

389
00:45:46,240 --> 00:45:56,140
Sorry. Since the separating points are going to be 25% power, 50 percentile and 75 percentile if we can separate with a percentile.

390
00:45:56,410 --> 00:46:04,780
It allows us to make sure that the number of variance in each part, in each beans can be the same.

391
00:46:05,650 --> 00:46:12,850
Although this method is not searching for the true distribution, but it can partially reflect as a distribution.

392
00:46:17,440 --> 00:46:22,990
The next method is to generate dynamic variance features based on clustering.

393
00:46:23,560 --> 00:46:34,000
And this is also very straightforward but better than the the previous two method because it takes the distribution of the land into account.

394
00:46:34,570 --> 00:46:47,260
And basically we will feed a Gaussian mixture on the land and find the in clusters and things by using M algorithm.

395
00:46:48,430 --> 00:46:55,419
And this has been advantageous since we are considering the distribution of the

396
00:46:55,420 --> 00:47:02,500
land and we can find the concentration of the lands and to generate beings.

397
00:47:02,770 --> 00:47:12,159
But it also has some limitations because we need to define the number of beings and we know that M algorithm depends a lot on the

398
00:47:12,160 --> 00:47:24,940
initialization initialization and it depends a lot of a lot on the inner rotation of the number of beings and the the choice of parameters.

399
00:47:24,940 --> 00:47:32,019
So this is very tricky and the difference in visualization can be two very different results.

400
00:47:32,020 --> 00:47:34,060
So we will be very careful on that.

401
00:47:42,780 --> 00:47:51,360
Then comes the approach of the truck, the dynamic foreign fighters based on kitty, which is kernel density estimation.

402
00:47:51,780 --> 00:47:57,570
This is applicable for those supervised generals when the costs are available.

403
00:47:58,530 --> 00:48:06,510
So the goal here is to define the base in a way that maximizes the discriminative power of the resulting features.

404
00:48:07,650 --> 00:48:15,840
This model works by approximating the density of the distribution of the mass separately for samples in each path.

405
00:48:16,260 --> 00:48:19,080
As the right graph shows, oh,

406
00:48:20,010 --> 00:48:30,420
the orange light on the blue line shows the plus one on the cross zero and identify the cross points of the two distributions.

407
00:48:30,960 --> 00:48:33,960
Here we have four cross points here.

408
00:48:35,340 --> 00:48:43,620
Later we use those points of being boundaries for features and optionally inferring out features or low discriminative power.

409
00:48:44,130 --> 00:48:47,310
So that's the three approaches.

410
00:48:48,120 --> 00:48:51,309
Each approach you will choose the first.

411
00:48:51,310 --> 00:48:58,380
Based on the variables we define the being boundaries for each instructive features

412
00:48:58,620 --> 00:49:08,430
and the way we will count the number of the violence in the each being on.

413
00:49:11,040 --> 00:49:20,700
Until now, we have already resigned from our functions to Trump features for each method and drugs.

414
00:49:20,940 --> 00:49:27,480
Oh and the right to vote and apply the method on some clinical data.

415
00:49:28,140 --> 00:49:40,230
For example, the breast cancer data. And for the next step, we will evaluate debriefing through the predictive models.

416
00:49:42,210 --> 00:49:49,240
For example, the logistic regression is random for us and the Carolinas neighbors.

417
00:49:49,800 --> 00:49:57,690
We will compare the different classifier first to choosing the model with the highest accuracy,

418
00:49:58,320 --> 00:50:05,640
and we will also use this method on other data to see whether it will produce generalized results.

419
00:50:06,120 --> 00:50:10,500
In addition to this, we focus on the efficiency of the implementation.

420
00:50:11,040 --> 00:50:18,360
On the one hand, we have already adjusted the algorithms based on the skeletal algorithm in the paper.

421
00:50:19,440 --> 00:50:30,690
For example, we have for the clustering method we have already, or use a different algorithm to do the cost through.

422
00:50:32,010 --> 00:50:41,640
Besides the clustering, we have already chose the like like means clustering to improve the efficiency.

423
00:50:41,940 --> 00:50:52,740
On the other hand, we will use our CPE to improve the efficiency in our oh, I think for our project.

424
00:50:52,920 --> 00:51:03,150
The first challenge is to fully understand the genomics representation and the significance of the some topic like for a lot of beings here.

425
00:51:04,320 --> 00:51:17,130
The second is that we try some data manipulation functions and machine learning functions similar to those in Python hackers in Scala.

426
00:51:18,240 --> 00:51:25,020
We also write our step to improve the efficiency and that of our presentation.

427
00:51:26,100 --> 00:51:31,630
Thank you. Any questions?

428
00:51:34,680 --> 00:51:38,970
So just to understand, you're just just taking one dimensional.

429
00:51:39,180 --> 00:51:44,580
So currently the data is one dimensional, just data you're trying to collect cluster classified.

430
00:51:45,360 --> 00:51:51,809
And are you going to take a multidimensional data eventually or what, what what I because of that, the method is very different.

431
00:51:51,810 --> 00:51:53,970
Where are you just doing the one dimensional.

432
00:51:56,860 --> 00:52:10,540
Oh, clearly we are in striking features for a vector offering the length of this lens we are adopting by our own stack, the x after the matrix.

433
00:52:10,960 --> 00:52:19,450
So for example, the simple one, we can obtain like ten rolls for simple one,

434
00:52:19,900 --> 00:52:27,340
and each one of these ends, each of the lens unit and four for example, that you go with.

435
00:52:27,340 --> 00:52:35,170
We just find the minimum of the whole length and the maximum and divide it into equal with Venus.

436
00:52:35,710 --> 00:52:39,100
So you're basically converting it to one dimensional data and I try, correct?

437
00:52:39,110 --> 00:52:42,490
Right. Yeah. Okay. Yeah. Thank you.

438
00:52:42,670 --> 00:52:50,940
Thank you. And six.

439
00:53:03,310 --> 00:53:06,850
Good morning, everyone. This is Group six.

440
00:53:06,850 --> 00:53:14,860
And today we're going to present our work about implementation of kernel ratio estimation art and kernel ratio estimation.

441
00:53:14,860 --> 00:53:18,399
It's actually altruism derived from the kernel density estimation.

442
00:53:18,400 --> 00:53:22,420
So I will give you a little bit background about what the Colonel density estimation is.

443
00:53:22,810 --> 00:53:32,770
I assume most of you have heard about this term before, and this is actually an algorithm which will produce a smooth,

444
00:53:32,770 --> 00:53:42,040
continuous surface where each location in the study area is assigned to a density value irrespective of arbitrary administrative boundaries.

445
00:53:42,730 --> 00:53:47,110
And here we are talking about the kernel density estimation in the context of spatial analysis.

446
00:53:47,560 --> 00:53:51,850
So there are two distinct methods to measure the distance of kernel density function.

447
00:53:52,330 --> 00:53:54,459
The first method is state side method,

448
00:53:54,460 --> 00:54:01,450
which where the density calculations perform on side to evaluate the density for every location in the study area.

449
00:54:01,930 --> 00:54:08,740
And the second method is the case site method, which only looks at the case locations and their defined surrounding locations.

450
00:54:09,280 --> 00:54:11,259
Ending our case in the tax dataset.

451
00:54:11,260 --> 00:54:19,540
The case location will be the retail store in Columbus, and there's a limitation for kernel density estimation for sure,

452
00:54:19,540 --> 00:54:25,710
because the search radius is fixed over a constant number for every point of interest,

453
00:54:25,720 --> 00:54:30,610
and it actually didn't consider the heterogeneity issue for across the surface.

454
00:54:31,090 --> 00:54:38,170
And therefore it will be helpful to think using a static bandwidth estimate or instead of a fixed bandwidth one.

455
00:54:38,650 --> 00:54:45,250
And as I mentioned before, from the kernel density estimation, we used a fixed bandwidth for density calculation.

456
00:54:45,760 --> 00:54:54,130
The kernel ratio estimation, we will use an adaptive bandwidth that vary across the study area to adapt to either the event points or the background.

457
00:54:55,030 --> 00:55:03,309
Intuitively speaking, you may think a retail store from the suburb, an area, the search radius for not retail store,

458
00:55:03,310 --> 00:55:09,430
it will larger than the retail store in the urban area, even though they serve roughly the same number of population.

459
00:55:09,430 --> 00:55:12,879
But because the population density vary across the surface.

460
00:55:12,880 --> 00:55:17,230
So really the assertion bandwidth should depends on where they locate.

461
00:55:17,890 --> 00:55:26,920
And there are also some paper shows that adaptive bandwidth is better than a fixed bandwidth estimating the probability density of event points.

462
00:55:28,240 --> 00:55:36,900
And here is the expression about what the case that adaptive bandwidth estimate here is and they are the EXI.

463
00:55:37,000 --> 00:55:43,479
Why is the location of case i and P is a function centered at x, y and based on the local population.

464
00:55:43,480 --> 00:55:51,430
So as I mentioned, unlike the kernel that's destination where H will be a constant number and it's a hyper

465
00:55:51,430 --> 00:55:56,620
parameter pre defined by researchers based on their knowledge about certain type of subject.

466
00:55:57,040 --> 00:56:05,709
Here the H will be a variable determined by the local population information which local I in the neighborhood of the point of

467
00:56:05,710 --> 00:56:14,710
interest and the rest of the function will pretty much the same as the kernel density estimation and our contribution here.

468
00:56:15,140 --> 00:56:22,960
Since we we find there are two packages in our about to calculate the kernel density estimation for spatial data,

469
00:56:23,350 --> 00:56:29,169
but neither one can apply to the idea of using an optimal bandwidth or we could see adaptive

470
00:56:29,170 --> 00:56:35,229
bandwidth and the idea of kernel ratio estimation only can be contacted through a ARQ house,

471
00:56:35,230 --> 00:56:45,280
which is an extension to a box of eyes. So there is no package we finding are can implement this exact idea and they are high level.

472
00:56:45,280 --> 00:56:49,030
What will be the kernel that the ratio estimation function looks like?

473
00:56:49,630 --> 00:56:54,940
So the input of the function will be a the raster data of the population information,

474
00:56:54,940 --> 00:57:02,410
so the population support and B will be the spatial points matrix of point of interest and the output will be the

475
00:57:03,040 --> 00:57:10,030
just so raster data of kernel ratio density across the study area which defined by the extent of spatial point.

476
00:57:10,810 --> 00:57:14,650
And roughly speaking, we have two big steps for this function.

477
00:57:14,650 --> 00:57:19,690
The first one is calculating the search bandwidth for each point based on the population support,

478
00:57:19,690 --> 00:57:24,370
and second would be calculating the kernel ratio based on the bandwidth.

479
00:57:24,880 --> 00:57:28,690
So for the first steps we actually defined, as I said,

480
00:57:28,720 --> 00:57:39,100
neighborhood and searching using a loop to search the population as a part for a for each point of interest and its neighborhood.

481
00:57:39,490 --> 00:57:49,510
And then to reach to a certain number of population that would be a hyper parameter predefined by the user for say six, 6000 people.

482
00:57:49,870 --> 00:57:59,110
And until the search reached to that limit, we will determine that the radius of that neighborhood will be the search bag with.

483
00:57:59,830 --> 00:58:05,260
Until now we have implement the idea of. Calculating the search and searching bandwidth.

484
00:58:05,650 --> 00:58:12,640
And here are just a toy example to show to show the comparison of using static bandwidth and adaptive bandwidth.

485
00:58:13,090 --> 00:58:19,930
So the first map, the figure one, is actually to show the searching area by using a static bandwidth method.

486
00:58:21,550 --> 00:58:29,350
As you can see, the search area for say you roughly will be the same for each point of interest no matter where they locate.

487
00:58:29,920 --> 00:58:35,110
And that's roughly how the search area looks like.

488
00:58:35,290 --> 00:58:42,840
If you're using the kernel density estimation and for the adaptive bandwidth, it's actually calculated by our function.

489
00:58:42,850 --> 00:58:50,229
So. And theoretically speaking, you can see for each location point of interest,

490
00:58:50,230 --> 00:58:56,110
there will be a unique search bandwidth which determined by its population support.

491
00:58:56,590 --> 00:59:06,639
And it's also aligned with our intuition, which you can see in the Northwest and Columbus there is actually a sub urban area and the search

492
00:59:06,640 --> 00:59:12,310
radius for it that returns to store will a larger than the one actually near the downtown Columbus.

493
00:59:13,480 --> 00:59:20,350
And for the future steps, we will first implement the kernel function.

494
00:59:20,680 --> 00:59:25,690
Now we have the Gaussian kernel density function and the uniform current density function.

495
00:59:25,690 --> 00:59:28,840
But we were thinking adding more in the future.

496
00:59:29,140 --> 00:59:38,560
And also the most computational challenge in part will be to calculate the distance matrix of between the cell cells of the output,

497
00:59:38,560 --> 00:59:40,600
raster file and point of interest.

498
00:59:41,110 --> 00:59:52,209
So the number of rows for that matrix will be the number of cells from the output raster file, which are pretty much determined by the study extent.

499
00:59:52,210 --> 00:59:59,080
And also the resolution of the raster file and the number of column will be the number of point of interest.

500
00:59:59,560 --> 01:00:03,010
But roughly speaking, that would be a pretty large matrix.

501
01:00:03,700 --> 01:00:07,240
So we'll calculate the distance between themselves and the point of interest.

502
01:00:07,720 --> 01:00:17,130
And we were thinking to attach elements which are larger than the searching bandwidth which we determine in the previous step into the area into zero.

503
01:00:17,140 --> 01:00:28,270
And so that ideally to convert the matrix into a sparse matrix and apply the kernel function and also the matrix multiplication at sparse matrix.

504
01:00:28,630 --> 01:00:34,030
So hopefully it will speed up the the calculation and it won't take too long to get the result.

505
01:00:34,540 --> 01:00:37,719
And that's all for our today's presentation.

506
01:00:37,720 --> 01:00:48,480
And this is our reference. And thank you so much for listening. Submitted for questions.

507
01:00:53,610 --> 01:01:00,509
So. So you haven't talked about the implementation details much.

508
01:01:00,510 --> 01:01:08,280
So I guess this is a your how many how large is how large data are you trying to scale this method?

509
01:01:08,730 --> 01:01:16,590
Oh, yeah. So for our test case, we have 7000 around the point of interest and the extent.

510
01:01:16,590 --> 01:01:22,950
Well we pretty much the city of Columbus which will not be, is not that large for.

511
01:01:23,250 --> 01:01:29,430
Yeah, but it's a 7000 point scale pretty large if you do because you need some somebody

512
01:01:29,430 --> 01:01:37,200
within the cubic or or or at least the the square quadratic complexity.

513
01:01:37,200 --> 01:01:43,980
So what is the, you know what is telling the company you know tell you you need to solve to to actually

514
01:01:44,220 --> 01:01:50,230
make the implementation work working in the in you know in your future aspects of

515
01:01:50,340 --> 01:01:56,100
what we think been pretty much the distance metric system is not sparse matrix we

516
01:01:56,100 --> 01:02:01,559
think they will occupy all of the memory and that's really the most challenging part.

517
01:02:01,560 --> 01:02:08,879
But we we hope the sparse nature can help us to address that our packages that gives us part of the matrix as a piece,

518
01:02:08,880 --> 01:02:19,680
as if you especially if you're okay with that these canyons they should be provide that kind of um yeah I'm sure that we thank you.

519
01:02:20,160 --> 01:02:24,930
Okay. Okay so.

520
01:02:27,680 --> 01:02:33,800
Last but not least, I think we have got a five.

521
01:02:50,070 --> 01:02:53,729
All right. Thanks, everyone. We drew five for our project.

522
01:02:53,730 --> 01:03:02,100
We worked on a new package we call Arena two, which is to help improve our image modeling in times and in our.

523
01:03:06,390 --> 01:03:10,430
If you have. Okay. Yeah. All right.

524
01:03:10,640 --> 01:03:19,770
So aama modeling, which stands for auto regressive moving average modeling, is commonly used for modeling time series data.

525
01:03:21,410 --> 01:03:28,820
It's one of the most general basic, basic kind of methods for modeling time series.

526
01:03:31,520 --> 01:03:39,650
So stats the stats package. An R implements an original function for fitment for fitting our models.

527
01:03:41,270 --> 01:03:53,630
But we've found that there's an optimization dash kind of big mathematical issue with the current arena function.

528
01:03:53,970 --> 01:03:58,040
Ah. Which leads to some mathematically untenable results.

529
01:04:00,080 --> 01:04:10,310
So our package aims to fix these issues and implement some other functionality that's useful for working with our models.

530
01:04:14,620 --> 01:04:19,899
So very briefly, we'll give an introduction to Reema modeling because not everyone is super familiar time series.

531
01:04:19,900 --> 01:04:23,920
So we let random variable collected time points one up through n.

532
01:04:24,670 --> 01:04:36,790
An air model or an auto regressive model with p coefficients has the form x two equals five one times x t minus one so forth up to the point x,

533
01:04:36,790 --> 01:04:45,550
t minus p plus some white noise error term and then moving average model has Q coefficients

534
01:04:45,880 --> 01:04:51,310
where the coefficients are now on the white noise of the previous observations,

535
01:04:51,820 --> 01:04:56,590
and in our model is simply just combining both in R and a model together.

536
01:05:01,610 --> 01:05:04,700
Yeah. Okay.

537
01:05:04,880 --> 01:05:10,220
So now we'll talk a little bit more about kind of where the R implementation is failing.

538
01:05:11,360 --> 01:05:20,510
Um, so we find that adding parameters to the current ah,

539
01:05:21,140 --> 01:05:31,610
implementation arena can yield likelihoods that actually decrease, which is like mathematically impossible.

540
01:05:34,790 --> 01:05:44,239
So we will, let's see. So this is kind of akin to like working in linear regression and seeing something

541
01:05:44,240 --> 01:05:50,960
where adding an extra covariate yields a decreased residual sum of squares.

542
01:05:52,040 --> 01:06:04,790
Um, and so to demonstrate this, we'll look at an arena model with the following parameters and just give an example of the agreement

543
01:06:04,790 --> 01:06:11,300
implementation and are yielding a decreased likelihood after adding an additional parameter.

544
01:06:14,820 --> 01:06:24,810
So here is a plot of our generated data where the x axis represents time and the y axis represents those generated values over time.

545
01:06:25,980 --> 01:06:28,350
And so using this time series data,

546
01:06:29,160 --> 01:06:37,110
we want to show an example of where the array of function in the stats library in our fails to optimize the likelihood of our model.

547
01:06:37,650 --> 01:06:42,690
So in the first two lines of code here we have Arma two, two and Arma two, one, Arma two,

548
01:06:42,690 --> 01:06:49,980
two is our model where we add the parameter and we can think of Arma two one as the original model.

549
01:06:50,340 --> 01:06:58,440
And so when we actually observe the lot, the log likelihoods of the Arma two one in the Arma two two models,

550
01:06:58,440 --> 01:07:01,800
we see that there is a decrease in the log likelihoods,

551
01:07:01,980 --> 01:07:10,380
which is not something we would expect for a function that's supposed to maximize the likelihood of our model.

552
01:07:10,830 --> 01:07:18,720
And so, again, this is just demonstrating a case where this this function sometimes fails to maximize the model likelihood.

553
01:07:20,830 --> 01:07:31,209
And so in our packs, in our package or Rama, too, we provide a function that addresses this issue by implementation of a random restart algorithm.

554
01:07:31,210 --> 01:07:38,080
And what this random restart algorithm does is it improves the parameter estimation in such

555
01:07:38,080 --> 01:07:44,440
a way that when we look at the log likelihoods after adding in a parameter into the model,

556
01:07:44,920 --> 01:07:49,149
we no longer see that the log likelihood is going to decrease.

557
01:07:49,150 --> 01:07:55,720
We will instead see a value that's either equal to our original model or greater than our original model.

558
01:08:00,870 --> 01:08:05,140
All right. So here is an example of our implementation of a green line.

559
01:08:06,210 --> 01:08:18,210
We're using the same modeling parameters as in the previous example, but we see here that our implementation results in an increased log likelihood.

560
01:08:19,110 --> 01:08:26,640
So we get a net a difference of 3.39 for log likelihood units difference here.

561
01:08:27,030 --> 01:08:36,270
But most importantly, we're seeing an increase in the log likelihood, which is a mathematically tenable result.

562
01:08:42,050 --> 01:08:46,820
So the package also contains a few other features that helps improve almo modeling.

563
01:08:46,820 --> 01:08:51,770
R So some of these features include a plot dot, a remap function.

564
01:08:51,770 --> 01:09:01,310
So if you have an estimated a remote model based off of our function, you can plot that object and it will visualize it using G2 port,

565
01:09:02,360 --> 01:09:07,880
which is very helpful if you're trying to understand how well the model fits the data.

566
01:09:08,450 --> 01:09:14,750
We also provide a function auto a remote too that automatically picks the number of remote parameters based on the AC.

567
01:09:15,110 --> 01:09:21,080
This this function essentially mimics the function forecast auto remote function, which does the same thing,

568
01:09:21,410 --> 01:09:30,680
but it uses our improved our improved algorithm for maximizing the likelihood because that's the maximum.

569
01:09:30,740 --> 01:09:35,240
If the likelihood is not properly maximized, the AIC is an invalid measurement.

570
01:09:37,550 --> 01:09:45,560
We also have a function that will create a table of AC values for AMO models of varying sizes which will help facilitate model selection.

571
01:09:47,770 --> 01:09:57,280
And we also have a function to simulate our model parameters so that if you want to get some random parameters for the model,

572
01:09:57,490 --> 01:10:06,129
the model is both causal and convertible, which essentially you can't pick any value for model parameters.

573
01:10:06,130 --> 01:10:10,180
They have to follow certain criteria, which is kind of challenging.

574
01:10:10,690 --> 01:10:17,709
We have a function that facilitates this and then we also provide functions to inspect the validity cause of causality,

575
01:10:17,710 --> 01:10:20,740
invert, invert ability of AMO models.

576
01:10:20,740 --> 01:10:27,250
And I won't go into too much details of what that means, but it's very helpful if you're making an AMO model.

577
01:10:28,870 --> 01:10:38,380
So future work that we want to do is add simulate a remote to function, which will basically take a fitted model and simulate from that model.

578
01:10:40,660 --> 01:10:46,330
We also would like to determine the effectiveness of the coefficient sampler and compare to alternative techniques.

579
01:10:49,840 --> 01:10:54,069
One thing we would want to do is implement a better stopping rule for the random restart algorithm.

580
01:10:54,070 --> 01:11:00,370
Currently, it's a parameter that we input into the model saying we want to try maybe ten random restarts,

581
01:11:00,670 --> 01:11:07,540
but there is published work out there on good criteria for stopping a random restart rule.

582
01:11:07,540 --> 01:11:11,889
So we want to implement that and we want to demonstrate how our algorithm may have

583
01:11:11,890 --> 01:11:16,720
improved the results of published papers that use the stats in the function.

584
01:11:17,890 --> 01:11:22,450
And then finally, we want to submit this to Crann and then hopefully write a paper on, on the results.

585
01:11:24,750 --> 01:11:28,190
Do you have a one on one?

586
01:11:28,190 --> 01:11:35,850
Maybe a little more than one minute for the question. So the question is this what is the.

587
01:11:40,170 --> 01:11:44,150
There's a lot of opinions we want to throw in that.

588
01:11:44,910 --> 01:11:57,930
I'm also. So basically for your excuse, but with two different programs with uh, you know, uh, the one with the yeah.

589
01:11:58,080 --> 01:12:03,840
So for, yeah, for both of you. So P and Q as two and two is what you would want.

590
01:12:04,170 --> 01:12:13,200
Right. New York. New York. Like you, it is lower or higher than the ones that are, you know,

591
01:12:13,950 --> 01:12:19,590
like I'm confused because typically like in the Time series, probably you would decide your parameters and you're like.

592
01:12:20,460 --> 01:12:26,340
Yeah, yeah, yeah. That's there's several ways you can pick the number of model parameters.

593
01:12:26,340 --> 01:12:36,540
ACF is an important thing, but several um, combinations of air and made coefficients often result in valid ACF values.

594
01:12:36,900 --> 01:12:40,070
And so then another typical way to pick is based off of AIC.

595
01:12:40,500 --> 01:12:44,760
Um, there's a lot of textbooks that kind of encourage this procedure where you check

596
01:12:44,760 --> 01:12:49,050
the ACF and then you may have a subset of three or four models that are valid.

597
01:12:49,350 --> 01:12:51,600
And then from those models you pick the AIC.

598
01:12:51,930 --> 01:12:58,649
So your argument is basically the population is not the calculate, not the estimation for log likelihoods,

599
01:12:58,650 --> 01:13:04,080
but the maximization of the log likelihood that it fails to properly maximize the log like.

600
01:13:04,920 --> 01:13:08,549
So we should, we should let this go and then let's take it off line then here.

601
01:13:08,550 --> 01:13:13,950
Yeah, yeah. So whoever this was, please take it.

602
01:13:14,040 --> 01:13:24,029
Take it. Okay, so, so yeah, I, I was very, very impressed to what you guys have have presented on Monday and Wednesday.

603
01:13:24,030 --> 01:13:27,090
I really am proud of you guys again, as I said.

604
01:13:27,900 --> 01:13:34,590
And what I, I mean, I know this project has been, you know, quite stressful for some of the people.

605
01:13:34,860 --> 01:13:40,530
But what I really hope, I hope for is that when you write the resume here, if you have a line,

606
01:13:40,530 --> 01:13:46,080
the software I developed and put it put it as a as a some something that you can be proud of.

607
01:13:46,470 --> 01:13:54,340
That's what I would like to see. And, you know, as some of you might indicate, if you wanted to write into our journal or some other small paper,

608
01:13:54,360 --> 01:13:59,189
just a two page paper, you can you can write it because you're going to write the report.

609
01:13:59,190 --> 01:14:05,999
You can probably do that, try to do that, that that'll be a nice thing to do without having to have otherwise necessarily.

610
01:14:06,000 --> 01:14:09,060
You can write your own paper and try to see where it goes.

611
01:14:09,510 --> 01:14:17,100
I think these are all great things you can pursue and I think most of you actually meet the criteria that I was expecting.

612
01:14:17,100 --> 01:14:22,229
So, you know, we'll have to see what the final report will will look like.

613
01:14:22,230 --> 01:14:25,280
But it looks like you guys did did great.

614
01:14:25,290 --> 01:14:35,279
If you are available in the office hours in this Wednesday where I can create another office hours because you might want to get a lot of feedback.

615
01:14:35,280 --> 01:14:45,300
So I'll, I'll schedule some time, you know, maybe today or tomorrow afternoon like a 530 or something so that I can give you a feedback on nine.

616
01:14:45,960 --> 01:14:50,910
So and what I want you to give a feedback is, you know, if you're in the case,

617
01:14:50,910 --> 01:14:56,520
you're worried about, oh, is this good enough or do I need to do more or not to, you know,

618
01:14:56,850 --> 01:15:02,940
I'm not going to guarantee that you're going to get full credit, but if you successfully, uh, you know,

619
01:15:03,030 --> 01:15:08,459
finish it in this way, you know, you don't, you don't need to like, do infinite amount of things.

620
01:15:08,460 --> 01:15:12,900
I just wanted to say this is a good for the class, even though you might want to develop more.

621
01:15:13,260 --> 01:15:21,600
So that kind of feedback I'm happy to provide if you wanted to get and I have probably some suggestion to how to improve your method.

622
01:15:21,990 --> 01:15:26,790
Well individually in the case I didn't give a specific feedback.

623
01:15:27,480 --> 01:15:31,260
Okay. So that's, that's what I wanted to say.

624
01:15:31,860 --> 01:15:36,450
And I want one thing, it's a little bit sensitive topic, but I think it's a good to bring up.

625
01:15:37,260 --> 01:15:44,540
So when you do, the actual project is always, you know, there's a fairness problem, always happens, right?

626
01:15:44,550 --> 01:15:51,290
So I think you guys really did a good job in making as a team and I try to present as a team, but sometimes, you know,

627
01:15:51,360 --> 01:16:00,659
because everyone has different backgrounds and so on, it's a, it's a it's a it happens where the amount of contribution is just different.

628
01:16:00,660 --> 01:16:11,760
So for example, like, so as long as you guys are happy with, you know, getting the, the equal credit, you don't have to say anything.

629
01:16:11,760 --> 01:16:15,600
But if you feel like is it fair or unfair?

630
01:16:15,600 --> 01:16:24,360
So my, my recommended criteria is that so maybe like I it's a if you're a group of three,

631
01:16:24,840 --> 01:16:29,190
I would say, oh, I did half of the work and the other two people did half the work.

632
01:16:29,370 --> 01:16:36,299
That's probably okay, I would say, but I did like 80% of work and 90% the work.

633
01:16:36,300 --> 01:16:45,960
Again, that, that, that, that that goes into unfair kind of territory or this person did that 0% basically like that that is a problem.

634
01:16:46,140 --> 01:16:55,140
So if you feel like that kind of unfairness problem coming up, you know, you can email me and talk about that.

635
01:16:55,140 --> 01:17:01,290
I'm not going to make it as a big deal. And we you know, we're going to find a good resolution.

636
01:17:01,290 --> 01:17:04,499
But if you believe that this project,

637
01:17:04,500 --> 01:17:11,940
your group has there is unfair thing happened and you you feel like you have to say at least I'll have I'm happy to

638
01:17:11,940 --> 01:17:20,549
listen to you and try to figure out what the fair solution in the case without trying to upset other team members.

639
01:17:20,550 --> 01:17:25,380
So I'm happy to hear about that. Know there are handled that case by case.

640
01:17:25,500 --> 01:17:28,140
Okay. So if those things happen, okay.

641
01:17:28,440 --> 01:17:39,330
So that's what all I wanted to say and I, you know, you guys be proud of us, should be proud of what you have have gone so far.

642
01:17:39,340 --> 01:17:47,520
And the presentation was great, but actual content, I believe that quote there is a substantial amount of benefit to the community.

643
01:17:48,000 --> 01:17:55,620
So I strongly encourage you to pursue that kind of development, either for your research or for you.

644
01:17:56,040 --> 01:18:04,259
Expanding your horizon will be your experience. I really encourage you to do that and it's good to see people in person this many.

645
01:18:04,260 --> 01:18:09,839
I hope that I'll try to figure out what what I need to do to increase the turnout,

646
01:18:09,840 --> 01:18:15,510
because I, I know that the your your morning class is hard about it.

647
01:18:15,510 --> 01:18:19,410
I think it's there is a benefit of doing the in-person interaction.

648
01:18:19,420 --> 01:18:26,370
So if you have any suggestion to have how going how I improve my lecture in terms of to increase the turnout.

649
01:18:26,850 --> 01:18:31,610
Happy to hear about that. Yeah. And yeah, that's that's all.

650
01:18:31,610 --> 01:18:37,889
And I it's a really pleasure to spend the semester with you.

651
01:18:37,890 --> 01:18:44,580
And I'm, I'm guessing that you're still stressed with a lot of finals and stuff, but I, I wish you all the good luck.

652
01:18:44,580 --> 01:18:49,170
And, you know, don't be stressed out about this final project too much.

653
01:18:49,530 --> 01:18:57,450
So I'll I'm happy to discuss with you how much additional effort you need to spend for for this project.

654
01:18:58,020 --> 01:19:04,740
Okay. Thank you. Okay. You know, your next time you do it within the class or from it from our.

