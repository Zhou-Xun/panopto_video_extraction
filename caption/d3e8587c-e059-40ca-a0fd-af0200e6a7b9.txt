1
00:00:20,310 --> 00:00:27,530
Yeah. Yeah.

2
00:00:54,840 --> 00:01:13,360
And it was. I.

3
00:01:21,240 --> 00:01:34,210
Don't you think that you. Yeah.

4
00:01:48,130 --> 00:01:52,510
Good morning. Good morning, everybody.

5
00:01:56,560 --> 00:02:03,370
So hopefully you all have a great weekend, a restful weekend.

6
00:02:05,590 --> 00:02:09,040
If you were like me, you were staring at a tree in the sky.

7
00:02:09,820 --> 00:02:13,240
Just staring at the sky in the tree for all of Saturday.

8
00:02:13,720 --> 00:02:18,280
But if not. Either way, I hope you have a restful week.

9
00:02:18,280 --> 00:02:23,850
You. So just to start with some announcements.

10
00:02:24,000 --> 00:02:29,460
So apologies for those who are looking for a recording from Thursday.

11
00:02:32,080 --> 00:02:36,960
We're trying out. And when I say we, I mean the department or the school.

12
00:02:36,970 --> 00:02:43,060
I don't know. Trying a new system of recording lectures automatically.

13
00:02:43,540 --> 00:02:47,830
Apparently I messed with it on Thursday, thinking I was helping.

14
00:02:49,810 --> 00:02:55,900
And even though I'm, like, super paranoid that it's on there or that it's not on.

15
00:02:55,900 --> 00:03:01,190
But I'm going to do what they told me and just. Not mess with anything.

16
00:03:01,760 --> 00:03:05,090
And apparently it's just supposed to record and automatically load.

17
00:03:05,460 --> 00:03:11,500
I'm going to give you apologies again. I can if I messed with it again, where I keep going until we get it right.

18
00:03:11,510 --> 00:03:16,760
But apparently what it's supposed to do is record in the background and automatically upload.

19
00:03:17,810 --> 00:03:22,550
So we will will see. Hopefully I will not be the person who messes it up again.

20
00:03:23,570 --> 00:03:28,440
Um. So that's announcements for today.

21
00:03:28,440 --> 00:03:38,099
And again, just a reminder, if you haven't had a chance to complete the survey, the internal survey, please do so by the end of the day.

22
00:03:38,100 --> 00:03:43,200
And then I'll have then we can work on the groupings for the rest of the week.

23
00:03:46,280 --> 00:03:50,990
So today. Well, I just share some announcements.

24
00:03:51,620 --> 00:03:53,569
Just a very brief review.

25
00:03:53,570 --> 00:04:06,140
And then we'll get into what we're talking about this week, which is our framing for that for evaluation as we move throughout the rest of the class.

26
00:04:08,880 --> 00:04:13,950
The last week, we went through an ecological view of programs.

27
00:04:15,420 --> 00:04:26,730
Programs? They operate in systems. So in this particular diagram, a program is work is operating within level one, the agency,

28
00:04:27,120 --> 00:04:35,220
but the agency is operating within the community and that community is also operating within larger forces.

29
00:04:36,780 --> 00:04:41,670
And the implication of this is that programs are dynamic.

30
00:04:42,600 --> 00:04:46,710
This is part of the reason why I got involved in evaluation in the first place,

31
00:04:47,100 --> 00:04:58,500
because I realized when I started in this role I was doing HIV education work and I just thought it was interesting and actually appropriate that.

32
00:05:00,060 --> 00:05:12,330
Staff in other departments were changing around programs that were designed for particular communities, but it was so high level, for example.

33
00:05:14,910 --> 00:05:26,010
I had a colleague who was implementing an intervention for African-American women to prevent HIV infection, and it got changed around.

34
00:05:27,900 --> 00:05:31,350
And the reason why I got changed around is because there were certain issues

35
00:05:31,350 --> 00:05:36,960
in that particular group of African-American women who were participating.

36
00:05:38,250 --> 00:05:44,879
And so for me, it was interesting to make those choices and understanding, okay, well,

37
00:05:44,880 --> 00:05:49,470
how can we really say something is effective or not effective if it's being changed?

38
00:05:49,950 --> 00:05:54,270
So that was kind of part also part of the beginning of my journey down this down this road.

39
00:05:54,780 --> 00:06:06,600
So programs are always changing. And I think in my years of doing this, have ever seen something that is complete 100% fidelity to the plant program.

40
00:06:07,720 --> 00:06:19,460
But again, the implication is that programs are dynamic, they change, but also how people assess merit worth of significance also changes.

41
00:06:19,900 --> 00:06:27,880
And so evaluation is a way of capturing program operations and outcomes in this particular snapshot of time.

42
00:06:29,910 --> 00:06:37,350
We also talked about different types of evaluation needs assessment, which is used to.

43
00:06:39,730 --> 00:06:44,320
Identify a different social needs and the giving community or given social space.

44
00:06:45,190 --> 00:06:51,430
Process which is designed to understand how a project or program is being implemented.

45
00:06:51,880 --> 00:06:57,450
Outcome is what? I like something thinking I'm going to be super paranoid about this.

46
00:06:57,460 --> 00:07:00,690
Give me a second. Okay. I'm just making that up.

47
00:07:01,680 --> 00:07:15,080
That's going to be on the record. Outcome, which is about understanding short term, mid-term, long term outcomes of a program and then economic,

48
00:07:15,350 --> 00:07:20,570
which is focused on the cost and implications of costs and then evaluation.

49
00:07:21,710 --> 00:07:31,280
I put this comment up here because in the aim of looking for a picture to fill this particular space I came across,

50
00:07:31,280 --> 00:07:36,170
apparently there is a simile for evaluation being like cooking soup.

51
00:07:37,160 --> 00:07:43,969
So formative forms of evaluation are like the chef cooking and deciding whether or

52
00:07:43,970 --> 00:07:51,380
not to and different spices and then the summative types which include outcome.

53
00:07:51,860 --> 00:07:57,110
Economic are really about well what does the person eating the soup think?

54
00:07:58,130 --> 00:08:05,690
I like this comic too, because it also shows a type of evaluation that we're just a side note, so you can write it down if you want.

55
00:08:05,900 --> 00:08:10,879
But another type of evaluation that actually I really enjoy doing,

56
00:08:10,880 --> 00:08:19,280
which is developmental and I think actually because it it's accommodating of changes in systems and context because again,

57
00:08:19,280 --> 00:08:22,940
think about it, a lot of times people when they're implementing programs,

58
00:08:23,270 --> 00:08:33,050
if you've ever done it, fail placements, you know, before you came to grad school yesterday because you are working and in grad school,

59
00:08:33,560 --> 00:08:37,580
those programs are kind of sometimes you feel like you're building as you're going.

60
00:08:37,970 --> 00:08:45,980
So developmental approaches or developmental that type of evaluation is really meant to accommodate that and model that.

61
00:08:46,400 --> 00:08:54,190
So even though we won't talk about it in this class. It's just, again, another type of evaluation you might come across.

62
00:08:54,880 --> 00:08:59,440
And if you just happen to like evaluation so much that you want to read comics about it.

63
00:09:00,160 --> 00:09:12,070
Spectrum. Chris Liza is a comic who does evaluation or comics for us because evaluators are nothing but people who just love to look at comics.

64
00:09:14,670 --> 00:09:18,790
So just, you know, again, another. Test.

65
00:09:19,990 --> 00:09:24,670
Quiz, if you will. Three different types of evaluation.

66
00:09:25,180 --> 00:09:34,990
So first, a client would like to identify more efficient ways to utilize program funds and an existing foster parent training program.

67
00:09:36,680 --> 00:09:42,890
The title is efficiency. I'm going to play with this pen today, see if it works.

68
00:09:43,880 --> 00:09:52,960
If it doesn't, I'll be sad. No. So some things that are signaling to me that this is efficiency or economic.

69
00:09:53,470 --> 00:09:57,140
Yes. Can you see? No. Oh, yes, you can.

70
00:09:59,280 --> 00:10:02,530
I'm so I'm. I'm amused by the smallest things I'm telling.

71
00:10:04,410 --> 00:10:12,690
So here's the thing. No funds. This idea of efficient again, think about efficient efficiency.

72
00:10:14,070 --> 00:10:19,980
Those are signal words to me that this is a cost of a cost type evaluation.

73
00:10:21,680 --> 00:10:28,790
Second one, a client would like to better understand how HIV prevention services are being provided

74
00:10:28,820 --> 00:10:34,400
through south eastern Michigan in an effort to develop a new model for prevention services.

75
00:10:35,180 --> 00:10:43,500
So this is a needs assessment. And some things that can kind of signal back to you.

76
00:10:44,580 --> 00:10:47,220
The client wants to know when they're talking to you.

77
00:10:47,540 --> 00:10:55,950
They're saying they want to better understand how something is being provided, in this case, HIV prevention services.

78
00:10:56,400 --> 00:11:00,480
So they're wanting to look at what's already in place.

79
00:11:01,570 --> 00:11:05,950
And then the why? Because they're trying to develop something new.

80
00:11:07,890 --> 00:11:15,090
So those two things to me signal when the client speaking to me that what will be most appropriate is a needs assessment.

81
00:11:16,590 --> 00:11:21,990
You can also it's helpful to think about the stage that the program is in.

82
00:11:22,620 --> 00:11:29,590
So, again, this program doesn't exist yet. So they're trying to do something new.

83
00:11:31,170 --> 00:11:36,480
And then finally, Prof. Oh, I just told you what it was, but I'll read it out loud anyway.

84
00:11:37,200 --> 00:11:42,870
A client would like to document a pilot care management program for Westland Seniors

85
00:11:43,230 --> 00:11:48,840
with low income in an effort to scale up the program to all of Wayne County.

86
00:11:49,320 --> 00:11:59,220
So this is a process. And some things that if a client said this to me would signal that it's a process.

87
00:11:59,970 --> 00:12:03,120
They want to document the program.

88
00:12:03,540 --> 00:12:07,890
They didn't say a document and they could also be talking about outcomes.

89
00:12:08,190 --> 00:12:17,010
But the fact that they want to document aspects of the program is one signal to me that a process evaluation is warranted here.

90
00:12:17,760 --> 00:12:21,420
The other thing that signals to me is this idea of scaling up.

91
00:12:22,140 --> 00:12:25,260
So this is talking about replication.

92
00:12:32,380 --> 00:12:37,150
And the replication is also meant to.

93
00:12:39,720 --> 00:12:43,830
When I'm thinking about doing a process evaluation, I'm thinking about, okay,

94
00:12:44,160 --> 00:12:50,730
so what aspects of this program right here will be applicable to Wayne County?

95
00:12:51,150 --> 00:13:00,450
And just for those who aren't familiar with the geography, Wayne County or Wayne County is the county that Detroit is located in?

96
00:13:00,930 --> 00:13:05,980
Westland is a suburb of Detroit. So.

97
00:13:07,020 --> 00:13:13,049
As I'm thinking through this from an evaluators.

98
00:13:13,050 --> 00:13:21,060
Landes Evaluators, I am thinking, okay, what do I need to know about this particular program and how can I capture that?

99
00:13:21,480 --> 00:13:25,590
And then I'm also thinking I need to learn a little bit about Wayne County.

100
00:13:25,740 --> 00:13:32,580
Fortunately, I know about Wayne County. Not everything, obviously, but I know about Wayne County because it's where I was born and raised.

101
00:13:40,380 --> 00:13:46,320
If you ever want, you know, to go on the Professor Red Sox history tour, go visit Romulus.

102
00:13:46,320 --> 00:13:51,450
That's where I grew up. And then I came right here to Ann Arbor.

103
00:13:52,420 --> 00:14:02,680
I've been here almost ever since. Um, so today what we're going to talk about is the CDC framework for program evaluation.

104
00:14:03,460 --> 00:14:07,900
Really, this week is devoted to kind of the scaffolding, as I mentioned,

105
00:14:08,320 --> 00:14:14,560
for how we're going to think about evaluation and approach evaluation in this course.

106
00:14:17,940 --> 00:14:25,440
Today we'll do this framework. But tomorrow or Thursday, rather, we'll do culturally responsive and equitable evaluation approaches.

107
00:14:27,490 --> 00:14:37,340
So I want to kind of set the stage and give. Use of the ability to kind of distinguish the difference between a framework and an approach.

108
00:14:38,920 --> 00:14:42,490
A framework is really just a basic conceptual structure.

109
00:14:42,850 --> 00:14:50,990
It's a supporting structure. And it's not it's not adding anything necessarily that is theoretically new.

110
00:14:51,010 --> 00:14:54,880
It's just a way of organizing something that's already known.

111
00:14:55,300 --> 00:14:56,410
And so in this case,

112
00:14:56,710 --> 00:15:08,080
the program for about the framework for program evolved from the CBC is just a way of presenting how we go about our work already in public health.

113
00:15:09,070 --> 00:15:12,340
It's just offering kind of a way of understanding it.

114
00:15:14,100 --> 00:15:23,280
I define a definition for approach. Approach is a distinct way to think about design and conduct evaluation efforts.

115
00:15:24,330 --> 00:15:30,110
So. Thinking about approaches, it's really about adding.

116
00:15:30,590 --> 00:15:34,910
I mean, one thing that distinguishes it is that when.

117
00:15:36,090 --> 00:15:43,110
A new approach to evaluation is presented. It's really thinking about kind of a new theoretical justification for doing evaluation.

118
00:15:43,530 --> 00:15:50,940
So when on Thursday we start talking about culturally responsive and equitable evaluation, we're really talking about why?

119
00:15:51,120 --> 00:16:03,579
Why are we doing this work this way? In just a second.

120
00:16:03,580 --> 00:16:07,030
The term I paid for notes. Okay.

121
00:16:07,690 --> 00:16:11,200
So. As you may have seen in the reading.

122
00:16:12,450 --> 00:16:20,010
The CDC's framework is comprised of six steps or six stages, and those stages are non-linear.

123
00:16:20,400 --> 00:16:30,540
So even though it starts technically at engaging stakeholders and then goes through this kind of circular, circular.

124
00:16:31,740 --> 00:16:40,070
Or circle. You can find yourself as an evaluator hopping from point to point.

125
00:16:42,200 --> 00:16:52,490
So for example, we might be thinking, you know, this actually is something that happens not not super often, but it's helpful.

126
00:16:53,390 --> 00:16:56,720
When I'm working with a client to describe a program,

127
00:16:57,530 --> 00:17:04,370
I'm also talking about them or starting to initiate conversations about how they use what we've learned.

128
00:17:06,270 --> 00:17:17,340
Or I might give in to gather incredible evidence and discover that there's a really good source of info or data that we're missing.

129
00:17:18,150 --> 00:17:24,930
And so I will go back to focus evaluation design or even go back to describing the program.

130
00:17:28,570 --> 00:17:31,930
But. And.

131
00:17:33,530 --> 00:17:39,560
Those standards that we talked about on day one are at the center of this framework.

132
00:17:40,310 --> 00:17:46,340
So, again, just a reminder, the professional standards of utility, is this something that's useful?

133
00:17:47,870 --> 00:17:51,200
Feasibility. Can I get it done? Can I do it?

134
00:17:52,210 --> 00:17:58,630
In that doing can be tangible like to have the resources but also intangible do.

135
00:17:58,660 --> 00:18:03,460
Is there a political will to do this work? Propriety.

136
00:18:03,820 --> 00:18:11,400
Am I going about this work in an ethical way? And then accuracy is the methodology sound.

137
00:18:13,910 --> 00:18:22,250
You do a time check? So.

138
00:18:23,390 --> 00:18:34,340
We are going to play this lovely video from the CBC so that you can see how they have gone about describing their framework.

139
00:18:35,320 --> 00:18:38,570
So it's about 9 minutes long. So. Have a seat.

140
00:18:40,090 --> 00:18:44,600
See if. In the late 1990s,

141
00:18:44,840 --> 00:18:49,909
the CDC convened an evaluation working group charged with developing a framework

142
00:18:49,910 --> 00:18:53,870
that summarizes and organizes the basic elements of program evaluation.

143
00:18:55,100 --> 00:19:02,090
The CDC framework for Program Evaluation was developed as a guide to public health professionals for systematic planning for evaluation.

144
00:19:03,290 --> 00:19:09,440
The framework was developed to summarize and organize the essential elements of program evaluation,

145
00:19:10,460 --> 00:19:13,940
provide a common frame of reference for conducting evaluations.

146
00:19:14,990 --> 00:19:21,290
Clarify the steps in program evaluation, review standards for effective program evaluation.

147
00:19:22,370 --> 00:19:26,570
Address misconceptions about the purposes and methods of program evaluation.

148
00:19:28,810 --> 00:19:34,510
The framework includes six steps in evaluation practice and four standards for effective evaluation.

149
00:19:39,120 --> 00:19:45,210
Central to the framework are the standards adopted from the Joint Committee on Standards for Educational Evaluation.

150
00:19:46,320 --> 00:19:49,380
The fourth standards are utility.

151
00:19:50,220 --> 00:19:54,959
These considerations are directed toward ensuring that an evaluation will serve the information

152
00:19:54,960 --> 00:20:01,110
needs of intended users and include identifying and addressing needs of relevant stakeholders.

153
00:20:02,190 --> 00:20:09,420
Ensuring competent, trustworthy and credible evaluators, collecting a broad range of information.

154
00:20:09,660 --> 00:20:13,560
Addressing the interests of the program, clients and stakeholders.

155
00:20:14,640 --> 00:20:18,120
Describing the perspectives. Underlying interpretation of findings.

156
00:20:19,710 --> 00:20:22,800
Reporting findings in a clear and easy to understand way.

157
00:20:24,060 --> 00:20:31,290
Reporting as evaluation findings become available. Evaluating in a way that encourages follow through by stakeholders.

158
00:20:32,970 --> 00:20:41,640
Feasibility. These considerations are intended to ensure that an evaluation will be realistic, prudent, diplomatic and frugal.

159
00:20:42,810 --> 00:20:51,060
Evaluation procedures should be practical. Evaluation should be carried out in a way that avoids politicizing or misusing the results.

160
00:20:52,050 --> 00:20:56,880
The evaluation should be cost effective. Propriety.

161
00:20:57,870 --> 00:21:03,210
These considerations are intended to ensure that an evaluation will be conducted legally, ethically,

162
00:21:03,450 --> 00:21:09,390
and with due regard to the welfare of those involved in the evaluation, as well as those affected by its results.

163
00:21:10,500 --> 00:21:11,850
Service Orientation.

164
00:21:12,390 --> 00:21:20,010
Evaluation should be designed to assist organizations to address and effectively serve the needs of the full range of targeted participants.

165
00:21:20,880 --> 00:21:26,340
These standards are similar in nature to those used in institutional review of human subjects research.

166
00:21:28,300 --> 00:21:35,560
Accuracy. These considerations are intended to ensure that an evaluation will reveal and convey technically

167
00:21:35,560 --> 00:21:40,870
adequate information about the features that determine worth or merit of the program being evaluated.

168
00:21:41,740 --> 00:21:47,560
These include reliable documentation of findings using appropriate analyzes with justifiable conclusions.

169
00:21:50,630 --> 00:21:54,860
Step one involves engaging the stakeholders. Who are the stakeholders?

170
00:21:55,580 --> 00:21:58,430
Anyone who might be affected by the program or policy.

171
00:21:59,540 --> 00:22:11,300
Examples of stakeholders might be program, health department staff, program participants, clients, staff, and other programs potential competitors.

172
00:22:13,100 --> 00:22:17,720
Why is it important to include stakeholders to get agreement on program goals?

173
00:22:18,590 --> 00:22:22,310
To get agreement on purpose of evaluation one size doesn't fit all.

174
00:22:23,480 --> 00:22:32,150
To bring together lay and professional resources. To build capacity to address health needs and give more control over factors affecting health.

175
00:22:33,590 --> 00:22:40,700
Because it increases credibility of evaluation, increases likelihood that recommendations from evaluations will be implemented.

176
00:22:43,500 --> 00:22:48,990
The second step of the framework is to describe the program. It is helpful to think in terms of the logic model.

177
00:22:49,440 --> 00:22:54,150
Description of the program should include information about the long term goals and objectives of the program,

178
00:22:54,660 --> 00:23:00,810
as well as the strategies for reaching those goals. In particular, these aspects should be addressed.

179
00:23:01,680 --> 00:23:05,670
The need to be addressed by the program. The expected effects of the program.

180
00:23:06,240 --> 00:23:13,440
What constitutes success? The stage at which the program is and whether it be planning, implementation or effects.

181
00:23:15,210 --> 00:23:20,460
The context in which the program operates, including history, geography or politics.

182
00:23:21,150 --> 00:23:24,570
Social, economic conditions. And other things that could affect results.

183
00:23:25,980 --> 00:23:29,670
A flowchart or logic model that describes how the program is supposed to work.

184
00:23:32,930 --> 00:23:36,230
The third step of the framework is to focus the evaluation design.

185
00:23:37,130 --> 00:23:42,380
At issue is striking a balance between meeting the concerns of stakeholders and using resources efficiently.

186
00:23:43,490 --> 00:23:48,590
Things to consider at this step are defining the purpose of the evaluation as established

187
00:23:48,590 --> 00:23:54,170
by stakeholders ensuring user participation and determining the focus of evaluation.

188
00:23:55,160 --> 00:23:59,840
Identifying and prioritizing the different uses for which evaluation results might be used.

189
00:24:01,430 --> 00:24:05,960
Determining questions to be asked. Take some time here to collect questions.

190
00:24:06,680 --> 00:24:11,810
Identify need process impact and or outcome evaluation.

191
00:24:14,060 --> 00:24:19,940
Specifying evaluation methods. This includes the design of evaluation and selection of measure.

192
00:24:22,190 --> 00:24:25,340
Developing explicit agreements that summarize procedures.

193
00:24:25,910 --> 00:24:31,700
Clarify roles and responsibilities. Use of resources and protection of human subjects.

194
00:24:35,090 --> 00:24:44,120
The fourth step in the framework is gathering credible evidence considerations and gathering credible evidence include indicators to be used.

195
00:24:45,050 --> 00:24:50,030
What sources of evidence are available or useful? What is the quality of the data?

196
00:24:51,020 --> 00:24:56,240
How much evidence is necessary? What are the logistics for gathering this information?

197
00:24:57,290 --> 00:25:00,310
Use of multiple procedures for gathering data is important.

198
00:25:03,300 --> 00:25:06,510
The fifth step in the framework is justification of conclusions.

199
00:25:07,470 --> 00:25:11,670
Evaluation conclusions are justified when they are linked to the evidence gathered

200
00:25:11,940 --> 00:25:16,320
and judged against agreed upon values or standards set by the stakeholders.

201
00:25:17,130 --> 00:25:22,500
Stakeholders must agree that conclusions are justified before they will use the evaluation results with confidence.

202
00:25:24,210 --> 00:25:31,080
Elements involved in ensuring justification for conclusions are use of appropriate analysis for examining data,

203
00:25:31,500 --> 00:25:34,500
summarizing findings and looking for patterns in the results.

204
00:25:36,120 --> 00:25:43,530
Interpreting what the findings mean. Making judgments regarding the merit worth and significance of the program.

205
00:25:44,610 --> 00:25:49,320
This involves comparison of findings and interpretations against agreed upon standards.

206
00:25:50,940 --> 00:25:55,050
Standards reflect the values of stakeholders and are the basis for forming judgments.

207
00:25:56,310 --> 00:26:00,720
Finally, recommendations are actions for consideration based on results.

208
00:26:01,500 --> 00:26:03,780
Recommendations regarding the use of evaluation.

209
00:26:03,780 --> 00:26:09,210
Results should be made with regard to the organizational context in which programing decisions will be made.

210
00:26:14,680 --> 00:26:20,770
The final step in the framework involves ensuring that lessons learned from the evaluation will be shared in a planned,

211
00:26:20,770 --> 00:26:23,920
thoughtful way so that they may be used effectively.

212
00:26:24,940 --> 00:26:30,000
Elements that ensure effective use of evaluation information are design.

213
00:26:30,700 --> 00:26:38,110
Remember the third step of the framework? The evaluation design should be focused at that step to ensure effective use of evaluation results.

214
00:26:39,730 --> 00:26:46,120
Preparation steps taken to plan for or rehearse eventual use of the evaluation findings.

215
00:26:47,350 --> 00:26:50,620
How might your findings affect the decision making of the stakeholders?

216
00:26:52,720 --> 00:27:00,940
Feedback. Providing feedback of findings to all parties of the evaluation builds trust and keeps the evaluation activities on track.

217
00:27:02,590 --> 00:27:07,120
Follow up. This includes providing support to the users of the evaluation.

218
00:27:07,870 --> 00:27:11,380
Follow up can also help avoid possible misuse of findings.

219
00:27:14,000 --> 00:27:22,520
Dissemination, findings and lessons learned about the program should be communicated to relevant audiences in a timely and appropriate manner.

220
00:27:23,210 --> 00:27:30,820
Reports can take many forms. Policy Brief Press Release, full report, brief report or executive summary.

221
00:27:32,000 --> 00:27:43,540
And that's it. We've completed the framework. The CDC convened an evaluation.

222
00:27:43,540 --> 00:27:49,570
Working. For me.

223
00:27:49,580 --> 00:27:55,770
Question. Any questions so far? All right.

224
00:27:56,490 --> 00:28:04,530
So for the rest of class, I will just be providing some additional food for thought and know that again,

225
00:28:04,530 --> 00:28:09,720
the rest of the class is structured to expand on each of these steps.

226
00:28:11,010 --> 00:28:19,860
It might be helpful to understand that, though, the place that your final assignment fits in is that you're working through steps one through three.

227
00:28:21,270 --> 00:28:29,790
That first step focus evaluation design is usually where the plan emerges, and that's what you'll be working through for the final assignment.

228
00:28:31,930 --> 00:28:37,900
So let's start with are engaging stakeholders or investor parties.

229
00:28:39,730 --> 00:28:46,780
So in this class, I'd like to use three basic buckets of invested parties.

230
00:28:47,770 --> 00:28:54,880
You might see two like the reading actually have while the MMW are hit to I believe.

231
00:28:57,650 --> 00:29:02,270
But I like to use this kind of third three bucket framework.

232
00:29:03,840 --> 00:29:12,120
One. The apparatus of your program, the primary users of the program, and then those affected by the program.

233
00:29:12,990 --> 00:29:16,220
So your operators are pretty self-explanatory.

234
00:29:16,230 --> 00:29:23,670
The people who are running the program could include the direct, direct service staff.

235
00:29:24,570 --> 00:29:29,280
Any agency staff or organization staff that are working in the background.

236
00:29:30,750 --> 00:29:37,020
Primary users. Those are your participants, which can be very narrow.

237
00:29:37,730 --> 00:29:46,140
If you know, for example, your program is with a group of kids in a particular social studies class in a particular school,

238
00:29:46,740 --> 00:30:02,460
but it could be a little bit more nebulous and fuzzy. For example, if you're doing a population level intervention to promote vaccine uptake.

239
00:30:04,700 --> 00:30:13,370
And so defining who the users of the program are can sometimes be straightforward, but sometimes take a little bit more energy to understand.

240
00:30:14,450 --> 00:30:21,770
And then finally, those who are affected by your program, also pretty fuzzy and nebulous sometimes.

241
00:30:23,850 --> 00:30:32,520
Oftentimes it is, you know, community members who may have or would be impacted by the results of a program.

242
00:30:33,330 --> 00:30:39,659
But it can also include funders who's funding the program and who would have some investment

243
00:30:39,660 --> 00:30:45,220
and understanding the evaluation results or wanting to understand the evaluation results.

244
00:30:45,690 --> 00:30:52,770
Because, again, thinking about what we talked about accountability last week, it's really about who.

245
00:30:53,650 --> 00:30:57,250
What is this program? And who is this evaluation accountable.

246
00:30:57,400 --> 00:31:14,000
Accountable to? I also want to just take a second to distinguish clients from investor parties.

247
00:31:14,480 --> 00:31:20,570
So all clients are invested parties, but not all investor parties are clients.

248
00:31:21,140 --> 00:31:30,290
So as a value, as an evaluator, I'm accountable to my client, particularly in regard to logistical project related matters.

249
00:31:31,220 --> 00:31:39,980
Contracting. Now navigating or negotiating relationships with other investor parties.

250
00:31:41,030 --> 00:31:47,030
Deliverables. So those are your reports, your dissemination, materials, surveys, etc.,

251
00:31:47,840 --> 00:31:54,830
evaluation plan as well as the person that I'm generally opening and closing the project with.

252
00:31:57,430 --> 00:32:01,420
They because they are a central source for logistical matters.

253
00:32:01,720 --> 00:32:11,140
It can sometimes be a little dicey balancing that relationship with that of other invested parties, and it can create tension.

254
00:32:11,620 --> 00:32:16,089
So, you know, part again, I'll stress that as an evaluator,

255
00:32:16,090 --> 00:32:21,700
it helps to have some basic understanding of facilitation skills because you're always

256
00:32:21,700 --> 00:32:28,540
going to be facilitating difficult conversations that may emerge surprisingly.

257
00:32:29,810 --> 00:32:39,750
So for example, I have a recent meeting. Might have been last week or the week before where I was working with a relatively new client.

258
00:32:39,760 --> 00:32:48,650
I'm working with a relatively new client to. Do a logic modeling project for their organization.

259
00:32:50,670 --> 00:32:56,160
They're very social justice minded. And so to me, it was a given.

260
00:32:56,430 --> 00:33:02,430
As an evaluator, of course, you're going to have your primary users involved in this logic modeling process.

261
00:33:03,000 --> 00:33:11,520
But they we spent the rest of the meeting actually discussing and negotiating their involvement.

262
00:33:11,820 --> 00:33:19,500
And the reason why ultimately we decided not to engage primary users at this time was that the client,

263
00:33:19,500 --> 00:33:23,430
they had concerns about involving them before the operator.

264
00:33:23,430 --> 00:33:25,860
So the state, the staff were all on the same page.

265
00:33:26,400 --> 00:33:38,430
So it was important to be open to listening and to be prepared to justify your position, but also to adjust where needed.

266
00:33:38,940 --> 00:33:43,040
So there issue wasn't, oh, we don't want primary users involved.

267
00:33:43,050 --> 00:33:48,510
It was we don't even know what page we're on with each other.

268
00:33:48,990 --> 00:34:00,090
So we would like to take time first to get together as a staff and then in a later stage bring in our primary users.

269
00:34:05,680 --> 00:34:14,140
Describing the program. Good program descriptions start with engagement.

270
00:34:15,160 --> 00:34:27,640
This is a particular framework of public engagement, and it goes from here where you're at a passive engagement.

271
00:34:28,790 --> 00:34:33,020
I shouldn't put a x. I'll just put a circle. Because it's not necessary.

272
00:34:33,050 --> 00:34:40,290
This is not a negative thing, but. And this model of engagement.

273
00:34:40,310 --> 00:34:48,410
And this is what we often see, where we as public health professionals say, hey, let's get some feedback from the community.

274
00:34:48,770 --> 00:34:52,549
Let's do this survey here and find out what they want.

275
00:34:52,550 --> 00:34:55,000
And then maybe we will, you know,

276
00:34:55,070 --> 00:35:03,440
decide whether or not and I'm being I'm being snotty with my next comment will decide whether or not their feedback is good enough to incorporate.

277
00:35:05,540 --> 00:35:14,479
There is a place for that sometimes. But what we are aiming for, not the snotty attitude, but, but the, you know,

278
00:35:14,480 --> 00:35:20,030
sometimes one sided feedback is, is, is warranted depending on the context.

279
00:35:20,720 --> 00:35:31,760
But what we're aiming for as much as possible is to get to this engagement where we're either being a source of empowerment to a community.

280
00:35:34,210 --> 00:35:37,930
Or leadership where they are leading the work.

281
00:35:38,650 --> 00:35:48,730
A good example of that model of work is community based participatory research, which I know a lot of students are familiar with in evaluation.

282
00:35:49,330 --> 00:36:00,850
We I wouldn't say we formally do CB, CB PR, but only because when I talked about research versus evaluation on the first day,

283
00:36:01,270 --> 00:36:09,820
I mean, typically what we're doing is not considered scholarly research and CBP as a scholarly research framing.

284
00:36:11,110 --> 00:36:17,950
But the idea is the same that as much as possible, what we're aiming for is that.

285
00:36:19,740 --> 00:36:29,760
A client and associated invested parties are able to take the work in and have it have a life after we're gone.

286
00:36:32,070 --> 00:36:38,920
I really sometimes I will say to my clients, I don't I want to put myself out of work with you.

287
00:36:38,940 --> 00:36:42,870
I want to work with you again. So, like, how can we make this happen?

288
00:36:42,950 --> 00:36:49,679
How you know, what will help build your capacity to evaluate your programs internally and to do

289
00:36:49,680 --> 00:36:54,900
it in a way that's rigorous with high quality and accountable to your communities?

290
00:36:57,760 --> 00:37:01,740
This is just another example that I include, included in the notes.

291
00:37:02,260 --> 00:37:10,270
This one is a little bit detailed, more detailed, and this model's a little bit more common to to see in the literature.

292
00:37:12,130 --> 00:37:22,880
But essentially says the same thing. We can either start from outreach where we're asking people what they want and then maybe incorporating it.

293
00:37:22,930 --> 00:37:26,110
But the power lies with us to incorporate it.

294
00:37:27,400 --> 00:37:31,480
Whereas at the other end of the scale is shared leadership.

295
00:37:31,750 --> 00:37:37,540
How can we work together to make decisions about what's going on in this evaluation process?

296
00:37:38,500 --> 00:37:45,160
And so, again, it's about power sharing versus holding the power as the evaluator.

297
00:37:45,610 --> 00:37:54,940
Because the truth is, we really do often enter communities and the assumption is that we are the experts from those who we're working with.

298
00:37:55,480 --> 00:38:02,710
And so you have to balance that with the fact that we would consider them the experts, and we're providing a service.

299
00:38:06,350 --> 00:38:11,350
The reason why. One reason why we care about engagement aside from the US.

300
00:38:11,810 --> 00:38:16,660
Oh actually before I go into that. Yes. Uh huh.

301
00:38:17,750 --> 00:38:33,910
Well. You know, you're talking about the yellow one.

302
00:38:35,170 --> 00:38:44,810
What? The yellow. Both of them are public participation, public engagement models.

303
00:38:45,390 --> 00:38:45,680
Yeah.

304
00:38:45,890 --> 00:38:55,520
But with I've just adapted them because I find them a little bit more useful them in detail than I have seen other models just in our literature.

305
00:38:56,510 --> 00:38:59,760
But that's where they that that's their that's their scholarly home.

306
00:38:59,840 --> 00:39:05,470
Public participation and public engagement. And you should be.

307
00:39:05,980 --> 00:39:11,320
You should be able to see the citations at the bottom if you want to read more.

308
00:39:11,500 --> 00:39:21,740
But they're both public engagement ones. So like I was saying, we care about engagement aside for just promoting equity.

309
00:39:22,130 --> 00:39:34,290
We care about it because. And part of what we're doing as evaluators, especially at this stage of describing a program, is also describing a problem.

310
00:39:34,930 --> 00:39:44,960
And problems have to be visible in order for them to be to warn or to demand attention again.

311
00:39:44,980 --> 00:39:53,020
You know, think about that. Meet the social needs we talked about needs need to be felt or they need to be demanded.

312
00:39:54,370 --> 00:40:02,120
In order for them to get attention. You can think about visibility with four indicators.

313
00:40:03,230 --> 00:40:08,120
One proximity. If I see that there's a problem around me.

314
00:40:10,370 --> 00:40:15,650
It's more likely to be visible. You know, just think about some of the.

315
00:40:16,940 --> 00:40:26,630
Think about I mean, you know, you know, wars in other countries or, you know, gang violence in another state.

316
00:40:27,470 --> 00:40:35,630
We know their problems. But because we're not actually in them, it can be kind of hard to process them on a daily basis.

317
00:40:37,070 --> 00:40:41,960
And that relates to intimacy, the level of personal familiar already with the problem.

318
00:40:43,820 --> 00:40:47,900
So not only can I see the problem. Is it a problem for me?

319
00:40:49,900 --> 00:40:54,400
Awareness is another indicator of visibility.

320
00:40:54,730 --> 00:40:58,560
So that's the degree to which a problem has a presence in your daily thoughts.

321
00:40:59,840 --> 00:41:06,670
And then finally magnitude, which is the scale or enormity of the problem in public health.

322
00:41:06,680 --> 00:41:09,980
We are taught to think about magnitude.

323
00:41:10,160 --> 00:41:18,950
I mean, it's pretty much driven into us. It's why I will sometimes say that as a public health professional, I actually care about people.

324
00:41:18,950 --> 00:41:29,390
I just care about groups because I'm thinking about scale versus a problem that might be very located, very centralized to like one or two people.

325
00:41:30,930 --> 00:41:40,240
Talking about disability and Halloween. That's a lot. Not that we're going to be.

326
00:41:41,710 --> 00:41:45,880
So the question is, when we're thinking about visibility, visibility from whose perspective?

327
00:41:46,570 --> 00:41:51,130
Stakeholders, which is a evaluators client.

328
00:41:53,220 --> 00:42:02,310
It's actually a balance of everybody sort of. But ultimately, like as public health professionals, we're going to define it based on magnitude.

329
00:42:03,000 --> 00:42:07,470
So when I'm writing an evaluation plan and when you're writing your evaluation plan,

330
00:42:08,040 --> 00:42:15,029
you're writing about the magnitude of a problem using your tried and true methods,

331
00:42:15,030 --> 00:42:22,710
epidemiological data, things of that nature, theory, empirical evidence.

332
00:42:23,490 --> 00:42:27,570
But what's helpful and important to keep in mind is that.

333
00:42:30,190 --> 00:42:35,230
That's not necessarily the lens that our clients are coming from or our invested parties.

334
00:42:36,720 --> 00:42:40,200
They're thinking about proximity, intimacy and awareness.

335
00:42:42,900 --> 00:42:48,720
And in many ways, the idea of defining a problem based on magnitude is relatively new.

336
00:42:49,660 --> 00:42:53,410
When I started in this World Grants.

337
00:42:54,520 --> 00:42:58,240
Were handed out in many ways based on.

338
00:42:59,430 --> 00:43:06,990
Intimacy, awareness and proximity. In other words, taking somebody's word for it that it was a problem.

339
00:43:08,400 --> 00:43:18,720
And so the idea of defining whether or not a problem is a problem based on data is something that is not always been the case.

340
00:43:21,940 --> 00:43:29,140
The other thing to keep in mind is that. Especially with those other the other three.

341
00:43:29,470 --> 00:43:35,180
I mean also what magnitude but. Proximity, intimacy and awareness.

342
00:43:36,260 --> 00:43:46,310
Those can be pretty subjective. So I think of this line I like this diagram is kind of confusing, but I like to think of it as.

343
00:43:47,520 --> 00:43:53,280
So this line here that's changing is a subjective measure of visibility.

344
00:43:53,760 --> 00:43:57,060
And so sometimes problems and acceptability, rather.

345
00:43:59,930 --> 00:44:06,409
So sometimes a problem can be super you know, visible.

346
00:44:06,410 --> 00:44:12,170
It's or our idea of what's acceptable and what's considered a problem will change over time.

347
00:44:15,430 --> 00:44:21,190
But then there's also an objective area where just. Something is unacceptable.

348
00:44:21,190 --> 00:44:23,080
Something is considered a problem.

349
00:44:23,830 --> 00:44:31,780
And it's only when those subjective and those objective markers line up where you will see the demand for corrective action.

350
00:44:32,620 --> 00:44:36,579
So it's helpful as an evaluator to not only understand the magnitude,

351
00:44:36,580 --> 00:44:44,740
like what's objectively a problem, but to understand is this a problem to my invested parties?

352
00:44:45,950 --> 00:44:56,770
And this stage of describing a program is a helpful place to really take that time to understand where your parties are coming from.

353
00:45:00,710 --> 00:45:14,670
Yes. And the impact.

354
00:45:17,540 --> 00:45:23,260
Small impact on day to day living. I would consider that.

355
00:45:27,360 --> 00:45:30,900
So the question is. How. How would you?

356
00:45:32,580 --> 00:45:38,850
I think I'm going to struggle to reword it. So how how would you account for impact in thinking about this with the.

357
00:45:40,690 --> 00:45:46,230
Make sure I'm saying that right. So is the impact. Okay.

358
00:45:46,590 --> 00:45:51,659
Try again. Cause I'm like, I understood it, but I'm like, I can't reword it.

359
00:45:51,660 --> 00:46:00,600
So, yeah, that's okay. So, for example, like. Variety is just a fruit, but.

360
00:46:04,030 --> 00:46:08,530
More people, for example. So when you're.

361
00:46:16,710 --> 00:46:20,800
Or does it also account for? Impact on those.

362
00:46:21,880 --> 00:46:25,190
Okay. So the question is, what do you think about impact?

363
00:46:25,210 --> 00:46:38,350
Does it account for the number of individuals or kind of like the person size of the problem, but also the level of impact that the problem has?

364
00:46:39,350 --> 00:46:42,660
And I would I would say both. I would say both.

365
00:46:42,670 --> 00:46:46,400
So when you're thinking about a problem.

366
00:46:49,290 --> 00:46:59,760
I think of a problem that impacts a lot of people, but let's say mask wearing, so which is also subjective in terms of the magnitude of their problem.

367
00:47:00,300 --> 00:47:09,970
But. You know, for the most part, objectively, mask wearing does not have a high level of impact, but.

368
00:47:10,850 --> 00:47:15,419
It impacts a lot of people. It's not necessarily a problem.

369
00:47:15,420 --> 00:47:17,670
So that's not the best example. But.

370
00:47:19,200 --> 00:47:26,760
When you're thinking about measuring it and thinking about describing it, you want to think about how many people does it impact.

371
00:47:27,630 --> 00:47:33,780
But also, you know, the magnitude of it or how much it impacts somebody.

372
00:47:34,410 --> 00:47:43,440
It's probably the I should have just said yes. But you do want to consider different dimensions of impact when you're thinking about magnitude.

373
00:47:45,970 --> 00:47:52,380
Does that help? Answer your question. What about others that they answer?

374
00:47:53,490 --> 00:47:55,710
Or are you like, yeah, whatever. Okay.

375
00:48:02,250 --> 00:48:11,640
And then I will just say that all of these considerations get funded or folded into the logic modeling process, which is the ultimate aim.

376
00:48:12,600 --> 00:48:15,740
Of this. Of this. Of this stage.

377
00:48:18,610 --> 00:48:22,500
And again, just another another this, you know,

378
00:48:23,320 --> 00:48:33,430
visual version of a logic model where one in two are things that an agency is doing to get three, four and five.

379
00:48:34,300 --> 00:48:38,140
I mean, they keep driving that home because it really will help.

380
00:48:38,150 --> 00:48:43,400
I think when it gets to when we start talking about things like outputs.

381
00:48:44,440 --> 00:48:47,300
This line, right? Oh, I keep forgetting. I got this pin.

382
00:48:47,600 --> 00:48:55,910
This line right here is really hard for a lot of us to distinguish between activities and outcomes or outputs.

383
00:48:57,190 --> 00:49:01,630
So again, thinking about it, as do. And get.

384
00:49:02,460 --> 00:49:07,950
And help may help simplify that in your head because that's where the line is.

385
00:49:17,860 --> 00:49:24,490
Okay. We're still good on time. So the third stage focus evaluation design.

386
00:49:27,270 --> 00:49:30,870
A big part of this stage is really understanding of suffering.

387
00:49:31,500 --> 00:49:34,590
Please forgive the typo or forget the change.

388
00:49:34,590 --> 00:49:45,960
That is the purpose of an evaluation when it should benefit participants and it should serve to build a professional knowledge base.

389
00:49:47,250 --> 00:49:52,650
Just because it's not generalizable doesn't mean that it doesn't have useful information to share.

390
00:49:54,890 --> 00:49:59,110
And they are for general purposes or purpose.

391
00:49:59,140 --> 00:50:04,110
I. I don't know to. Doing evaluation.

392
00:50:04,440 --> 00:50:13,680
One is to gain insight, gain insight into program operations, gain insight into the outcomes that emerge from a project.

393
00:50:15,970 --> 00:50:19,820
The fourth the second one. Game or change practice.

394
00:50:20,480 --> 00:50:29,250
I'm getting Mundelein. Let me get some water. Change practice.

395
00:50:31,290 --> 00:50:37,980
So that second one is just referring to how can this evaluation help us to improve our practice?

396
00:50:39,990 --> 00:50:43,950
Their purpose that you will see assess affects.

397
00:50:45,720 --> 00:50:50,430
What kind of changes are we seeing because of this program?

398
00:50:52,250 --> 00:50:57,830
And then finally, the fourth purpose affect those who participate in the evaluation.

399
00:50:59,060 --> 00:51:03,140
And while this most typically relates to the results.

400
00:51:04,090 --> 00:51:10,030
So we take the results and we apply them to change practice, change the program.

401
00:51:10,450 --> 00:51:13,090
It can also refer to the implementation.

402
00:51:13,720 --> 00:51:24,160
For example, we might do an evaluation where embedded in that is a process of helping an organization build their capacity to do evaluation.

403
00:51:26,850 --> 00:51:30,020
So both purposes can be formally.

404
00:51:30,510 --> 00:51:36,990
That type of purpose can be formally folded into the plan of the evaluation plan.

405
00:51:39,940 --> 00:51:45,370
Another aspect of focusing the evaluation is selecting research design.

406
00:51:45,940 --> 00:51:49,330
We'll talk more about the details of different research design.

407
00:51:49,810 --> 00:52:00,370
But what I just want to leave you with for now is that selecting research design is not as straightforward as you think it might be,

408
00:52:01,270 --> 00:52:04,900
especially when we're talking about doing evaluation,

409
00:52:05,140 --> 00:52:14,500
because we do account for that context in a different way than you would with a scholarly research project.

410
00:52:15,460 --> 00:52:18,900
So for example, yes, I need to consider rigor.

411
00:52:18,910 --> 00:52:24,070
What is the most appropriate research design to answer the questions I have?

412
00:52:25,180 --> 00:52:27,400
But again, it's also a feasibility.

413
00:52:29,990 --> 00:52:38,300
Evaluation budgets, for example, often run low compared to what you might see coming out of, like, NIH funded study.

414
00:52:38,720 --> 00:52:43,100
But we're not getting $1,000,000 to run experimental design.

415
00:52:43,990 --> 00:52:47,410
Unless we're just really, really, really lucky or blessed or whatever.

416
00:52:48,340 --> 00:52:52,510
Most often we're getting 10%, 10% of the program budget.

417
00:52:52,510 --> 00:52:57,480
That's the kind of standard marker which can be $10,000.

418
00:52:57,490 --> 00:53:01,330
So how can I do this work with $10,000? I have to keep that in mind.

419
00:53:02,290 --> 00:53:08,720
Ethical considerations. The ethics of control groups, for example.

420
00:53:09,750 --> 00:53:15,810
Is it really ethical to deny one group a treatment that we think might be effective?

421
00:53:16,560 --> 00:53:20,850
And so we have to account for that in different ways.

422
00:53:21,780 --> 00:53:26,970
There are some clients that I've had who are really super against control groups in any form or fashion,

423
00:53:28,560 --> 00:53:34,020
are really not interested in kind of setting aside a group to not receive a treatment.

424
00:53:34,140 --> 00:53:37,530
And that's completely understandable. So I have to account for that.

425
00:53:38,700 --> 00:53:47,460
And then as well as things like time, do I have time? I once got an evaluation a couple of years ago where I had like a month.

426
00:53:49,040 --> 00:53:57,149
That was a really crazy month. But it was important and I could go on to why we only had a month.

427
00:53:57,150 --> 00:54:06,290
But it wasn't our fault. It was the Thunder's fault. Because they didn't want to give the money out until a month before the project was due.

428
00:54:06,920 --> 00:54:12,510
Like, thanks. That was, like I said, it was fun.

429
00:54:12,660 --> 00:54:16,450
So all those things, you know, need to be accounted for when we're talking about design.

430
00:54:17,400 --> 00:54:21,390
Um, another thing at this stage is thinking about measurement.

431
00:54:22,860 --> 00:54:28,260
Yes. The variables that you select must be clearly focused on the valuation questions.

432
00:54:28,830 --> 00:54:30,989
So when you when it comes down to it,

433
00:54:30,990 --> 00:54:37,620
it's important to take time to think about what questions you're asking and plan to answer with the evaluation process,

434
00:54:38,850 --> 00:54:48,080
because that is the foundation for deciding, okay, what variables are we going to measure, what are our indicators, etcetera, etcetera.

435
00:54:50,100 --> 00:54:52,860
Measurement should also be culturally sensitive.

436
00:54:53,700 --> 00:55:02,850
For example, are we offering items in a survey that are meaningful and understandable to the people who are going to take the survey?

437
00:55:04,110 --> 00:55:07,920
Or are they offensive, unknowingly or knowingly?

438
00:55:10,820 --> 00:55:19,130
We can't always know the answers to that ourselves. Thus we promote use our working with vested parties.

439
00:55:25,400 --> 00:55:35,140
Gathering credible evidence. What I'll leave you with today is to think about issues of propriety.

440
00:55:36,730 --> 00:55:42,940
So when we're doing data collection, it's important that we have good representation.

441
00:55:43,630 --> 00:55:47,950
So if you have taken stats before, you know, for example,

442
00:55:48,280 --> 00:55:58,600
it's helpful to know what sample size do I need to have enough power to identify any changes that I need to identify?

443
00:56:00,070 --> 00:56:03,130
But we also have to be concerned with things such as privacy.

444
00:56:04,750 --> 00:56:09,610
Data should not be collected by people who have authority over research participants.

445
00:56:12,410 --> 00:56:16,910
So if, for example, I'm working with a client and the client says.

446
00:56:17,970 --> 00:56:28,180
That's right. Yeah. For example, I had a client that. We had youth take a survey, but some of the youth didn't end up.

447
00:56:29,480 --> 00:56:36,740
Filling up a cold. We were using a code to be able to identify who they were without them writing their name on it.

448
00:56:37,370 --> 00:56:44,749
Not all the youth actually filled the cold out, so the client, in an effort to be helpful,

449
00:56:44,750 --> 00:56:49,940
said, okay, well, you know, let me look at the handwriting on the on the survey.

450
00:56:50,270 --> 00:56:55,250
I can tell you who did it, or I can just, like, go find out who's the one who completed the survey.

451
00:56:55,790 --> 00:57:00,499
And I had to, you know, graciously say no, that, you know, there's some ethical concerns,

452
00:57:00,500 --> 00:57:04,940
but you actually know what they responded, especially since we're asking about your program.

453
00:57:06,680 --> 00:57:09,020
So it's not a matter of, you know, some you know,

454
00:57:09,020 --> 00:57:14,300
some evil twisted person going on with their mustache going, oh, I'm going to know what the people said.

455
00:57:15,880 --> 00:57:20,960
You know, it's more subtle than that, but it's something you should keep out a lookout for.

456
00:57:22,160 --> 00:57:31,310
And then obviously, whatever steps you can take to protect participants, identity is helpful and then voluntary participation.

457
00:57:31,700 --> 00:57:36,440
All participants should be aware of and agree to their participation and data collection.

458
00:57:38,320 --> 00:57:46,830
Um. Even though the video meant the video mentioned IRP issues and having IRP oversight.

459
00:57:47,100 --> 00:57:50,430
So the Institutional Review Board, if you're not familiar,

460
00:57:50,820 --> 00:57:57,959
is essentially a board of people who are assessing whether or not research

461
00:57:57,960 --> 00:58:05,310
subjects are being treated fairly and ethically through their participation,

462
00:58:05,970 --> 00:58:15,840
but with a valuation. It's a little bit more nuanced because many evaluation projects are not considered research by RB standards,

463
00:58:16,410 --> 00:58:22,680
which helps me when I'm, you know, is very helpful because I don't have to go through IRP.

464
00:58:23,430 --> 00:58:26,639
But what that means is that as a evaluator,

465
00:58:26,640 --> 00:58:36,180
I have to be responsible and make sure that my team is responsible and operating ethically without having that external oversight.

466
00:58:37,020 --> 00:58:42,250
So it's kind of a, you know. I don't know.

467
00:58:42,320 --> 00:58:51,160
It's a good thing and it's a bad thing that we're not always considered research from the purposes of IAB definition of research,

468
00:58:52,450 --> 00:58:58,180
because we don't have our research. If you're interested, it is because our research subject is the program, not the people.

469
00:58:58,960 --> 00:59:02,410
Fine distinction, but enough to get us out of doing IAB.

470
00:59:02,680 --> 00:59:06,810
But we still have the responsibility to make sure that we're doing this work in an ethical way.

471
00:59:10,830 --> 00:59:24,350
And then with data analysis. One thing just to point out is that we can always manipulate data, but in a way that would support our biases.

472
00:59:27,390 --> 00:59:32,520
If you've ever analyzed data before, you're probably aware that you can just run.

473
00:59:33,150 --> 00:59:38,040
You can choose to run a mean, for example, excluding a certain population.

474
00:59:38,040 --> 00:59:45,000
You can or, you know, certain. People in the data set.

475
00:59:45,010 --> 00:59:52,530
You can do all kinds of things that would make it that you can make it so that you can support your own biases.

476
00:59:53,130 --> 00:59:56,880
But, you know, suffice to say, it's not ethical to do so.

477
00:59:57,210 --> 01:00:03,990
And so, again, important. Check your biases, manage your biases as you're going through the project.

478
01:00:06,400 --> 01:00:17,740
In an evaluation plan. It's also even helpful to outline how you and your clients or other investor parties will go about doing the work ethically.

479
01:00:23,430 --> 01:00:30,870
Just a fine conclusion. So just a fine conclusion.

480
01:00:31,740 --> 01:00:39,930
It really does go beyond identifying whether or not you have some results and whether or not those results are statistically significant.

481
01:00:41,430 --> 01:00:45,330
In the video and in the reading, it points out that.

482
01:00:47,170 --> 01:00:56,410
When you're justifying conclusions, you're also thinking about the standards by which a program will have merit worth and or significance.

483
01:01:00,950 --> 01:01:06,980
I'm just going to skip over this, I. Because I just want to point out, again, thinking about visibility.

484
01:01:08,110 --> 01:01:14,769
What what can what standards exist in part out governed by what's visible.

485
01:01:14,770 --> 01:01:17,920
Right. So that was the only reason I want to point that out.

486
01:01:18,370 --> 01:01:22,000
Um, but there are potential sources of standards.

487
01:01:22,940 --> 01:01:26,999
Participant Me? Community.

488
01:01:27,000 --> 01:01:30,120
Values and norms. Feasibility.

489
01:01:30,660 --> 01:01:40,490
Sustainability of the program. Fixed criteria from other sources like a request for proposals.

490
01:01:41,270 --> 01:01:47,030
Well say might say okay, this program will be successful if blankety blank.

491
01:01:48,020 --> 01:01:56,900
Professional standards like in association standards and even political will can all go into that mix of what's

492
01:01:57,680 --> 01:02:06,620
finally determined to be the standards by which we determine whether or not something was effective or successful or,

493
01:02:07,760 --> 01:02:13,640
you know, whether or not something was fidelity to a curriculum versus adapting a curriculum.

494
01:02:15,200 --> 01:02:26,690
So we benefit as evaluators from having facilitation skills to negotiate those different sources and to be able to outline with our invested parties.

495
01:02:28,380 --> 01:02:34,470
What standards are we going to use to say that something was either successful or not successful?

496
01:02:41,490 --> 01:02:47,990
And then finally we land on. Using and sharing the lessons learned.

497
01:02:49,360 --> 01:02:56,830
In short, appropriate use of findings are for internal or external decision making.

498
01:02:57,750 --> 01:03:05,400
Inappropriate use of finding might include justifying decisions already made public relations.

499
01:03:06,290 --> 01:03:13,360
Or fulfilling funding requirements. And again, knowing the difference can be subtle.

500
01:03:14,020 --> 01:03:17,570
I had a client. I'm trying to.

501
01:03:19,050 --> 01:03:23,400
Scrub out details. I am a client.

502
01:03:24,420 --> 01:03:32,650
They're really. With about evaluation of the different initiatives they fund to be able to.

503
01:03:34,990 --> 01:03:41,470
Disseminate positive findings. They just wanted to be able to say, we serve this many people.

504
01:03:41,770 --> 01:03:50,100
We are awesome. But that ran counter to what they first said.

505
01:03:50,570 --> 01:03:57,140
The reason why they were doing evaluation, which was to improve a particular health care field.

506
01:03:59,340 --> 01:04:06,510
And so it took a while for us as a team to realize, Oh, they really are just doing this for public relations.

507
01:04:06,510 --> 01:04:15,690
And anything else that we suggest to them in terms of methods is going to be kind of pushed back on because they just need the numbers.

508
01:04:15,690 --> 01:04:20,100
They just need numbers so they can put them in newsletters and other places.

509
01:04:22,250 --> 01:04:34,489
That was an inappropriate use of findings. Especially since it often those things tend to lead to pressure to modify methods or

510
01:04:34,490 --> 01:04:41,540
to modify results or to find results that support what it is they already want to do.

511
01:04:44,600 --> 01:04:50,660
Yes. Have you found? You just let me see a lot of.

512
01:04:56,360 --> 01:05:00,290
For topping the top rate for a lot more money under the program.

513
01:05:02,790 --> 01:05:06,930
Part of it. So how do you balance the pressure to, like, present the numbers?

514
01:05:07,930 --> 01:05:11,140
Warming to present the numbers is not always unethical.

515
01:05:11,950 --> 01:05:18,010
I mean, you know, there's a sense of accountability if I say I'm going to fund if I'm going to serve.

516
01:05:19,210 --> 01:05:28,870
300 people and I only serve 50. I need to know why I only serve 50 or you know, and that you know.

517
01:05:28,870 --> 01:05:35,480
That's helpful. But. When it kind of leans toward being unethical.

518
01:05:35,690 --> 01:05:44,580
It's also about. I would say, like as an evaluator, part of what I do is kind of a capacity building slash teacher hat.

519
01:05:46,260 --> 01:05:52,440
And so sometimes we have to put on that hat to to discuss with clients or other invested parties.

520
01:05:52,860 --> 01:05:58,380
You know, numbers are great, but based on what you are saying to me, you know,

521
01:05:58,650 --> 01:06:04,170
it would also be helpful to have this qualitative data on staff's experience of doing something,

522
01:06:04,170 --> 01:06:13,260
for example, or, you know, qualitative interviews with with your participants.

523
01:06:15,890 --> 01:06:22,010
If that doesn't work, sometimes it just means that, you know,

524
01:06:22,010 --> 01:06:29,780
and then there's also like opportunities to negotiate and to build something that makes sense to all parties.

525
01:06:30,110 --> 01:06:38,150
But if it really does veer toward unethical behavior, there have been times like in the case of the project that I was just describing,

526
01:06:38,630 --> 01:06:45,050
where it's just untenable and we in our parts of our relationship with each other.

527
01:06:47,130 --> 01:06:57,060
And I have done that because it just became very clear that it wasn't a good fit and I wasn't really supportive of what you were asking me to do.

528
01:06:59,070 --> 01:07:06,930
But again, I try to get I try to I try to find out these things as much as possible before we get going.

529
01:07:06,930 --> 01:07:12,000
Before I sign a contract, I need to be able to tell, you know, so that we're not like in the middle,

530
01:07:12,390 --> 01:07:17,520
but let's kill this contract because we're just not getting along.

531
01:07:26,210 --> 01:07:33,080
So accurate reporting and reporting is not just about writing a report.

532
01:07:34,730 --> 01:07:38,600
The video highlighted that there's different ways of reporting.

533
01:07:39,200 --> 01:07:45,230
There's obviously the report, but there's also executive summaries.

534
01:07:45,620 --> 01:07:50,480
There's also video. There's also storytelling.

535
01:07:54,240 --> 01:07:59,340
I was. I was in the lounge, the student lounge across the other building.

536
01:07:59,340 --> 01:08:06,360
And I saw those graphic recordings of I don't know what they were from, I'm guessing,

537
01:08:06,360 --> 01:08:09,720
students who were talking about what they want to do with their futures.

538
01:08:12,240 --> 01:08:14,700
That sort of other interview upon me, I love that.

539
01:08:14,700 --> 01:08:25,200
I was like, this is awful, but but I am actually, you know, part of what I also do in my practice is do art space work.

540
01:08:25,680 --> 01:08:30,540
So I've done some illustrated results as well.

541
01:08:30,570 --> 01:08:37,410
So there is also a account that's also falling under this umbrella of of reporting.

542
01:08:38,330 --> 01:08:46,250
And it is an ethical responsibility, as I like to stress, that data is data.

543
01:08:47,700 --> 01:08:55,650
It's pieces of people's lives, right? I'm asking oftentimes personal questions from people.

544
01:08:55,680 --> 01:09:00,360
How do you feel about this? What's your you know, your behavior?

545
01:09:00,370 --> 01:09:03,810
What's your eating behavior? These are pieces of people's lives.

546
01:09:05,100 --> 01:09:11,310
And so I think of it as an ethical responsibility to share back what I've learned.

547
01:09:16,750 --> 01:09:19,660
And to be accurate about it again, not just.

548
01:09:22,340 --> 01:09:32,060
Not just telling clients or investor parties what they want to hear, but telling them from our perspective as the team, what do we see?

549
01:09:32,090 --> 01:09:36,499
What did we learn? When did we perceive? And this is why we perceive that.

550
01:09:36,500 --> 01:09:45,800
This is why we saw it and in whatever way possible, communicating what undergirds our decision making.

551
01:09:49,370 --> 01:09:56,360
I also think it's it's helpful, but it's also kind of expensive to do generate informed discussion.

552
01:09:56,990 --> 01:10:08,090
So very rarely do I just give a client a report and not have some type of follow up meeting after it at the at the very least.

553
01:10:10,140 --> 01:10:13,890
Because again, the resources that are being put into evaluation.

554
01:10:14,770 --> 01:10:19,570
Are also should be used to be useful to the client.

555
01:10:22,240 --> 01:10:25,220
All relevant parties should be included in the reporting.

556
01:10:25,930 --> 01:10:35,049
This is probably one area where it becomes somewhat difficult to do and where we get pushback because again, when we're talking about,

557
01:10:35,050 --> 01:10:42,100
for example, community members, it's not like community members are often getting paid to sit and listen to you.

558
01:10:42,100 --> 01:10:46,450
Wax philosophical about why the program needs to do this or should do this.

559
01:10:48,890 --> 01:10:55,280
So more and more I try to work with my clients to account for that in the budget

560
01:10:55,550 --> 01:11:01,160
and account for community members participation in some type of compensation.

561
01:11:03,280 --> 01:11:06,940
And then finally, negative and problematic findings should be reported.

562
01:11:06,970 --> 01:11:20,200
Sensitivity. Sensitivity. Sensitivity. With this this aspect in particular negative and problematic findings.

563
01:11:21,040 --> 01:11:26,230
I try to work with clients even from the beginning so that.

564
01:11:29,630 --> 01:11:34,550
It's a more comfortable space to discuss negative and problematic findings.

565
01:11:35,930 --> 01:11:47,360
So priming the pump, if you will. So I will start kind of priming clients at the beginning of an engagement that well, to talk, you know,

566
01:11:47,360 --> 01:11:54,350
we'll talk about, okay, how would you feel if we get these results that don't look the way you want them to?

567
01:11:55,220 --> 01:12:02,780
But also trying to reframe that idea of negative and problematic findings so that the understanding is,

568
01:12:02,780 --> 01:12:08,840
is that there's always areas for improvement as well as there's always strengths in a given program,

569
01:12:10,340 --> 01:12:18,110
because oftentimes clients are afraid of evaluators. We have you know, as I mentioned last week, we do have a lot of influence in that.

570
01:12:18,110 --> 01:12:23,240
Our report can go on to either decide if a program is funded or not funded.

571
01:12:24,500 --> 01:12:26,960
So it's just helpful to.

572
01:12:28,530 --> 01:12:37,350
Help clients feel comfortable in that kind of in that type of relationship where in some ways we do have power that we don't even necessarily ask for.

573
01:12:43,310 --> 01:12:47,150
Okay. So I'll just end today by saying that.

574
01:12:48,560 --> 01:12:55,550
Court evaluations in addition to being useful saw utility feasible.

575
01:12:58,980 --> 01:13:07,890
Propriety while having propriety being ethical and being accurate, they should also be developed with a culturally sensitive lens.

576
01:13:08,160 --> 01:13:18,600
So working with our invested parties, ensuring the processes are culturally appropriate, and then providing meaningful products or deliverables.

577
01:13:19,020 --> 01:13:25,140
And on Thursday, we'll talk about culturally responsive and equitable approaches, which.

578
01:13:26,220 --> 01:13:34,740
Take this idea of cultural sensitivity and expand it and give it more of a theoretical and sound justification.

579
01:13:35,700 --> 01:13:38,850
So any questions before we end for today? Okay.

580
01:13:39,330 --> 01:13:55,470
I saw your hand first. Okay.

581
01:13:55,780 --> 01:14:05,950
So the question is how often are our evaluators working directly with investor parties versus giving clients the tools to work with invested parties?

582
01:14:08,660 --> 01:14:15,390
If I may. It's hard to say. It really does depend on, honestly, someone's approach to evaluation.

583
01:14:15,450 --> 01:14:22,940
Somebody like me, I you know, and my colleagues, where our emphasis is on culturally responsive and equitable approaches,

584
01:14:23,640 --> 01:14:27,870
or we also have colleagues who do participatory evaluation.

585
01:14:28,050 --> 01:14:36,510
Then there's empowerment evaluation. We're working with parties a lot because that's just kind of part of how we conceptualize the work.

586
01:14:37,410 --> 01:14:42,990
It doesn't make sense to us not to do that, whereas there are some who are more.

587
01:14:46,950 --> 01:14:52,080
And then there's a reading that kind of shows like a tree and there's like a tree that's like more.

588
01:14:52,230 --> 01:14:54,960
There's a branch that's, like, really super focused on rigor.

589
01:14:55,740 --> 01:15:02,050
They aren't always they don't always really focus super much on working with invested part.

590
01:15:04,330 --> 01:15:07,660
So it really does depend. And I'm.

591
01:15:08,290 --> 01:15:13,050
I'm not going to go down that rabbit hole. No. But I'm going to stop there.

592
01:15:14,700 --> 01:15:22,620
I start telling our professors what I said about how they understand why they let me into the program.

593
01:15:22,620 --> 01:15:26,730
I complained all the time. Yes. My question is of.

594
01:15:35,950 --> 01:15:39,990
And they're all valuable. But you can't actually act upon all of.

595
01:15:47,300 --> 01:15:55,420
So how do you weigh? Yes.

596
01:15:56,300 --> 01:16:03,420
Yes. So if there's a question. So the question was, you know, you generate evaluation questions.

597
01:16:03,420 --> 01:16:12,720
All of them are useful. All of them are important. But some of them you can't act on some of them or maybe even some of them you can't.

598
01:16:14,860 --> 01:16:18,660
They're not maybe that important to your clients.

599
01:16:20,280 --> 01:16:27,530
And do not include them. Yes. If you're like my director, though, who is a hustler.

600
01:16:28,040 --> 01:16:32,509
She she puts those questions in her pocket. Serious.

601
01:16:32,510 --> 01:16:38,570
I love it. I wish I could be like her when I grew up. She puts them in their pocket, and then she negotiates another project.

602
01:16:40,300 --> 01:16:46,290
But. I haven't got there yet. So I just keep them and I look at them sadly.

603
01:16:46,890 --> 01:16:50,220
But but the truth is, you can't answer everything.

604
01:16:50,580 --> 01:16:56,100
So you have to negotiate with all parties involved.

605
01:16:56,730 --> 01:17:06,680
What are the priority questions? Because again, we were thinking and it has implications not just for the questions, but also for things like methods.

606
01:17:07,130 --> 01:17:13,430
If the difference between a survey being a reasonable 10 minutes long and a survey being 60 minutes long.

607
01:17:15,160 --> 01:17:18,790
And then that has implications for your sample size or.

608
01:17:19,860 --> 01:17:26,190
For example, you get three people to do the 60 Minutes survey versus the 300 who will do the ten minute.

609
01:17:27,610 --> 01:17:34,150
So unfortunately, yes, sometimes we we, we there are questions that are important from our perspective,

610
01:17:34,480 --> 01:17:42,270
but we do not and we do not seek to answer them. When you look at the time before.

611
01:17:42,390 --> 01:17:45,690
Okay. One more question and then I have to let you go.

612
01:17:55,000 --> 01:17:58,180
I know, like. I love.

613
01:18:03,390 --> 01:18:12,370
Our. We like to take priority.

614
01:18:16,140 --> 01:18:20,190
Yeah. So the question is, are they like places where you can pulse results?

615
01:18:22,330 --> 01:18:24,340
Nothing that is not peer review.

616
01:18:24,370 --> 01:18:33,680
So typically what people do if they want to, you know, disseminate widely to the field is that they we do have peer reviewed journals, too.

617
01:18:33,910 --> 01:18:38,799
So we I mean, you can you can publish in any journal that fits the article.

618
01:18:38,800 --> 01:18:48,960
So if I do. A valuation of a family in a family based intervention published in Journal for Family Studies.

619
01:18:50,200 --> 01:18:53,769
Or I could publish in the American Journal of Evaluation.

620
01:18:53,770 --> 01:19:05,800
Or I could publish an Apache Journal. Other than that, I can't think of that's a good idea, but I can't think of anything.

621
01:19:06,670 --> 01:19:14,730
Where? It would be widely available that the only thing that I can think of that would be close is that.

622
01:19:16,620 --> 01:19:22,390
You partner with some of the think tanks? To get it get it published that way.

623
01:19:24,040 --> 01:19:27,850
Mm hmm. Um. Something.

624
01:19:30,320 --> 01:19:33,320
Yes. Yeah. You have to. Yeah. Thank you.

625
01:19:33,620 --> 01:19:40,670
Yeah. You have to negotiate that with, you know, negotiate either before you start or during the process.

626
01:19:41,610 --> 01:19:45,170
Because you can't just publish on somebody's stuff without.

627
01:19:45,420 --> 01:19:50,230
Well, you could go to jail, but. It's not ethical.

628
01:19:50,620 --> 01:19:55,390
Okay. So all right. So on Thursday, we'll talk about Cory.

629
01:19:56,470 --> 01:19:57,340
Have a good day.

