1
00:00:05,258 --> 00:00:10,130
Hello, this lecture will cover
measures of association in regression.

2
00:00:10,130 --> 00:00:14,200
Specifically we will talk about
inference used in linear regression, and

3
00:00:14,200 --> 00:00:20,070
the concept of goodness-of-fit, or
the coefficient of determination.

4
00:00:20,070 --> 00:00:23,960
In our companion lectures on correlation
and linear regression, we have

5
00:00:23,960 --> 00:00:29,250
discussed two values for quantifying the
association of two continuous measures.

6
00:00:29,250 --> 00:00:34,570
We have r, Pearson's correlation
coefficient, and we have b, which is

7
00:00:34,570 --> 00:00:41,770
the estimate of the population slope beta
computed through least-squares regression.

8
00:00:41,770 --> 00:00:45,910
In our previous lectures,
we had equations that showed how r and

9
00:00:45,910 --> 00:00:48,390
b were computed from the data.

10
00:00:48,390 --> 00:00:53,430
They were both based on the sample
covariance of x and y, and

11
00:00:53,430 --> 00:00:59,100
each also incorporated some of
the variability in x and/or y.

12
00:00:59,100 --> 00:01:01,790
Using the two equations that
I have shown you for r and

13
00:01:01,790 --> 00:01:08,010
b, we can solve both to get
the covariance by itself.

14
00:01:08,010 --> 00:01:12,880
I can then set these two equations equal
to each other, giving me a relationship

15
00:01:12,880 --> 00:01:18,480
between Pearson's correlation
coefficient r and the slope estimate b.

16
00:01:18,480 --> 00:01:23,410
Eventually leading to this equation here,
that tells me how the slope coefficient b

17
00:01:23,410 --> 00:01:26,970
is related to the Pearson's
correlation coefficient r.

18
00:01:26,970 --> 00:01:32,520
It is r scaled by the ratio of
the standard deviation of my y measures,

19
00:01:32,520 --> 00:01:35,880
and the center deviation of my x measures.

20
00:01:35,880 --> 00:01:40,548
From this equation, we have the following
relationship between the slope and

21
00:01:40,548 --> 00:01:42,530
Pearson's correlation coefficient.

22
00:01:42,530 --> 00:01:48,080
First, the slope and Pearson's correlation
coefficient must have the same sign.

23
00:01:48,080 --> 00:01:53,420
In the equation, r is multiplied by a
ratio of two positive numbers, therefore,

24
00:01:53,420 --> 00:01:59,330
if r is positive b must be positive and
if r is negative, b must be negative.

25
00:01:59,330 --> 00:02:01,860
If r has significance from zero,

26
00:02:01,860 --> 00:02:06,670
then b must also be different
from zero and vice versa.

27
00:02:06,670 --> 00:02:09,670
They both give the same conclusion about

28
00:02:09,670 --> 00:02:13,810
the statistical significance of
the relationship of x and y.

29
00:02:13,810 --> 00:02:20,370
If r is exactly zero, then we can see in
the equation that b must be exactly zero.

30
00:02:20,370 --> 00:02:25,990
However, all other values of r do not
have an exact corresponding value for

31
00:02:25,990 --> 00:02:27,760
the slope.

32
00:02:27,760 --> 00:02:31,710
Here I show you an example of
two scatter plots of data.

33
00:02:31,710 --> 00:02:34,980
Both of them have the same
slope of 0.49 for

34
00:02:34,980 --> 00:02:38,590
the least squares line that
fits through the data.

35
00:02:38,590 --> 00:02:42,710
However, you can see the plot on the left
has points that are much more compressed

36
00:02:42,710 --> 00:02:43,320
around the line,

37
00:02:43,320 --> 00:02:47,770
and the plot on the right has points that
are less compressed around the line.

38
00:02:47,770 --> 00:02:51,965
Therefore, the plot on the left has
a Pearson's correlation of 0.57,

39
00:02:51,965 --> 00:02:57,260
while the plot on the right
has a correlation of 0.2.

40
00:02:57,260 --> 00:03:01,980
Again, the correlation coefficient
can vary and have the same slope.

41
00:03:01,980 --> 00:03:06,810
Conversely, we can vary the slope for
the same Pearson correlation coefficient

42
00:03:06,810 --> 00:03:10,998
based upon the scatter of
the points around the fitted line.

43
00:03:10,998 --> 00:03:14,280
Now, as Pearson's correlation
coefficient r can be used for

44
00:03:14,280 --> 00:03:19,090
inference regarding the population
correlation coefficient row,

45
00:03:19,090 --> 00:03:25,860
my sample slot b can be used to make
influence about the population slope beta.

46
00:03:25,860 --> 00:03:28,240
Testing for an association between x and

47
00:03:28,240 --> 00:03:33,730
y is the same as having a null hypothesis
that the population slope is zero,

48
00:03:33,730 --> 00:03:39,020
versus the alternative that
the population slope is not zero.

49
00:03:39,020 --> 00:03:45,430
Again my signal for my statistic will
be the magnitude of my observed slope,

50
00:03:45,430 --> 00:03:48,810
and the noise is computed
by this equation here.

51
00:03:48,810 --> 00:03:52,890
Again with like correlation
the computation of the noise is not

52
00:03:52,890 --> 00:03:59,040
necessarily intuitive, and I simply supply
this equation for your own information.

53
00:03:59,040 --> 00:04:03,060
However, we return to our example
here which had an intercept of 1,

54
00:04:03,060 --> 00:04:07,510
and a slope of 0.32 fitted to the points.

55
00:04:07,510 --> 00:04:10,960
Through the computation of standard
error that I showed earlier,

56
00:04:10,960 --> 00:04:15,420
my slope coefficient has
a standard error of 0.03.

57
00:04:15,420 --> 00:04:20,290
Again this means from sample to sample
there is a small amount of variability

58
00:04:20,290 --> 00:04:24,860
in the slopes that would be
computed among the samples.

59
00:04:24,860 --> 00:04:28,900
I can compute a signal to noise ratio
by simply taking the ratio of my slope

60
00:04:28,900 --> 00:04:31,190
to its noise.

61
00:04:31,190 --> 00:04:35,420
Or I can compute a 95% confidence
interval for the true population slope,

62
00:04:35,420 --> 00:04:41,510
by simply taking my observed slope, and
adding or subtracting 2 times that noise.

63
00:04:41,510 --> 00:04:45,235
The signal to noise
ratio is 0.32 over 0.03,

64
00:04:45,235 --> 00:04:50,250
again leading to a statistic of
10.82 with a very small p value.

65
00:04:50,250 --> 00:04:54,990
The statistic, again, is looked up
in a standard normal distribution.

66
00:04:54,990 --> 00:04:58,980
You will recall that the signal to noise
ratio is similar to the one we saw

67
00:04:58,980 --> 00:05:02,033
with the correlation coefficient.

68
00:05:02,033 --> 00:05:07,228
The corresponding 95% confidence
interval for the population slope,

69
00:05:07,228 --> 00:05:11,764
is my observed slope of 0.32 plus or
minus 2 times 0.03,

70
00:05:11,764 --> 00:05:15,820
giving me a confidence
interval of 0.26 to 0.38.

71
00:05:15,820 --> 00:05:20,255
Again, this confidence interval is
distance from the null value of 0, and

72
00:05:20,255 --> 00:05:24,018
therefore I would expect that
the p-value would be very small.

73
00:05:24,018 --> 00:05:29,046
Both of these give me very strong
evidence that the population slope

74
00:05:29,046 --> 00:05:33,897
is very different from zero, and
there is an association between

75
00:05:33,897 --> 00:05:38,416
percentage of firearm ownership and
male firearm suicide.

76
00:05:38,416 --> 00:05:43,680
Now typically, the fit of a regression
model is summarized in a table.

77
00:05:43,680 --> 00:05:45,810
For simple regression there are two rows.

78
00:05:45,810 --> 00:05:52,040
There is information on the intercept, a,
and there is information on the slope b.

79
00:05:52,040 --> 00:05:56,290
The first column will
present the estimate.

80
00:05:56,290 --> 00:06:00,920
The second column, again,
it gives the standard error or the noise.

81
00:06:00,920 --> 00:06:04,130
There is a standard error for
the intercept presented in this table.

82
00:06:04,130 --> 00:06:07,990
We are rarely interested in doing
inference on the intercept.

83
00:06:07,990 --> 00:06:10,840
It does not quantify the association
between the two variables,

84
00:06:10,840 --> 00:06:15,110
it is only the slope, although this
information is given in the table.

85
00:06:15,110 --> 00:06:19,100
There is then a ratio of the estimate to
the standard error, this is the t value or

86
00:06:19,100 --> 00:06:21,110
the t statistic.

87
00:06:21,110 --> 00:06:24,620
And then the p value is the corresponding
area in the normal distribution

88
00:06:24,620 --> 00:06:27,170
corresponding to that t value.

89
00:06:27,170 --> 00:06:30,520
Again, the p value on
the intercept is irrelevant for

90
00:06:30,520 --> 00:06:32,870
our association between our variables.

91
00:06:32,870 --> 00:06:38,660
We only focus on the p value for the slope
which, again, is very, very small.

92
00:06:38,660 --> 00:06:41,440
The MSE is 7.619.

93
00:06:41,440 --> 00:06:45,960
Again, this was the residual variability
in the points around the line.

94
00:06:45,960 --> 00:06:48,440
We would like that value to be small, but

95
00:06:48,440 --> 00:06:53,400
there will always be residual variability
on the points around the line.

96
00:06:53,400 --> 00:06:57,670
There is one new quantity presented on
the slide, and this is r squared, and

97
00:06:57,670 --> 00:07:01,740
I have given you
an R-squared value of 0.709.

98
00:07:01,740 --> 00:07:06,130
R squared is more technically known
as the coefficient of determination.

99
00:07:06,130 --> 00:07:11,086
It is a number that goes from 0 to 1,
and it quantifies the proportion

100
00:07:11,086 --> 00:07:15,374
of variation in y,
that is explained by a regression model.

101
00:07:15,374 --> 00:07:19,911
Again, it is similar to correlation,
it tells us how tightly or

102
00:07:19,911 --> 00:07:25,290
how loosely scattered my points
are around my fitted line.

103
00:07:25,290 --> 00:07:28,360
We call the coefficient of
determination R-squared,

104
00:07:28,360 --> 00:07:31,360
because we know that
the coefficient of determination

105
00:07:31,360 --> 00:07:36,050
is the square of Pearson's
correlation coefficient.

106
00:07:36,050 --> 00:07:40,150
So if we are given an R-squared value,
we know the magnitude of Pearson's

107
00:07:40,150 --> 00:07:44,730
correlation, we simply take the square
root of the coefficient of determination.

108
00:07:44,730 --> 00:07:49,760
However, we do not know the sign of
Pearson's correlation coefficient,

109
00:07:49,760 --> 00:07:53,960
because the square root of
R-squared will always be positive.

110
00:07:53,960 --> 00:07:57,700
So, we have to look at the sign on
the slope to determine whether or

111
00:07:57,700 --> 00:08:01,740
not Pearson's correlation is positive or
negative.

112
00:08:01,740 --> 00:08:05,940
Now, if Pearson's correlation coefficient
is zero, then as we said earlier,

113
00:08:05,940 --> 00:08:11,280
the slope must be zero, and therefore the
coefficient of determination must be zero.

114
00:08:11,280 --> 00:08:16,350
All three of these things tell me that x
and y have no association with each other,

115
00:08:16,350 --> 00:08:20,410
and therefore x explains none
of the variability in y.

116
00:08:20,410 --> 00:08:24,815
x has no information to tell me
why a y value would be what it is.

117
00:08:24,815 --> 00:08:30,971
R-squared is often cited as a measure
of a model's goodness-of-fit.

118
00:08:30,971 --> 00:08:33,810
As R-squared increases from 0 to 1,

119
00:08:33,810 --> 00:08:39,070
we know that x provides more and
more information about y.

120
00:08:39,070 --> 00:08:42,460
So we have now covered estimation,
inference and

121
00:08:42,460 --> 00:08:47,000
goodness of fit, related to
a simple linear regression model.

122
00:08:47,000 --> 00:08:51,400
If you progress further in your studies of
biostatistics, you will encounter several

123
00:08:51,400 --> 00:08:55,630
generalizations of simple linear
regression, including multiple linear

124
00:08:55,630 --> 00:09:00,190
regression, which is when you have more
than one x variable in your model.

125
00:09:00,190 --> 00:09:03,980
And logistic regression
which is when the y variable

126
00:09:03,980 --> 00:09:06,880
is binary rather than continuous.

