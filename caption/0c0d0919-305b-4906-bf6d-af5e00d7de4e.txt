1
00:00:05,290 --> 00:00:15,110
Okay. If you.

2
00:00:18,770 --> 00:00:27,110
Wow. Only seven people. So.

3
00:00:31,270 --> 00:00:34,590
I don't know. I heard some important announcements.

4
00:00:34,600 --> 00:00:41,140
I'm just wondering whether I should wait or maybe it's the the announcements.

5
00:00:45,210 --> 00:00:52,080
Maybe let me do the announcements. Like, you know, sort of maybe doing it right after the break because.

6
00:00:54,290 --> 00:00:58,880
At least a few more. Be glad to be here. Okay.

7
00:00:59,210 --> 00:01:05,030
So did you have so you wanted to talk about the second oldest for you?

8
00:01:06,170 --> 00:01:13,640
And often those do remind me. I have three or four announcements to make.

9
00:01:16,580 --> 00:01:21,780
Okay. So let's.

10
00:01:26,110 --> 00:01:31,120
Let's see. Anybody has questions for me.

11
00:01:35,740 --> 00:01:48,460
I know I'm seeing a lot of you and I can see the activity which is which is great like and hopefully everybody is working in groups and you know,

12
00:01:48,970 --> 00:01:53,770
I think I'm seeing like five groups today. I saw three yesterday.

13
00:01:57,010 --> 00:02:00,930
The big thought in the night.

14
00:02:00,930 --> 00:02:06,390
Also, I saw you guys on Monday by Zoom.

15
00:02:06,880 --> 00:02:12,210
Yeah, yeah, yeah. So. So at least, you know, I mean, this is all good.

16
00:02:17,590 --> 00:02:22,220
That's what this project is supposed to be about. Should be. Okay.

17
00:02:22,270 --> 00:02:30,940
So today we were sort of the last module.

18
00:02:31,750 --> 00:02:40,510
And again, you'll see that many of the ideas that we are going to talk about, this is not new to you.

19
00:02:43,150 --> 00:02:58,059
And we will sort of reuse many of the concepts, but at the same time introduce some some new concepts.

20
00:02:58,060 --> 00:03:04,480
But primarily we will reuse, recycle what you have learned so far.

21
00:03:05,130 --> 00:03:09,130
And so this is about module selection.

22
00:03:09,940 --> 00:03:13,030
And here is an outline. So we'll talk about.

23
00:03:15,010 --> 00:03:20,920
So maybe before even going into the outline, let me just say this.

24
00:03:21,790 --> 00:03:28,779
So many of you who have been seeing and you have kind of talked to me via email,

25
00:03:28,780 --> 00:03:38,169
via zoom in person, the first thing I ever asked you is that what is your focus in this project?

26
00:03:38,170 --> 00:03:49,390
Is it inference or is it prediction? And I kind of tried to channel you like to think about this already in regression analysis.

27
00:03:50,920 --> 00:03:53,660
The generally there are two broad objectives.

28
00:03:53,770 --> 00:04:04,570
One is inference where, you know, you do the point, and often the point estimates you do construct confidence interval to do hypothesis tests.

29
00:04:05,140 --> 00:04:14,830
And then the other broad sort of objective is prediction, where the goal is to develop the best model for predicting future responses.

30
00:04:15,730 --> 00:04:28,240
And in data analysis, you know, the objectives, what your objective is basically drives your model selection algorithm.

31
00:04:29,050 --> 00:04:35,670
So that's why no wonder whenever you came to me, walked into my office or we came into the Zoom room,

32
00:04:36,310 --> 00:04:40,570
the first thing I asked you like, what is your objective?

33
00:04:41,230 --> 00:04:44,620
Is it inference or is it is it prediction?

34
00:04:45,340 --> 00:04:52,960
And that's sort of the context that we are going to keep that in mind,

35
00:04:52,990 --> 00:05:07,120
very clear in mind that our objectives will determine or drive the model selection inference is more clearcut because you

36
00:05:07,120 --> 00:05:15,489
are interested in understanding the association of the outcome with certain variables and in understanding that association,

37
00:05:15,490 --> 00:05:21,610
you want to adjust for confounders. You want to look for interaction effect modification.

38
00:05:22,510 --> 00:05:30,040
So it's more clear cut, whereas in prediction there are many possible strategies of model selection.

39
00:05:30,940 --> 00:05:37,330
So in this module, we are going to primarily focus on prediction.

40
00:05:39,820 --> 00:05:45,700
Although I want to like I said, I wanted to start with that preamble like what is your object?

41
00:05:46,150 --> 00:05:50,200
And that's something that you should always ask yourself first.

42
00:05:52,720 --> 00:06:05,620
So here I will kind of bring in the context of inference early on, but then pretty much the whole module is focused on model selection for prediction,

43
00:06:05,620 --> 00:06:14,800
because for inference we have kind of really, you know, sort of we have done as, I mean all tool so far.

44
00:06:15,250 --> 00:06:22,810
That is what our focus has been. Yep. So we'll talk about that, the prediction context.

45
00:06:23,260 --> 00:06:27,610
We'll talk about maximum models, the criteria for comparing models.

46
00:06:27,610 --> 00:06:33,610
And then we'll also talk about automated model selection and couple examples.

47
00:06:34,420 --> 00:06:39,700
So there is this is from Chapter ten of your textbook.

48
00:06:40,870 --> 00:06:44,620
And there's also an alternative reading listed here.

49
00:06:45,820 --> 00:06:50,470
So again, bringing home that point, as I was mentioning, model selection for inference.

50
00:06:50,860 --> 00:06:55,240
You know, what we have been dealing with so far, we want be to have to be close to vector.

51
00:06:56,230 --> 00:07:06,750
So this requires pre-specified scientific knowledge about the known or the the hypothesized causal biological mechanisms.

52
00:07:06,760 --> 00:07:13,149
So that's why I said if you're objective, the inference association like go back to the literature,

53
00:07:13,150 --> 00:07:17,020
see, you know what, what kind of evidence is there?

54
00:07:17,100 --> 00:07:26,940
I mean, so that could help you gather the pre-specified scientific knowledge and then and then sort of lay out your model.

55
00:07:27,480 --> 00:07:31,290
And the focus is an unbiased point, estimation and valid inference.

56
00:07:32,130 --> 00:07:44,220
On the other hand, for prediction, the model selection is guided by the rational that we want the best model for predicting future responses.

57
00:07:44,640 --> 00:07:47,670
And that's the key word.

58
00:07:48,300 --> 00:07:58,560
So the focus is more on the fitted model rather than the individual pattern of this beta herd can even be biased.

59
00:08:00,180 --> 00:08:07,050
So so as you can see that, you know, like the objective is clearly different.

60
00:08:07,380 --> 00:08:14,670
And so far we have focused on inference from models. As I said, we want in a given set of parameters, given set of coordinates.

61
00:08:14,670 --> 00:08:20,760
So like betterment estimation wanting to point as well as interval estimation interpreting parameters,

62
00:08:21,090 --> 00:08:28,440
hypothesis testing, assessing relationships between covariance scale of the or variables.

63
00:08:28,440 --> 00:08:33,629
Are there interactions then model diagnostics to guide us through that process?

64
00:08:33,630 --> 00:08:44,550
So the up to now what we have done is that the scenario with the covariance that are to be included in the model were largely predetermined,

65
00:08:45,540 --> 00:08:55,590
predetermined based on again what we know about the scientific mechanism, underlying scientific mechanism based on prior literature.

66
00:08:56,700 --> 00:09:00,930
So that was the premise up to now largely determined.

67
00:09:01,380 --> 00:09:07,410
Now what, what should be included in the model and potentially except for some kind of,

68
00:09:07,410 --> 00:09:14,190
you know, nonlinear transformations, interactions, etc. or like, you know,

69
00:09:14,190 --> 00:09:24,930
kind of converting a continuous variable to a categorical variable to kind of make the model more flexible, like the scale of the variable,

70
00:09:25,980 --> 00:09:33,780
things like that, which may have been added after earlier residual diagnostics like the fix that you did with the model.

71
00:09:34,080 --> 00:09:42,030
So that has been our focus. But now we want the best model for predicting future responses.

72
00:09:42,540 --> 00:09:45,809
If the number of covariance is too large piece to last,

73
00:09:45,810 --> 00:09:55,620
then there can be overfitting because the model might sort of captured the noise in the current observations too closely,

74
00:09:55,890 --> 00:10:03,390
and consequently our predictions will be poor on future observations.

75
00:10:03,840 --> 00:10:11,250
Future meaning? What do I mean by future observations? Observations that are not included in training the model.

76
00:10:11,490 --> 00:10:17,580
So here is a new working arm or ward that you will hear training the model means

77
00:10:17,880 --> 00:10:24,150
and the data on or the training data means the data on which the model was built.

78
00:10:25,530 --> 00:10:36,510
And by future observations I mean data or observations that read in part or the parent included in the model building.

79
00:10:37,620 --> 00:10:45,900
You'll also hear can like test data for this data means again things data that wasn't used to train the model.

80
00:10:46,860 --> 00:10:56,339
Okay so so that's that's the challenge now is these too large there may be overfitting and we might check the noise

81
00:10:56,340 --> 00:11:06,180
too closely noise in the current data and the model might be lousy in terms of predicting future observations.

82
00:11:07,620 --> 00:11:25,290
So the, so the idea is that we need some sort of algorithms to select models that are that quote unquote are best,

83
00:11:25,290 --> 00:11:29,100
best in under certain criteria for predictive purposes.

84
00:11:29,970 --> 00:11:33,570
Once again, before we kind of plunge right into.

85
00:11:33,570 --> 00:11:36,630
How do we do that? Please.

86
00:11:37,290 --> 00:11:43,230
You know, kind of again, I'm really reiterating the inference versus the prediction objective.

87
00:11:44,370 --> 00:11:50,010
So far, all we have done was model selection for inference objective.

88
00:11:50,460 --> 00:11:52,800
And, you know, typically one goal mediator,

89
00:11:52,830 --> 00:12:03,600
a specific set of covariates are of interest like and in your project we said religiosity and ideal are our primary predictors of interest

90
00:12:04,440 --> 00:12:15,780
and we wish to be able to draw some kind of causal inference about the effects of these primary predictors and outcome depression and.

91
00:12:18,070 --> 00:12:30,459
And the kind of things we worry about in terms of model selection in that scenario is that it like is the association between X and Y,

92
00:12:30,460 --> 00:12:39,010
between religiosity and depression, is that somehow confounded by some hard core video?

93
00:12:39,160 --> 00:12:49,840
Z What do I need to adjust for when looking at the association between the primary predictor X and the outcome?

94
00:12:49,840 --> 00:13:00,610
Why then are their effect modifiers meaning of interactions between between variables and so on and so forth?

95
00:13:00,610 --> 00:13:02,409
So that again, once again,

96
00:13:02,410 --> 00:13:16,210
the inference requires pre-specified scientific knowledge about like the underlying the causal mechanism that might link X, Y and Z.

97
00:13:16,810 --> 00:13:25,330
So religiosity, depression and may does state this number of kids, things like that.

98
00:13:25,960 --> 00:13:29,740
So the so the prediction asks on the other.

99
00:13:29,950 --> 00:13:35,650
So that's the inference and scenario and that's what we have been kind of focused on.

100
00:13:36,160 --> 00:13:41,379
If you were to turn that problem into a prediction problem,

101
00:13:41,380 --> 00:13:49,150
like the same problem that we have cause for you in the project, if you were to turn it into a prediction problem,

102
00:13:49,600 --> 00:14:00,579
then prediction would ask the somewhat less specific question what what kind of question would a task it would ask,

103
00:14:00,580 --> 00:14:13,360
like read all those X, Y and Z and like which of those, you know, maybe does number of kids age religiosity?

104
00:14:13,360 --> 00:14:23,210
ADL None of it adds up. Which of those would help me best predict depression for new observations?

105
00:14:25,770 --> 00:14:36,040
So I'm less concerned about the causal mechanism or the causal effects between wire nets.

106
00:14:38,010 --> 00:14:51,420
I mean, you could incorporate prior mechanistic biological knowledge into the model building for prediction, but it's strictly not necessary to do so.

107
00:14:53,570 --> 00:15:06,260
So. So I know that I'm kind of docking a lot without turning any slides, and I'm kind of probably giving you more of a philosophical perspective.

108
00:15:06,500 --> 00:15:15,530
But I want everybody to actually understand this, like where the where the philosophical distinction is,

109
00:15:15,530 --> 00:15:18,830
because this is something that will guide your modern selection.

110
00:15:21,240 --> 00:15:31,140
Everybody. Fine. Good. And, you know, like, if you came to me in any form, zoom room in person in a.

111
00:15:32,440 --> 00:15:39,160
I asked that question first when you came to ask me about the project, and now you know why.

112
00:15:40,210 --> 00:15:45,200
Yes. So. I just wanted to basically be using all of the words.

113
00:15:48,680 --> 00:15:52,480
I can't do that. Yeah. Come to that.

114
00:15:52,750 --> 00:15:56,020
But I want you to understand this distinction first. And.

115
00:15:56,250 --> 00:16:02,500
And then from this point on would be just go right into prediction model selection for prediction.

116
00:16:02,500 --> 00:16:10,030
Because I have convinced you by now that the model selection for inference we have pretty much covered.

117
00:16:11,080 --> 00:16:21,280
That is what our focus has been so far. So now we are going to delve into prediction and and sort of go from there.

118
00:16:22,150 --> 00:16:35,880
So. So. The selection of friends is because many of you may know you miss the the big picture point.

119
00:16:35,890 --> 00:16:37,960
The model selection for inference is.

120
00:16:40,240 --> 00:16:46,300
That's why I say when you when you are talking about model selection for inference start with the scientific question.

121
00:16:46,840 --> 00:16:55,749
Fix your hypothesis and go and do literature search and start with with that model.

122
00:16:55,750 --> 00:17:04,150
The for the most extensive model that you will want to like include the variables

123
00:17:04,570 --> 00:17:10,110
that need to be adjusted for to address that specific scientific question.

124
00:17:12,310 --> 00:17:18,150
And then go right as the prediction you basically say.

125
00:17:18,730 --> 00:17:34,000
I don't I don't blame everybody. But which is the best subset of variables that will give me the best model for predicting future operations.

126
00:17:34,060 --> 00:17:40,930
You don't. You don't care about. I mean, probably that's too harsh about what you do.

127
00:17:41,020 --> 00:17:52,990
The focus is not on understanding the underlying biological mechanism as much or or kind of understanding the causal relationship.

128
00:17:55,020 --> 00:17:59,339
You get a lot of perplexity versus parsimony.

129
00:17:59,340 --> 00:18:04,140
Suppose we have 30 equal variants in the true model and we have 50 observations.

130
00:18:04,710 --> 00:18:09,660
We could consider the following two alternatives. We could fit the model using all of the covariance.

131
00:18:11,310 --> 00:18:21,240
So 30 covariance 50 observations. And in this case beta hat is unbiased for beta, but of course it will have a baby.

132
00:18:21,240 --> 00:18:26,940
But how big is the other approaches?

133
00:18:26,940 --> 00:18:36,930
We could fit the model using the five strongest predictors, strongest in the sense best in predicting the outcome.

134
00:18:38,280 --> 00:18:46,350
In this case beta hat will be on may be biased for beta, but it will definitely have a lower opinions.

135
00:18:47,340 --> 00:18:58,860
And the argument is that a thought so death coming that I mean if I could sort of simply say coming back to your question for inference,

136
00:18:59,310 --> 00:19:05,280
if the objective is inference, you kind of although you wouldn't do something as crazy as this,

137
00:19:05,520 --> 00:19:16,200
but you kind of have to stick to one step because giving unbiased estimates of beta is your focus,

138
00:19:16,200 --> 00:19:21,899
because you are trying to understand the causal mechanism for prediction.

139
00:19:21,900 --> 00:19:29,430
You could do either either approach one or two, approach two could perform better and depending on the circumstances.

140
00:19:30,710 --> 00:19:41,090
Sorry, but if it weren't for friends, we find that some of the that the variables are not associated with the python filter is

141
00:19:41,090 --> 00:19:48,460
more like we can establish a confounding there so we don't need to use them for the.

142
00:19:49,250 --> 00:19:59,120
We do need to. We do have some same kind of basis to include them in the book in blue or exclude is it to include or

143
00:19:59,330 --> 00:20:06,440
to exclude is it is a different question because and I totally understand where you're coming from,

144
00:20:07,340 --> 00:20:12,260
but whether to include in the model or not, that's a different question.

145
00:20:12,260 --> 00:20:15,389
And you know, like pick up, pick up, pick up.

146
00:20:15,390 --> 00:20:25,190
I mean, some clinical epidemiology journal, you will see that when look, when some of the focus is on association.

147
00:20:25,700 --> 00:20:39,680
Table three of the of the People, which shows the results from a multivariable analyzes multiple linear regression or multiple logistic regression.

148
00:20:40,250 --> 00:20:48,190
You will see that there may be variables that are listed in that they would that are not significant.

149
00:20:48,200 --> 00:20:51,620
And what is the rationale for that? The rationale for that.

150
00:20:51,650 --> 00:21:06,170
So if if if let's say, for example, I'm just making it to see age or gender like and you are looking at sort of hypertension,

151
00:21:07,910 --> 00:21:13,010
age and gender in your dataset, but not important.

152
00:21:13,820 --> 00:21:22,070
They are not significant and you are trying to relate blood pressure to like just making it up like.

153
00:21:25,340 --> 00:21:28,610
BMI. Okay, that's your primary.

154
00:21:28,610 --> 00:21:38,179
BMI is their primary body and you kind of fit the model and adjust for different types of albeit some may be

155
00:21:38,180 --> 00:21:46,850
confounders effect modifiers and lower before you do finding your data age and gender are not significant.

156
00:21:47,660 --> 00:21:51,200
So your question is, do I keep it? Do I kick it out?

157
00:21:52,310 --> 00:21:55,700
And what I would say and this is what you would see,

158
00:21:55,970 --> 00:22:01,940
what I would say is like if the objective is inference and you are really your

159
00:22:01,940 --> 00:22:06,890
focus is trying to understand the association between BMI and blood pressure.

160
00:22:08,150 --> 00:22:12,470
Even though the median gender are not you are not significant in your data.

161
00:22:12,710 --> 00:22:22,820
You will be able to model with those non-significant variables included because the thing is that you are you want to go on with even general,

162
00:22:23,270 --> 00:22:29,990
you know sort of for hypertension that we know that but in your data let's say don't go that non-significant

163
00:22:30,170 --> 00:22:37,530
so the point is you have exhausted for those that's the point because the focus is not a pandemic,

164
00:22:37,970 --> 00:22:43,340
but the focus is have you exhausted four factors that are known?

165
00:22:46,160 --> 00:22:53,570
Got it. Just add it now. Yeah. And so whether to keep it or not is, is not the question.

166
00:22:55,040 --> 00:23:04,490
Because keeping them doesn't do doesn't like how glossy the model selection,

167
00:23:04,970 --> 00:23:10,580
the objective of model selection there is not whether you keep it gender in the model or not.

168
00:23:12,760 --> 00:23:18,930
Just I guess just because we're doing point with prediction the in the sense of just like make sure that I

169
00:23:18,930 --> 00:23:26,430
understand this is for long as section of an inference it is unless it poses a problem to the regression.

170
00:23:26,850 --> 00:23:28,890
You keep it. Yeah. Yeah.

171
00:23:29,640 --> 00:23:44,250
Unless it poses a problem to the regression to understanding the underlying the, the the association between BMI and hypertension and blood pressure.

172
00:23:45,630 --> 00:23:48,690
You you. You keep it. You keep it.

173
00:23:48,690 --> 00:23:55,620
Because the focus is to understand the causal mechanism there is for prediction.

174
00:24:00,150 --> 00:24:04,560
Doesn't you know, you don't care because you want to get the best predictive model.

175
00:24:06,780 --> 00:24:10,950
So you keep the variable. That's how you get the union.

176
00:24:12,420 --> 00:24:16,080
Yeah. If you if you have what they call linearity, then of course this.

177
00:24:18,110 --> 00:24:21,290
That's. And that's what she was asking. Yeah. So.

178
00:24:21,670 --> 00:24:25,100
So that's the that's the basic premise. Okay.

179
00:24:28,370 --> 00:24:33,320
And for prediction, like I said, you could use either of these two approaches.

180
00:24:33,650 --> 00:24:44,180
And either of these two would perform better. Um, and strictly speaking, you do not have to incorporate prior knowledge into the model building.

181
00:24:46,130 --> 00:24:49,459
You could, but it is not strictly necessary for prediction.

182
00:24:49,460 --> 00:24:51,230
But it's one inference you have.

183
00:24:51,920 --> 00:25:00,770
And that's why, again, when you when you came and told me that my objective is inference, I said, you know, I do, starting with the hypothesis.

184
00:25:00,770 --> 00:25:06,379
And several of you actually came with a very nice hypothesis based on, you know, like your literature review.

185
00:25:06,380 --> 00:25:10,960
But, but then some people said, no, no, no, I want to do predictions.

186
00:25:10,970 --> 00:25:15,670
And I said, okay. Go free.

187
00:25:15,730 --> 00:25:22,000
I mean, you know. Okay. So no model selection for prediction, the big picture.

188
00:25:22,750 --> 00:25:30,610
So I'm glad that I know like we spent almost 25 minutes kind of on this philosophical discussion.

189
00:25:30,610 --> 00:25:34,210
But this is this is very, very important. Very important.

190
00:25:38,050 --> 00:25:46,990
So now we will basically shift gears and we will talk about model selection for prediction.

191
00:25:47,440 --> 00:25:50,230
And here is the big picture.

192
00:25:50,920 --> 00:26:00,159
So the modern selection methods, there have been many strategies that have been proposed for model selection for the purpose of prediction.

193
00:26:00,160 --> 00:26:07,030
And generally each of the methods consists of the same essential steps.

194
00:26:09,230 --> 00:26:13,300
So the first is to specify a maximum slothful model.

195
00:26:13,750 --> 00:26:20,260
So you construct a list of every possible covariate which could be included for a list of all the candidate.

196
00:26:20,260 --> 00:26:26,350
Covariates needs to specify criterion for comparing competing models.

197
00:26:29,500 --> 00:26:34,570
The third step is, and there are several possibilities,

198
00:26:35,920 --> 00:26:47,950
and the criteria criteria would also involve a choosing the, you know, like the training data and the test data,

199
00:26:47,950 --> 00:26:57,460
how well the model fits the current data, the training data, and how well the model predicts future responses and which is the test data.

200
00:26:57,790 --> 00:27:01,720
So questions like that will also come into play.

201
00:27:03,340 --> 00:27:08,919
And so these are the new words that you learn. Training, data test, data training.

202
00:27:08,920 --> 00:27:11,800
It is the data on which you build your model.

203
00:27:12,190 --> 00:27:19,360
And test data is the data that was not included in building the model, but for testing how well your model predicts.

204
00:27:20,410 --> 00:27:31,270
Then the third step is to select a sequence of models best models where best is defined by some criteria in two,

205
00:27:32,320 --> 00:27:37,780
in step two, and this list of sequence of models,

206
00:27:38,140 --> 00:27:48,879
you can sort of construct algorithms, you can look at all possible subsets, regression, and then you can look at automated selection procedures.

207
00:27:48,880 --> 00:27:53,200
We talk about that forward, backward and hybrid selection procedures.

208
00:27:55,150 --> 00:28:03,220
And then the last step is, once you have selected, quote unquote, your best models for prediction purposes,

209
00:28:03,580 --> 00:28:14,740
then you want to estimate the error based on a test dataset and you want to validate that model.

210
00:28:15,700 --> 00:28:33,540
So the accuracy of the prediction when we apply this best model to an external dataset, and that external dataset would be independent test data,

211
00:28:33,550 --> 00:28:45,940
or there are very clever ways of using your training data to actually construct the test dataset, and that's called cross validation.

212
00:28:46,660 --> 00:28:51,580
Okay. And so these are the four steps.

213
00:28:52,450 --> 00:28:55,680
So the first step is specifying the maximum model.

214
00:28:56,820 --> 00:29:06,490
And the maximum model is basically where do you have all the poor, which are candidates for inclusion in the final model?

215
00:29:07,060 --> 00:29:16,420
And so here you essentially use some some kind of common sense, some statistical and some subject matter knowledge.

216
00:29:17,590 --> 00:29:28,510
Then when deciding on candidates for inclusion in the final model, which which candidates are which variables are going to be inputs.

217
00:29:29,290 --> 00:29:35,559
And at this stage, it's necessary to inspect the data carefully before choosing any candidate coordinates.

218
00:29:35,560 --> 00:29:43,299
For example, the covariance must be scored on a meaningful scale, will be it must have sufficient variability.

219
00:29:43,300 --> 00:29:50,740
So, you know, kind of don't not variables, categorical variables where you have four categories,

220
00:29:50,740 --> 00:29:57,670
but essentially a 90% and 5% of the data is in one category.

221
00:29:57,670 --> 00:30:07,700
So like really sparse data set. Like we give an example of race ethnicity as a variable with African-Americans.

222
00:30:07,750 --> 00:30:13,510
Composition of Asian Americans. Asian and Pacific Islander and Hispanic other.

223
00:30:14,110 --> 00:30:20,080
You are doing a study in a population of predominantly white populations?

224
00:30:20,170 --> 00:30:27,680
Of course. I mean, it's going to be very sparse in data with respect to race.

225
00:30:27,730 --> 00:30:37,270
So so you have to have openness that will have sufficient variability and then nobody should be highly predictive by the remaining covariates.

226
00:30:37,900 --> 00:30:48,550
So, you know, vice is you have to be aware of beware of potential high value variables.

227
00:30:49,060 --> 00:30:56,920
The size of the maximum would be. And again, these are practical limitations that you can enforce.

228
00:30:57,730 --> 00:31:02,770
You can understand them by sample size. Just simple, commonsense rules.

229
00:31:03,370 --> 00:31:09,250
I want at least five or more subjects bar parameter to be estimated.

230
00:31:09,760 --> 00:31:14,940
So you wouldn't you wouldn't fit.

231
00:31:15,100 --> 00:31:22,840
Try to fit the model with be equal to 30 on a sample size of an equal to 50.

232
00:31:23,650 --> 00:31:31,870
Right. So so those kind of practical considerations, limitations you can impose or you will impose.

233
00:31:32,530 --> 00:31:38,110
And statisticians and investigators should work closely together in constructing a maximum model.

234
00:31:38,120 --> 00:31:48,520
So this is more about kind of collaboration, common sense, imposing practical limitations and so on.

235
00:31:50,760 --> 00:31:54,660
The next step. As you can see, we skipped step two.

236
00:31:54,870 --> 00:32:00,260
We will talk about skip step two in detail. But let's talk about step three first.

237
00:32:00,270 --> 00:32:03,209
So select model given up criteria.

238
00:32:03,210 --> 00:32:16,140
We go back and talk about what these criteria are, but let's skip one step and go to step three select model given a criterion.

239
00:32:16,140 --> 00:32:27,090
So we will introduce the approach to sticking P value as a criterion, and selection by P value is a very commonly used and traditional method,

240
00:32:29,220 --> 00:32:36,240
but we will introduce other criteria in detail later, and that's what step two is.

241
00:32:37,170 --> 00:32:48,420
So the so the first. So if you have peak obedience, so there are how many possible models there are, if they're not people, really, it's.

242
00:32:51,990 --> 00:32:59,040
There are possibly two to the power B minus one moles that you could fit.

243
00:33:01,510 --> 00:33:06,030
Well as all possible regression to consider.

244
00:33:09,480 --> 00:33:26,490
So the all possible subsets regression basically pretty much fits all possible models and the and if and from each of those models.

245
00:33:26,490 --> 00:33:35,940
So when I say all possible regression, all possible models, I mean like so if you have pre predictor so you could have models that have only one,

246
00:33:37,080 --> 00:33:48,510
you could have models that have two, you could have models that can three and so on up to a model that has all three.

247
00:33:49,650 --> 00:33:53,520
Right? So that's why it's stupid about B minus one model.

248
00:33:53,530 --> 00:34:02,250
Then the all possible subsets regression literally does that constructs all the to the part B minus one models.

249
00:34:03,060 --> 00:34:12,410
And based on the criterion that you have chosen in step two for comparing models, uh,

250
00:34:13,470 --> 00:34:20,480
the final model is the one that is judged to be the best based on that basic idea.

251
00:34:21,840 --> 00:34:26,940
And we'll talk about, you know, this criterion in step. But that's the that's the idea.

252
00:34:28,590 --> 00:34:32,580
So that's what all possible regressions means.

253
00:34:33,900 --> 00:34:42,030
But, you know, this is you want to avoid fitting and comparing all possible models because this is like a brute force method.

254
00:34:42,030 --> 00:34:46,710
It's kind of a unintelligent method.

255
00:34:47,160 --> 00:35:04,830
Right. You are you're kind of putting the the onus completely on the on the on the software, not the user as an intelligent analyst and so on.

256
00:35:05,910 --> 00:35:13,889
And so it set out what would be more desirable is to apply computationally efficient selection methods.

257
00:35:13,890 --> 00:35:19,260
So this is more like a kind of a, as I said, an intelligent method.

258
00:35:22,740 --> 00:35:29,459
So what are some, some other alternatives like?

259
00:35:29,460 --> 00:35:33,210
But we can get computationally efficient selection methods.

260
00:35:34,470 --> 00:35:45,720
So there are three classical variations on this key and these are forward selection, backward elimination and stepwise regression.

261
00:35:46,470 --> 00:35:51,030
There are also other methods, some which are beyond the scope of 650.

262
00:35:51,030 --> 00:35:51,780
For example,

263
00:35:51,780 --> 00:36:03,899
we have last for list absolute shrinkage and selection operator of available selection and the regular regularization are done simultaneous liberties

264
00:36:03,900 --> 00:36:17,700
are yeah we are not going to talk about less so here we are going to talk about these three variations on the and the team of automated selection.

265
00:36:19,020 --> 00:36:29,219
So what is forward selection and the first algorithm that we talk about in forward selection and we pre specify

266
00:36:29,220 --> 00:36:39,720
a P-value threshold for inclusion and that is denoted by P subscript by I stands for inclusion for example,

267
00:36:39,720 --> 00:36:45,210
we can set it at .05. And what is the basic idea or the basic concept?

268
00:36:45,690 --> 00:36:51,030
You start with the null model. The non model only has the intercept.

269
00:36:51,690 --> 00:36:59,760
That's my m zero model. And at each step you add the most significant covariate provided.

270
00:37:00,210 --> 00:37:04,480
The P value for the covariant is less than b i.

271
00:37:06,840 --> 00:37:11,050
Okay. So the cooperate with the lawyers.

272
00:37:11,070 --> 00:37:22,680
So you look at every other covidiot, every covidiot among those P and the one that has the smallest P value gets entered into the model.

273
00:37:26,160 --> 00:37:36,660
So in the lesson in step two or in in one model, then you'll have the intercept and one copy that had the lowest p value.

274
00:37:37,230 --> 00:37:50,130
Right. What happens in the next step? Now you look at all the remaining p minus one covariance and you see which one has the lowest p value.

275
00:37:50,580 --> 00:37:58,020
Given you have a model with the intercept and the first variable that you selected.

276
00:38:00,490 --> 00:38:05,080
And choose that variable as the second one to enter and so on.

277
00:38:05,080 --> 00:38:20,680
You continue the the process and you only terminate when no more covariates meet the binary of inclusion or all the coordinates have been added.

278
00:38:23,680 --> 00:38:30,410
Is this awesome? Like this? Is this. Conditions like this are one condition based.

279
00:38:31,380 --> 00:38:34,530
Various bad ones. Yeah.

280
00:38:34,950 --> 00:38:40,560
So step wise, is, is, is a more economical than forward selection.

281
00:38:40,980 --> 00:38:46,050
It is upon it is conditional because conditional on the fourth variable meaning the model.

282
00:38:46,470 --> 00:38:51,720
What is among the p minus one for vineyard slept which one has the smallest p value.

283
00:38:51,720 --> 00:38:54,629
So from that respect its condition. But it's not stepwise.

284
00:38:54,630 --> 00:39:08,130
Stepwise is we can be stepwise so that forward selection you are adding one variable at a time and and you continue

285
00:39:08,130 --> 00:39:16,070
the process until there are no more covidiots left that meet the pyramid or all candidates have been added.

286
00:39:16,080 --> 00:39:21,300
Yes. Geez.

287
00:39:29,840 --> 00:39:38,970
To the various. Yes.

288
00:39:39,550 --> 00:39:43,620
So from that respect, it's conditional, as Jack was pointing out.

289
00:39:45,090 --> 00:39:55,650
Okay. So now the next one is kind of a reverse of that backward elimination.

290
00:39:57,180 --> 00:40:05,129
In backward elimination. You do that opposite. You start with the full model in forward selection.

291
00:40:05,130 --> 00:40:08,520
You start with the non model in backward elimination.

292
00:40:08,820 --> 00:40:13,410
You start with the full model, the maximum model.

293
00:40:13,530 --> 00:40:19,320
And the maximum model is basically emby, which is all the pico mediates.

294
00:40:20,280 --> 00:40:31,920
Now you put you specify your p value for all meeting, for kicking out variables and that is B or for example, it would fix it at 1 to 5.

295
00:40:32,160 --> 00:40:38,850
And at each step what you are doing is basically you are eliminating the least significant covariate provided.

296
00:40:39,240 --> 00:40:43,080
B value for that covariate is greater than B0.

297
00:40:46,060 --> 00:40:54,160
And same sort of logic. But now you started with the Mexican model and you are kicking out one variable at a time.

298
00:40:55,600 --> 00:41:09,310
So the corporate with the highest p value sides that BS greater than b all gets removed too up in the model at each step.

299
00:41:12,000 --> 00:41:18,510
And the procedure terminates when no comedy meets the pure rule.

300
00:41:19,800 --> 00:41:29,940
In other words, be greater than your SATs or all covariates are significant in the sense that the p value is less than B or so.

301
00:41:29,940 --> 00:41:33,150
There are no more variables that can be kicked up. Yes.

302
00:41:33,420 --> 00:41:44,040
So it's rational for one another for selection. So instead of like instead of playing it as a way to do is once you picked a variable

303
00:41:44,340 --> 00:41:50,160
and you like to repress all of the variables on its residual residuals on the outcome.

304
00:41:53,230 --> 00:42:00,410
I am not sure I understand your question. So let's say you start with the model and you have a go there.

305
00:42:00,840 --> 00:42:04,080
Yeah. In the future with the new one up the next step.

306
00:42:04,100 --> 00:42:13,910
Is it possible for you to progress the outcome against this selected variant, obtain its residuals, and then use its residuals for.

307
00:42:15,180 --> 00:42:20,570
On the. GROSS Is that is that equivalent to the.

308
00:42:28,660 --> 00:42:32,260
It would be another mechanism of forward selection,

309
00:42:33,220 --> 00:42:42,880
but I would be thinking a little bit as to whether it will like for all scenarios, we could give you the exact same result.

310
00:42:43,150 --> 00:42:51,280
It would be another mechanism because remember now we are doing the forward selection in a feedback loop we can for the idea of seeing your

311
00:42:51,280 --> 00:43:04,090
distress equalizing and I think they will be equivalent within one would think a little bit more because remember what gets in to the model next.

312
00:43:04,660 --> 00:43:17,500
We are trying to say that in the presence of that variable, that first and third, which has the smallest p value among the set of B minus one.

313
00:43:17,770 --> 00:43:22,120
So that is the only part I have to but but,

314
00:43:22,600 --> 00:43:30,250
but I agree with you that you are loading and that will give you a mechanism for bringing forward some action.

315
00:43:34,990 --> 00:43:50,840
For four years. That.

316
00:43:50,840 --> 00:43:59,750
That's why that's why we had that philosophical discussion about, you know, this is this is this is open ended.

317
00:44:00,350 --> 00:44:04,490
No, you are not doing anything else, but.

318
00:44:08,760 --> 00:44:17,140
No, no, no. That's why I said you don't kiss. That's the basic difference between inference versus prediction.

319
00:44:17,210 --> 00:44:21,540
No, it's a very, very automated method.

320
00:44:26,920 --> 00:44:30,490
Yeah. Because 14 friends who won big had to be unbiased.

321
00:44:31,180 --> 00:44:41,760
For prediction, Rita head would be biased and you would still get the only the basic premises they would get.

322
00:44:41,770 --> 00:44:46,090
We were trying to get estimates with lowest variance.

323
00:44:47,530 --> 00:44:53,019
So again, it's a bias variance tradeoff done something means better.

324
00:44:53,020 --> 00:44:56,950
So no, no, you don't know. You don't do any anything.

325
00:44:59,920 --> 00:45:06,040
This. I will come to that.

326
00:45:08,020 --> 00:45:11,860
But but right now it's only based on the P-value. Okay.

327
00:45:13,270 --> 00:45:21,190
So that's backward elimination. And the third one.

328
00:45:21,520 --> 00:45:27,909
The third automated algorithm is called stepwise regression.

329
00:45:27,910 --> 00:45:36,070
It's a more much more economical algorithm than either the forward or the backward.

330
00:45:37,900 --> 00:45:51,910
And the idea here is you still begin with the null model, only the intercept, but you proceed as in forward selection.

331
00:45:52,630 --> 00:45:56,980
So you specify A, B and a B or.

332
00:45:57,970 --> 00:46:03,070
The only requirement is that the P all has to be greater than equal to VI.

333
00:46:03,340 --> 00:46:12,730
But the basic idea is that each step you add the most significant already provided B is less than VII,

334
00:46:12,940 --> 00:46:18,100
but you also have the proposition that you can kick out a body.

335
00:46:18,210 --> 00:46:26,320
So you kind of do at each step, you can simultaneously add one and pick one up.

336
00:46:28,330 --> 00:46:36,460
If there is a variable that meets the criterion of omission based on P greater than B old.

337
00:46:37,360 --> 00:46:42,870
So it's it's more economical from that step than the one forward step.

338
00:46:42,880 --> 00:46:51,270
But there is also a potential of a backward step. And the procedure terminates when normal covidiots are added or deleted.

339
00:46:51,280 --> 00:46:54,100
So the idea is after adding each new variable,

340
00:46:54,400 --> 00:47:02,860
you seek to remove one variable that no longer provides an improvement in the model fit in the presence of the added variable.

341
00:47:05,020 --> 00:47:15,730
So in other words, it's, it's in some way it's essentially forward selection, but with the provision of a possible deletion.

342
00:47:17,440 --> 00:47:21,760
And it more closely mimics all possible regression.

343
00:47:22,180 --> 00:47:29,800
But now we have a computational advantage because it's a more efficient algorithm computationally.

344
00:47:31,690 --> 00:47:35,950
You are kind of reaching the quote unquote, optimum faster.

345
00:47:40,930 --> 00:47:47,560
The opinion is quite varied regarding the appropriate feel, the kick out, the omission fee.

346
00:47:48,190 --> 00:47:54,820
The only requirement, as I mentioned, is that fuel has to be greater than equal to VII.

347
00:47:55,540 --> 00:48:06,610
And the reason for that is that, especially in the earlier stages, that because the statistic involves Sigma Head Square in the current model,

348
00:48:07,390 --> 00:48:13,710
if there are too few covariates in the model, Sigma Head Square may be an overestimate.

349
00:48:13,720 --> 00:48:23,590
We know that. So consequently, the P values may be inflated and may artificially exceed the feel.

350
00:48:23,590 --> 00:48:28,030
So that's the reason we you we impose that restriction.

351
00:48:28,300 --> 00:48:32,680
But the opinion is quite varied regarding the appropriate bill.

352
00:48:32,680 --> 00:48:37,389
Introduce a point one or 2.25 and VII.

353
00:48:37,390 --> 00:48:44,230
The inclusion is often or more commonly said to the 5% level of significance.

354
00:48:46,230 --> 00:48:50,390
So that's the there's three automated algorithms.

355
00:48:50,400 --> 00:49:01,050
There are limitations. This is a greedy search and greedy in the in the sense, you know, like.

356
00:49:02,850 --> 00:49:08,280
And not necessarily optimal in the sense that you were mentioning.

357
00:49:08,280 --> 00:49:21,630
Like I basically don't do anything as. It's so it's it's a it's more of a mechanistic only the focus is on the people and

358
00:49:21,640 --> 00:49:28,780
the I and this sometimes will be it's may be significant in the presence of others,

359
00:49:28,780 --> 00:49:38,560
but there is no provision to incorporate that. It's very automated in the way it's in selecting the variables for inclusion or deletion.

360
00:49:39,430 --> 00:49:46,460
I cannot say no know I have to keep this in the model that this altered these methods.

361
00:49:46,480 --> 00:49:47,590
Did you have a question. Yeah.

362
00:49:48,670 --> 00:49:59,080
So how many presented you can this therapy infinite number of variants you Geller you can on but as I said you can impose practicality.

363
00:49:59,110 --> 00:50:01,870
This is the first step. You impose practical limitations.

364
00:50:01,870 --> 00:50:15,850
No I actually have and equal greater than equal to five bar for you can use practical limitations like that even before coming to this step.

365
00:50:16,880 --> 00:50:22,300
Um, there are other methods again that are beyond the scope of things.

366
00:50:22,300 --> 00:50:25,120
We think there's a very good selection.

367
00:50:25,120 --> 00:50:30,810
You know, I don't know how many of you have heard about principle component analysis like dimension reduction type of.

368
00:50:33,280 --> 00:50:37,810
So we are not going to talk about those because those are.

369
00:50:38,860 --> 00:50:45,969
How many of you have heard about dimension reduction? It's a principal component of the service.

370
00:50:45,970 --> 00:50:54,880
And so those are traditionally part of a multibillion dollar business schools or high dimensional data like force.

371
00:50:55,630 --> 00:51:04,990
But here are the premises we use. We use practical limitations in sort of selecting candidates that use those.

372
00:51:05,410 --> 00:51:12,700
And maybe some preprocessing has already been done in in providing you a list of candidates that equals.

373
00:51:16,350 --> 00:51:20,100
These tend to break down when B is very large.

374
00:51:20,790 --> 00:51:24,449
For example, if P is greater than N, you can do backward elimination.

375
00:51:24,450 --> 00:51:32,490
And this comes to your question and see that if B is very, very large for Blagden.

376
00:51:32,670 --> 00:51:39,720
So you have to do some kind of a dimension reduction before you sort of come to this step.

377
00:51:41,820 --> 00:51:46,830
My by calling it a large Phillips B produced questionable B values.

378
00:51:47,610 --> 00:51:53,459
And the other reason these are I mean,

379
00:51:53,460 --> 00:52:01,620
I necessarily don't like these alternative selection procedures is because, I mean, they can be used as a guide.

380
00:52:02,460 --> 00:52:14,580
But basically the F tests are problematic both during and after selection because the F tests in forward

381
00:52:14,580 --> 00:52:19,590
selection will be conservative for the early stages because there are very few variables in the model.

382
00:52:19,980 --> 00:52:31,530
So the Sigma hat, where of course will be large and in the denominator is in the F statistic,

383
00:52:32,370 --> 00:52:37,860
is not going to be the sigma head squared from the optimal model of the full model.

384
00:52:38,190 --> 00:52:41,670
And this is especially true in the early stages of formal selection.

385
00:52:42,450 --> 00:52:49,190
There is multiple comparison problem during selection because you are asking the same dataset over and over again.

386
00:52:49,200 --> 00:52:55,110
Too many questions, too many hypotheses, and you cannot get a simple p value simply by chance.

387
00:52:55,950 --> 00:52:58,890
I'm going to get a small p value simply by chance.

388
00:52:59,130 --> 00:53:06,080
And then the third point, which is my biggest problem, and which is where I'm going to mention this, which is where,

389
00:53:06,090 --> 00:53:14,310
again, if you came to me for projects related questions that I asked you, which you did focus and don't mix up,

390
00:53:15,030 --> 00:53:21,630
is you use this automated selection procedure and then the post selection inference,

391
00:53:22,200 --> 00:53:33,840
the F tests in the final model are not valid because the predicted results for the FSA a view that B is fixed.

392
00:53:34,440 --> 00:53:40,170
But with the automated selection, we are in some way also estimating the P.

393
00:53:44,250 --> 00:54:01,110
The number of predictors. So this is my biggest reason why I, I mean, why I don't necessarily subscribe to these automatic selection procedures,

394
00:54:01,920 --> 00:54:08,070
especially if you are in the inference sort of.

395
00:54:10,780 --> 00:54:14,530
Okay. So that's.

396
00:54:19,570 --> 00:54:23,890
Implementation is easy, straightforward in SAS and our board.

397
00:54:24,460 --> 00:54:29,300
So that's not the issue. If you had a question, I guess another because another.

398
00:54:30,290 --> 00:54:38,760
Testing is that you are. Think like the best ones don't necessarily have a full gauge of what happens.

399
00:54:39,530 --> 00:54:47,939
Yeah. Yeah, that that is exactly that point. That is the point that here I was making for forward selection earlier.

400
00:54:47,940 --> 00:54:55,530
But that that is what carries. So the FBC is not valued in the final model.

401
00:54:58,590 --> 00:55:03,750
So I am going to quickly go through this example and then we are going to take a break.

402
00:55:03,780 --> 00:55:12,060
This is a study that was carried out for 31 adult males in order to develop a prediction model for oxygen uptake.

403
00:55:14,340 --> 00:55:23,010
The study subjects ran one mile on a treadmill with an oxygen mask and heart monitors strapped to them and data recorded on age,

404
00:55:23,010 --> 00:55:28,440
weight, time to run, one mile resting balls, maximum balls and balls at the end of the run.

405
00:55:28,890 --> 00:55:32,670
So there are one, two, three, four, five, six, four radiates.

406
00:55:34,230 --> 00:55:44,220
And and we want to sort of do a prediction model.

407
00:55:44,550 --> 00:55:53,390
So first is we wanted to see if there are any linearity issues, is any corrective action required?

408
00:55:53,400 --> 00:55:59,790
So these are the vials from 46 covidiots.

409
00:55:59,790 --> 00:56:03,600
And, you know, like they all meet the value of less than ten criteria.

410
00:56:03,690 --> 00:56:11,339
No ball ending balls, maximum balls have a whiff of 8158176.

411
00:56:11,340 --> 00:56:24,479
But again, you know, I'm not going to sort of this team are below the threshold then generate a potential model using forward

412
00:56:24,480 --> 00:56:30,850
selection with this is more of something that you do hands on but I still wanted to show this example.

413
00:56:30,930 --> 00:56:40,140
API equals 2.15 value for entry and here is the model based on forward selection.

414
00:56:40,530 --> 00:56:49,350
So the first variable to enter the model listed on and then it's then ending bars maximum paths.

415
00:56:49,380 --> 00:56:53,820
Now I'm going to ask you this question before going to the next slide.

416
00:56:54,120 --> 00:57:00,180
Suppose I made the API for entry equal 2.25.

417
00:57:00,690 --> 00:57:04,679
Can you tell me how would your results compare with this model?

418
00:57:04,680 --> 00:57:12,640
Would it be a smaller model or a bigger one? Smaller model.

419
00:57:13,660 --> 00:57:17,410
Right. If the p value for entry is.

420
00:57:19,730 --> 00:57:23,230
Want to take you. Exactly.

421
00:57:23,670 --> 00:57:27,770
So the picture. Right. So I would have a smaller model.

422
00:57:28,340 --> 00:57:34,370
And yes, of course, the only one that made this criterion is done.

423
00:57:34,580 --> 00:57:40,340
So I would stop here. Okay.

424
00:57:40,730 --> 00:57:46,120
Now we select a model to back for the elimination with below equal to point one.

425
00:57:46,130 --> 00:57:51,230
So here is the kick out probability. So recall now in backward elimination.

426
00:57:51,230 --> 00:57:58,250
We start with the free model, the maximum model which is which is all the six covariates and we kick out one variable at a time.

427
00:57:59,390 --> 00:58:04,640
So the two variables we kick out are unstressed and first.

428
00:58:04,640 --> 00:58:17,820
And then we. Interestingly, in this dataset as you can see the qualitatively with be equal 2.1 and B are equal 2.1.

429
00:58:17,820 --> 00:58:30,300
You get equivalent models correct in terms of like which coordinates are there because for API equal 2.1, the other four were chosen.

430
00:58:31,290 --> 00:58:36,359
Okay, so here is b b all equal 2.1, that model.

431
00:58:36,360 --> 00:58:41,759
Now if I'd done the backward selection, but now would be all equal.

432
00:58:41,760 --> 00:58:47,640
2.5 Can you tell me how your results would compare with this model?

433
00:58:47,970 --> 00:58:51,720
Would it be a smaller or a bigger model?

434
00:58:53,730 --> 00:59:07,260
Or maybe I should pose the question as when I say smaller, I mean like so this model, because we kicked out two variables has four covariance.

435
00:59:07,770 --> 00:59:17,280
So now if I use feel equal 2.5 and do a backward elimination, would I kick out more or less?

436
00:59:25,150 --> 00:59:30,490
For retirement. 4.5. But I one to point.

437
00:59:31,110 --> 00:59:35,970
Exactly. So I kick out less so. Meaning I would have a bigger money.

438
00:59:37,680 --> 00:59:43,890
Correct. And that's exactly what happens. I only kick out resting balls.

439
00:59:44,130 --> 00:59:47,700
So the model that I'm left with has all the other five.

440
00:59:48,300 --> 01:00:01,020
So that's mine. Oh, and then, Gabby, I can categorize it just for illustration into the these two categories.

441
01:00:01,290 --> 01:00:08,730
Here are the frequencies. Now, with new AIDS, if I select the model two forward selection.

442
01:00:09,840 --> 01:00:13,979
So I still get this beat on AIDS.

443
01:00:13,980 --> 01:00:26,760
Balls and balls, Max. So just a very, you know, short example, an automatic example.

444
01:00:27,990 --> 01:00:38,220
So in closing, I want to make a few remarks and then we will go to the next step and maybe take a break and go to the next step.

445
01:00:38,790 --> 01:00:47,310
So the automated model selection algorithms are useful when the prior hypotheses are not strong.

446
01:00:49,320 --> 01:00:54,030
In other words, we have a fishing expedition is warranted.

447
01:00:58,460 --> 01:01:01,490
Like everybody with me. So I'm repeating it.

448
01:01:01,850 --> 01:01:10,339
Automated model selection algorithms can be useful when mostly when you don't have much prior information.

449
01:01:10,340 --> 01:01:14,600
Prior hypotheses are not strong and you want to go fishing expedition.

450
01:01:16,970 --> 01:01:20,270
The objective is prediction as opposed to estimation inference.

451
01:01:22,990 --> 01:01:30,310
And as they pointed out again, he said objectivity, estimation, inference, then the results.

452
01:01:31,180 --> 01:01:42,489
Based on that, you have to be careful because the results based on automated or coded selection would at most be viewed as hypothesis generating Y.

453
01:01:42,490 --> 01:01:52,240
Because, as we said, as we argued that the forced selection of statistics, the in the final model are not going to be valid.

454
01:01:54,780 --> 01:02:00,720
Okay. So with that, let's take a break.

455
01:02:00,900 --> 01:02:11,720
And so we have talked about step one and step three for the big picture on model selection for prediction.

456
01:02:11,740 --> 01:02:17,400
Now we will talk about criteria for comparing competing models.

457
01:02:17,430 --> 01:02:21,360
So steps two and step four.

458
01:02:22,560 --> 01:02:27,410
So here are the five criteria that we will talk about.

459
01:02:27,420 --> 01:02:33,180
Three of them you already know. And we are going to introduce a couple more.

460
01:02:35,190 --> 01:02:39,420
So let's take a break. It's 9 to 8. Let's come back at 918, please.

461
01:03:15,400 --> 01:03:22,320
Hello. That's a whole.

462
01:03:29,070 --> 01:03:33,530
Yeah. I mean I mean. But. But I just. So we groups coming.

463
01:03:34,200 --> 01:03:42,990
Oh. And I think I will follow them for a few days.

464
01:03:43,950 --> 01:03:50,470
Yes. He was like. We have a plus.

465
01:03:51,410 --> 01:03:52,880
So how many could be done?

466
01:03:53,270 --> 01:04:04,790
I mean, I've always wanted to see is this, you know, please at least like two or three people so that you can we can discuss it at the.

467
01:04:08,070 --> 01:04:12,350
I mean, do you think that you know.

468
01:04:12,990 --> 01:04:20,210
Yes. Oh, yeah. No, I this one. The reason I'm asking is all the other groups that they used to be on this.

469
01:04:20,310 --> 01:04:25,650
Obviously, it's not something I could have done. So that's why I don't like them.

470
01:04:25,950 --> 01:04:29,130
There isn't that good. Coming up, 1015.

471
01:04:29,340 --> 01:04:34,190
Detective Fluff. Oh. Oh, okay.

472
01:04:34,200 --> 01:04:38,590
Oh, it is. And one of them on.

473
01:04:42,570 --> 01:04:47,060
Yes. I think some of them would be.

474
01:04:48,710 --> 01:05:02,740
I mean, like. Bosom buddies. Maybe I could franchise a little bit on that and just just talk about, like, just cause.

475
01:05:04,360 --> 01:05:08,590
So. Yeah, that's fine. That's fine. But your group doesn't want to meet.

476
01:05:09,190 --> 01:05:14,231
No, no. They still want to have a house like the other two have done a gym class at 1030, and we.

477
01:05:15,720 --> 01:05:24,030
Okay. So I as I mentioned in the beginning, I've a few announcements.

478
01:05:25,290 --> 01:05:29,340
You, me, as you already noted on canvas.

479
01:05:29,370 --> 01:05:41,910
Exam two has been graded. You can collect your exams either from the data size or or from your mailbox, whichever he chooses.

480
01:05:42,090 --> 01:05:49,470
Please lethargy assessment. Here is a histogram of the exam to score.

481
01:05:52,200 --> 01:06:00,359
The total number of points was 85 and the median was 78.5.

482
01:06:00,360 --> 01:06:13,620
The mean was 73.6 and the first and third quartiles were 60.25 and 83.75.

483
01:06:13,620 --> 01:06:24,270
So generally a good job and solutions have been posted on canvas.

484
01:06:24,540 --> 01:06:32,280
Please review. As before, there isn't particularly any question that we taught.

485
01:06:33,540 --> 01:06:38,069
You know, sort of many people made mistakes or there were common mistakes.

486
01:06:38,070 --> 01:06:49,500
So, so, so nothing like a common phenomenon to discuss, but to go and take a look at the solutions.

487
01:06:49,500 --> 01:07:02,610
And if there are and compare it with your exam and service and if there are any questions, please reach out and talk to me.

488
01:07:05,130 --> 01:07:10,320
So that's the first thing that I wanted to mention about Exam two.

489
01:07:11,430 --> 01:07:25,650
The second thing is you will get your presentation order for the project again and there will be an announcement on canvas by tomorrow morning.

490
01:07:26,700 --> 01:07:34,200
And remember, next week, December 16th, we are doing the project presentations.

491
01:07:35,070 --> 01:07:41,130
Do keep in mind that you have to submit your slides by December 5th.

492
01:07:42,450 --> 01:07:48,660
I think we said 5 p.m. Go back and take a look at the instructions,

493
01:07:49,050 --> 01:08:00,740
but you have to upload your presentation slides by Monday and after that you cannot make a change to your slides.

494
01:08:03,750 --> 01:08:12,450
And as I mentioned, the presentation order you will receive via an announcement on canvas by tomorrow morning.

495
01:08:12,510 --> 01:08:16,620
So that's the second announcement I wanted to make.

496
01:08:18,120 --> 01:08:24,090
And then finally, I hope I was recording this.

497
01:08:26,040 --> 01:08:37,860
Yes, I am. Okay. So again, so those were about the exam two scores and the presentation and the slides.

498
01:08:38,310 --> 01:08:49,230
Those were the announcements I wanted to make. And finally, another request the teaching evaluation system is now on.

499
01:08:50,220 --> 01:08:56,129
Please, if you can take a few minutes and go and fill out the evaluation.

500
01:08:56,130 --> 01:09:00,480
So I would appreciate that so far.

501
01:09:02,070 --> 01:09:12,330
I know you all are very busy and of semester stuff, but I would appreciate if you could figure out the evaluations.

502
01:09:12,690 --> 01:09:26,810
Okay. So with that, these are all the announcements I want to make and now we are going to pick up from where we left them.

503
01:09:27,330 --> 01:09:38,100
So this is step two criteria for comparing the models and there are several measures by which to compare these different models.

504
01:09:39,420 --> 01:09:48,960
Our square adjusted our squared sigma heads, but our MSE, these three we already have talked about,

505
01:09:49,710 --> 01:09:54,720
you know, b this is the least preferable because of, you know,

506
01:09:55,020 --> 01:10:01,469
we all know that as you add more convenience that squared will increase the

507
01:10:01,470 --> 01:10:08,490
adjusted R squared and the MSE kind of put a penalty for more complicated models.

508
01:10:13,730 --> 01:10:19,400
So from that respect makes an.

509
01:10:23,860 --> 01:10:30,340
Takes into account the the number of variables in the morning.

510
01:10:30,450 --> 01:10:40,450
So I have made some compromise between parsimony and productiveness.

511
01:10:41,920 --> 01:10:48,550
The next two are new statistics that we are going to introduce.

512
01:10:49,480 --> 01:10:58,750
The first is called breasts and and the second is Mallows Sippy.

513
01:10:59,260 --> 01:11:04,420
And these are more about prediction for new observations a little more.

514
01:11:10,680 --> 01:11:19,010
Relevant. In the context of production.

515
01:11:21,350 --> 01:11:36,080
For the press, as you will see later, it sort of uses that lead one out of the nation and it's kind of a way of validating.

516
01:11:36,500 --> 01:11:40,819
It gives a better assessment of prediction and ability for new observations,

517
01:11:40,820 --> 01:11:51,920
and Malus c.P explicitly takes into account the bias versus variance of the of the predictions.

518
01:11:52,280 --> 01:12:00,710
There are also other criteria called EIC API key information criterion and vision information criterion.

519
01:12:02,510 --> 01:12:06,589
We are not going to go into those in 650.

520
01:12:06,590 --> 01:12:11,360
You see that in 651. When you talk about generalizing in models.

521
01:12:12,290 --> 01:12:17,089
But there are. I just want you to be aware that there are other information criteria.

522
01:12:17,090 --> 01:12:22,330
We are not going to formally define those in 650. So all of these five.

523
01:12:23,360 --> 01:12:27,670
Each of these have their advantages and disadvantages.

524
01:12:29,210 --> 01:12:32,300
So let's kind of look at it one more time.

525
01:12:32,720 --> 01:12:37,760
So Oscar, we know, is the easiest, simplest.

526
01:12:38,570 --> 01:12:48,440
It's the correlation between why habit and why squared of that split of the Pearson correlation coefficient.

527
01:12:49,130 --> 01:12:52,250
And it is not decreasing and covariates and added.

528
01:12:52,610 --> 01:12:59,930
The interpretation is very straightforward. It's the percent of variance of the outcome that is explained by the predictors.

529
01:13:01,010 --> 01:13:11,030
So from that respect, it's, it's, it's the simplest, easiest, but it has this disadvantage that it will it is not decreasing.

530
01:13:11,510 --> 01:13:16,160
Mm hmm. So the more covariate to add article will increase.

531
01:13:16,640 --> 01:13:21,050
Mm hmm. So it doesn't sort of, like, get just for parsimony.

532
01:13:23,000 --> 01:13:27,590
Um, from that respect, it's the least attractive criterion.

533
01:13:29,210 --> 01:13:33,349
What about the least objective criterion?

534
01:13:33,350 --> 01:13:38,360
Again, because it will tend to always choose the biggest possible model.

535
01:13:41,180 --> 01:13:50,180
So the next one is the adjusted R-squared we have talked about maybe puts a penalty on that and

536
01:13:50,180 --> 01:13:57,020
the number of poor mediates added and it can increase or decrease when covariates are added.

537
01:13:58,820 --> 01:14:03,139
And it is sort of similar from that respect.

538
01:14:03,140 --> 01:14:11,270
Model selection based on adjusted output would be equivalent to model selection based on MSE or sigma squared,

539
01:14:11,270 --> 01:14:18,680
because both of these, both of the adjusted output and Sigma had square kind of put a penalty on larger models.

540
01:14:21,380 --> 01:14:34,370
Sigma had squared or the MSE will tend to decrease when we include important covidiots,

541
01:14:34,880 --> 01:14:45,140
which it should do, but it will tend to increase when we include unimportant covariance.

542
01:14:46,190 --> 01:14:56,030
So again, bottom line is all that tested output and the sigma had squared and MSE puts a penalty in larger models.

543
01:14:56,360 --> 01:15:08,180
And so from that respect, they are both more attractive than the unadjusted r squared, which is sort of naive to to model complexity.

544
01:15:09,860 --> 01:15:21,500
Okay. So, so these two adjusted R squared and sigma squared are kind of well accepted criterion.

545
01:15:29,360 --> 01:15:39,400
From that perspective. What about the new tools did this?

546
01:15:39,610 --> 01:15:44,990
So Osgood assisted us as he might have cut all our functions of the residuals.

547
01:15:45,010 --> 01:15:49,830
We saw this. Of the Epsilon heads.

548
01:15:50,370 --> 01:15:54,300
So it reflects how well the model fits the current data.

549
01:15:55,260 --> 01:16:01,990
The training error. Because they are based on the residuals.

550
01:16:03,730 --> 01:16:09,520
So why they reflect how well the model fits the current data,

551
01:16:09,520 --> 01:16:18,610
but they don't tell us anything about how well the model fits future data, future responses.

552
01:16:21,120 --> 01:16:25,770
Everybody sees that it's basically based on the training data.

553
01:16:26,340 --> 01:16:37,770
So it reflects in some way the training error, but doesn't say anything about how well the model might do on future responses.

554
01:16:39,570 --> 01:16:57,420
Did you have a question? Yeah.

555
01:16:57,510 --> 01:17:01,920
Basically decrease is not upset by corresponding loss of freedom.

556
01:17:01,990 --> 01:17:14,700
B essentially means what I explain in words that if you add important obedience, then MSE will decrease.

557
01:17:16,910 --> 01:17:23,590
But if you add unimportant variables, then what will happen to MSE?

558
01:17:24,320 --> 01:17:33,230
MSE will increase. So just by adding more variance, it doesn't decrease mse.

559
01:17:33,410 --> 01:17:38,270
It will depend on whether this pull we need to do or are adding are important or unimportant.

560
01:17:39,680 --> 01:17:45,890
But as our squared will increase, no matter how bad it could be, it's an important or unimportant.

561
01:17:49,120 --> 01:17:54,579
Okay. And the same thing just did after. So from that respect, I just did.

562
01:17:54,580 --> 01:17:58,600
Our squid and sigma had squid are much more preferred criteria.

563
01:17:59,410 --> 01:18:13,210
But still, all these three, the first three, do not reflect how well the model fits, how well the model predicts future responses.

564
01:18:13,750 --> 01:18:27,310
So we need some kind of criterion that reflects how the model might fit or might predict future responses.

565
01:18:28,540 --> 01:18:38,379
And why is that important? Because you actually this was kind of like some way this was an exam question.

566
01:18:38,380 --> 01:18:45,790
And we also proved it in class that the residuals, the mea underestimate the crew errors.

567
01:18:47,680 --> 01:18:56,320
And so, in other words, it seems since the MSE is based on the residual sum of squares, of the residuals,

568
01:18:57,860 --> 01:19:06,940
that which is a reflection of the training set error, the training set error can be overly optimistic.

569
01:19:08,710 --> 01:19:18,819
There is another way that we call this. So we say that the training test, the training set error is not an honest estimate of prediction accuracy.

570
01:19:18,820 --> 01:19:24,990
Why not honest? Because it might be more optimistic if it underestimates the true.

571
01:19:26,500 --> 01:19:38,440
So that is C tools. We'd all underestimate the true errors and they will also underestimate the testers

572
01:19:38,440 --> 01:19:44,440
distress meaning on an external dataset that was not used to build the model.

573
01:19:45,010 --> 01:19:53,499
We don't know how the model is going to perform and most often are almost

574
01:19:53,500 --> 01:20:02,960
always the training set error is going to be more optimistic than the best set.

575
01:20:04,840 --> 01:20:19,250
So that's why we need some statistics that actually captured how well the model predicts future responses.

576
01:20:19,270 --> 01:20:31,690
Clearly, these three guys tell us how the model keeps the current data, but they cannot tell us anything about the future responses.

577
01:20:31,930 --> 01:20:37,030
So we need some new statistics for that. And what are those new statistics?

578
01:20:38,020 --> 01:20:49,360
So let's get to the two defining statistics that can tell us how well the model predicts future responses like a test error.

579
01:20:51,160 --> 01:21:01,540
Okay. So here are two statistics that and again, the concept is very simple.

580
01:21:01,890 --> 01:21:13,690
If you remember I had I said one time that most powerful ideas are actually inherently very simple.

581
01:21:14,020 --> 01:21:23,710
So this is where I introduce the prediction error residuals.

582
01:21:24,010 --> 01:21:33,370
So you guys, you all are familiar with that? With our with the externally standardized test, the live formal procedure, the jackknife procedure.

583
01:21:33,760 --> 01:21:46,600
So now let's define delete. And residuals are prediction error residuals says Epsilon hat without the value subject as

584
01:21:46,600 --> 01:21:57,460
observed value of Y for the right subject minus the predicted value of R for the R subject.

585
01:21:57,820 --> 01:22:01,450
When the model was built without diet subject.

586
01:22:03,890 --> 01:22:09,940
Isn't this a beautiful idea? So basically, what am I doing? I'm reusing my data every time.

587
01:22:09,950 --> 01:22:22,460
Conceptually, this is what is going on. Every time when I'm predicting for and for an observation for a subject I am actually building the model

588
01:22:22,460 --> 01:22:34,010
with everybody is other than that subject and now I'm using that model to predict for the Iot subject.

589
01:22:34,520 --> 01:22:40,010
So that's my why are you without the Iot subject?

590
01:22:40,730 --> 01:22:45,310
So this list of what would you do to turn this to the test? It looks a lot like an internal.

591
01:22:45,320 --> 01:22:50,470
So internally like you would think, oh well let's suppose.

592
01:22:51,570 --> 01:22:54,970
It looks like the external deficit.

593
01:22:55,850 --> 01:23:05,510
But the explanation looks a lot. It's I guess it could be.

594
01:23:06,100 --> 01:23:10,190
It wasn't exactly. We don't standardize it.

595
01:23:10,190 --> 01:23:13,510
Exactly. You are spot on. But you also.

596
01:23:14,990 --> 01:23:18,580
So to get yourself square. Yeah. Yeah.

597
01:23:18,630 --> 01:23:22,580
No, no. Yeah. So this is basically you can.

598
01:23:23,300 --> 01:23:26,600
And this this is the. But but I want you to.

599
01:23:27,260 --> 01:23:34,340
Don't get bogged down by the weeds. I want you to get the big picture, the concept.

600
01:23:35,450 --> 01:23:42,080
And that idea is so clever. I want to see kind of glittering eyes at that.

601
01:23:44,410 --> 01:23:51,040
But I mean, please don't get bogged down by the kind of the is there a square group?

602
01:23:51,310 --> 01:23:54,970
Is that do you see what is happening?

603
01:23:56,410 --> 01:24:03,489
Like I am, I have the same data set I don't have the luxury of and this is what happens a lot in practice.

604
01:24:03,490 --> 01:24:07,930
I don't have the luxury of an external data set of another independent.

605
01:24:07,930 --> 01:24:20,320
Those data I am using my the data at hand because it is subject, but each subject is being held out as a best data.

606
01:24:22,730 --> 01:24:26,360
And building the model on the remaining and minus one.

607
01:24:26,360 --> 01:24:33,860
And I'm pretty predicting for the I it's subject based on the model that I built using the N minus five subjects.

608
01:24:34,160 --> 01:24:43,940
So what a beautifully clever idea. This is also called the using the same sample and that's exactly what I do.

609
01:24:44,090 --> 01:24:48,319
So that is my y you had I without the I it subject.

610
01:24:48,320 --> 01:24:52,270
It's the predicted value for the subject from a fitted model with the I.

611
01:24:52,270 --> 01:24:55,550
The observation was left out. Not now.

612
01:24:55,700 --> 01:25:04,670
This is independent of y and this residual is also known as the press residual.

613
01:25:07,190 --> 01:25:22,290
Press. And it's denoted by press a subscript I that actually we were leaving out systematically

614
01:25:23,010 --> 01:25:29,270
you are removing each division pretend it is the future data it is in the future.

615
01:25:29,400 --> 01:25:39,660
Then you are measuring the performance of prediction. So instead of defining the R square and sigma squared based on the residuals there like that,

616
01:25:39,660 --> 01:25:45,990
you should also residuals you use this press interaction does.

617
01:25:48,210 --> 01:25:55,740
This epsilon had I without die its subject and gone struck counterparts off our

618
01:25:55,770 --> 01:26:02,670
square and MSI and those are referred to as threats and are spread spread.

619
01:26:05,080 --> 01:26:20,260
So you define the prediction sum of squares, the press statistic as the sum of squares of the press residuals, not the real residuals.

620
01:26:20,560 --> 01:26:24,190
So this is the sum of squares of.

621
01:26:26,680 --> 01:26:46,060
Press assiduous. And you can sure that this is algebraically welland the sum of I for one twin epsilon I had divided by one minus h by square.

622
01:26:47,740 --> 01:26:51,100
So you are not you don't necessarily have to repeat the model.

623
01:26:53,560 --> 01:27:03,100
And because of the way that I construct the residual it's account, it accounts for the high level of DJIA, but not high residuals.

624
01:27:03,100 --> 01:27:10,680
So it's sort of a an observation that that is our outlier doesn't have the ability to mask its outline.

625
01:27:10,720 --> 01:27:17,470
This for prediction purposes, it's reasonable to select the model with the lowest press.

626
01:27:19,060 --> 01:27:26,200
And another measure that we can construct based on the press residuals is the prediction R squared.

627
01:27:26,470 --> 01:27:37,030
The R squared spread, which is given by one minus instead of the SS are over ss y.

628
01:27:37,030 --> 01:27:40,540
Now I use one minus press over. Is this y?

629
01:27:40,540 --> 01:27:49,239
Because the press is like, you know, I thought so what,

630
01:27:49,240 --> 01:28:04,090
what percentage of the variation is the press statistic is like is this e except that it is based on the lead footnote or the press residual.

631
01:28:05,080 --> 01:28:08,219
Okay, so this is R squared spread one minus press.

632
01:28:08,220 --> 01:28:11,280
So what is this y and output?

633
01:28:11,290 --> 01:28:15,490
Brad would measure the ability to predict future responses.

634
01:28:16,930 --> 01:28:22,329
R squared and adjusted r squared relate to the model's ability to predict garden data.

635
01:28:22,330 --> 01:28:27,760
So this is the main distinction between these set so far scarce.

636
01:28:27,760 --> 01:28:31,420
Yes. So what does it mean? This is not necessary to release this model?

637
01:28:32,140 --> 01:28:36,850
Basically, that like conceptually, if you looked at Epsilon,

638
01:28:36,850 --> 01:28:46,660
I had either died of the reaction each time you are fitting in models because each time you are making one observation out of sample.

639
01:28:48,340 --> 01:28:53,720
So conceptually as if you are fitting in models but you don't have to.

640
01:28:54,040 --> 01:29:00,040
That's what it means by not necessary to defeat the model because algebraically you can show that

641
01:29:00,040 --> 01:29:06,249
this is equivalent to summation I from one when epsilon I had square over one minus j squared.

642
01:29:06,250 --> 01:29:11,190
So you get it from one regression 50. Okay.

643
01:29:13,530 --> 01:29:17,400
This is also known as Leap one out cross-validation.

644
01:29:19,320 --> 01:29:23,460
So in closing, here is a residual taxonomy.

645
01:29:23,910 --> 01:29:31,860
You know, there are these different sets of residuals, ordinary pressure, standardized, internally standardized, externally standardized.

646
01:29:32,430 --> 01:29:44,400
And again, this is just a summary table giving you the definition, the expressions and the approximate distributions.

647
01:29:46,910 --> 01:29:50,600
The last criterion we are going to talk about is called Mallow.

648
01:29:50,610 --> 01:30:03,890
C.P. And all I am going to mention here is that this is a statistic.

649
01:30:03,900 --> 01:30:09,530
This is a new statistic, once again relevant for the prediction context.

650
01:30:09,530 --> 01:30:15,350
More. This is a measure that combines.

651
01:30:16,460 --> 01:30:34,240
Statistics that combines. Bias and variance of the prediction.

652
01:30:49,400 --> 01:30:58,090
And so the total variation in the prediction of why that is due to bias plus Fabian.

653
01:30:58,110 --> 01:31:00,010
So it's this a,

654
01:31:00,130 --> 01:31:15,700
this gamma B is a statistic that combines the bias in billions and Mel C P is an estimate of this combined effect of bias and variance.

655
01:31:17,590 --> 01:31:23,110
And the PS upstream denotes a model with p parameters.

656
01:31:23,920 --> 01:31:27,370
So basically what, what, what we do, again,

657
01:31:27,370 --> 01:31:37,600
I'm not going to go into the technical details from the model that is being evaluated is that if you look at SFE,

658
01:31:38,110 --> 01:31:41,260
that would underestimate the test data.

659
01:31:41,770 --> 01:31:49,900
It estimates the training tested, but it would underestimate that estimate because it's not going to be an honest estimate.

660
01:31:49,920 --> 01:31:53,080
It's going to be overly optimistic based on the training set.

661
01:31:55,570 --> 01:32:09,430
So what matters if it does, it kind of strikes a balance between bias and variance and the the model that has the smallest c p, you select that model.

662
01:32:09,490 --> 01:32:29,860
So that's the basic criterion. So these all these criteria help us in comparing competing models and specifically C, B and press.

663
01:32:30,160 --> 01:32:41,500
We did not talk about the CBC are geared towards, you know, assessing how the model predicts, how well the model predicts future responses.

664
01:32:44,590 --> 01:32:49,050
A very small example there.

665
01:32:49,810 --> 01:33:00,250
See, for example, this is the true model Y is 1.1 plus point one times it's 1.2 times X two plus one,

666
01:33:00,250 --> 01:33:04,809
two, one or one times X plus one, four times it's called plus epsilon.

667
01:33:04,810 --> 01:33:10,000
So this is the true model, let's say, and I simulate data from this model.

668
01:33:13,240 --> 01:33:17,050
And now I feel that the.

669
01:33:17,060 --> 01:33:23,920
Q what with the simulated data and assumed that the true model is unknown, you want to select the best predictive model.

670
01:33:24,550 --> 01:33:28,430
So here are measures for three possible models.

671
01:33:29,770 --> 01:33:39,190
The first model has X, two and X for the second model has x2x4 and x one and the third model has x2x4, x one and x three.

672
01:33:41,380 --> 01:33:48,790
So the type B here are the five criteria. And you will do according to our expert who will choose the model with the highest R squared.

673
01:33:48,790 --> 01:33:57,790
Right? So no surprise that the model that has the one, all the four variables, you know, has the highest R squared.

674
01:33:58,120 --> 01:34:02,739
Not a surprise. We know that that's the one did what about sigma squared?

675
01:34:02,740 --> 01:34:06,520
He would choose the model that has the lowest sigma head square.

676
01:34:06,910 --> 01:34:12,370
So based on that he would choose the model x2x4x1.

677
01:34:12,850 --> 01:34:18,190
What about adjusted r squared? Once again, he would choose the model with the highest adjusted r squared.

678
01:34:19,000 --> 01:34:25,780
So this model here. What about press r squared r squared?

679
01:34:25,780 --> 01:34:33,250
Press up. Based on the best guess he does, he would choose the model with the highest r squared spread.

680
01:34:34,180 --> 01:34:39,670
It's on 3065. What about c b?

681
01:34:40,360 --> 01:34:43,450
He would choose the model with the smallest c p.

682
01:34:47,610 --> 01:34:58,740
Okay. So here is an example to show once again based on simulated data that even though x three is in the true model,

683
01:35:00,090 --> 01:35:05,970
its effect is not large enough to offset the cost to estimate it.

684
01:35:10,180 --> 01:35:22,000
So basically this is the model that would be chosen by all the four criteria except our square, which always chooses the maximum model.

685
01:35:25,730 --> 01:35:30,200
Okay. Model validation.

686
01:35:32,180 --> 01:35:43,850
This is the last step. Ultimately, we want to find the the ones we have chosen, the final model.

687
01:35:44,330 --> 01:35:52,460
We want to test the final model on an external data set or some newly collected independent dataset.

688
01:35:54,890 --> 01:36:06,140
Now, as I said, the best is if you have an external test data and all the samples you presented,

689
01:36:06,140 --> 01:36:11,450
the sample that was used to build the, build the model.

690
01:36:13,010 --> 01:36:21,140
So that's the best. But, uh, but oftentimes we don't have that luxury.

691
01:36:24,860 --> 01:36:34,520
And if we don't have that luxury, then an alternative that is very useful is called careful cross validation.

692
01:36:34,880 --> 01:36:42,950
So again, reusing your training data and here is the idea of careful cross validation.

693
01:36:42,950 --> 01:36:50,029
Once again, a very simple idea. So you have you don't have the luxury of an external test data.

694
01:36:50,030 --> 01:36:55,430
So you have this training data that you use to build your models.

695
01:36:55,730 --> 01:37:07,670
How do you now validate your model? Based on the data, I tend to sometimes people do even a simpler that I don't have you done this?

696
01:37:07,670 --> 01:37:10,790
Like even do a simple data splitting.

697
01:37:11,110 --> 01:37:23,090
So instead of growing the building, the model on the full training data right at the get go, they split the data into two third, one third.

698
01:37:23,870 --> 01:37:32,060
So two third of the data is used to train or build the model and the held data one third is used to test.

699
01:37:32,780 --> 01:37:35,150
So this is a very simple idea, data splitting.

700
01:37:37,450 --> 01:37:45,880
And you can develop the model with the training dataset and assess its predictive ability in the testing like the one third held out dataset.

701
01:37:46,760 --> 01:37:52,610
The disadvantage of this data splitting method is you will need a large sample size to start with to do that.

702
01:37:52,730 --> 01:37:56,990
The third one could split in eight if you don't have large sample size.

703
01:37:56,990 --> 01:38:02,140
That gave full price validation is a very clever and useful alternative.

704
01:38:02,150 --> 01:38:07,980
What do you do? You take the training data and you split and the industry standard.

705
01:38:08,000 --> 01:38:17,570
So I'm going to maybe describing as tenfold the industry standard is basically a ten fold cross-validation.

706
01:38:19,430 --> 01:38:33,620
Then full cross validation means what you split the data into ten parts randomly, randomly split the data in two key equally sized datasets.

707
01:38:34,430 --> 01:38:40,100
Gaze let's at ten here. You want me to digging for E today?

708
01:38:40,190 --> 01:38:47,690
12k you train the model or so for ten full cross-validation.

709
01:38:47,690 --> 01:38:55,079
What will happen is you train the you know p so you b1 b2 to beat in the first

710
01:38:55,080 --> 01:39:01,400
step you brought the model on B1 to be known and use DTM as your test data.

711
01:39:01,940 --> 01:39:13,600
In the second step, you use train the model on B2 to be dead and let's use that B1 as training.

712
01:39:13,610 --> 01:39:23,150
I didn't keep on rotating. So you use 9% of the data for training and one then for testing.

713
01:39:23,960 --> 01:39:28,580
So you keep on rotating instead of the leave one out.

714
01:39:28,820 --> 01:39:43,520
Now your key is like your B key is sort of the leave out sample and you, you kind of keep on rotating this and you get a cross-validation,

715
01:39:43,520 --> 01:39:52,520
you get a press statistic, and that's what we using your data are training the dummy.

716
01:39:52,520 --> 01:39:57,889
And so it's a very clever way and as you can see get equal to do the data splitting is

717
01:39:57,890 --> 01:40:06,770
basically a special case of the key full cross-validation so so the idea again is very,

718
01:40:06,770 --> 01:40:16,579
very simple and now you are essentially training the using your data to get the probability they didn't endorse.

719
01:40:16,580 --> 01:40:22,520
And then, you know, like you kind of aggregate your performance speed to test statistic or whatever.

720
01:40:23,570 --> 01:40:30,590
And get your honest estimate of the test set data.

721
01:40:31,310 --> 01:40:35,990
So. So that's that's well, that's that's the idea.

722
01:40:35,990 --> 01:40:39,800
And this is the ten fold cross-validation is kind of the industry standard.

723
01:40:40,790 --> 01:40:46,160
And I think I'm going to stop there and see if there are questions.

724
01:40:52,130 --> 01:40:58,430
So all the sort of simple, clever ideas.

725
01:41:04,130 --> 01:41:12,100
But that's the kind of premise for model selection for prediction.

726
01:41:13,400 --> 01:41:17,360
Okay. If there are no questions, I'll stop and I'll see you.

727
01:41:17,420 --> 01:41:23,720
Many of you are tragic. And thank you so.

728
01:41:58,230 --> 01:41:58,500
Yeah.

