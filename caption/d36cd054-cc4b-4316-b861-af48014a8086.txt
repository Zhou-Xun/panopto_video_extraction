1
00:00:00,600 --> 00:00:10,310
It's particularly. All right, fabulous.

2
00:00:10,520 --> 00:00:16,550
Hi, everyone. Welcome to week four where we're just about the midpoint.

3
00:00:17,120 --> 00:00:24,440
So today we're going to step outside of our data management work and get into some descriptive statistics.

4
00:00:27,670 --> 00:00:30,970
Okay. We are recording. I've got the screen capturing.

5
00:00:31,510 --> 00:00:34,540
And just a reminder, no late penalty for your homeworks.

6
00:00:35,140 --> 00:00:39,040
Homework two was due today to keep on pace with everything there.

7
00:00:39,760 --> 00:00:43,210
And then homework three about descriptive statistics will be due next week.

8
00:00:43,960 --> 00:00:48,970
And I also want to make a plug for Student Drop-In hours or on Friday afternoon with me over Zoom.

9
00:00:49,180 --> 00:00:54,190
I had my first attendee last week and I got extremely excited, so please join me.

10
00:00:54,520 --> 00:01:04,989
I'm enthusiastic to talk with you. Whether it's about working this class or other types of projects going forward, here's our outline.

11
00:01:04,990 --> 00:01:11,110
We're going to go over our homework. To recap last class, we'll talk about univariate descriptors.

12
00:01:11,110 --> 00:01:17,080
Statistics have a break and then divide it. Descriptive statistics, I think we'll probably end up wrapping up about it here.

13
00:01:18,430 --> 00:01:21,759
We'll push a little bit to the next level. All right.

14
00:01:21,760 --> 00:01:24,850
Homework to review. This is the biggest one of the whole class.

15
00:01:24,850 --> 00:01:30,670
And this this, I believe, will be consistent, likely with your experience in the workforce as well.

16
00:01:30,880 --> 00:01:37,150
Where and in your I'll you projects potentially this is certainly true for all of my work.

17
00:01:38,160 --> 00:01:46,680
The bulk of my time is spent on data wrangling, data management and the actual analysis, the regression that ends up being relatively quick.

18
00:01:46,710 --> 00:01:51,240
If we've built our data, set infrastructure in a clear way.

19
00:01:51,960 --> 00:01:59,160
So talk to me about your experiences here. There has been a really wonderful discussion happening on the discussion thread for class.

20
00:01:59,160 --> 00:02:03,900
People are being helpful to each other, offering suggestions.

21
00:02:04,080 --> 00:02:07,440
People are giving really clear descriptions of problems and error messages.

22
00:02:08,760 --> 00:02:17,160
Yeah, I'm impressed at the level of detail you're providing, which is making it easier for me and other folks to try to troubleshoot along with you.

23
00:02:17,790 --> 00:02:21,719
So what kind of pain points came up during the question?

24
00:02:21,720 --> 00:02:30,600
You'll see either. The answer Key for Homework two is posted early to the site, but what type of issues challenges came up along the way for you?

25
00:02:30,630 --> 00:02:36,750
Anything you want to talk about as a group? That was I came into one problem at the very end of saving the dataset because it

26
00:02:36,750 --> 00:02:40,610
saved it for some reason was like a character instead of like an actual dataset.

27
00:02:40,620 --> 00:02:44,610
All of which was odd because I thought I use the same code.

28
00:02:44,910 --> 00:02:49,060
So it's seeming as a single vector, that's a character type vector kind of.

29
00:02:49,080 --> 00:02:56,760
It seemed like it kind of just the save the date of all, if that makes sense, to go and save the actual data points.

30
00:02:56,760 --> 00:03:00,540
It just saves the dataset. So I wonder if I just did something wrong.

31
00:03:01,080 --> 00:03:09,510
Yeah. So one thing that can happen if in the actual save function you don't want to put quotes around the object name.

32
00:03:10,200 --> 00:03:15,590
If you put quotes around the object name, it doesn't know it's an object and thinks you just want to save that character expressions.

33
00:03:16,590 --> 00:03:24,660
So save rounded parentheses and then object name without quotes and then comma the file path with quotes.

34
00:03:25,860 --> 00:03:29,280
Knowing when to selectively deploy quotes is tricky, tricky.

35
00:03:29,280 --> 00:03:33,780
So when we're talking in this case, when we're trying to tell it which object we want to save,

36
00:03:33,780 --> 00:03:41,500
in this case, it's like 99% sure that might be what happened to you, but I was able to figure it out.

37
00:03:41,520 --> 00:03:50,240
Nice. Yeah. Other. Yeah, but.

38
00:03:50,260 --> 00:03:54,760
And he did come in relatively clean for most large datasets.

39
00:03:56,520 --> 00:04:04,110
Generally there's a lot more QC that has to go in a long way, but this is a great place to get started.

40
00:04:06,460 --> 00:04:14,160
All right. Well, so let's learn to review the answer sheet and we'll have some feedback for you based on your submissions there.

41
00:04:14,710 --> 00:04:21,160
The subsequent homework assignments, including Homework three, will build on this dataset that you built here.

42
00:04:21,880 --> 00:04:24,660
But I'll have a copy of it for you as a new project.

43
00:04:24,670 --> 00:04:31,750
So if you struggle to build the data set and homework to that is no way will penalize you for how much.

44
00:04:32,170 --> 00:04:40,480
So yeah, there'll be a new copy of the dataset, but the idea would be in the real world would be always using the dataset from the previous examples.

45
00:04:45,770 --> 00:04:49,510
Right. What do we learn last night?

46
00:04:49,550 --> 00:04:55,840
Okay. So one of the first things we talked about last class was the purpose of exploring data and really

47
00:04:55,840 --> 00:05:01,750
trying to give ourselves some activities that we want to become automatic when we get a new dataset.

48
00:05:01,990 --> 00:05:07,030
So what are some activities that you're starting to develop instinctually?

49
00:05:07,030 --> 00:05:12,820
What do you want to do when you go to do this? What are some data exploring activities that are of interest to you?

50
00:05:12,880 --> 00:05:17,590
Your work pipeline? Yeah, it's.

51
00:05:19,250 --> 00:05:26,450
Yeah. We want to know what type of object is it? And if we're talking about a vector object, is that character and factor date.

52
00:05:26,990 --> 00:05:30,890
All kinds of options. Exactly. What else are people curious about?

53
00:05:31,490 --> 00:05:36,770
Oh, and what's a function to do that we can use the structure function as tr are we use class?

54
00:05:37,780 --> 00:05:43,660
Multiple ways to do the same thing. What's another exploring thing people are curious about?

55
00:05:46,620 --> 00:05:54,550
I want to make sure. Like all the. Variables and observations are there for the columns in rows essentially.

56
00:05:55,210 --> 00:06:02,830
Yeah I see. Are there the expected number of columns and rows so I can check that with the demand function for dimension.

57
00:06:03,190 --> 00:06:07,060
I want to figure out what variable are the variables in the columns.

58
00:06:07,810 --> 00:06:12,750
Sometimes you get a data set, the variables are growth and most our packages assume the other.

59
00:06:12,760 --> 00:06:20,379
So we want to transform. It is that now I want to figure out are their names of these variables or do I need to spend my

60
00:06:20,380 --> 00:06:26,480
first couple of days with the data set out applying some names or some labels for these activities?

61
00:06:27,430 --> 00:06:35,950
Anything else we missed here? Again, these are things that you don't want to do no matter what, without anybody asking you to do them.

62
00:06:36,130 --> 00:06:43,030
Usually I'll also check to make sure everything aligns well with the code book that they've provided me, or the data dictionary that provided me.

63
00:06:44,110 --> 00:06:48,130
So I know how missing this is coded all kinds of things there.

64
00:06:48,340 --> 00:06:51,870
And if they didn't give me a code while wild consult.

65
00:06:58,550 --> 00:07:02,870
Another major thing that comes up when we're managing our employee data is often

66
00:07:02,870 --> 00:07:06,200
our data come from a bunch of different people or a bunch of different sources,

67
00:07:06,440 --> 00:07:13,550
and we want to stitch them together. So one way to do that is to join a family of functions for anybody to talk me through.

68
00:07:14,150 --> 00:07:18,910
How does the giant family of functions work? Supremacism.

69
00:07:28,720 --> 00:07:31,180
So they were. What are they joining together?

70
00:07:36,560 --> 00:07:44,900
So typically we'll have one data set, comma a second data set comma and then whatever variable we want to join them by.

71
00:07:45,000 --> 00:07:53,630
So we need to have something in common between them. So usually it's a participant idea, but depending on your study design, that might vary there.

72
00:07:54,740 --> 00:07:57,440
And we had some options in there so we can decide.

73
00:07:58,870 --> 00:08:05,980
Do we want to join together participants that were present in either dataset, in which case we might use a full join.

74
00:08:06,670 --> 00:08:12,900
Otherwise, there's a whole bunch of other. Related functions like left join right giant,

75
00:08:13,530 --> 00:08:21,420
where we're only grabbing people who are maybe in common across both datasets or definitely were in the left dataset,

76
00:08:21,900 --> 00:08:25,200
in which case is going to put some missing values throughout.

77
00:08:26,160 --> 00:08:29,550
So you want to keep track of how many participants are in your dataset.

78
00:08:31,140 --> 00:08:36,330
How many do you expect to have after your join and then confirm it after you've done the join?

79
00:08:36,330 --> 00:08:40,590
So go through that logic check of what do you expect, what do you get and do they match?

80
00:08:41,100 --> 00:08:48,300
Because this joint stuff, this is a common pain point where we can accidentally lose some participants along the way.

81
00:08:49,020 --> 00:08:53,399
Want to make sure if we are going to exclude anyone from our analysis that we're doing.

82
00:08:53,400 --> 00:09:04,200
That was kind of a conscious decision. We're not letting our do that for us and we want to be in the driver's seat for those types of decisions.

83
00:09:06,480 --> 00:09:09,629
We also talked about creating new variables. This is extremely common.

84
00:09:09,630 --> 00:09:16,050
Some of the examples folks hear from this class about ways that this might be relevant in their own work would be,

85
00:09:16,050 --> 00:09:21,570
if you want to sum a series of subscales into a total score.

86
00:09:21,570 --> 00:09:30,570
We saw how we could do that. We saw how we could create bins of a continuous variable based on clinical plot

87
00:09:30,600 --> 00:09:35,920
points or other reasons that you might have to separate people into categories.

88
00:09:36,780 --> 00:09:40,410
And we could do that one with the case, one function.

89
00:09:41,370 --> 00:09:48,660
We saw how you could also just if you have a continuous variable, you could cut it into groups with a cut interval family as a function.

90
00:09:48,960 --> 00:09:59,580
So lots of approaches to not just work with the variables we're given, but to take those variables and create a new version of them.

91
00:09:59,580 --> 00:10:06,660
So that is more informative for your research. QUESTION We also saw there's a function called Mutate,

92
00:10:06,930 --> 00:10:14,760
which we can use to keep our original dataset the same and add those new variables onto the right side of it.

93
00:10:15,480 --> 00:10:18,120
So that can be one useful strategy.

94
00:10:19,690 --> 00:10:26,950
Any any comments, questions that are coming up as we're remembering these types of key ideas about joining about screening variables.

95
00:10:34,170 --> 00:10:38,550
I was just curious, I suppose, to think about it, but I'm curious.

96
00:10:40,000 --> 00:10:42,280
If you had like a ton of the datasets,

97
00:10:43,030 --> 00:10:52,930
do you have to do like the left drawing for each one or is there something that allows you to put them all in one line to merge into ones?

98
00:10:53,320 --> 00:10:57,240
Yeah, this is a great question and this is a common thread I noticed across a couple of the discussion.

99
00:10:57,250 --> 00:11:05,410
Lots of people when I watched you through joints for this stuff, I had you join two data sets at a time.

100
00:11:05,620 --> 00:11:10,750
So when you have more than two datasets, join the first to check your dimensions.

101
00:11:10,930 --> 00:11:14,319
Now to that new dataset. Add a third. Check your dimensions.

102
00:11:14,320 --> 00:11:16,120
Now to that new data at a fourth.

103
00:11:17,080 --> 00:11:25,660
And I personally like that methodical approach because it allows me to check errors at regular intervals along the way.

104
00:11:25,990 --> 00:11:30,570
And the joint family functions only accept two data sets at once.

105
00:11:30,590 --> 00:11:37,060
So a couple of people try to put all four data sets in one joint function and we were getting error messages along the way.

106
00:11:37,840 --> 00:11:42,920
There is a way to. Join them all in one step if you need it.

107
00:11:42,930 --> 00:11:51,780
I think I also posted that in a discussion thread. You first stitch all of the datasets together into a list and then you still use the join function.

108
00:11:51,780 --> 00:11:57,360
But you apply that to. A reduced version of that list.

109
00:11:57,370 --> 00:12:03,330
So it is possible and there some template code in there and the discussions about how to do it.

110
00:12:03,540 --> 00:12:07,410
And I think this is like a common interest that multiple people have.

111
00:12:08,130 --> 00:12:14,510
Yeah, sometimes I don't want to check it every time. I just want to have one line of code that I'll be able to do it and lists.

112
00:12:14,520 --> 00:12:23,550
If you remember, ah, from the first week of class I had an umbrella with that and I think the, the umbrella metaphor works well for our list.

113
00:12:23,820 --> 00:12:27,480
So this is an object that essentially contains other objects.

114
00:12:27,720 --> 00:12:34,770
So it's an umbrella that contains other objects. So in this way we would have a list that contains four other data sets.

115
00:12:35,460 --> 00:12:40,200
So we would have one name that actually corresponds to those. For example, the size of interest chest you.

116
00:12:42,600 --> 00:12:47,460
And you'll start to make your workflow your own and see what are your preferences there.

117
00:12:48,000 --> 00:12:53,430
By all means, you do not have to implement things that are my data management choices.

118
00:12:53,640 --> 00:12:58,770
You can use these kind of efficient lines of code to crank it all out in one line.

119
00:12:58,770 --> 00:13:02,760
If that works well for you, especially if it's a repeated task you're doing.

120
00:13:02,770 --> 00:13:07,260
Officer, what else is on your mind?

121
00:13:16,300 --> 00:13:22,900
Another topic. Oh, yeah. And this is I guess this is a subset of that creating new variable.

122
00:13:23,440 --> 00:13:26,830
But sometimes you want to convert across variable types. That's very common.

123
00:13:28,600 --> 00:13:32,820
Okay. Any. Excuse me. Hair.

124
00:13:41,300 --> 00:13:46,640
Any anything lingering about last class, about data management, data wrangling?

125
00:13:47,030 --> 00:13:56,510
Hopefully, you're starting to think about some examples where those might come in in your pilot project or your research interest.

126
00:13:57,080 --> 00:14:01,010
There is a ton of publicly available data out there that you can really play with, too.

127
00:14:01,040 --> 00:14:07,520
So you don't have to wait for an assignment or an external group to start practicing.

128
00:14:07,730 --> 00:14:16,910
I find these ideas in our stick in my head best when I'm directly applying them or trying to find some example or a way to practice it.

129
00:14:20,340 --> 00:14:26,760
All right. Well, as we transition now, we're going to use those nice rainfall datasets that we've generated,

130
00:14:27,570 --> 00:14:30,240
and we're going to try to generate some descriptive statistics.

131
00:14:30,750 --> 00:14:38,820
So our goals in this sequence of slides will be to describe distributions of variables using central tendency and spread.

132
00:14:39,660 --> 00:14:47,850
And really where we'll take that to the next level in this class, we're going to learn how to create reproducible and professional tables.

133
00:14:47,850 --> 00:14:54,970
And ah, so one of the things that. I had to do what I first started coding.

134
00:14:54,990 --> 00:15:01,890
I would like calculate the mean and copy that value and paste it into my word document table.

135
00:15:02,340 --> 00:15:07,950
That is a fragile process because I could have a copy paste error, or I could.

136
00:15:08,460 --> 00:15:12,440
If I have to update my code, I might forget to update one of those values.

137
00:15:12,450 --> 00:15:17,519
We're going to learn how to automatically create tables and work from our code so

138
00:15:17,520 --> 00:15:24,180
there's no copy paste to make this process very smooth and a lot less error prone.

139
00:15:25,380 --> 00:15:29,910
Okay, so what do we mean by univariate descriptive statistics?

140
00:15:30,570 --> 00:15:35,330
This is a way in which we can describe the distribution of one variable at a time.

141
00:15:35,340 --> 00:15:37,260
That's what uni is for. So one.

142
00:15:37,980 --> 00:15:44,810
And fundamentally, this is going to depend on the type of variable or the shape of the variable, and this is going to be a common theme.

143
00:15:44,820 --> 00:15:53,520
So once next week when we're doing in our selection of choices for what style of graphics to use is going to depend on the type of variable.

144
00:15:53,970 --> 00:16:00,480
So we want to spend some time thinking about we were thinking about this last class of like when we're exploring the data,

145
00:16:00,480 --> 00:16:01,740
what type of variable is it?

146
00:16:01,740 --> 00:16:09,810
Is it numeric, is a character and based on what type of that variable it is is going to influence our options for statistics,

147
00:16:09,930 --> 00:16:13,730
for graphing everything down the line. Right.

148
00:16:14,980 --> 00:16:17,110
For numeric variable. Here's some examples.

149
00:16:17,110 --> 00:16:25,230
So our data can come in many shapes and we want to be able to understand the shape of our data prior to hypothesis testing,

150
00:16:25,240 --> 00:16:29,020
seeing if our data meet the assumptions for any kind of statistical test.

151
00:16:29,710 --> 00:16:33,820
So can anybody help me out? How would they describe the shape of this data right here?

152
00:16:38,740 --> 00:16:44,410
Yeah. I'm hearing some normally distributed. We've got a bell curve distribution that's relatively symmetric.

153
00:16:44,740 --> 00:16:48,220
Can anybody help me out here? How would you describe this data set?

154
00:16:51,030 --> 00:16:55,440
Yeah, we got it right here. And so we've got this long tail on the right hand side.

155
00:16:55,650 --> 00:16:59,270
What about this one? Anybody have an idea with this shape of the data?

156
00:16:59,280 --> 00:17:08,189
Looks like. But I don't just like double peaks.

157
00:17:08,190 --> 00:17:11,440
But I do. Is there, like, another term? Yeah, we've got double peaks.

158
00:17:11,460 --> 00:17:14,370
One way to describe it might be bimodal distribution.

159
00:17:15,750 --> 00:17:22,469
We see this a lot with, like, genetic data, for example, or tri modal distribution, but so the data can come in many shapes.

160
00:17:22,470 --> 00:17:27,540
We got to figure out what the shape is before we do our hypothesis testing.

161
00:17:30,090 --> 00:17:37,320
So when we're figuring out the shape of the data, we can do that graphically.

162
00:17:37,330 --> 00:17:41,700
We also can do it by calculating summary statistics on the distribution.

163
00:17:42,330 --> 00:17:48,340
So one of the common summary statistics that we calculate is a measure of the central tendency.

164
00:17:48,360 --> 00:17:56,940
So where is the middle of the data? There's a couple of options for this I'm highlighting here so we can calculate the mean or the average.

165
00:17:58,500 --> 00:18:05,790
That's most typically done. If we have this kind of normal distribution, we can also calculate the median or the 50th percentile.

166
00:18:08,340 --> 00:18:14,670
We also want to calculate the spread. So how tight or how spread out is that data?

167
00:18:14,760 --> 00:18:18,720
Some of those metrics here include the standard deviation or the interquartile range.

168
00:18:18,930 --> 00:18:22,380
Can anybody help me out? What's the interquartile range? What do we mean by that?

169
00:18:26,150 --> 00:18:29,240
Yeah. Good shot. Your minimum value.

170
00:18:29,540 --> 00:18:33,610
Maximum value. And then like your average and earth.

171
00:18:33,620 --> 00:18:37,669
Yeah. Your median value. Yeah, yeah, we're close.

172
00:18:37,670 --> 00:18:42,800
So you're describing something that I would typically call the five number summary or the six number summary,

173
00:18:42,830 --> 00:18:51,420
which would have like the zeroth percentile, the 25th percentile, 50th percentile, 75th percentile, 100th percentile or the minimum,

174
00:18:51,420 --> 00:18:56,120
maximum and or quartile range is the difference between the 25th of the 75th.

175
00:18:56,660 --> 00:19:02,060
So it's a single number of value reflecting the difference between those two, right?

176
00:19:02,270 --> 00:19:07,009
Yeah. Target way through this. The range is another measure of spread.

177
00:19:07,010 --> 00:19:17,510
It's the spread of the extreme. Uh, the minimum, the maximum and these quintiles quintile is the generic term for any percentile.

178
00:19:18,050 --> 00:19:25,400
But here I'm presenting a specific type of quantile called a quartile, which essentially breaks the data up into quarters.

179
00:19:26,280 --> 00:19:30,980
Um, 24 that's 50th percentile is the same thing as the median.

180
00:19:31,640 --> 00:19:35,690
And the interquartile range is the difference between the 75th and the 25th.

181
00:19:36,200 --> 00:19:40,099
So this class is not meant to introduce these concepts.

182
00:19:40,100 --> 00:19:45,739
Hopefully folks may have heard them elsewhere in some other classes.

183
00:19:45,740 --> 00:19:54,240
So hopefully this is a recap. But if this is new information for you, please feel free to come chat with me, okay?

184
00:19:54,980 --> 00:19:58,010
Oh, yeah. And we also something we remember about.

185
00:19:59,470 --> 00:20:05,680
Work and R is that our doesn't just care about the values, it also cares about the position.

186
00:20:05,890 --> 00:20:12,879
So we can also ask information about what value is in first position or what value is in last position.

187
00:20:12,880 --> 00:20:17,500
So so there can be some additional information we can ask for that specific to our.

188
00:20:20,630 --> 00:20:27,600
There is some lovely functions that can do this for us, and thankfully these have relatively intuitive names.

189
00:20:27,600 --> 00:20:33,659
So to calculate the average or the mean, we can use that main function, the median function, the SD function,

190
00:20:33,660 --> 00:20:39,740
and these are present in the base R packages, meaning that we don't need to load any fancy new package.

191
00:20:39,750 --> 00:20:43,110
These functions are ready to roll with us as soon as we turn our on.

192
00:20:45,350 --> 00:20:52,190
There's some additional answer. The quantile function by default calculates quartiles,

193
00:20:52,910 --> 00:20:58,310
but you can ask for different percentiles if you want the 95th percentile or something like that.

194
00:20:59,860 --> 00:21:07,629
IQ ah grabs that interquartile range. So we just describe in summary grabs what they call the six number summary.

195
00:21:07,630 --> 00:21:13,450
In this case we just talked about what an IQ was, but the six numbers summary in this case.

196
00:21:14,550 --> 00:21:18,390
Is a combination of these five quartiles.

197
00:21:19,470 --> 00:21:22,920
Plus the media. No, that is the media.

198
00:21:23,070 --> 00:21:26,250
These five quartiles plus the mean point about the right thing.

199
00:21:27,300 --> 00:21:32,220
So there's various combinations of these functions you can use to get the same information.

200
00:21:32,910 --> 00:21:36,670
So you could run the quantile in the mean, or you can just run the summary.

201
00:21:36,670 --> 00:21:46,409
You'll start to develop which ones of these functions make. Most are like sticks in your brain and just use those and then all the rest you

202
00:21:46,410 --> 00:21:50,700
can let them slip away because there's many ways to achieve the same outcome.

203
00:21:52,080 --> 00:21:55,590
On here are some functions to grab positional information.

204
00:21:55,920 --> 00:22:02,790
There's also a min function for the minimum, a max function for the maximum, as are several related.

205
00:22:09,050 --> 00:22:13,460
Those were base are functions.

206
00:22:13,580 --> 00:22:20,830
There are some. Tiny verse versions of how to do this as well.

207
00:22:20,890 --> 00:22:25,630
Maybe do this a little bit more efficiently or in a presentation style that might be.

208
00:22:27,650 --> 00:22:31,430
More public face day. So there's a function called summarize.

209
00:22:31,730 --> 00:22:38,030
You can use this. It accepts both British and the US version of spelling, so you could do with the S or Z.

210
00:22:38,720 --> 00:22:45,470
And this is going to calculate the summary statistics and put it in a dataframe which stands sometimes it can be very

211
00:22:45,470 --> 00:22:52,580
powerful to have our results in a dataframe because then we can use that dataframe and perform functions on it,

212
00:22:52,580 --> 00:22:56,270
we can graph it, we can export it, these kinds of things.

213
00:22:56,510 --> 00:23:04,820
So as opposed to just having the output print and show up in our console, now we have the output as a dataframe that we can play around with.

214
00:23:06,080 --> 00:23:09,220
So in this case. How do we read this?

215
00:23:09,250 --> 00:23:13,000
What does this initial symbol tell you? What you remind me. Do we have to type this?

216
00:23:13,030 --> 00:23:18,240
What's this thing called? There's a purpose there.

217
00:23:21,720 --> 00:23:27,480
This initial samba this onto the command prompt. This is our way of telling you that it's ready to accept cuts.

218
00:23:27,550 --> 00:23:34,140
You do not have to type this initial symbol. So what we've got going on here is we're taking the enhanced data set that

219
00:23:34,140 --> 00:23:38,700
we built in the last class and we're piping it into the summarized function.

220
00:23:39,960 --> 00:23:43,740
So this is the object that we're performing the summarize function on.

221
00:23:44,880 --> 00:23:52,020
And then within that summary function, we're saying calculate for me or make a column in this output dataframe.

222
00:23:52,080 --> 00:23:54,990
This output dataframe. I want a column called Minimum.

223
00:23:55,590 --> 00:24:01,020
How are we going to calculate that where you use the main function on the edge variable from enhanced?

224
00:24:02,070 --> 00:24:07,160
I'm saying I also want a column for me. I'm going to use the mean function.

225
00:24:07,170 --> 00:24:11,280
I'm going to calculate the mean of the Age column and then use that.

226
00:24:11,640 --> 00:24:14,310
And then the third thing I want maximum column.

227
00:24:14,310 --> 00:24:22,680
So I'm using the maximum function to calculate the maximum age, maximum value in the age column of the enhanced dataset.

228
00:24:25,820 --> 00:24:30,470
So this is the name of the output column. These are names you can make up.

229
00:24:30,860 --> 00:24:39,050
Call them whatever you want in your output dataset. And these are the actions that you want to be performed and put in that column.

230
00:24:41,120 --> 00:24:49,490
So this is going to produce an output table and give us a column called Minimum with the minimum age of the enhanced dataset, which is zero.

231
00:24:50,060 --> 00:24:52,879
It's going to give us a column called Mean because we asked for it.

232
00:24:52,880 --> 00:24:58,010
So we called me and I column called Maximum and it calculates all those values and puts it in a nice little table.

233
00:24:58,610 --> 00:25:02,830
So this is a quite simple table because we just have one variable.

234
00:25:02,840 --> 00:25:09,320
But you could imagine a scenario where you want to export and share many rows of this table with other folks.

235
00:25:16,730 --> 00:25:25,700
Let's pass. All right, so here's what. Here are some ways to calculate distributions of continuous variables or numeric variables.

236
00:25:26,000 --> 00:25:40,110
Any comments and questions about these? You see how it all comes down to what type of variable is that first?

237
00:25:42,070 --> 00:25:48,160
So in this next section we'll leave behind the numeric variables and we'll talk about factor character variables.

238
00:25:48,940 --> 00:25:53,830
So really two different pathways depending on what type of variable you're working with.

239
00:25:56,070 --> 00:25:59,310
So in this case, if we've got a character or a factor variable.

240
00:26:00,090 --> 00:26:11,280
So sometimes it's very common and epidemiologists have an educational attainment variable and you might have some observations like this one.

241
00:26:11,610 --> 00:26:15,330
She's a high school education participant who had a college education.

242
00:26:16,440 --> 00:26:26,460
In this type of structure. One common way to describe categorical data like this is to provide the number or the count

243
00:26:27,060 --> 00:26:32,670
of observations in each category and the frequency or the percent of the total per group.

244
00:26:35,060 --> 00:26:40,190
So in this case here, if this is our data set, we just have five observations in the data set.

245
00:26:41,990 --> 00:26:49,850
The distribution might be. We have a count of two people in the high school category reflecting 40% of the total data.

246
00:26:50,180 --> 00:26:54,890
And we have a count of three people in the college category reflecting 60% of the data.

247
00:26:55,550 --> 00:27:02,540
So we are presenting completely different information than the numeric variables where we are presenting mean and standard deviation.

248
00:27:02,820 --> 00:27:06,320
Here we're just present day number and percent our frequency.

249
00:27:10,060 --> 00:27:15,280
There's a number of useful functions for those, and this is a non-exhaustive list.

250
00:27:15,850 --> 00:27:22,840
So these are from the base, our packages so we can run the table on which does the count calculates the number of group.

251
00:27:23,810 --> 00:27:32,170
The prop table function calculates the proportion per group and also does the number of per group count, also does the number paragraph.

252
00:27:32,180 --> 00:27:34,070
So there are some redundancies on here.

253
00:27:38,750 --> 00:27:45,200
If we want to make a table, output table like we were just doing with the summarize function, we can do something similar.

254
00:27:46,250 --> 00:27:50,899
So we're going to take the enhanced dataset and pipe it into something.

255
00:27:50,900 --> 00:27:57,870
What are we going to play for them to? We are going to pipe it into a function called count.

256
00:27:57,880 --> 00:28:04,740
So it's saying count the number of people in each education category from the enhanced dataset.

257
00:28:06,250 --> 00:28:11,800
So that's the first one. So that's going to count the number that's going to create these first two columns of the output data set.

258
00:28:12,220 --> 00:28:14,230
And then I'm piping the output of that.

259
00:28:14,230 --> 00:28:20,740
So I'm essentially typing these first two columns into our meeting function to calculate the percent of the total.

260
00:28:21,640 --> 00:28:25,150
So I'm getting that to set to suppress process.

261
00:28:29,940 --> 00:28:38,680
We'll have lots of examples of how to practice this. These earlier steps are almost.

262
00:28:40,820 --> 00:28:47,480
Because my my actual favorite way to do all of those calculations is wrapped in this.

263
00:28:48,430 --> 00:28:53,410
Magical function called the table summary function from the summary package.

264
00:28:53,740 --> 00:28:58,620
So here is the. Output table of this function.

265
00:28:59,070 --> 00:29:05,850
And there is no understatement to say like this would have saved like hundreds of hours of my life.

266
00:29:06,060 --> 00:29:12,510
But I am glad that it exists now and everyone can use it and going forward to save all that what we can.

267
00:29:13,290 --> 00:29:22,950
So essentially this creates a univariate descriptive statistics table where we have all of the variables in the first column.

268
00:29:23,910 --> 00:29:33,990
And some of those variables, for example, are those continuous like H and then we have the second column provides the summary statistics.

269
00:29:34,470 --> 00:29:44,350
So for the continuous ones. In this case, I calculated the median, the 25th percentile, 75th percentile.

270
00:29:44,860 --> 00:29:45,729
And then in this case,

271
00:29:45,730 --> 00:29:55,330
for a categorical or a factor variable and knew what all the levels of that factor at work and I calculated the N or the number and the percent.

272
00:29:56,410 --> 00:30:01,180
So we can see that 26% of the data have an education level less than high school.

273
00:30:02,460 --> 00:30:11,390
This function is smart enough to know. Based on what type of variable you input, it will calculate different types of statistics.

274
00:30:11,870 --> 00:30:15,590
So it's asking, is this variable numeric or a factor?

275
00:30:15,890 --> 00:30:20,540
Okay, this is numeric. I'm going to calculate a media. Is this variable numeric or a factor?

276
00:30:20,540 --> 00:30:23,310
Oh, it's a factor. Okay. I'm going to calculate a number and percents.

277
00:30:24,140 --> 00:30:31,160
So all of your data are cleaning steps that you did earlier to make sure your data variables are the types you want.

278
00:30:31,400 --> 00:30:34,450
It pays off. In dividends here.

279
00:30:36,080 --> 00:30:39,139
Okay. So here's how it kind of works by default.

280
00:30:39,140 --> 00:30:45,049
It calculates for numerical goals, calculates the median 25th percentile and 75th percentile.

281
00:30:45,050 --> 00:30:50,570
But you can ask for different statistics, like if you want the mean and standard deviation,

282
00:30:51,260 --> 00:30:54,320
you can specify that for factor variables, this is the default.

283
00:30:58,730 --> 00:31:02,180
We got this table from this little cove.

284
00:31:02,510 --> 00:31:06,290
It's really quite incredible. So we're taking the enhanced data set.

285
00:31:07,590 --> 00:31:15,030
Piping at first into the select function. If you remember from last class, select as a function to grab just the columns you want.

286
00:31:15,660 --> 00:31:21,050
So I'm grabbing just the columns of age and education. Age and education.

287
00:31:21,080 --> 00:31:26,510
So this is where I'm selecting what rows I want in the output table and then I type that into the table.

288
00:31:26,510 --> 00:31:29,960
Summary. So if I want to do everything with the default.

289
00:31:32,380 --> 00:31:37,630
I go next. This is lovely. And then this can be exported to where you can share this with your collaborators.

290
00:31:38,770 --> 00:31:42,220
If you update your dataset, for example, if you.

291
00:31:43,600 --> 00:31:49,030
Decide on, you know what? We want to restrict our analysis just to nonsmokers.

292
00:31:50,320 --> 00:31:54,930
We just substitute your version of the data set here and recreate the output.

293
00:31:54,940 --> 00:32:00,010
So no, no copy paste errors. We really want all of our results to be automated.

294
00:32:07,890 --> 00:32:13,650
You can customize this output so you can use options to provide labels to adjust.

295
00:32:13,650 --> 00:32:18,820
What types of summary statistics that are generated. Adjust the number of decimal places.

296
00:32:18,840 --> 00:32:23,040
There's dozens of ways to customize this. So for example, here.

297
00:32:24,130 --> 00:32:30,910
Within that table summary function. I'm saying give me a label for the age variable.

298
00:32:30,910 --> 00:32:32,770
I want the label to be age in years.

299
00:32:33,830 --> 00:32:43,459
For the digits on the age variable to have no decimal points because by convention we usually report ages without decimals.

300
00:32:43,460 --> 00:32:47,000
After the first statistic I'm saying for the age variable,

301
00:32:47,000 --> 00:32:55,850
I'd rather present the mean in the standard deviation instead of the median, the 25th percentile and 75%.

302
00:32:56,240 --> 00:33:00,860
So you can see how you can using code, customize how you want the output to look.

303
00:33:02,660 --> 00:33:04,940
So this is whatever feature you want to customize.

304
00:33:05,240 --> 00:33:10,820
This is the variable that you want adjusted and then this is the instructions you're giving for how to modify it.

305
00:33:12,790 --> 00:33:15,520
All right, let's recap. Okay. So we've been on a little bit of a journey.

306
00:33:15,820 --> 00:33:25,360
So, you know, very descriptive statistics are a way of getting to know our data, a way of talking about the distribution of one variable at a time.

307
00:33:25,360 --> 00:33:29,140
So just considering each variable in isolation, what does it look like?

308
00:33:34,620 --> 00:33:42,060
For numeric variables are some conventions there. We usually describe them in some measure of central tendency and some measure of spread,

309
00:33:42,330 --> 00:33:49,649
and you can have your preferences in there for what types of descriptions you like for character and factor variables.

310
00:33:49,650 --> 00:33:53,160
We typically describe them with number and frequency.

311
00:33:55,390 --> 00:34:01,690
And you can automate all of this with a function called table summary in the summary package,

312
00:34:01,990 --> 00:34:07,270
which helps you create reproducible tables that you can share with your colleagues.

313
00:34:07,510 --> 00:34:13,960
So I'll typically do these functions up here if I know the output is only going to be used by me.

314
00:34:14,320 --> 00:34:17,260
So if it's a work in progress, I just want to do something very quick.

315
00:34:17,830 --> 00:34:26,140
I'll use these functions because they they stick in my head, Mark, when I'm ready to create output that I want to share with a collaborator.

316
00:34:26,290 --> 00:34:33,160
Then I'll use the polished version. So you may find a different combination of these functions work well for you.

317
00:34:33,580 --> 00:34:38,260
But that in particular is how it works for me, where I'll use I'll use these for internal coding,

318
00:34:38,830 --> 00:34:43,630
quick coding, and I'll use this version when I want to produce something externally.

319
00:34:44,440 --> 00:34:47,440
The final time, what workflow you enjoy there?

320
00:34:49,090 --> 00:34:56,020
All right. Any comments and questions about these concepts of describing distributions of variables?

321
00:34:59,790 --> 00:35:06,280
Well, one thing I failed to mention on the table summary data set, and this will come up likely at the start of next class.

322
00:35:06,580 --> 00:35:11,020
But they also count for you, if applicable, the number of missing values.

323
00:35:12,020 --> 00:35:17,590
So you can see here there's 1300 people in our dataset that had missing educational attainment values.

324
00:35:17,840 --> 00:35:24,500
You notice they don't get a percent. So they're not by default, they're not calculated in the total frequency.

325
00:35:34,010 --> 00:35:37,850
Come on. What's on your mind? Excited.

326
00:35:37,850 --> 00:35:40,940
You want to try it and try this out with our data set? Yes.

327
00:35:41,540 --> 00:35:47,420
All right. Here's the link. We can click over to our our studio cloud workspace.

328
00:35:49,060 --> 00:35:52,380
Yes, I did. Right.

329
00:35:54,280 --> 00:35:57,670
So here we are in our class workspace.

330
00:35:58,960 --> 00:36:08,980
Logged in as my test unit. You should be able to see a Class three descriptive statistics project under my instructor account.

331
00:36:09,550 --> 00:36:17,950
So please click the plus button over here to make a copy for yourself so that you can can make all the edits you would like along with.

332
00:36:59,530 --> 00:37:02,649
How's our level of zoom in here going for the back row?

333
00:37:02,650 --> 00:37:08,290
Should I do another clicker? So, all right.

334
00:37:08,290 --> 00:37:11,650
So hopefully we're starting to become familiar with this.

335
00:37:12,670 --> 00:37:18,219
Um, interface. So on the left hand side, we have our console.

336
00:37:18,220 --> 00:37:22,300
This is where all of our code gets submitted and our output appears.

337
00:37:22,510 --> 00:37:26,230
Here's our command prompt. So our is telling us that it's ready to receive code.

338
00:37:27,680 --> 00:37:30,950
In our upper right hand corner here is our global environment. This is empty right now.

339
00:37:30,950 --> 00:37:35,600
We have no objects created and in our lower right hand corner, here is all of our.

340
00:37:36,550 --> 00:37:39,440
Files that are shared with you in this project.

341
00:37:39,460 --> 00:37:51,240
So if you can navigate down to the file, it's how's the extension of that Q&A for the class descriptive statistics.

342
00:37:51,250 --> 00:37:56,350
So this is our portal markdown file and now we'll get our classic four panel view.

343
00:37:58,880 --> 00:38:04,730
All right. So I clicked on the class three descriptive statistics that Q and D file in the lower right hand corner,

344
00:38:04,730 --> 00:38:08,330
and that should have popped up a quarter mark down in the upper left hand corner.

345
00:38:09,630 --> 00:38:13,560
Be sure to add your name here so that your team's ownership of your code.

346
00:38:17,500 --> 00:38:21,579
Because we'll often share code for different purposes.

347
00:38:21,580 --> 00:38:26,490
And we want to be able people be able to know who's who's been working with us, who should we ask questions to?

348
00:38:28,290 --> 00:38:29,690
All right. And this is our header.

349
00:38:29,780 --> 00:38:37,640
But the topics that our title or author, our date here or calculate today's date when we click the render button at the end,

350
00:38:38,240 --> 00:38:42,379
it's saying I want the output format to be a website or an HD amount.

351
00:38:42,380 --> 00:38:46,730
You can change that if you want, but this has the source view.

352
00:38:46,730 --> 00:38:54,140
So this where we're viewing the actual code. All right.

353
00:38:55,250 --> 00:38:59,750
The first major coach I have here is about installing packages.

354
00:38:59,990 --> 00:39:06,620
You'll notice I have this command here that says if our equals false, that means don't actually evaluate it.

355
00:39:06,620 --> 00:39:11,180
So when we put the render button at the end, our is going to skip this.

356
00:39:11,680 --> 00:39:19,810
It's not going to run it. It's not going to evaluate it. And I'm doing that on purpose because it takes some time to install these packages,

357
00:39:19,820 --> 00:39:23,090
like the time the first package maybe takes like 5 minutes and stuff.

358
00:39:24,440 --> 00:39:29,330
And what do you remember about that installation? How many times do we need to install a package?

359
00:39:31,050 --> 00:39:37,200
Yeah. Really just once per machine we're working on. And so this is already installed on our workspace.

360
00:39:37,710 --> 00:39:44,670
So this already exists. If you're working on your personal computer, you might change this so you out equals true and then run it yourself.

361
00:39:44,880 --> 00:39:52,200
All right. So I'm going to skip past this first one, but we can see that there are a few packages that we're going to use.

362
00:39:52,200 --> 00:39:58,470
So the type first was in our last packet on our last class, the here package was in our last class.

363
00:39:58,710 --> 00:40:02,040
And then these are two new ones. So we've got two new packages we're going to use.

364
00:40:02,310 --> 00:40:13,320
One is that get summary, which is the home of that table summary packet function that I was giving a rave review about.

365
00:40:13,530 --> 00:40:21,060
And then flex table is a package that kind of goes with it that helps us be able to export it into word for use.

366
00:40:22,500 --> 00:40:32,010
Okay. All right. So no, for the first chunk that I'm actually going to use, is the second one all about loading packages?

367
00:40:33,920 --> 00:40:37,060
So if we want some people are asking questions about like.

368
00:40:38,390 --> 00:40:42,650
How do you find out what packages are available? What data sets are available?

369
00:40:42,650 --> 00:40:52,340
We can use the search function. So this tells us what packages are already queued up.

370
00:40:52,520 --> 00:40:58,250
These are all of the base packages that are has activated for us just by turning our on.

371
00:40:58,610 --> 00:41:04,880
And we can use the LFS function to ask what are the objects that are in our global environment?

372
00:41:05,540 --> 00:41:12,980
Right now we've got nothing but in the future. If you ever want to ask What objects do you have the last function of that is for?

373
00:41:14,890 --> 00:41:18,850
All right. We're going to load these four packages because we'll need them in our environment.

374
00:41:19,000 --> 00:41:22,510
How many times do we need to load the packages?

375
00:41:24,850 --> 00:41:32,710
Every time. So every time you make a new quarto markdown document, I want you to have the packages that you're going to use up at the top of it,

376
00:41:33,190 --> 00:41:39,880
because when you click Render, when you have a of course on markdown document, it's going to this is going to act as a completely.

377
00:41:40,830 --> 00:41:45,330
Independent computational experience.

378
00:41:45,570 --> 00:41:46,920
When you when you click Render,

379
00:41:46,920 --> 00:41:54,770
it's going to ignore anything you've loaded on a previous markdown document and it will just be able to use the material,

380
00:41:54,790 --> 00:42:01,440
but it's coded in that document. So so we want to have all of our libraries loaded at the beginning so I can load these one by one if I want.

381
00:42:04,600 --> 00:42:08,860
Some ways I'm loading some commands.

382
00:42:08,860 --> 00:42:13,030
I'm using control center on a PC. You can use command center on a mac.

383
00:42:15,100 --> 00:42:24,540
I get these other packages going. And then a related packet from function session info tells us what's what we've got up to.

384
00:42:25,460 --> 00:42:31,970
All right. So this one, of course, is where I'm going to load the data that we built last class.

385
00:42:33,500 --> 00:42:42,360
So where do you think it's looking for our data sets. Do you have a sense of where our software is looking to grab our data set?

386
00:42:50,490 --> 00:42:53,760
Yeah. Have you ever shot the working directory?

387
00:42:55,070 --> 00:42:59,720
That is one option. There's a concept called the working directory.

388
00:43:01,390 --> 00:43:10,450
We can use a function called get WD, which means get working directory and that tells you where.

389
00:43:11,590 --> 00:43:16,629
Is our looking for things by default. So when I said get working directory,

390
00:43:16,630 --> 00:43:25,660
this is telling me where they are by default looking for things and it's looking in a folder called cloud and a subfolder called Project.

391
00:43:27,120 --> 00:43:34,260
And if we look over here, we see we've got a folder called Cloud and I sub folder, it's called Project OC.

392
00:43:34,290 --> 00:43:39,690
And so if you're working on your personal computer, I mean, you're not there's no way you have a cloud for earth.

393
00:43:40,290 --> 00:43:43,379
We are unlikely that you have a cloud folder on your personal computer.

394
00:43:43,380 --> 00:43:48,360
You might have a folder called My Documents. You might have a folder called Desktop, something like that.

395
00:43:48,990 --> 00:43:55,170
So this concept is going to be relative to wherever you're working personally.

396
00:43:57,570 --> 00:44:02,480
This can be a little bit. Fragile turtle like set.

397
00:44:03,490 --> 00:44:10,970
So the function that was mentioned are implied I guess is that working directory that's like if you want to.

398
00:44:12,850 --> 00:44:19,870
Change where I was looking by default. I don't. Personally, personally like to do that because that's a.

399
00:44:21,520 --> 00:44:29,740
Pelosi called a global option, and that will change hours behavior across every file you have open.

400
00:44:30,610 --> 00:44:39,459
So it's a way like if you have opened your homework from ViaSat 523 and you set your working directory to go to your inbox,

401
00:44:39,460 --> 00:44:46,240
that 523 folder that will also apply to this project, you do it in a bit, six, seven, four.

402
00:44:46,870 --> 00:44:50,080
So this setting you're working directory applies to everything.

403
00:44:51,070 --> 00:44:58,479
And if which is fine, if you're only working on one thing, but sometimes you're working on multiple, you might have your Illyria research going.

404
00:44:58,480 --> 00:45:03,790
You might have multiple classes working. So a related way that you might consider.

405
00:45:04,690 --> 00:45:07,799
I used the. Here functions.

406
00:45:07,800 --> 00:45:14,660
So this essentially looks wherever you've saved your code and it looks in the same folder.

407
00:45:14,870 --> 00:45:23,180
So this is saying wherever I have this handy look in that same folder for an R data set.

408
00:45:23,900 --> 00:45:28,520
So where do I have that Q&A? Over here. And it's going to look for a data set over here.

409
00:45:28,850 --> 00:45:32,450
So in that way, if I have another.

410
00:45:33,760 --> 00:45:37,770
A code file open for us at 523.

411
00:45:38,280 --> 00:45:42,180
I could pull up a data set from my bios that 523 folder on my computer.

412
00:45:42,450 --> 00:45:46,700
So in this way it can talk to multiple places.

413
00:45:46,740 --> 00:45:54,120
That's kind of a tricky concept. I'm glad you mentioned that because I brought that up earlier and I'm happy to talk about that in more detail.

414
00:45:54,390 --> 00:45:59,610
This is just a practice that I've been engaging with as as a startup.

415
00:46:00,660 --> 00:46:03,899
I don't know if you're a person, like I have like too many tabs open, for example,

416
00:46:03,900 --> 00:46:08,850
on my browser, I have too many are products are like scripts open at once.

417
00:46:09,090 --> 00:46:15,690
And using the here family as functions as opposed to setting my working directory allows me to keep them a little bit.

418
00:46:15,770 --> 00:46:21,630
So yeah. So is here equivalent to the set working structure?

419
00:46:21,650 --> 00:46:25,550
Is it kind of like. Specific to that project.

420
00:46:26,090 --> 00:46:32,900
It's specific to that when you call it. So if we were to just ask, like, I could just ask where is here?

421
00:46:33,830 --> 00:46:39,800
It's essentially a question. So this is saying here is this location, but it's not.

422
00:46:41,140 --> 00:46:47,650
Forcing you to do anything there. So it only applies like this specific line of code.

423
00:46:47,830 --> 00:46:53,420
It wouldn't apply if you had another file open. It's not like saying everything has to go here.

424
00:46:53,440 --> 00:47:00,110
It's like just in this moment. It's like a temporary here. Yeah, maybe it's like a temporary version of this.

425
00:47:00,150 --> 00:47:03,580
Okay. Okay. Versus this is, like, applies to everything.

426
00:47:04,570 --> 00:47:08,640
So what this is saying is load a dataset, what it's doing.

427
00:47:08,660 --> 00:47:11,470
Why load a dataset in our project folder?

428
00:47:12,930 --> 00:47:19,650
That's called enhanced class status that the RDA, the file extension that RDA is one that we can use for our data sets.

429
00:47:20,220 --> 00:47:23,370
And then I've got those verbose equals. True. That's an option there.

430
00:47:23,520 --> 00:47:29,340
We can look that up in the help your if we want. So I'm using the load function.

431
00:47:30,390 --> 00:47:35,280
So the default is verbose equals false. But this just means it's going to.

432
00:47:36,970 --> 00:47:41,170
Print the name of the object as as we load it, so we'll be able to see.

433
00:47:41,440 --> 00:47:46,950
So I'm going to run this line of code and I'm expecting an object to show up in my global environment.

434
00:47:46,960 --> 00:47:54,100
The same object that we built last class. All right.

435
00:47:54,130 --> 00:47:57,700
What happens when you run that load statement?

436
00:47:57,850 --> 00:48:02,260
What do you folks get? What type of object do we observe?

437
00:48:02,560 --> 00:48:06,790
What are the dimensions? Does this make sense with our prior work?

438
00:48:12,230 --> 00:48:24,460
We explore how we find out anything about it. So your data showed and it's seven point.

439
00:48:26,420 --> 00:48:30,799
Yes. We see, we got an object in our global environment. The object in this case called and hands.

440
00:48:30,800 --> 00:48:36,680
We can double click on it if we want. Pops up looking almost like an excel and we can see that here.

441
00:48:36,680 --> 00:48:43,250
So we use familiar columns, not just the columns that came from the enhanced website,

442
00:48:43,490 --> 00:48:49,370
but also the columns that we built in class, like the age groups variable, the race ethnicity variable.

443
00:48:49,370 --> 00:48:57,920
So there is this includes both the directly downloaded variables as well as the new ones that we've built.

444
00:48:59,150 --> 00:49:04,520
The dimensions are here, so our always gives us what is our give us the rows or the columns first.

445
00:49:05,820 --> 00:49:11,070
So he's going to give us a and we've got 90 to 154 rose 27 columns.

446
00:49:13,960 --> 00:49:17,860
And this for both is useful because it tells us what's the name of the object.

447
00:49:19,030 --> 00:49:23,889
So you notice when things are sort of in our dataset, they come in with a name already.

448
00:49:23,890 --> 00:49:30,310
We don't have to use the assignment operator. You know that dash with the arrow to assign it a name.

449
00:49:30,460 --> 00:49:33,940
If it's stored as an AR object, it comes in with a name already.

450
00:49:36,960 --> 00:49:42,390
Yep. Just got RV and it's still weird. Yeah.

451
00:49:42,420 --> 00:49:54,060
This file extension I've got RDA is a file type for R to source something on the data set so we can see it's located down here in our files.

452
00:49:58,920 --> 00:50:06,590
Class status that RDA I have there. Similarly, I have an RDA file that you can use to do your homework, the homework RDA file.

453
00:50:08,960 --> 00:50:15,670
How do we make an art so this is you build it with the we built at the very last line of code last class.

454
00:50:15,680 --> 00:50:18,830
It was like save the engines object.

455
00:50:20,050 --> 00:50:23,110
And we did file equals.

456
00:50:24,490 --> 00:50:27,670
Here and then the enhanced class.

457
00:50:30,160 --> 00:50:41,380
Plus they also. So this was the very last line of code, last class, and that's how we made that object that's now down here.

458
00:50:51,030 --> 00:50:57,399
These are good discussions, this type of work with like loading your packages, loading your dance up.

459
00:50:57,400 --> 00:51:00,940
These are tricky business and it's good to talk about it.

460
00:51:02,010 --> 00:51:05,190
Victor. All right. So let's do some summary.

461
00:51:05,220 --> 00:51:11,580
Univariate summary, descriptive statistics on our numeric variables.

462
00:51:11,910 --> 00:51:16,350
So one of the variables we were playing with last class was.

463
00:51:17,880 --> 00:51:21,990
A variable labeled by other hands called ridge edge.

464
00:51:22,230 --> 00:51:25,680
So I think this is participants age in years.

465
00:51:26,580 --> 00:51:38,570
So some of the functions we saw. And the PowerPoint slide that we can calculate summary descriptive statistics for one variable at a time.

466
00:51:38,870 --> 00:51:43,970
Why am I using the dollar sign here? Can anybody remember what the dollar sign means in our.

467
00:51:48,900 --> 00:51:58,799
The purpose of that dollars. This is, I think valuable that you want to extract from your fantastic.

468
00:51:58,800 --> 00:52:02,490
So I'm not just running the summary on the entire enhanced dataset.

469
00:52:02,700 --> 00:52:11,640
It's saying run the summary function just on this column, in this dataset, so we can use the dollar sign to call out specific columns by name.

470
00:52:13,280 --> 00:52:16,390
So let's try our way through it.

471
00:52:16,410 --> 00:52:20,700
What do you see in the output? Can anybody help me interpret this output?

472
00:52:20,700 --> 00:52:25,770
What's the the script? How do we how do we interpret these values here?

473
00:52:26,010 --> 00:52:46,330
What do you see? What are these numbers?

474
00:52:46,600 --> 00:52:50,440
What are these percentiles? Are they quartiles? What are these values?

475
00:53:04,630 --> 00:53:09,190
Yeah. Thanks. So here's our minimum or zero percentile.

476
00:53:09,400 --> 00:53:18,670
So that's the minimum age in this dataset is zero. This is the first quartile or the 25th percentile of that, if that is 11 years of age.

477
00:53:19,970 --> 00:53:23,990
Our median or the 50th percentile is 31 years of age.

478
00:53:25,290 --> 00:53:28,950
And we've got a mean of value of 34 years of age.

479
00:53:29,430 --> 00:53:38,160
Our third quartile or the 75th percentile. The dataset is 58 years of age and our maximum or hundredth percentile is 80 years of age.

480
00:53:38,460 --> 00:53:41,590
What do you think? What do you think about in your mind?

481
00:53:41,610 --> 00:53:46,190
Can you picture this distribution based on these numbers?

482
00:53:46,200 --> 00:53:52,739
And if so, what do you think about the shape of the data given the relationship between the media?

483
00:53:52,740 --> 00:54:00,120
And I mean, I give you any insights into the shape of the data and next life, as I said, we'll do plotting.

484
00:54:00,120 --> 00:54:02,100
So we'll be able to actually look at this.

485
00:54:02,700 --> 00:54:10,890
But sometimes when you've seen a few of these over time, just looking at the numbers of the median in the mean for you, some indication of the shape.

486
00:54:20,310 --> 00:54:24,990
Yeah. It might be some skew since the medium. We don't exactly line up.

487
00:54:25,290 --> 00:54:31,740
Yeah. So if we had a perfectly normal distribution, I would expect the median and the median to be right on top of each other.

488
00:54:32,760 --> 00:54:38,430
In this case as a mean as a little bit to the right of the media. I'm kind of picturing some bright skew there.

489
00:54:38,880 --> 00:54:41,370
But we'll be able to check that with that with a picture next time.

490
00:54:46,080 --> 00:54:54,180
Another common measure we might want some measure of the spread here is the standard deviation from grabbing that out.

491
00:54:54,180 --> 00:55:00,210
There we see the standard deviation of the age variable and the enhanced dataset is 25 years of age.

492
00:55:01,890 --> 00:55:07,500
So this is how I work. If I'm. Just looking at the output for myself.

493
00:55:07,500 --> 00:55:11,280
I'm not sharing it with anybody. I just want to be able to do this very quickly.

494
00:55:12,650 --> 00:55:24,130
If I want to start creating individual tables for myself or to share with some collaborators, I might start using these tiny first functions.

495
00:55:24,140 --> 00:55:31,550
So here I'm saying pipe the enhanced dataset into the summarize function, and now we're going to create an output table.

496
00:55:31,760 --> 00:55:37,220
And this output table is going to have one, two, three, four, five, six, seven columns.

497
00:55:37,880 --> 00:55:48,290
And in that first column, I want the minimum age. In the second column, a lot of quantile of age, but specifically I want the 25th quantile.

498
00:55:48,290 --> 00:55:51,739
So the 25th percentile and the third column.

499
00:55:51,740 --> 00:55:57,800
I like the mean fourth on the median fifth column, I want the 75th percentile.

500
00:55:58,700 --> 00:56:03,530
So from April 7th, the age so you get and these are completely customizable.

501
00:56:03,740 --> 00:56:07,490
You can do as many of these as you want and you separate them all by two columns.

502
00:56:12,010 --> 00:56:17,240
So let's create a table for ourselves. You can see where we've got the minimum age, 25th percentile.

503
00:56:17,260 --> 00:56:20,410
These numbers are this consistent with the numbers we got up above.

504
00:56:22,030 --> 00:56:26,560
Yeah. We get the same numbers either way. So this is really up to you of how you like to do it.

505
00:56:26,770 --> 00:56:31,150
And this little black arrow helps us navigate to the other end of that long table.

506
00:56:36,970 --> 00:56:38,050
Let's check our understanding.

507
00:56:38,080 --> 00:56:46,360
So last class there was another variable that we explored and we looked at this poverty income ratio variable and we saw how it was coded.

508
00:56:47,170 --> 00:56:53,530
So I'd like you to calculate descriptive statistics on this variable.

509
00:56:54,190 --> 00:56:59,290
Use any of the ways we've talked it out and start by creating a new R code.

510
00:56:59,560 --> 00:57:03,100
See if you can calculate descriptive statistics for this one variable.

511
00:58:55,040 --> 00:58:58,170
All right. Who can help me get started? How are we going to make our coach?

512
00:58:58,380 --> 00:59:04,770
How are we going to get some works to space to work in? Yeah.

513
00:59:12,640 --> 00:59:19,310
For Lawrence, I. Oh, yeah.

514
00:59:19,520 --> 00:59:22,840
I'm the keyboard, the back text. Yeah. So.

515
00:59:22,850 --> 00:59:25,910
And you're saying they're looking in the top right hand corner of our keyboard?

516
00:59:27,260 --> 00:59:30,640
So I've left him. Thank you. So we can type that.

517
00:59:30,650 --> 00:59:34,520
These are not apostrophes. These are back ticks. So, one, two, three.

518
00:59:35,660 --> 00:59:39,500
Curly bracket. Ah, and you notice we have to close it out too.

519
00:59:39,530 --> 00:59:42,980
So, one, two, three. So you can make this right here.

520
00:59:44,550 --> 00:59:55,680
If we got similarly, we could use the C++ app right here from a pop up, and I think there's a keystroke method for it.

521
00:59:55,690 --> 00:59:59,850
What's it called? Yes, there we go. So there is a.

522
01:00:02,220 --> 01:00:05,830
There's like. From.

523
01:00:07,210 --> 01:00:13,060
They called like keystroke command. So there are some shortcuts like keyboard shortcut you can do to generate new ones of these.

524
01:00:13,660 --> 01:00:18,940
So control alt high creates a new code chunk.

525
01:00:20,390 --> 01:00:27,170
So to create a new code just as a third way we can do it.

526
01:00:27,560 --> 01:00:31,440
You do control alt right.

527
01:00:33,740 --> 01:00:38,290
Or if you have a mac, it's command five. Your options.

528
01:00:39,370 --> 01:00:42,850
You feel like a wizard, though, when you can start making things really fast.

529
01:00:43,120 --> 01:00:46,000
All right, so we've got a coach chunk. What are we going to do with it? What?

530
01:00:46,300 --> 01:00:50,050
How, how how are we going to start calculating some of these values for it?

531
01:00:54,140 --> 01:01:04,700
Can we get some of these values? Yeah.

532
01:01:04,740 --> 01:01:08,850
He's not fabulous.

533
01:01:08,880 --> 01:01:11,310
One of the ways we can do is a summary function.

534
01:01:12,150 --> 01:01:21,870
We want to perform the summary function on the enhanced data set, but not the whole data set just to income ratio variable.

535
01:01:22,140 --> 01:01:25,320
So how do we call up just the poverty income ratio variable?

536
01:01:27,190 --> 01:01:33,980
Use the dollar sign I and we and then if we start typing slowly it auto populates for us.

537
01:01:34,820 --> 01:01:39,950
So this gets us a good number of what we asked for. So we asked for the minimum.

538
01:01:39,950 --> 01:01:47,040
We got the minimum, we ask for the 25th percentile. We got that the median we got that the median 75th percentile.

539
01:01:47,040 --> 01:01:50,950
And that's that was everything. Yes, sir.

540
01:01:51,280 --> 01:01:54,610
We also have a number here that we didn't have with age. What? What are these?

541
01:01:54,640 --> 01:01:58,500
What's this value out here? Yeah.

542
01:01:58,510 --> 01:02:03,550
I guess that's a number of missing values. Why don't we have any of these with the age variable?

543
01:02:10,310 --> 01:02:15,660
Yeah. I didn't have any missing. So you notice that if there is no message, it's not like it gives you an Nasiriya.

544
01:02:15,680 --> 01:02:23,120
It just like leaves off that part of the output. So you only get it at any entry if there is missing value.

545
01:02:24,350 --> 01:02:27,640
Did anybody do this a different way? Yep.

546
01:02:28,480 --> 01:02:34,420
Very nice. And how did that go? Was a great practice together.

547
01:02:34,660 --> 01:02:38,889
So that might be if we want to use the tidy verse way.

548
01:02:38,890 --> 01:02:42,040
If we want to pipe into a function. Yes. Yes.

549
01:02:42,040 --> 01:02:47,799
We knew that and object. And we want to pipe that into a function.

550
01:02:47,800 --> 01:02:53,440
What function do you want a to? Summarize this.

551
01:02:54,770 --> 01:02:58,160
And which one do you want to do first? Which one of these values?

552
01:02:58,230 --> 01:03:02,660
Do I calculate first? Let's do the minimum.

553
01:03:03,440 --> 01:03:06,649
Yeah. So we're going to calculate the minimum. The first thing we give it is what do we want?

554
01:03:06,650 --> 01:03:12,469
The name of the output column to be. So maybe minimum. I'm not feeling super creative right now.

555
01:03:12,470 --> 01:03:16,460
So then I'll do an equal sign and then I'll tell it how to calculate that minimum.

556
01:03:16,730 --> 01:03:20,960
So one way to calculate a minimum is with the main function.

557
01:03:21,590 --> 01:03:25,430
And what variable in my calculating the minimum minimum function on.

558
01:03:28,030 --> 01:03:35,290
Yeah. Yeah. Why don't I have to do a dollar sign off on PR here?

559
01:03:37,130 --> 01:03:40,320
Why am I leaving that off? Biting it into.

560
01:03:41,510 --> 01:03:44,569
Yeah. They already piped it into here so it already knows that.

561
01:03:44,570 --> 01:03:51,459
I'm always talking about that and hands off that. All right, so let's before I go, trying to add a bunch of columns, so I'll put this out.

562
01:03:51,460 --> 01:03:54,800
Let's just do one first. All right.

563
01:03:55,220 --> 01:03:58,310
Oh. Oh. Here's a good trick we got.

564
01:03:58,460 --> 01:04:05,120
Is this what happened when you said it didn't go well? So I did the whole thing together and kind of getting the any issue.

565
01:04:05,360 --> 01:04:10,909
Yeah. The issue this is a kind of a preview to something we're going to do at the end of the class.

566
01:04:10,910 --> 01:04:16,180
But. In The Help viewer, we can look up the main function.

567
01:04:19,310 --> 01:04:29,600
Turns out that. Every function in art is written by a different person, and every person has different detail preferences.

568
01:04:30,200 --> 01:04:36,170
So the person who wrote the summary function by default is like, Yeah, sure, go ahead, leave the missing in there.

569
01:04:36,740 --> 01:04:43,340
First we wrote the main function, so by default has the error out with missing.

570
01:04:43,790 --> 01:04:47,960
So it has a setting of eight erm equals false meaning.

571
01:04:48,260 --> 01:04:56,060
If there's any missing, don't remove it. So you can't calculate the minimum of a number that does not exist.

572
01:04:56,870 --> 01:05:05,930
So we have to override that default setting and put in a comma for and a dot R equals true.

573
01:05:06,770 --> 01:05:11,060
And we'll have a whole thing about. Missing data at the end of this quest.

574
01:05:11,990 --> 01:05:16,460
But now we run this and we get instead of a missing value, we get the value of one.

575
01:05:17,540 --> 01:05:26,720
So that's one strategy. And I'll do I'll try to, before I go, adding a bunch of columns, I'll try to run one column and and make sure that works.

576
01:05:27,140 --> 01:05:30,350
Because it's easier for me to troubleshoot one thing at a time.

577
01:05:31,370 --> 01:05:38,460
And you can look up the function in that one column and see see what your error situation is happening.

578
01:05:38,690 --> 01:05:42,710
If I want to add a second column. So if I want to add like the.

579
01:05:47,170 --> 01:05:53,630
Oh, like the 25th percentile. The percentile 25th.

580
01:05:54,500 --> 01:06:06,610
And I can use a function called quartile. And the MPR provides equals to first.

581
01:06:09,050 --> 01:06:17,020
Oh. He got very upset with me. What's your upset?

582
01:06:17,080 --> 01:06:20,400
Oh, quartile quantile is the function, not quartile.

583
01:06:20,700 --> 01:06:26,200
Thank you. Do you know how I was able to pass this error?

584
01:06:26,220 --> 01:06:29,250
It's, like, useful, I think, to work through errors together.

585
01:06:29,520 --> 01:06:40,710
So here. This was what it used to say. I ran a function called quartile, and it tells me in the output here, I can't find a function called quartile.

586
01:06:42,450 --> 01:06:45,059
That's because that function doesn't exist. I had a typo.

587
01:06:45,060 --> 01:06:53,040
I should have read quantile, but so it's actually like lovely and wonderful and nice to get error messages from there.

588
01:06:53,280 --> 01:06:58,860
Not every package gives you error messages, and this helps guide me on a way to find a solution.

589
01:06:59,760 --> 01:07:03,240
So sometimes if I get an issue where it can't find a function.

590
01:07:05,650 --> 01:07:12,880
Maybe I'm just hyper functional. Sometimes I get this message if that function is in a package that I haven't loaded yet.

591
01:07:13,960 --> 01:07:18,280
So sometimes this is a signifier to me. I need to do that library statement to load that package.

592
01:07:18,450 --> 01:07:25,270
It depends on what kind of situation I'm running. All right, so let's take that type of run, the control function.

593
01:07:27,410 --> 01:07:31,940
Again, another missing value issue. They're telling me this and I am equals false.

594
01:07:31,940 --> 01:07:36,390
You have addressed this yet, so I'm not for any. Erm.

595
01:07:36,990 --> 01:07:40,860
Sure. And now we can fill this up.

596
01:07:41,070 --> 01:07:49,770
So that's how I'm building complexity one step at a time, and I address my errors one at a time.

597
01:07:50,010 --> 01:07:54,839
So I think for me personally, my goal in coding is not to never have any errors,

598
01:07:54,840 --> 01:08:02,600
but to be able to work through them methodically and do my coding a little bit at a little bit of a time.

599
01:08:02,610 --> 01:08:08,190
So for me, that's easier to troubleshoot than if I try to run the whole big thing at once.

600
01:08:08,490 --> 01:08:14,310
Then I can't figure out where things went wrong. All right.

601
01:08:14,320 --> 01:08:20,620
And you can continue to expand that out for all of the other types of statistics that are requested.

602
01:08:21,700 --> 01:08:29,750
Any comments and questions about this aspect? Plus if it's the you do it.

603
01:08:30,040 --> 01:08:36,810
Rosie which calls? Why are you using here? Oh, so the default is adding a dot around because false.

604
01:08:37,210 --> 01:08:40,750
Which means how do we say this in words? This is saying.

605
01:08:42,030 --> 01:08:49,140
When it's running the minimum or the maximum function by default, is it r m means remove.

606
01:08:49,800 --> 01:08:52,959
So should it remove the missing by default?

607
01:08:52,960 --> 01:08:58,410
It's saying false meaning it's leaving the missing in there and trying to calculate.

608
01:09:00,250 --> 01:09:06,160
Maximum on something that's not a number. So that's why it's airing out, because by default it's saying, don't remove it.

609
01:09:06,190 --> 01:09:11,200
Leave the missing in there and missing values interrupt numeric calculations.

610
01:09:11,680 --> 01:09:18,220
So we have to override the default. This was like the preference set by whoever authored this package.

611
01:09:19,510 --> 01:09:23,120
Our preference would be to do we want to remove the missing?

612
01:09:23,140 --> 01:09:28,450
Yes, I want to throw those missing out of there. And then I want to press on forward with the calculation.

613
01:09:29,080 --> 01:09:36,040
So I want to flip this from false to true so that I'm telling the function to remove the missing and calculate it.

614
01:09:36,460 --> 01:09:41,220
This is not going to actually remove the missing from our data set. The data set is fine.

615
01:09:41,230 --> 01:09:49,690
We're not if we're not using the assignment operator to change the data set, this setting only applies for the confines of this function.

616
01:09:50,020 --> 01:09:54,040
So it's saying just while you're running this function, remove the missing. Go ahead.

617
01:09:55,070 --> 01:10:04,049
And I always look this up because every package has a different default way of handling mess and men.

618
01:10:04,050 --> 01:10:09,570
And Max, I didn't look up quantile because it's written by the same people, but we can look that one up too.

619
01:10:10,710 --> 01:10:15,570
You can see they have a quantile function and have that same default and ram equals false.

620
01:10:17,680 --> 01:10:20,770
Yeah. A bunch of, like, related packages all written by the same person.

621
01:10:20,770 --> 01:10:31,940
Have that same setting. Look, our normal, normal pressure points to experience in your analysis.

622
01:10:32,570 --> 01:10:39,080
So you may choose if this is like a super crucial variable for you, you may choose to exclude the people who have missing values,

623
01:10:39,590 --> 01:10:45,680
or you may just want to describe the magnitude of the issue and so forth.

624
01:10:46,430 --> 01:10:48,139
All right. Let's take a pause here.

625
01:10:48,140 --> 01:10:57,020
And when we come back after the break, we'll do some descriptive statistics on categorical variables before we go back to bivariate with the slides.

626
01:10:58,010 --> 01:11:02,300
So, yeah, let's take a five minute break and I'll be here if anybody has comments or questions during the break.

627
01:11:15,460 --> 01:11:21,200
And I don't know if this is for 20, but this doesn't add up.

628
01:11:42,060 --> 01:11:45,130
Let's do it today. Critics charge.

629
01:11:45,910 --> 01:11:56,860
Right now I have to take care of lives, but I can't figure out how to break down.

630
01:11:57,220 --> 01:12:00,280
I can see it. Okay.

631
01:12:00,760 --> 01:12:11,580
Oh, my love. Really? So as my eyes look at what I do, remember how I like.

632
01:12:12,050 --> 01:12:16,710
Oh, like. Four in the morning and it was safe to curl up now.

633
01:12:16,730 --> 01:12:19,910
That's why I always play golf on the cold tracks, because it all holds me.

634
01:12:20,780 --> 01:12:24,470
What I see is a doing unit,

635
01:12:24,720 --> 01:12:36,680
so I'm going to take a look at what I'm going to be going while more than half a lot of like I'm standing for like only as I start the practice,

636
01:12:36,680 --> 01:12:40,370
like, okay, let's go. Don't need to install the vaccine to control that.

637
01:12:41,690 --> 01:12:48,830
Let's put a hashtag. I thought she was trying to do that for how could you hold it against me for like.

638
01:12:49,460 --> 01:12:54,470
Well, we want to. Yeah, but then we come back, like, so which thing you go for?

639
01:12:54,470 --> 01:12:57,890
No, those are all for this one against all of us.

640
01:12:58,040 --> 01:13:01,580
We want I don't want to make that which I haven't done. That's true.

641
01:13:02,810 --> 01:13:06,260
Yeah. Oh, not with a vertical bar.

642
01:13:06,800 --> 01:13:10,040
Yeah, just this one. So that I remembered it. It's true.

643
01:13:10,090 --> 01:13:14,840
That's what you do. Is. Is that so close? All the way to the safe.

644
01:13:17,010 --> 01:13:22,090
Yeah. I was like, I'm just going to say, we're like, Your size was different.

645
01:13:22,170 --> 01:13:26,240
We announce here because I'm allowed specifically 13 dogs.

646
01:13:26,280 --> 01:13:29,480
And she was like, I'm not taking care of something like acid.

647
01:13:29,520 --> 01:13:33,630
And I was like, okay, okay, let's go. So it made it very nice.

648
01:13:34,440 --> 01:13:37,740
Let's go to that legal. Let's see what's happening here.

649
01:13:37,920 --> 01:13:46,290
Do you realize that you care for your big dogs or that's doing everything I want you to do?

650
01:13:46,290 --> 01:13:52,530
I figured, you know, like huge Holly Hunter puts out this massive.

651
01:13:52,710 --> 01:14:03,840
Yeah, yeah. So that was good. But my experience, like small dog, I think is about as far as they don't they don't make a lot and I don't know why.

652
01:14:04,080 --> 01:14:09,540
I didn't know who I was. So I was like, This is your reality show or something?

653
01:14:10,310 --> 01:14:14,490
You just those are just like, I'm right. Yeah, but I don't.

654
01:14:14,490 --> 01:14:20,729
Oh, which does that? What do you want to do? So we call you on the doorstep for a shot of you.

655
01:14:20,730 --> 01:14:28,139
And I see that as Yeah, this is the name of the On Chat Rewards program for fun.

656
01:14:28,140 --> 01:14:32,040
And they're I mean, I think that was good.

657
01:14:32,040 --> 01:14:36,680
I mean, look, I'm usually the best I like affected the most.

658
01:14:36,690 --> 01:14:40,080
Yeah, I get that. And then you want to.

659
01:14:40,090 --> 01:14:43,920
Yeah, that's low and whatever you call it.

660
01:14:43,940 --> 01:14:47,230
Oh yeah. Like this. Yeah.

661
01:14:48,150 --> 01:14:53,400
Whatever that is. Big dogs you can kind of slap. Not like it's a safety net.

662
01:14:53,550 --> 01:14:57,280
Yeah. And they like in it. So it's just kind of fun to.

663
01:14:57,430 --> 01:15:03,750
Oh yeah you can, you have to like fly or something.

664
01:15:03,900 --> 01:15:07,520
You have to. Yeah, it's kind of small.

665
01:15:07,800 --> 01:15:12,100
So all of us have like. Like a complex deal.

666
01:15:12,110 --> 01:15:18,110
Just like, start bargaining or like fighting. It's something, you know, when you're like, you're a foot tall.

667
01:15:18,110 --> 01:15:23,470
Yeah, that's my head. This family dog is.

668
01:15:24,470 --> 01:15:28,760
Oh, it's a data set new. Oh, I see.

669
01:15:29,750 --> 01:15:33,360
And he's like, it's like 15 or 16 then.

670
01:15:34,520 --> 01:15:39,410
So I said, I want to do whatever I do.

671
01:15:40,550 --> 01:15:47,330
I think that that's probably right. I think that's why I think you just have to dogs.

672
01:15:50,700 --> 01:15:54,100
Yeah. He was so small. He like I don't.

673
01:15:54,210 --> 01:15:59,460
I wonder. I don't know, like living across the road.

674
01:15:59,490 --> 01:16:05,730
Yeah, that's so bad. My mom and I really love it.

675
01:16:07,890 --> 01:16:15,430
It's so sad because people say, oh, you have to pay all new ideas and stuff elsewhere and.

676
01:16:15,720 --> 01:16:19,930
Okay, so when you're doing this. Oh, my God. Everything.

677
01:16:20,730 --> 01:16:26,010
Yeah, absolutely. From my heart. I know that you guys talk.

678
01:16:26,310 --> 01:16:30,690
If you want to say I was I have joked that you might do that after the second group.

679
01:16:30,960 --> 01:16:37,410
Oh, so just like, you know, once you get to like choosing where your, you know.

680
01:16:37,750 --> 01:16:45,600
Yeah, yeah, yeah. I mean, that's okay. So you don't have to print that out.

681
01:16:46,440 --> 01:16:50,160
And at the same thing, you can go pick this one now.

682
01:16:50,860 --> 01:16:59,830
You all done it? Yeah. Do you think we know what that's doing is just like 20 to affect your output?

683
01:17:00,480 --> 01:17:04,860
That one, actually. Yeah. But then you feel like you were there again.

684
01:17:05,350 --> 01:17:13,020
Like. Yeah, like I do. I like to just have an open tab, like, oh, sometimes.

685
01:17:13,020 --> 01:17:19,380
All excited. That's because. Yeah, you're right. It's always good to use if you already have the old ads and want to.

686
01:17:21,390 --> 01:17:25,620
Oh, yeah, yeah. The Social Network. Oh, I didn't even like that idea.

687
01:17:26,640 --> 01:17:35,370
Okay. And then as soon as it said, yes, you can download, I guess, like I said, you can't write a check.

688
01:17:35,370 --> 01:17:43,740
And so it was a conversation about really the work we do here.

689
01:17:43,850 --> 01:17:51,060
Yeah, we had no idea. X chromosome. And then that's going to save it here, like on Twitter as h html.

690
01:17:51,210 --> 01:18:05,610
And we just like to think, yeah, we're. Yeah, I, I guess my question is, if you write letters now that we write like is something you learn over time.

691
01:18:06,550 --> 01:18:09,770
Yeah. Yeah.

692
01:18:10,550 --> 01:18:15,360
I was like, Oh, yeah.

693
01:18:18,240 --> 01:18:21,660
All right, let's jump back into our data analysis.

694
01:18:21,660 --> 01:18:27,900
We're going to transition from continuous numeric variables and then we'll talk about categorical or factor variables.

695
01:18:28,980 --> 01:18:38,910
So one example of this type of factor variable we're going to work with is a self-reported race ethnicity variable.

696
01:18:38,910 --> 01:18:45,420
So similar to before. There's a way of doing this with just kind of my internal approach.

697
01:18:45,420 --> 01:18:52,140
So I'll use the table function. This is a quick one. It's good for looking at one variable at a time.

698
01:18:52,860 --> 01:18:56,820
How do we interpret this output? Can anybody help me here? What are these numbers mean?

699
01:18:59,190 --> 01:19:09,270
Well, how do we do these values mean? Police.

700
01:19:11,610 --> 01:19:16,420
Count observation calendar. Participant count for each category.

701
01:19:16,660 --> 01:19:22,660
Yeah. These are the number of participants who provided responses and each of these indicated categories.

702
01:19:22,660 --> 01:19:29,280
So 3150 people in the non-Hispanic white category, for example.

703
01:19:29,290 --> 01:19:34,030
So these are the numbers. We don't we haven't yet generated the proportions, but we'll get to it.

704
01:19:34,840 --> 01:19:37,930
So one way of getting to the proportions we can do.

705
01:19:39,870 --> 01:19:43,490
To the enhanced dataset into the cap function for that variable.

706
01:19:43,760 --> 01:19:49,730
And then type the counted values into the mutate to calculate the percentages.

707
01:19:50,420 --> 01:20:00,530
So kind of a two step approach. And then we get a tiny table with each of those categories.

708
01:20:00,770 --> 01:20:06,830
We have the same numbers as before. So that 3150. But now we also have the percent of the total.

709
01:20:17,090 --> 01:20:21,080
So far we've been working with one variable at a time.

710
01:20:21,350 --> 01:20:27,890
We're going to see and where we've been generating output, that's kind of confined to the our structure.

711
01:20:28,010 --> 01:20:34,910
Now we're going to generate output that might be more useful for sharing with with outside audiences as well.

712
01:20:36,870 --> 01:20:45,660
So we're going to work with a function called table summary to, uh, to build this.

713
01:20:46,290 --> 01:20:51,330
So first thing I'm going, we're going to pull up what are the column names of the data and enhanced dataset.

714
01:20:53,030 --> 01:20:56,330
So we've got all these columns here and we're going to think about.

715
01:20:57,590 --> 01:21:01,700
What variables? Does it even make sense to calculate descriptive statistics?

716
01:21:02,570 --> 01:21:06,220
So for example. What second does anybody remember?

717
01:21:06,320 --> 01:21:10,780
Second is an enhanced. Yeah.

718
01:21:10,800 --> 01:21:13,940
Sequence. All right. This is our participant ID. Is.

719
01:21:14,610 --> 01:21:17,820
Do you want descriptive statistics on participant I.D. in this study?

720
01:21:18,630 --> 01:21:20,700
No. Probably those numbers don't really.

721
01:21:20,910 --> 01:21:27,780
I mean, maybe if there was a setting where you wanted, like to know how many repeated measures you have for a participant.

722
01:21:27,780 --> 01:21:30,960
You might want something like that, but these are not informative.

723
01:21:30,970 --> 01:21:35,400
So I'm going to drop that variable. I don't want that in my classroom dataset.

724
01:21:37,380 --> 01:21:45,450
A couple of other variables I don't want in my descriptive table are the strata variable and the population sampling unit.

725
01:21:45,720 --> 01:21:50,880
These are. Like population sampling.

726
01:21:51,600 --> 01:21:56,940
Variables like telling us what county people were from or what household people are from.

727
01:21:57,630 --> 01:22:05,910
So those generally wouldn't be reported either. So what I'm doing in this table, this first aspect.

728
01:22:07,270 --> 01:22:10,680
Oops. Oh. I know that. I really clicked.

729
01:22:10,870 --> 01:22:16,070
Okay. I'm. I'm using the select function.

730
01:22:16,090 --> 01:22:20,830
Can anybody remind me what is the select function for as opposed to the filter function?

731
01:22:24,530 --> 01:22:34,080
Those two are related functions. One of the items for grabbing Rose and one of them's for grabbing columns.

732
01:22:34,410 --> 01:22:37,560
Do you think selectors for grabbing rows are for grabbing columns?

733
01:22:38,530 --> 01:22:43,330
Oh, yeah. This one's for grabbing Collins. And you can make that intuition, because these are all names of columns.

734
01:22:44,230 --> 01:22:47,380
So you can see I'm ignoring the second column.

735
01:22:47,620 --> 01:22:53,770
I'm grabbing just the columns I want. So I want a, I want x rays on this Z education.

736
01:22:54,040 --> 01:23:00,070
I want some of these cell count variables like lymphocytes and neutrophils.

737
01:23:00,610 --> 01:23:06,580
I want some of these exposure variables, iron status, variable arsenic, cadmium, iron coating.

738
01:23:07,990 --> 01:23:12,640
Then once I've selected just those variables, I'm typing that into something.

739
01:23:14,980 --> 01:23:18,280
What am I piping that into? I'm typing them into the table.

740
01:23:18,280 --> 01:23:26,819
Summary. Pack function, which is going to calculate those summary statistics on our variables.

741
01:23:26,820 --> 01:23:31,980
It's going to calculate different summary statistics if our variables are numeric versus factor.

742
01:23:32,970 --> 01:23:36,120
I'm giving a couple options here. We don't have to specify options.

743
01:23:36,120 --> 01:23:42,420
What I'm saying for continuous variables, I'd rather calculate the mean in the standard deviation rather than the median.

744
01:23:42,630 --> 01:23:51,420
You know, these are just your preferences. I'm providing some labels for some of these variables so I can put my cursor anywhere in this whole thing.

745
01:23:52,560 --> 01:23:58,650
Run it. It's going to think for a second. And then we get this kind of lovely table.

746
01:23:58,650 --> 01:24:02,070
So you notice that this formatting even comes through by default.

747
01:24:02,070 --> 01:24:08,340
So it's got some bold for the headers. It's got a superscript to describe how some of these statistics are done.

748
01:24:09,330 --> 01:24:13,590
And we've got our variable, we've got our units on that variable.

749
01:24:14,190 --> 01:24:17,460
So then what are these two numbers that are right here next to each?

750
01:24:22,080 --> 01:24:27,180
How do we find out what those are? We can follow this foot now all the way down to the bottom of the table.

751
01:24:29,980 --> 01:24:33,510
And we see that. Come on.

752
01:24:35,850 --> 01:24:41,450
We see that the statistics we reported are mean and standard deviation and percent.

753
01:24:42,960 --> 01:24:49,930
And this is smart enough, it can figure out which what variables are numeric, which variables are categorical.

754
01:24:50,550 --> 01:24:58,400
So variables like age are going to mean standard deviation, and our categorical variables like sex are going to end in percent.

755
01:24:59,900 --> 01:25:03,680
So here's the number and percent in each of these categories.

756
01:25:05,090 --> 01:25:10,280
Some of our variables have missing values, and those are also provided.

757
01:25:13,700 --> 01:25:22,430
So investing some time to learn this. Function and make sure all of your variables are assigned.

758
01:25:22,440 --> 01:25:32,130
The type you want can save a ton of time because then these products are generated very nicely going forward.

759
01:25:34,250 --> 01:25:41,420
Okay. What questions do you have about this Holzer's pique your interest in working with this type of table?

760
01:25:45,580 --> 01:25:48,770
Yeah. Plus, they're like.

761
01:25:49,930 --> 01:26:00,160
Scenarios where you would want to take out the missing numbers because it seems to take a whole lot of space and might not be very helpful.

762
01:26:01,140 --> 01:26:07,440
Yeah. One of personally, one of the things I do, I'll make a table like this, everybody.

763
01:26:07,800 --> 01:26:14,590
And then I will. Evaluate what are my must have variables because.

764
01:26:16,160 --> 01:26:21,020
Sometimes it's not useful for me to exclude everybody who's missing anything.

765
01:26:21,020 --> 01:26:24,920
Then my data set gets to just lost or it gets too weirdly selected.

766
01:26:25,670 --> 01:26:28,790
So usually I'll decide what are my most must have.

767
01:26:28,790 --> 01:26:35,420
So probably my primary exposure variable, my primary outcome variable, key demographics.

768
01:26:35,720 --> 01:26:44,540
So I'll exclude participants who are missing on those and I'll make a smaller dataset that is just not missing on my key criteria.

769
01:26:45,470 --> 01:26:52,280
And then I'll make a version of this table. We'll see how to do this, where you compare those included people to your excluded people.

770
01:26:53,150 --> 01:26:57,979
So you can see do the people you excluded differ meaningfully from the people in our study?

771
01:26:57,980 --> 01:27:10,020
So is your analytic sample generalizable to the overall cohort or has it have your decisions analysts to exclude missing data like meaningfully?

772
01:27:10,020 --> 01:27:12,980
It created a bias in that population.

773
01:27:13,670 --> 01:27:20,270
So we'll be able to make a version of this table that has a column for the included study sample column for the excluded.

774
01:27:20,450 --> 01:27:27,889
And we can see maybe the excluded people are all older or maybe the excluded people are all lower income.

775
01:27:27,890 --> 01:27:35,390
Maybe, maybe it was onerous to come back for that second study. And we're losing participants of certain characteristics,

776
01:27:35,570 --> 01:27:41,340
but based on the choices we're making as an analyst and it's helpful to know how those things.

777
01:27:41,390 --> 01:27:50,390
Yeah. So yeah, I usually make this first with everybody so I can kind of get an idea of where there's a missing data issue for me.

778
01:27:50,990 --> 01:27:53,629
And then I make choices about inclusion, exclusion.

779
01:27:53,630 --> 01:28:00,770
I'll usually make a flowchart for myself so I can see which variable I'm excluding, how many people are left in there.

780
01:28:02,510 --> 01:28:10,910
But by the time I'm at regression model stage, then I want to have like a nice, tidy data set where my key variables don't have anything.

781
01:28:11,390 --> 01:28:20,330
I've either excluded or I've done some imputation, I've done some something to address that missing data because most regressions that will

782
01:28:20,330 --> 01:28:25,670
start with like y you have already addressed the missing data question before you get to the.

783
01:28:28,390 --> 01:28:34,390
Yeah. And you'll see as the answers, like so many choices you can make and there's no exact one path forward.

784
01:28:34,390 --> 01:28:36,850
So you kind of, you pick something to justify it.

785
01:28:36,850 --> 01:28:42,780
Maybe you do a sensitivity analysis where you see, Hey, if I made a different choice, what would the impact on the results be?

786
01:28:48,370 --> 01:28:52,719
So yeah, investing in data, cleaning up front to make sure variables are factors,

787
01:28:52,720 --> 01:28:59,830
make sure variables are numeric and payoff in terms of ability to use these fancy functions online.

788
01:29:02,510 --> 01:29:07,910
Let's transition back to slides and we'll talk about this principle of bivariate statistics,

789
01:29:08,060 --> 01:29:17,230
which you could do library of included versus excluded, or you could do bivariate of case versus control or exposed versus not.

790
01:29:17,240 --> 01:29:25,399
So this is a generalized framework when we want to describe the distributions of variables by a second variable criteria.

791
01:29:25,400 --> 01:29:32,840
So here we've been comparing like values of one variable at a time, and that will compare two variables together.

792
01:29:40,130 --> 01:29:47,660
Okay. All right, so we're gonna talk about filtering and by very descriptive statistics.

793
01:29:51,080 --> 01:29:55,880
So first we'll talk about how to order and filter R&D to start based on certain participant criteria.

794
01:29:56,450 --> 01:30:00,530
And then we'll talk about how to describe the distributions of two variables at once.

795
01:30:08,380 --> 01:30:15,010
So a useful function we can use to reorder our dataset is the arranged function.

796
01:30:15,010 --> 01:30:23,770
So if our original dataset has participants in kind of a random age order and we would prefer to put it in order.

797
01:30:25,280 --> 01:30:27,540
We can say I take the enhanced data set dataset,

798
01:30:28,430 --> 01:30:34,640
good end of the range function and now that's essentially going to order it to be in ascending order based on page.

799
01:30:38,650 --> 01:30:43,540
We can flip the order. So here we're saying arrange it based on descending order of age.

800
01:30:44,350 --> 01:30:53,910
Now I. I don't really do this very often, to be honest.

801
01:30:53,930 --> 01:30:59,030
I think I remember, in fact, you have to do this all the time before doing merges.

802
01:30:59,090 --> 01:31:04,400
Is that a correct assumption? So are you. We don't really have to do that.

803
01:31:05,760 --> 01:31:11,250
I almost never do this unless I, like, really want to just zoom in and look at the data set in this way.

804
01:31:11,520 --> 01:31:15,880
But this is not like a required step before I go ahead with something else.

805
01:31:15,900 --> 01:31:20,400
This is more of a data exploration or visualization type activity.

806
01:31:24,800 --> 01:31:31,520
If I want to find participants that match some characteristic I can use as a filter function.

807
01:31:31,520 --> 01:31:37,760
I think we saw this in a previous slide deck because this is the sister function to select.

808
01:31:38,690 --> 01:31:44,809
But I just want to emphasize this again here, because this can be a gateway to doing bivariate test statistics.

809
01:31:44,810 --> 01:31:50,800
So we could filter out just the rows which matches certain criteria.

810
01:31:50,810 --> 01:31:59,810
So here I'm typing the enhanced dataset into the filter function and I'm using filter to select within the column called Disease.

811
01:32:01,550 --> 01:32:04,670
Values are, which is a disease, is equal to chaos.

812
01:32:05,450 --> 01:32:10,610
So I'm grabbing just certain rows and I just have these few rows for case.

813
01:32:10,940 --> 01:32:17,479
So one way of doing bivariate descriptive statistics, I could filter out the cases and then do univariate,

814
01:32:17,480 --> 01:32:22,250
and then I could filter out the controls and then do univariate on them.

815
01:32:22,760 --> 01:32:29,180
So essentially split the dataset into the two bins and calculate univariate statistics as planned previously.

816
01:32:29,210 --> 01:32:34,480
So this is one way of going about that. Right.

817
01:32:34,610 --> 01:32:40,660
So in general, the principle in bivariate descriptive statistics is while we want to look at the distributions of two variables at once.

818
01:32:42,010 --> 01:32:44,980
So usually for for me when I want to do this,

819
01:32:45,700 --> 01:32:52,840
I want the description of not a sample that the samples already been split based on some category or some criteria there.

820
01:32:56,130 --> 01:33:00,630
So for example, maybe I want differences in ages between cases and controls.

821
01:33:00,780 --> 01:33:04,500
Maybe I want differences in grades between courses.

822
01:33:04,800 --> 01:33:08,180
Maybe on differences and time since mammogram by insurance data.

823
01:33:08,190 --> 01:33:16,020
So I have some category. I want to split the data on that category and then calculate the distributions of the sample within that category.

824
01:33:23,180 --> 01:33:27,709
So one introductory option like gateway option you might start with,

825
01:33:27,710 --> 01:33:34,790
you can filter the data set by this categorical variable and then calculate the univariate descriptive statistics on each of those samples.

826
01:33:37,570 --> 01:33:48,310
So to implement that, we can we can use the group by variable to say what categorical variable do we want to group the data by,

827
01:33:49,330 --> 01:33:54,160
specify some calculations we want performed on those groups and then we can close that grouping.

828
01:33:56,300 --> 01:34:03,020
So in this case, I'm using group five. So I'm saying. Take that data, pipe it into the group buy function.

829
01:34:04,280 --> 01:34:12,319
I want it grouped by disease factor, so a case versus control setups and then I'm going to pipe that into the action.

830
01:34:12,320 --> 01:34:17,210
So what this is saying is my output, my output is going to be grouped by case or control status.

831
01:34:19,630 --> 01:34:23,080
And then I've got the second half here. Now I'm calculating this things from before.

832
01:34:23,080 --> 01:34:26,830
I want to summarize the median age, the minimum age, maximum age.

833
01:34:29,530 --> 01:34:35,680
So any descriptors you want. You can have as many of them as you want separated by commas within that summarized function.

834
01:34:36,670 --> 01:34:41,680
And I can do that table out here. So this is an extension of the univariate table.

835
01:34:41,890 --> 01:34:46,120
When we did it, univariate, we just had one row because it was for the whole data set.

836
01:34:46,690 --> 01:34:51,880
So now we have two rows because we have one for each category or maybe you have multiple categories in that variable.

837
01:34:53,750 --> 01:35:01,960
Now we unwrap it and we put the dataset back to how it was. So this is one item bill table.

838
01:35:02,420 --> 01:35:05,540
And so we say it's bivariate because we've got two variables, right?

839
01:35:05,540 --> 01:35:10,189
We've got in our rows, we have one of the variables. So that is the status.

840
01:35:10,190 --> 01:35:13,210
And in the columns we have another variable or the age of.

841
01:35:20,660 --> 01:35:31,270
Another way that we can use this table summary that we have taken the time to learn for univariate statistics and it is highly applicable here.

842
01:35:31,280 --> 01:35:36,019
So this may look familiar. These variables that we were looking at last time,

843
01:35:36,020 --> 01:35:42,800
age and educational attainment categories last time we were looking at overall in everybody in the sample.

844
01:35:43,310 --> 01:35:48,800
Now I'm splitting it by a second variable. I'm splitting it by participant sex or male versus female.

845
01:35:52,950 --> 01:35:58,439
So we're running this. And that's why I started taking the things that piping it into the select

846
01:35:58,440 --> 01:36:02,790
function selects is the function where we grab which columns are of interest.

847
01:36:03,150 --> 01:36:10,139
So the columns are age, education and sex. So you notice I'm grabbing both, call both the variables.

848
01:36:10,140 --> 01:36:16,290
I want to appear in the rows of the table as well as that extra variable that I want to appear in the columns.

849
01:36:18,510 --> 01:36:24,810
Bring that into the table. Summary And the only difference I have between the univariate and survivor, I say what variable I want to split by.

850
01:36:26,430 --> 01:36:33,030
So by taking the time to learn this function once now all of your future extensions can implement this as well.

851
01:36:33,510 --> 01:36:39,720
Similarly, I would have a version where I would have an indicator variable for included or excluded status.

852
01:36:39,900 --> 01:36:50,240
So I would do it by include it. Yes. No. And then I'd have a column for the included and a column for that excluded animation.

853
01:36:50,370 --> 01:36:53,460
So here's where you would say the variable you want the column split by.

854
01:36:54,630 --> 01:36:58,050
And if you want, you can even add a P value.

855
01:36:58,770 --> 01:37:02,220
And we'll get into this with hypothesis testing, I think, in two weeks.

856
01:37:03,150 --> 01:37:13,200
But this has a nice extension where we add the P value and this gives us a fourth column where we get test the test X where it's assessing.

857
01:37:13,470 --> 01:37:17,880
Are there any differences in the distribution of this variable by these two groups?

858
01:37:19,020 --> 01:37:23,550
So are there any differences in the distribution of age between males and females?

859
01:37:23,910 --> 01:37:27,960
Are there any differences on the distribution of educational attainment by males and females?

860
01:37:28,320 --> 01:37:38,520
So it's really an elegant way of creating a lot of information in a small space and in a format that you can share with your collaborators.

861
01:37:42,980 --> 01:37:54,260
Very. So to recap, if we want to visualize or look at our data in certain order,

862
01:37:54,980 --> 01:38:01,100
we can use a range if we want to pull out participants or rows meeting certain criteria with news filter.

863
01:38:02,270 --> 01:38:07,730
So those are kind of introductory ways we can separate the datasets into groups and calculate univariate descriptors.

864
01:38:09,260 --> 01:38:15,799
Or we can do it all on one data set together so we can use the group by function with those summarized

865
01:38:15,800 --> 01:38:20,750
function that were in the first half of lecture function that was on the first half of lecture.

866
01:38:21,470 --> 01:38:26,260
And then we can put it all together with the table summary function to specify what variables meant by.

867
01:38:27,250 --> 01:38:34,320
I typically for any given manuscript, I probably have at least three of these kind of tables.

868
01:38:34,330 --> 01:38:40,960
Some of them are in the supplement. The first one is usually in the supplement it's described that included versus excluded sample.

869
01:38:41,800 --> 01:38:46,740
The second one is usually in the main manuscript that's left. Do you have the case?

870
01:38:46,750 --> 01:38:50,420
Yes or no? Or do you have the outcome variable? I mean, yes or no?

871
01:38:50,860 --> 01:38:54,100
And then I usually also have one. Do you have the exposure variable, yes or no?

872
01:38:54,760 --> 01:38:59,740
So three flavors of this table at various points in the manuscript.

873
01:39:04,100 --> 01:39:09,170
What kind of comments, questions do you have about. These type of statistics are.

874
01:39:09,230 --> 01:39:14,120
Is this something you've generated before in other classes? Yes, it's familiar.

875
01:39:14,180 --> 01:39:18,250
Okay. Are these coping strategies that you've used in other classes?

876
01:39:19,660 --> 01:39:25,569
Especially the summary function. Summary function. Excellent. All right.

877
01:39:25,570 --> 01:39:30,610
Let's jump back in here. And this is the same project from before.

878
01:39:44,870 --> 01:39:49,310
All right. So first off, we want to practice some of this.

879
01:39:50,240 --> 01:39:56,540
If we want to order the data, we can use the arrange. So first we can look at the current order of the rose.

880
01:39:59,780 --> 01:40:03,330
Does this look like it's an age order right now.

881
01:40:04,310 --> 01:40:07,550
Now it's like pretty scrambled. So if I want.

882
01:40:09,550 --> 01:40:16,540
To rearrange it here in my slides I was just providing one variable, but you can provide as many variables as you want.

883
01:40:16,750 --> 01:40:22,540
That's for helping if there's types. So for a variable like age, you can imagine there's a bunch of people with the same age.

884
01:40:22,750 --> 01:40:26,740
So it's saying in the event of a tie, then go choose the second variable.

885
01:40:29,830 --> 01:40:36,390
So now we've got all. Zero year olds, particularly those that are male, are first.

886
01:40:36,400 --> 01:40:41,130
So you can view it by multiple features.

887
01:40:41,140 --> 01:40:47,280
Here I'm doing descending ages, so now we've got all of our 80 year olds who are male at the top.

888
01:40:47,550 --> 01:40:53,180
And you notice that I am. Just doing this for visualization purposes.

889
01:40:53,180 --> 01:40:57,380
I don't have an assignment operator. I'm not like building a new object.

890
01:40:57,800 --> 01:41:03,410
I'm not changing the original object. Like if we go ahead and click on the enhanced object.

891
01:41:04,640 --> 01:41:08,360
It's still in that original scrambled up order.

892
01:41:08,360 --> 01:41:13,490
So I'm not I'm not messing with the data set. This is just purely for visualization of.

893
01:41:14,630 --> 01:41:19,480
Yes. All right.

894
01:41:19,690 --> 01:41:25,360
So now if I want to filter the data, I'm just participants meeting certain criteria.

895
01:41:28,120 --> 01:41:35,520
Let's practice it first. So let's say we want to do an analysis where we only look at the female participants first.

896
01:41:37,040 --> 01:41:42,190
Usually before I do any kind of. Manipulation of the data set.

897
01:41:42,200 --> 01:41:48,450
I want to know, what do I expect first when I first run the table function and see how many female participants I have.

898
01:41:50,060 --> 01:41:57,080
All right. So I'm expecting if I filter the dancer on female status, I'm expecting to have 4600 participants.

899
01:41:57,080 --> 01:42:02,750
Let's. Then we can say, All right, let's subset that data.

900
01:42:03,060 --> 01:42:06,480
Let's take pipeline dataset into the folder function.

901
01:42:07,380 --> 01:42:14,520
Which participants do I want to graph? I want to grab participants from the variable sex wherever it's equal to female.

902
01:42:14,880 --> 01:42:18,030
And I'm assigning that as a new object I'm calling enhanced subset.

903
01:42:18,300 --> 01:42:23,010
So I'm expecting a new dataset to pop up in the upper right hand corner.

904
01:42:26,420 --> 01:42:32,579
All right. We did that. We got a new data set and we see that it has that 4600 participants.

905
01:42:32,580 --> 01:42:36,150
I was expecting if I click on it, I can double check.

906
01:42:36,390 --> 01:42:38,730
We see that these are all female participants.

907
01:42:39,860 --> 01:42:46,730
So I'm doing that like logic checking whenever I'm manipulating the size of the data, just like I did when I was doing Giants.

908
01:42:49,560 --> 01:42:54,750
Let's say we want to pick participant criteria based on multiple features.

909
01:42:55,380 --> 01:43:03,960
So this might be a scenario where I want to know people who are iron deficient and who are meeting a certain age threshold.

910
01:43:04,380 --> 01:43:07,620
So, first of all, make a table. See, what do I expect?

911
01:43:11,960 --> 01:43:20,360
So this is saying those that are iron deficient and age greater than 60, this is 376 people.

912
01:43:20,630 --> 01:43:26,210
So by subset, I'm expecting to get 360 people, 326 people.

913
01:43:26,450 --> 01:43:30,620
So I'm saying take that dataset, plug it into filter function.

914
01:43:31,220 --> 01:43:36,800
And I want to filter on two criteria. So I want to filter on the iron status variable equal to deficient.

915
01:43:37,130 --> 01:43:40,520
And I want participants who have age greater than 60.

916
01:43:41,460 --> 01:43:44,490
And I've signed that as a new data set I'm calling enhanced subset.

917
01:43:44,880 --> 01:43:49,290
So I expect that to overwrite this dataset and have a new slice that.

918
01:43:49,300 --> 01:43:55,670
376. And that's what I see when we get here.

919
01:43:55,670 --> 01:44:04,430
And if I go to click Hub, I see these are all older participants and if we scroll over to their iron status.

920
01:44:08,440 --> 01:44:10,240
They're all in the deficient category.

921
01:44:10,870 --> 01:44:17,920
So you can string together as many combinations of participant criteria that you need for your particular research question.

922
01:44:19,180 --> 01:44:22,600
Then run your univariate statistics on that subset, if you like.

923
01:44:24,400 --> 01:44:27,790
Another way to do it, where you leave the whole data set the same.

924
01:44:28,480 --> 01:44:35,680
So now I'm running everything on that data. So I'm just saying I want to group it by a characteristic, calculate those summary statistics.

925
01:44:37,240 --> 01:44:43,060
So we get that tiny table where we have rows for those criteria and then columns for the second variable.

926
01:44:46,000 --> 01:44:49,780
Here's the same way to do that for categorical variables.

927
01:44:51,070 --> 01:44:56,560
So now we have in the rows we have one variable, and in the columns we have a second variable.

928
01:44:57,400 --> 01:45:00,970
So we're getting all the combinations of those two different categorical variables.

929
01:45:03,130 --> 01:45:04,900
Rushing a little bit at the end of class.

930
01:45:04,900 --> 01:45:15,670
I would like to just show you that we could do this in a bivariate table as well as running this whole function.

931
01:45:17,350 --> 01:45:19,720
We're going to look at it now.

932
01:45:21,730 --> 01:45:29,410
So now we've created a table where we have columns for the overall sample and then for that variable that we want the dataset split by.

933
01:45:30,820 --> 01:45:34,510
And if you're ready to make it, you can save it as a word document.

934
01:45:34,750 --> 01:45:42,690
So let's all show up in our files folder. I've got the summary statistics table as a word document.

935
01:45:55,810 --> 01:46:05,630
I don't know where you are, but we have a nice table that we can export and share with our collaborators.

936
01:46:08,520 --> 01:46:15,440
All right. So this should be all of the steps that you'll need to do the homework assignment this week.

937
01:46:15,450 --> 01:46:23,050
I'm just going to show you. That assignment under descriptive statistics.

938
01:46:27,520 --> 01:46:31,870
I'll ask you to calculate univariate descriptive statistics for a numeric variable.

939
01:46:32,890 --> 01:46:37,540
Computer statistics for a factor variable filter the dataset,

940
01:46:37,540 --> 01:46:46,659
then calculate variance and then this last one to calculate bivariate and you'll use the dataset you built in homework.

941
01:46:46,660 --> 01:46:51,760
Two For this homework you don't have to queue it up again.

942
01:46:51,760 --> 01:46:58,930
I have it here for you in the files. So there's another copy of it in our week three project you can want.

943
01:46:58,930 --> 01:47:08,890
Everybody can work from the same parent once. So this homework template will help you pull up that dataset right here from this location.

944
01:47:09,220 --> 01:47:12,310
So this is going to be pulling from your dataset right here.

945
01:47:13,270 --> 01:47:19,210
Thank you all for your attention. I appreciate it. And next class, we'll talk about missing data in more detail.

