1
00:00:00,210 --> 00:00:13,500
Right. So. So part of it was saying, hey, uh, all right.

2
00:00:13,770 --> 00:00:18,590
The sooner we start, the sooner we can go outside. So let's get going here.

3
00:00:19,160 --> 00:00:22,350
Halloween costumes. Let's say you have a Halloween costume.

4
00:00:22,950 --> 00:00:26,190
Do I have one? I have tons of Halloween costumes. Are you going to be something?

5
00:00:26,550 --> 00:00:32,460
What's not a plastic? How many of my faculty do you think dress up on Halloween?

6
00:00:32,930 --> 00:00:37,050
5 to 1. It's me.

7
00:00:39,480 --> 00:00:45,930
So I don't technically dress up a muslim teaching on it. So maybe I should dress up on Wednesday.

8
00:00:45,960 --> 00:00:50,850
All right. So you really do this? Should Atwood dress up?

9
00:00:51,600 --> 00:00:58,150
That would be. No. No, I don't think so. Yeah, no.

10
00:00:59,100 --> 00:01:05,610
Our previous administrator, Larry Boden, used to dress up every year, but that would be a good benefit to coming in first.

11
00:01:05,610 --> 00:01:08,880
And it wouldn't be 50 years that they would get everybody to come to class.

12
00:01:09,580 --> 00:01:15,330
Yeah, I don't believe it, sir. You're just trying to make me feel like I'm past it.

13
00:01:16,450 --> 00:01:20,780
No, I like the challenge, everybody.

14
00:01:21,210 --> 00:01:25,410
I'm wearing a costume on a Wednesday class. All right.

15
00:01:26,340 --> 00:01:34,820
I'm going to be the only human being on Wednesday wearing a helmet. All right, I'll play Farkle.

16
00:01:35,600 --> 00:01:38,780
Right. I meant to download the data today, and I forgot. I realized.

17
00:01:39,260 --> 00:01:42,469
See how we're doing? I said to enough of you.

18
00:01:42,470 --> 00:01:46,880
Now, I think this is going to be the final exam is I'm going to give you the circle data

19
00:01:47,540 --> 00:01:51,140
and ask you some questions about it based upon what we've learned in this class.

20
00:01:51,890 --> 00:01:55,460
So, again, I don't want I don't want to report. I don't want a sample report.

21
00:01:57,200 --> 00:02:02,690
It'll be shorter, easier to answer questions, but you would have to use the methods of this class or to answer them.

22
00:02:02,960 --> 00:02:09,770
All right. So that's my plan right now, since I've had everybody do this, let's see what actually happened.

23
00:02:10,670 --> 00:02:14,750
I'm curious to know what the mean differences over time for everybody.

24
00:02:16,790 --> 00:02:25,919
But besides that. I have talked a lot in this class about estimating equations and trying to make all 650 classes kind of fit.

25
00:02:25,920 --> 00:02:31,710
One big, heavy idea of how we estimate regression parameters.

26
00:02:32,550 --> 00:02:40,379
If you want more on that, Tim Johnson referred me to a great book by Jim Johnson Wakefield,

27
00:02:40,380 --> 00:02:45,280
who at Seattle, and he taught the regression classes that I took in Seattle.

28
00:02:45,300 --> 00:02:51,870
He didn't teach them to me. He taught them after I left. But he came up with this book called Bayesian Frequentist Regression Methods.

29
00:02:51,870 --> 00:02:59,280
You can ignore the Bayesian everything frequentist that we have learned in 650 5153 are in this.

30
00:02:59,760 --> 00:03:08,520
It's a little bit written of PhD students, but mostly what I see in this class actually appears somewhere in what he says.

31
00:03:08,520 --> 00:03:13,680
Right. There's an estimating equation from these squares. We can generalize it and then and so forth.

32
00:03:13,690 --> 00:03:19,649
So if you want a free text work because you're in Michigan, everybody gets a free PDF of this.

33
00:03:19,650 --> 00:03:25,080
And so I just put it on the campus website. If you want to keep a look at it, please do.

34
00:03:26,070 --> 00:03:33,300
It's a really nice book and I was wondering how we can incorporate it into our into our series of regression classes.

35
00:03:34,530 --> 00:03:38,380
Again, not quite at the level of this text, so check that out if you want.

36
00:03:38,400 --> 00:03:51,210
It's they're trying to figure out which costume I'm going to wear, what I don't know, what's furry.

37
00:03:52,800 --> 00:03:59,850
And I go through our code again today and I want to continue upset of my picture.

38
00:04:00,540 --> 00:04:03,390
Here we go. So I get to a certain point.

39
00:04:04,530 --> 00:04:13,050
Last class showing you what I was going to analyze the dataset with binary outcomes for homework for and you each have a different one.

40
00:04:15,960 --> 00:04:20,340
I got some feedback in office hours that I think maybe carries over to other individuals.

41
00:04:22,290 --> 00:04:30,450
We struggle in the 650 sequence. It's called Applied Theory is Important.

42
00:04:31,720 --> 00:04:37,720
I like nothing worse or I hate nothing more than someone who uses methods and doesn't know why they work.

43
00:04:38,440 --> 00:04:45,340
Right. I get a little concerned with some of the data science machine learning folks don't really know what the thing is their running does.

44
00:04:48,840 --> 00:04:50,969
So I'm trying to balance more of the applied.

45
00:04:50,970 --> 00:04:57,000
Sometimes the theory in my lecture doesn't carry over to what we do in the homeworks, and there's a disconnect there.

46
00:04:57,000 --> 00:04:59,520
And some of you would like more of a connect to connect.

47
00:05:00,090 --> 00:05:06,840
So I'm going to do that today with G and I don't know what I'll do with the element and stuff,

48
00:05:06,840 --> 00:05:11,400
but maybe I'll try and reverse back to that homework assignment later.

49
00:05:12,120 --> 00:05:14,910
So before I go into fitting some more,

50
00:05:16,140 --> 00:05:25,290
I want to connect the lecture notes to this dataset and what I'm trying to do with it again for for folks who don't like matrix algebra, right?

51
00:05:25,770 --> 00:05:29,010
And you know, you have to survive.

52
00:05:30,180 --> 00:05:34,709
Let's look at it some more because that throw a lot of matrices that you're in.

53
00:05:34,710 --> 00:05:38,610
Don't ever really devolve them into into individual elements. All right.

54
00:05:38,820 --> 00:05:44,200
So in this homework assignment that I am running, I have a binary outcome.

55
00:05:50,920 --> 00:05:57,040
And the number of observations per person ranges anywhere from 2 to 6.

56
00:05:57,070 --> 00:06:02,410
The goal was to get everybody at six quarters, but they didn't quite witchy that goal.

57
00:06:02,770 --> 00:06:07,900
And again, we took our people with one observation in my analysis in order to fit in our structure.

58
00:06:08,710 --> 00:06:17,220
So am I is two through six. Going to fit a generalized linear model.

59
00:06:20,640 --> 00:06:40,690
So some function of the mean. So the mean of any person at any time point, there's a link function in that in my approach.

60
00:06:40,690 --> 00:06:46,630
So I didn't have an intercept in what I'm going to do because I had to find my design matrix.

61
00:06:48,330 --> 00:06:52,370
To be such. No, I shouldn't be in one.

62
00:06:55,170 --> 00:06:58,830
George. To.

63
00:07:05,660 --> 00:07:09,140
So I have a generalized linear model for any individual observation.

64
00:07:09,150 --> 00:07:17,470
No vectors right now. So there were four indicator variables in my dataset, in my design matrix.

65
00:07:18,790 --> 00:07:29,230
And again this is an indicator that the person was male in the time that's than or equal to three.

66
00:07:32,260 --> 00:07:38,050
It's just a one or a zero based on time and whether they're male or female.

67
00:07:40,080 --> 00:07:46,470
This was an indicator of what the person was male in time greater than or equal to for.

68
00:07:48,570 --> 00:07:53,160
And then we have the same thing for female indicator of female.

69
00:07:55,130 --> 00:08:02,040
Time. Just three indicator of.

70
00:08:07,940 --> 00:08:16,590
Oops. It's the design matrix is going to be zeros and ones.

71
00:08:16,600 --> 00:08:22,149
So I have incorporated time as categorical and it's very categorical.

72
00:08:22,150 --> 00:08:25,560
There's only one category, there is three or less for four more.

73
00:08:26,640 --> 00:08:31,950
So there's four means I'm sitting here, right, each of the two time points and male or female plus you.

74
00:08:37,650 --> 00:08:43,500
You know. AJ What is new? AJ here is the probability that my outcome is a one.

75
00:08:44,100 --> 00:09:05,420
Talking about binary outcomes here. We use the canonical link when you use the logic link here so that the mean is related to covariates,

76
00:09:05,430 --> 00:09:10,230
it's actually the logic of the mean is a function of covariance linear combination.

77
00:09:12,730 --> 00:09:17,290
Data here is that a vector of coefficients? Beta 1 to 2.

78
00:09:18,220 --> 00:09:24,740
It's like three. Two, three, four.

79
00:09:27,450 --> 00:09:37,320
I've got a design vector here. So at any time point for any person there is a vector of four values.

80
00:09:38,280 --> 00:09:45,900
So this is 1000 real time interval two three.

81
00:09:50,040 --> 00:09:56,310
It is zero one for male.

82
00:09:57,510 --> 00:10:03,390
Time for another four. It is 0010.

83
00:10:05,060 --> 00:10:16,930
For female. Timeless or equal to 3 minutes 0001 female time.

84
00:10:27,960 --> 00:10:38,910
So that means at any point in time and at any point in time is again, this vector times the vector of coefficients on the large scale.

85
00:10:42,240 --> 00:10:48,730
And so therefore meurig. Again the source.

86
00:10:53,300 --> 00:10:57,410
The inverse logic function, which sometimes we call the export function.

87
00:11:01,450 --> 00:11:06,320
Is e to the linear combination divided by one plus two.

88
00:11:06,330 --> 00:11:12,310
The linear combination, right? That number goes between zero and one. It's a probability.

89
00:11:19,700 --> 00:11:24,590
Right. I have now specified the marginal distribution of any single observation.

90
00:11:27,350 --> 00:11:35,270
So this is why we call GE a marginal approach. I do not model the joint distribution of somebodies outcomes.

91
00:11:35,990 --> 00:11:39,770
I specify the marginal distribution of each of their outcomes separately.

92
00:11:55,190 --> 00:11:59,210
All right. Again.

93
00:11:59,360 --> 00:12:01,159
Sorry if you don't get that, I am recording it.

94
00:12:01,160 --> 00:12:09,889
So if you're keeping up with me, if I'm really going too fast, please don't see no reason to go into a video.

95
00:12:09,890 --> 00:12:15,110
Just find one instead of handwritten notes for a person.

96
00:12:15,110 --> 00:12:19,280
Now I've specified all of the marginal components of each outcome.

97
00:12:19,760 --> 00:12:25,390
Let's talk about someone now who has all six outcomes. So all six quarters, they have a zero or 140 phoneme.

98
00:12:26,990 --> 00:12:34,819
So now we have a vector of observations for each person. So there's 1i12 through I six.

99
00:12:34,820 --> 00:12:42,830
Right. And I have a vector of means one like to.

100
00:12:45,140 --> 00:13:00,440
My six. Now I say that.

101
00:13:01,560 --> 00:13:08,430
The vector of means on the logit scale are a linear combination of covariance.

102
00:13:08,430 --> 00:13:18,239
So now XA is a matrix. It has one rule for every time point that this person has the same beta coefficients.

103
00:13:18,240 --> 00:13:25,320
Just now that each person has a vector of outcomes, they have to have a design matrix rather than the design vector for each of the outcomes.

104
00:13:27,220 --> 00:13:30,750
What that explicitly what is exide?

105
00:13:30,750 --> 00:13:34,710
What is the design matrix? So.

106
00:13:37,450 --> 00:13:43,590
One, two, three, four, five, six. Tom Brown.

107
00:13:43,650 --> 00:13:48,780
Why do you have an intercept? It's interesting.

108
00:13:51,450 --> 00:14:13,520
When. Right.

109
00:14:14,510 --> 00:14:17,930
So if they're male, I have an indicator that they're male.

110
00:14:17,930 --> 00:14:20,990
And their observation came from the first three observations.

111
00:14:21,980 --> 00:14:27,920
The next three rows tell me that they're male. And their observations came from the latter three quarters four, five and six.

112
00:14:37,940 --> 00:14:47,780
And if they're female. 0010001.

113
00:14:57,000 --> 00:15:03,270
What range? There is my.

114
00:15:11,640 --> 00:15:17,450
Listing like the cursor and. Oh, the cursor.

115
00:15:17,450 --> 00:15:23,810
Thank you. Females.

116
00:15:24,860 --> 00:15:28,370
Right. So every male or every female has the same.

117
00:15:28,760 --> 00:15:33,200
Every female has the same design matrix. Every male that has the same design matrix.

118
00:15:55,560 --> 00:16:01,170
And remember, when we write a function of a vector, that's just the function applied to each element of the vector.

119
00:16:02,870 --> 00:16:10,390
Right. Come.

120
00:16:17,180 --> 00:16:23,210
I mean, is the inverse of the linear predictor right geometries linear prediction?

121
00:16:25,180 --> 00:16:28,700
But yeah, we have a function applied to a vector.

122
00:16:28,700 --> 00:16:32,200
It just means the function applied to each of the elements of the vector.

123
00:16:36,920 --> 00:16:43,190
Okay. So now for each person, but I have a design matrix, I have a vector of coefficients.

124
00:16:43,190 --> 00:16:50,000
I specified that the mean on some function level is related to those covariates and the linear predictor.

125
00:16:50,900 --> 00:16:57,760
And I want to estimate the regression parameters. Yeah, the thing works.

126
00:16:57,760 --> 00:17:00,820
The other thing I need is to talk about the variance.

127
00:17:08,770 --> 00:17:18,219
Yes. I want black. It's good. And it's doing that now.

128
00:17:18,220 --> 00:17:27,830
I have grass. Here we go.

129
00:17:28,200 --> 00:17:34,930
I don't know. All right. Which.

130
00:17:41,480 --> 00:17:44,780
So I wrote this in the notes.

131
00:17:44,930 --> 00:17:52,910
We're going to specify a variance covariance structure, sort of quasi a quasi structure that networks.

132
00:18:13,720 --> 00:18:16,240
So this feature I wonder Janice,

133
00:18:16,240 --> 00:18:25,330
just any over dispersion in the data not accounted for by the variance function so it's often forced to be one for Poisson and binary data.

134
00:18:25,870 --> 00:18:31,870
But you can estimate it and engie the default in R is to estimate it, not to force it to be one.

135
00:18:36,460 --> 00:18:45,830
And this is the way we write a variance covariance structure. The end of the one half is simply a diagonal matrix.

136
00:18:48,980 --> 00:18:54,299
And I have binomial data. So the variance says new times.

137
00:18:54,300 --> 00:18:57,420
One minus new and square root.

138
00:19:00,380 --> 00:19:12,300
And the second observation has me and why me like to one minus that square root down to the sixth one.

139
00:19:12,350 --> 00:19:19,730
My eyes. I just do that.

140
00:19:27,070 --> 00:19:31,390
Well, those are my choices. 90 or 72.

141
00:19:34,510 --> 00:19:42,230
This is listed. When I was in the one place it was.

142
00:19:43,630 --> 00:19:47,030
One half and everything else is zeros.

143
00:19:47,050 --> 00:19:53,200
We're just specifying the marginal variance from the marginal standard deviation of each of the six observations.

144
00:19:57,410 --> 00:20:01,940
And again, when we pre and post multiply by this, we end up with a variance along the diagonal.

145
00:20:03,140 --> 00:20:08,540
So we do this in matrix algebra with lots.

146
00:20:17,310 --> 00:20:22,320
Is there a way for me? Somehow it's all I can get.

147
00:20:25,710 --> 00:20:29,280
All right. Well, stick with that. All right.

148
00:20:30,690 --> 00:20:38,010
So now I'm trying to connect all six observations from the same person and into some sort of quasi looking distribution.

149
00:20:38,640 --> 00:20:42,720
Okay. I haven't specified the joint distribution of all six observations.

150
00:20:43,350 --> 00:20:51,900
I've specified their marginal variances, and I'm going to connect them through this other matrix that I haven't talked about yet, which is Lambda I.

151
00:20:53,770 --> 00:20:57,070
And so this is the working correlation in G lingo.

152
00:20:59,130 --> 00:21:05,770
Correlation. So that could be.

153
00:21:07,670 --> 00:21:13,340
One word could be that. So the sentence.

154
00:21:20,450 --> 00:21:26,659
It could be a11. And then there is a correlation among any two observations.

155
00:21:26,660 --> 00:21:30,140
That's constant exchange exchangeable.

156
00:21:33,070 --> 00:21:36,610
Or it could be the entire thing here.

157
00:21:37,360 --> 00:21:40,420
One, two, three, four, five, six.

158
00:21:41,020 --> 00:21:47,250
Row, row, square root of the third and fourth row in this house.

159
00:21:48,920 --> 00:21:52,520
Exclude fourth or fifth.

160
00:21:53,900 --> 00:22:03,680
And everything else. It's a banded correlation matrix and correlation decreases as to observations from further apart six observations.

161
00:22:04,520 --> 00:22:21,569
So that's this Lambda Survived matrix. So now I have specified all the components of what I need in order to estimate my regression parameters.

162
00:22:21,570 --> 00:22:29,740
The data parameters. So we estimate data.

163
00:22:33,090 --> 00:22:39,239
Estimating an equation and using an estimating equation, of course,

164
00:22:39,240 --> 00:22:45,510
to call it generalized estimating equation, because we're basing it off of a generalized linear model.

165
00:22:48,380 --> 00:22:51,380
So again, we have to add up across subjects.

166
00:22:51,380 --> 00:22:58,580
Now, I have specified this structure for one individual and the structure is the same for all individuals.

167
00:22:59,630 --> 00:23:00,950
So I'm going to add all that up.

168
00:23:02,470 --> 00:23:13,090
So we have this matrix we call Delta I, I have this variance covariance matrix that I just specified times what I observe.

169
00:23:14,270 --> 00:23:21,200
The residuals, the observed minus what the model says it should be on average, then zero vector.

170
00:23:21,950 --> 00:23:38,830
Here's a series of equations here. One for each of the betas. And where Delta II Delta is a matrix is a key.

171
00:23:39,310 --> 00:23:42,520
I am. I am.

172
00:23:42,520 --> 00:23:47,590
I is the number of observations. P is the number of regression parameters.

173
00:23:52,680 --> 00:24:02,630
Matrix of derivatives. Element.

174
00:24:04,830 --> 00:24:15,780
He changed. Equal to the derivative of a person's mean at a given time point.

175
00:24:17,160 --> 00:24:29,660
With respect to that progression parameter. Okay. So I mean, at any one time point is a function of a bunch of covariates.

176
00:24:29,660 --> 00:24:35,550
Sometimes the regression parameters I take a derivative with respect to each of those parameters in the linear predictor.

177
00:24:39,850 --> 00:24:48,330
State. David Kay. She inverse i j.

178
00:24:52,260 --> 00:24:57,900
Again remember GMU is the is the linear predictor new as g inverse of the linear predictor.

179
00:24:59,040 --> 00:25:02,460
Well, this was irrelevant with normal data because we had the G was the identity.

180
00:25:05,180 --> 00:25:10,489
So that's the derivative with respect to being a case of the inverse of the link function,

181
00:25:10,490 --> 00:25:18,500
which for binary data if I use the logit link is x AJ data the supposed to be I don't need to write it.

182
00:25:18,500 --> 00:25:23,410
All right. Excellent. Enough. XP, of course, which.

183
00:25:28,210 --> 00:25:40,510
Divided by one class. So again, sj beda is a sum of its times better.

184
00:25:40,530 --> 00:25:43,640
Many, many times you take a derivative with respect to each of those matters.

185
00:25:44,930 --> 00:25:47,630
Again, some of this cancels out with the variance function.

186
00:25:48,620 --> 00:25:56,320
If you do the algebra because this is the canonical and canonical link related to the variance function in the binomial the binary case.

187
00:25:58,970 --> 00:26:01,330
So all of that goes into this estimating equation.

188
00:26:01,340 --> 00:26:10,130
So we're going to make six of derivatives times our variance covariance matrix kind of times the vector of the observed wise minus the means.

189
00:26:12,760 --> 00:26:15,870
Again, this looks like every other estimating equation you've seen.

190
00:26:15,870 --> 00:26:20,610
And 650 and 651. This is the most general case.

191
00:26:21,480 --> 00:26:26,490
All of those fit into this framework. And if we have independence,

192
00:26:26,940 --> 00:26:36,900
if we have independent sitting inside here and start to simplify back down to 650 and 651 and even what kind of modeling that she'll have or an 11.

193
00:26:38,300 --> 00:26:44,910
And again, for normal outcomes, this vector, this matrix of derivatives is just the design matrix.

194
00:26:46,460 --> 00:26:49,940
Because it was better times x you take the derivative, you get x.

195
00:26:51,650 --> 00:26:55,790
It just gets more complicated with with no other options.

196
00:26:57,770 --> 00:27:02,160
Right. So that's what our is doing. It's figuring all of that out.

197
00:27:02,170 --> 00:27:08,530
If you've told it what kind of data you have, what the link function is, what the variance function is, what kind of correlation structure you want.

198
00:27:09,190 --> 00:27:14,090
It then has the machinery to solve this equation for you without having programing.

199
00:27:16,330 --> 00:27:20,620
So I don't think it's useful to help people program it themselves.

200
00:27:21,590 --> 00:27:25,520
But that's something we could do. Perfect timing.

201
00:27:25,640 --> 00:27:31,130
I've got just about today. Okay.

202
00:27:31,220 --> 00:27:40,670
So the the code in my slides, all the formulas in my slides are applying to this analysis right here.

203
00:27:42,090 --> 00:27:46,110
So I think I have loaded. Yep, I loaded the.

204
00:27:46,290 --> 00:27:50,280
So there the the forex x variables for my data here.

205
00:27:51,630 --> 00:27:56,250
Indicators of gender and time again.

206
00:27:56,280 --> 00:28:01,770
The very first model that I set for you last time was an independent correlation structure.

207
00:28:02,760 --> 00:28:06,630
So if you are assuming in and seriously.

208
00:28:23,860 --> 00:28:33,460
If you are fitting G.E. with an independent correlation structure, you are fitting a glamor that is exactly the same as a glam.

209
00:28:35,620 --> 00:28:40,929
And again, I have forced the scale parameter to be one of the few parameters.

210
00:28:40,930 --> 00:28:46,780
One here. You don't have to do that. The reason I did it is because I wanted to connect it back with jobs.

211
00:28:46,870 --> 00:28:51,310
So again, this is what we got for our regression coefficients.

212
00:28:51,730 --> 00:28:57,980
The naive standard error is definitely wrong here if there's any correlation in the data, because we assumed independence.

213
00:28:58,000 --> 00:29:00,910
We've tried to, and that's about that with normal data. The same thing holds here.

214
00:29:02,470 --> 00:29:08,650
Those new standards are either too big or too small, depending on whether we have a between or a within a person comparison.

215
00:29:09,550 --> 00:29:12,820
The robust standard errors are what we call the sandwich standard errors.

216
00:29:12,820 --> 00:29:16,510
They appear to senators and there they are.

217
00:29:17,080 --> 00:29:19,270
And then, of course, there's the statistics for both of those,

218
00:29:20,110 --> 00:29:27,070
which doesn't give you P values because there's a constant debate on what's the appropriate of distribution.

219
00:29:29,080 --> 00:29:34,000
I just compare it to a normal zero one. Asymptotically, everything gets close to a normal zero one.

220
00:29:34,690 --> 00:29:38,380
So with enough data, that's a sufficient comparison for a few value.

221
00:29:39,160 --> 00:29:42,940
But there's lots of work out there on other sorts of no distributions.

222
00:29:43,690 --> 00:29:53,349
So just to show you, if I got rid of this, so again, this is not the default.

223
00:29:53,350 --> 00:29:57,790
The default is that's what's not with overlap. So is this.

224
00:30:00,560 --> 00:30:04,710
They want me. Maybe I did show you this last time. I was so rushed last time.

225
00:30:04,830 --> 00:30:09,850
If you want a fantastic algebra exercise, I forgot what I did here.

226
00:30:12,320 --> 00:30:19,500
I will tell you what the scale parameter is. If it's estimated in this case, it's so darn close to one, it's really didn't matter what we did.

227
00:30:21,050 --> 00:30:26,060
So there isn't much of a dispersion playing out really over dispersion.

228
00:30:27,810 --> 00:30:30,870
But there's no need to estimate that parameter. One seems to be close enough.

229
00:30:31,720 --> 00:30:34,080
So again, if you want to do some model comparisons,

230
00:30:34,080 --> 00:30:39,480
I ask you to compare exchangeable independence and air one structures and you want to pick among them.

231
00:30:39,900 --> 00:30:44,570
There is no hypothesis testing. You have to do it by some quantity.

232
00:30:45,100 --> 00:30:48,750
QIC is the only one that's program that I know anything about.

233
00:30:49,740 --> 00:30:54,890
There is no AC I so forth because those are likelihood methods and we don't have a likelihood here.

234
00:30:54,900 --> 00:31:02,700
There is no likelihood in hand and I didn't run the other model.

235
00:31:07,860 --> 00:31:11,320
So there's lots of other numbers. There's a quasi likelihood.

236
00:31:11,340 --> 00:31:15,900
So there is an actual number that looks like a likelihood because of likelihood.

237
00:31:17,190 --> 00:31:22,380
But again, comparing quasi likelihoods, you can multiply by two and look it up in a case where a distribution,

238
00:31:22,760 --> 00:31:28,320
right, that's likelihood theory quantifier is there's a accuracy value back up there on the left.

239
00:31:31,430 --> 00:31:38,870
Let's go back here. So when I said G.E. with an X, with an independent correlation structure, and I said,

240
00:31:38,870 --> 00:31:45,020
show me the coefficients again, you got naive and you got sandwich variance estimate.

241
00:31:47,210 --> 00:31:55,850
You didn't need the G function to do this. You simply could fit the GLM function because that assumes independence already.

242
00:31:57,500 --> 00:32:03,860
And then to use the library I gave you earlier this semester, just add sandwich variants estimated to this model.

243
00:32:05,800 --> 00:32:11,420
And if you look at those if you look at the estimates. Model base standard errors.

244
00:32:12,700 --> 00:32:19,360
And the sandwich standard is using this approach. They are identical to what she did.

245
00:32:21,500 --> 00:32:27,260
So. I use this approach very often when I analyze data with Judy.

246
00:32:28,760 --> 00:32:35,010
Independence is the easiest thing to assume, and I can just fix the standard errors later with a robust standard or robust.

247
00:32:35,300 --> 00:32:38,690
Say that exchangeable empirical standard error.

248
00:32:39,740 --> 00:32:42,740
In the end, I don't really care what the correlation structure is.

249
00:32:43,370 --> 00:32:49,900
I just want to fix the standard errors to get good inference. And I know that my estimates are consistent, right?

250
00:32:51,080 --> 00:32:55,850
In large samples they're going to get close to the truth regardless of what my matrix aims.

251
00:32:57,080 --> 00:33:00,860
But let's fit other working correlation structures.

252
00:33:02,870 --> 00:33:05,600
So now I'm going to Fiji once again, same model.

253
00:33:06,560 --> 00:33:15,770
The correlation structure is exchangeable and like I noticed last time, I don't know why I didn't bother to do this.

254
00:33:18,110 --> 00:33:22,690
What's. I can run and ramble.

255
00:33:27,640 --> 00:33:32,680
This is all iterative. And remember, GLM is an iterative process.

256
00:33:33,680 --> 00:33:37,220
Iterate away to these squares, but it goes very, very fast.

257
00:33:37,340 --> 00:33:45,020
But G, as I pointed out last time in class, the G function and R and I think every G program just assumes independence in the beginning.

258
00:33:45,050 --> 00:33:49,700
Let's just start with those coefficient estimates and then we'll iterate back.

259
00:33:49,700 --> 00:33:56,180
We'll get a new estimate of Rho. We'll put that in the weight matrix re estimated beta until things converge.

260
00:33:57,050 --> 00:34:00,110
So again, this bugs me.

261
00:34:00,110 --> 00:34:05,540
Someone who wrote this program decided that they needed to tell me what the starting values were for the regression coefficients.

262
00:34:06,830 --> 00:34:13,700
So whenever I run simulations, that thing pops up a thousand times my screen and I can't turn it off.

263
00:34:14,330 --> 00:34:17,150
There was no argument to shut it off anyway,

264
00:34:17,480 --> 00:34:22,760
so I don't know why someone felt it necessary to tell me what the initial regression estimates were, but there they are.

265
00:34:23,120 --> 00:34:29,699
So I have fit now exchangeable correlation. The estimates again, remember,

266
00:34:29,700 --> 00:34:33,870
the estimates here are not going to be the same as they were with independents

267
00:34:34,950 --> 00:34:39,300
because there's a wait matrix inside the estimation of data that's now different.

268
00:34:39,960 --> 00:34:43,710
So you're going to get two different sets of estimates. Both are consistent.

269
00:34:46,930 --> 00:34:50,110
But it doesn't necessarily mean that one will be closer to the truth than the other.

270
00:34:50,560 --> 00:34:57,160
They're just in large samples, you know, that you're doing something that a lot of people we get values that are close to the truth.

271
00:34:58,210 --> 00:35:04,030
That naive standard error is based upon a working correlation structure of exchange ability.

272
00:35:05,900 --> 00:35:11,150
So you could use those model based standard errors and you could say, you know what, I have already accounted for the correlation.

273
00:35:11,540 --> 00:35:15,820
They don't need a further sandwich variance estimate. But you can get it.

274
00:35:16,370 --> 00:35:22,010
And you could see that the naive and the robust, although a little bit different, aren't that far away from each other.

275
00:35:23,090 --> 00:35:28,030
So you could choose to use the 90 senators, you could choose to use the empirical standard errors.

276
00:35:28,050 --> 00:35:34,150
That's up to you. As I said, I get people if whenever I see an applied the average.

277
00:35:34,640 --> 00:35:38,230
They're just naturally is the empirical sandwich variance estimate or.

278
00:35:39,200 --> 00:35:44,210
Again, my argument is if you're going to use the empirical standard error, why would you bother to model the correlation?

279
00:35:45,400 --> 00:35:54,760
Yes. So if we do decide to use this sandwich, like you said, we can't use likelihood theory.

280
00:35:54,760 --> 00:35:59,229
Right. Or we can't use likely here. Even with normal models.

281
00:35:59,230 --> 00:36:07,430
Back when. So the the standard errors that are from the sandwich formula warrants in the likelihood.

282
00:36:09,770 --> 00:36:14,530
So I'm just I'm curious about like what they're calling the robust Z.

283
00:36:14,540 --> 00:36:18,770
Is that like a wall? The robust Z is the wall statistic.

284
00:36:19,250 --> 00:36:23,990
It is the estimate, the coefficient estimate divided by its standard error.

285
00:36:25,730 --> 00:36:33,680
Right. Did I make that explicit? So if you take column one divide by column two, you get the naive Z.

286
00:36:34,700 --> 00:36:39,560
If you take column one divided by column for you, get the robust receipt.

287
00:36:40,190 --> 00:36:45,230
And you know, those are WALTZER tests that you can compare to 1.96 for critical time.

288
00:36:46,390 --> 00:36:52,790
Okay. Those are valid. Those are valid about those tests. Asymptotically, those things should be close to a normal zero one.

289
00:36:56,960 --> 00:37:03,080
And then, of course, I want to see and I have to use a different yield package or a G package to get that.

290
00:37:04,580 --> 00:37:06,680
I believe it's a little bit lower. We'll compare them in a second.

291
00:37:08,000 --> 00:37:12,590
The other thing you can get now is if you were that interested in modeling the correlation,

292
00:37:13,400 --> 00:37:17,210
I'm curious to know what is the correlation between any two observations on a person?

293
00:37:18,800 --> 00:37:28,900
And G tells me that it's approximately 8.16.166, but pretty common.

294
00:37:29,300 --> 00:37:32,430
So that's a pretty average amount of correlation here.

295
00:37:34,010 --> 00:37:40,940
So again, remember, correlation is a concept. A correlation matrix is a concept that came from the multivariate normal distribution.

296
00:37:41,600 --> 00:37:46,370
When we start talking about correlation of Poisson random variables or correlation of binary,

297
00:37:47,540 --> 00:37:53,120
it gets a little bit it's not quite the same thing, but it is a rough interpretation.

298
00:37:53,130 --> 00:37:56,990
There is a mild amount of correlation with 10% correlation. Again.

299
00:37:57,020 --> 00:38:04,280
Think about this in terms of variability. When there's correlation, positive correlation, that means that there is variability between people.

300
00:38:07,010 --> 00:38:10,640
My observations tend to cluster together a little bit like one six.

301
00:38:14,280 --> 00:38:20,580
And my observations tend to look like each themselves and we're all kind of spread out through each other, but it's not huge.

302
00:38:21,660 --> 00:38:26,760
Well, this is binary. That's right. And fitting binary again.

303
00:38:26,770 --> 00:38:32,460
What does it mean for a01 and a01 random variable have .16 correlation.

304
00:38:33,510 --> 00:38:37,150
Okay. All right.

305
00:38:37,170 --> 00:38:40,720
And then the other option I'm going to give you in this homework is ar1 correlation.

306
00:38:40,750 --> 00:38:53,020
Now, we think the correlation might decrease over time. So same model scale six.

307
00:38:53,070 --> 00:38:57,450
True. So here the correlation structure is a R.

308
00:38:57,570 --> 00:39:02,010
Yes. And there is ar2 correlation structure.

309
00:39:02,040 --> 00:39:06,720
There is other correlation structures. So this movie means what is the value of M.

310
00:39:07,950 --> 00:39:15,750
And so end is one. And again, if you say Aram and you don't specify MVP, I think the default is one.

311
00:39:16,320 --> 00:39:22,080
But I'm not sure you can figure that out by trying it. Okay. So all of this coach's index right here is error one.

312
00:39:23,640 --> 00:39:29,370
Everything else is the same. And again, here I get the results.

313
00:39:29,790 --> 00:39:31,200
I get coefficient estimates,

314
00:39:31,650 --> 00:39:39,600
I get standard errors with the assumption of error one correlation and then a sandwich variance estimate for anything that's left in the residuals,

315
00:39:40,360 --> 00:39:49,080
you know, not huge differences. So again, remember, you compare a robust Z to a critical value of 1.9, six or two of our two.

316
00:39:50,040 --> 00:39:56,580
Didn't really matter when we use for standard error here. There are some significant differences according to this model.

317
00:39:57,660 --> 00:40:01,530
Now, remember what all of these coefficients are.

318
00:40:02,850 --> 00:40:08,250
These coefficients are simply the mean of the males between three or below,

319
00:40:09,000 --> 00:40:19,020
the mean above for for males and similarly for females three and below and females three and above on the logit scale.

320
00:40:21,500 --> 00:40:24,280
So these these aren't really what I'm interested in.

321
00:40:24,290 --> 00:40:28,640
These are telling me whether the means are different from zero or the coefficients are different from zero.

322
00:40:29,730 --> 00:40:33,530
I don't really care about that. I can take care of. Two of them are different from each other.

323
00:40:34,310 --> 00:40:38,330
So we're going to do that in a second. So I'm not surprised that these are big, but they're not what I'm interested in.

324
00:40:39,290 --> 00:40:42,650
But our doesn't know that, and it just gives me that table.

325
00:40:44,450 --> 00:40:50,120
And again, if you want the working correlation structure from energy fit, there's the code.

326
00:40:50,300 --> 00:40:55,280
And we can see that the correlation starts at 2.22 and drops off pretty quickly.

327
00:40:59,850 --> 00:41:04,560
Again, whatever that means, whatever that means. In terms of binary data, I don't really know.

328
00:41:04,870 --> 00:41:09,270
But so there are the choices of the three models.

329
00:41:13,920 --> 00:41:21,489
Not much difference. The crises may not be different, but if you have longitudinal data,

330
00:41:21,490 --> 00:41:27,280
don't ever go with independence just because the independence is close to the other two right here.

331
00:41:27,610 --> 00:41:32,950
Don't pick number one. You have to believe there's some correlation in the data and the.

332
00:41:32,950 --> 00:41:37,930
Q I just can't. Again, the correlation here is fairly small, so this isn't picking it up very much.

333
00:41:38,440 --> 00:41:45,220
So I would say exchangeable. And everyone basically said just as well and exchangeable is a little bit smaller.

334
00:41:46,450 --> 00:41:56,060
So maybe I'll go with exchangeable here. And that's what I did and said, my final model is model two.

335
00:41:56,330 --> 00:41:59,900
So the second approach was exchangeable correlation structure.

336
00:42:01,730 --> 00:42:09,200
So now what I want to do is show you again. And what I keep saying is I do not want to see a table of coefficient estimates on a large scale.

337
00:42:10,710 --> 00:42:14,580
It is useless to me. I don't know what the logic of negative three means.

338
00:42:16,530 --> 00:42:21,720
So I'm going to report my results as a as to show you a way that I would have approached this problem.

339
00:42:23,190 --> 00:42:29,909
So I'm going to save the coefficient estimates and to get the variance covariance matrix

340
00:42:29,910 --> 00:42:34,020
of these coefficient estimates and I'm going to base them upon the robust variance,

341
00:42:34,320 --> 00:42:38,190
not the model based. And they were pretty close to each other. So it's really not relevant here.

342
00:42:40,490 --> 00:42:45,230
I'm going to have to switch everything from a larger scale back to the original scale of binary data.

343
00:42:45,800 --> 00:42:51,380
So the inverse of the logit again, log it. The inverse of log is exponential.

344
00:42:52,190 --> 00:42:56,300
So I don't know who came up with exit as the inverse for logit.

345
00:42:58,550 --> 00:43:03,170
Yeah, it works out. So the exit function is simply e to something divided by one.

346
00:43:03,170 --> 00:43:06,740
Plus either that something that produces a number between zero and one.

347
00:43:08,060 --> 00:43:12,590
Right. So what do I want to know? So I'd like to know.

348
00:43:12,590 --> 00:43:15,740
Are males and females different before visit for.

349
00:43:16,250 --> 00:43:19,940
So the blue and the green at times three, two and one. Are they different from each other?

350
00:43:20,900 --> 00:43:28,340
I'd like to know. Do do the males have mean a mean that's different from the first three time points relative to the later three time points?

351
00:43:29,240 --> 00:43:35,000
And do females change over time? Is there a mean at the first three time points different from their mean of the last three time events?

352
00:43:35,540 --> 00:43:37,610
So I have three contrasts of interests here.

353
00:43:39,860 --> 00:43:47,480
And again, if you have a bunch of regression coefficients and you have the variance covariance matrix of the regression coefficients,

354
00:43:48,140 --> 00:43:55,310
you can compute the the value and the standard error of any linear combination of those things.

355
00:43:57,490 --> 00:44:00,610
So that's thinking there might be libraries that do this for you.

356
00:44:00,610 --> 00:44:04,270
But I was an up jay myself, so I have a contrast matrix here.

357
00:44:05,470 --> 00:44:13,240
So my first contrast says take the males at the first three timepoints and look at the difference to the last three time points.

358
00:44:13,240 --> 00:44:17,170
So one negative one zero zero the last two coefficients have to do with the females.

359
00:44:17,210 --> 00:44:18,730
I don't care about those in this question.

360
00:44:19,450 --> 00:44:25,299
My second contrast says compare the males of the first three time points to the females at the first three time points.

361
00:44:25,300 --> 00:44:29,740
So one zero, negative one zero. And then I want to see if there's a change in the females.

362
00:44:29,740 --> 00:44:36,220
So there's my four. Those are the three things I'm interested in. I want standard errors for those linear combinations.

363
00:44:38,140 --> 00:44:43,570
And hopefully you have seen all this, I'm sure, in your regression classes.

364
00:44:44,350 --> 00:44:54,100
So now that I have my contrast, I take that contrast matrix matrix times my coefficients and the first row is going to say beta one minus beta two.

365
00:44:55,030 --> 00:44:59,710
Second contrast is beta one minus beta three and the third one is beta three, minus beta four.

366
00:45:01,240 --> 00:45:08,200
And then to get the variance covariance of those three contrasts, again, it's a, it's a quadratic form there.

367
00:45:09,100 --> 00:45:15,910
You take the C matrix times of the variance covariance matrix times the transfer of the contrast matrix.

368
00:45:18,550 --> 00:45:22,540
And so cc, I guess instead of just highlighting and pointing towards.

369
00:45:27,590 --> 00:45:31,770
So cc era of the three contrasts that should be three numbers, right?

370
00:45:31,820 --> 00:45:37,400
The differences either within within gender over time or between the two genders.

371
00:45:40,220 --> 00:45:45,180
And then. This should be a three by three matrix.

372
00:45:51,180 --> 00:45:56,440
There it is. So this is the variance of my first question.

373
00:45:57,150 --> 00:46:00,930
Second one, third one. And these are the variances between them.

374
00:46:00,930 --> 00:46:04,559
I don't really care about the variances, but they're standard errors.

375
00:46:04,560 --> 00:46:14,310
The standard errors are the square root of the diagonal. And then I get a Z statistic which takes the value divided by its standard error.

376
00:46:14,550 --> 00:46:19,050
So we're taking a lot earlier, and I'm going to compare that to a standard normal zero on.

377
00:46:20,080 --> 00:46:27,400
So each value I want to find out where it is in a standard normal distribution and get a cumulative probability multiplied by two to get a p value.

378
00:46:29,760 --> 00:46:38,060
And then I would put this all into a table. So again, I have the contrasts on the logit scale.

379
00:46:40,320 --> 00:46:43,470
And I have inference related to this stuff on the larger scale.

380
00:46:43,770 --> 00:46:46,860
I want to get it on the other scale when it's sort of the inverse.

381
00:46:47,310 --> 00:46:59,190
So I take the next bit of the coefficients. I'm going to take the exit of the coefficients -1.9, six times their standard errors and 1.6 plus.

382
00:46:59,490 --> 00:47:07,050
So again, compute your confidence interval on the coefficient level and then translate it back to the other scale.

383
00:47:08,670 --> 00:47:12,850
And then I've got my three p values. So again, what does this look like?

384
00:47:12,870 --> 00:47:16,070
What am I doing here? No.

385
00:47:18,170 --> 00:47:24,830
How do you define that? The two functions I use the most in R that are not programed are expert in logic.

386
00:47:26,000 --> 00:47:37,510
I don't know why they don't exist in art. And you know, and let's give it some names here.

387
00:47:37,810 --> 00:47:49,100
So I want to give it some real names and some code names. So here is a table.

388
00:47:49,110 --> 00:47:55,860
And again, please don't give me seven decimal points. I don't want seven decimal points.

389
00:47:55,890 --> 00:48:02,260
Two is always sufficient. Three is okay. You know, it's two three.

390
00:48:03,690 --> 00:48:10,510
Skip runs a chance of going to small here. So remember, I had four regression equations.

391
00:48:11,750 --> 00:48:20,060
Four regression parameters. Excuse me. One, explaining whether it was male or female and whether it was time three and below or four and above.

392
00:48:21,230 --> 00:48:27,440
So I exponentially rated those coefficients. They gave me a probability of this on the exit scale.

393
00:48:28,040 --> 00:48:36,020
So I estimate the probability observed for me, for males early on is around 7.6% of males,

394
00:48:36,560 --> 00:48:45,560
about 4.7% of what's 4.7 for males later and correspondingly .06 for females goes up to

395
00:48:45,560 --> 00:48:50,360
about plan three for later in the study now and that's not too bad with my picture.

396
00:48:52,130 --> 00:48:55,520
They get a 95% confidence interval for those probabilities.

397
00:48:57,200 --> 00:49:02,320
So I don't have any odds ratios here because of the way I defined my X matrix.

398
00:49:02,600 --> 00:49:06,230
I didn't have an intercept that then. I was comparing a group to some reference group.

399
00:49:06,590 --> 00:49:11,810
I defined the four groups that didn't have an intercept so I could do this because there are no.

400
00:49:12,080 --> 00:49:16,220
Everybody. Every parameter is a group that I'm interested in. There's no comparison among them.

401
00:49:17,060 --> 00:49:20,690
So I have a 95% confidence interval for each of those probabilities.

402
00:49:23,180 --> 00:49:28,580
And then I have three P values. And again, you have to be careful when you just give a column a p values.

403
00:49:28,730 --> 00:49:33,170
I would tell my my boss or my audience or my professor.

404
00:49:33,860 --> 00:49:41,720
What do these P values mean? The first p value is comparing the difference between 0.047 and 076.

405
00:49:42,260 --> 00:49:49,190
Is there a change over time in males? And here I don't have enough evidence of a significant change in males.

406
00:49:50,250 --> 00:49:57,510
The second p value is comparing females early on with males early on, and that P values less than Plano five.

407
00:49:57,510 --> 00:50:06,210
So we might say early on in the study there was a difference between males and females, about five points, five points probability.

408
00:50:07,290 --> 00:50:10,440
And the last p value is the comparison within females over time.

409
00:50:10,860 --> 00:50:14,250
And their appearance, of course, doesn't appear to be much change in the females over time.

410
00:50:16,170 --> 00:50:20,640
This is much easier for an audience to understand than the original table.

411
00:50:21,880 --> 00:50:24,870
Okay, so again, think about that.

412
00:50:25,530 --> 00:50:39,960
When you are doing all of your analysis, especially with binary data, I want to show you though, did you and 651 ever use a log link with binary data?

413
00:50:42,170 --> 00:50:51,160
Use a log link with the sun data. Probably never fit a long length in a larger halo just yet.

414
00:50:52,240 --> 00:50:59,100
I don't know what an odds ratio means, but I do understand what a risk ratio means.

415
00:50:59,110 --> 00:51:03,969
If you tell me my probability is twice as large. I get my head around that.

416
00:51:03,970 --> 00:51:08,890
When you say my odds is twice as high. That's a little harder for me to figure out.

417
00:51:10,090 --> 00:51:17,560
So you can do this. England's. So I'm going to fit a logit model to the same data.

418
00:51:18,550 --> 00:51:25,600
So I'm going to say the zero one indicator of zero from here is now I'm going to have an intercept.

419
00:51:27,070 --> 00:51:31,330
So the intercept is reflecting males at the earlier time points.

420
00:51:32,380 --> 00:51:34,300
And then I have the other three variables in the model.

421
00:51:34,750 --> 00:51:42,760
So now the coefficients are going to estimate the relative change between the intercept and that indicator.

422
00:51:42,760 --> 00:51:45,850
Right. So we fit this model here.

423
00:51:50,010 --> 00:51:54,390
So we have taught you that. Then you then exponentially add those coefficients.

424
00:51:58,990 --> 00:52:08,680
Summary. Oh, Jamal, it's not serious, right?

425
00:52:08,710 --> 00:52:17,380
I don't want to see this. So that negative point, the second row there, first column, that estimate of negative .51.

426
00:52:18,310 --> 00:52:21,820
Right. That's the change in males over time.

427
00:52:22,630 --> 00:52:28,420
On the logic scale, if I exponentially rate that negative point of five when I get an odds ratio,

428
00:52:29,200 --> 00:52:33,850
what's the odds of therapy for me for males later in the study versus earlier?

429
00:52:36,490 --> 00:52:42,550
So we can do that and I get point five, 9.6.

430
00:52:44,380 --> 00:52:52,870
So the odds of zero, Thelma, off later on in the study is 60% of the odds of zero Bartholomew earlier in the study for males.

431
00:52:53,870 --> 00:53:00,020
What? Okay. And again, you could get this number.

432
00:53:02,230 --> 00:53:05,670
From my original table. I'm just retraumatizing everything.

433
00:53:07,420 --> 00:53:10,989
If I give you probabilities for males. Before and before.

434
00:53:10,990 --> 00:53:19,420
And after. Before and after. You can get an ratio probability divided by one minus probability divided by the other,

435
00:53:19,420 --> 00:53:25,990
probabilities of a by one minus that if you take 3.76 and 2.047 and you compute in odds ratio,

436
00:53:26,830 --> 00:53:31,340
it's going to be darn close to what this game right here is.

437
00:53:31,630 --> 00:53:38,500
We're all doing the same thing. One time I reported the actual probabilities because I have a court study here and so I can

438
00:53:38,500 --> 00:53:46,030
estimate probabilities or I can compute odds ratios if you want to compute risk ratios,

439
00:53:46,630 --> 00:53:50,650
if you want to compute the ratio of the two probabilities, it's not the two odds.

440
00:53:51,970 --> 00:53:53,950
The way you can do that is with a log link.

441
00:53:55,240 --> 00:54:00,640
And this is where quasi likelihood and this is where you probably knew this is 651 because you didn't spend a lot of time on quasi,

442
00:54:01,150 --> 00:54:05,850
quasi exponential families. So this could right here.

443
00:54:08,090 --> 00:54:15,880
It's fitting a binomial regression model. So what you have to do is say quasi.

444
00:54:15,940 --> 00:54:25,000
So this is a this is a specification of how if you wanted to find the the link in the variance function, I'd say of an exponential family,

445
00:54:25,600 --> 00:54:33,270
you use the quasi family and I want a log link but I want to go back to the original variance, the binomial variance of mutants when I'm with you.

446
00:54:34,030 --> 00:54:38,080
Right. So that's how you fit a log link with binary data.

447
00:54:40,690 --> 00:54:47,230
And I believe it's the same in Glamis. If you want to fit a GLM in 651 using a log link, read it.

448
00:54:48,190 --> 00:54:53,620
I believe the code is the same. It's just that you don't have a correlation structure and so forth.

449
00:54:54,010 --> 00:55:01,419
So I can set this model and it usually will converge again.

450
00:55:01,420 --> 00:55:04,600
Remember, this log link does have its problems.

451
00:55:05,860 --> 00:55:12,580
It doesn't necessarily produce a mean for everybody that's between zero and one because the log will allow it to go up to infinity.

452
00:55:13,710 --> 00:55:19,020
But I don't really care about the fit and values what I want in the coefficients.

453
00:55:19,410 --> 00:55:27,000
So now I have coefficients using a log link. So if I exponential data coefficient now it is no longer an odds ratio.

454
00:55:27,000 --> 00:55:30,390
It's a risk ratio. It's the ratio of the two probabilities.

455
00:55:31,770 --> 00:55:36,630
So now what I have done essentially in this model is I have the ratio of these two numbers.

456
00:55:37,640 --> 00:55:41,030
Rather than the ratio of their odds from those numbers.

457
00:55:42,910 --> 00:55:50,710
And if I exponential rate that I get at a risk ratio of 0.62 and I remember the odds ratio.

458
00:55:52,210 --> 00:56:00,460
Was about point six. And again, it all goes back to all the stuff we talk about where the overall probability of an event is low.

459
00:56:01,860 --> 00:56:06,719
An odds ratio and a risk ratio are essentially the same thing. And that's what the case is here.

460
00:56:06,720 --> 00:56:11,610
We have a very rare outcome and the odds ratio and the risk ratio are pretty close to each other.

461
00:56:13,290 --> 00:56:18,390
So again, the point of showing you all this is you have different ways with binary data and how you would report the data.

462
00:56:18,810 --> 00:56:23,460
Do you wanna report the probabilities? Do you want an odds ratio or a risk ratio?

463
00:56:24,180 --> 00:56:31,980
You can do all those things. I mean it's not that that hard to do with the machinery we have at of the week.

464
00:56:34,320 --> 00:56:36,180
It's everything. It's 3:00.

465
00:56:37,600 --> 00:56:43,739
Um, again, my goal originally was to have people to be able to have time to work on their homeworks because people are telling me,

466
00:56:43,740 --> 00:56:47,100
I can't come to your class, Dr. Brown, because I have so much homework and other stuff to do.

467
00:56:48,000 --> 00:56:53,160
So now you can come to class and do your homework. But anyways, it is 3:00 on a Friday.

468
00:56:53,490 --> 00:56:56,520
I'm happy to stay here and talk about what you're doing with homework.

469
00:56:56,520 --> 00:57:04,050
For example, please and go out and enjoy the nice weather, your candy that you got this morning.

470
00:57:05,340 --> 00:57:11,520
Are there still cupcakes in the counter? Yeah, but there are also other food has a lot of other food, but it's awesome.

471
00:57:11,940 --> 00:57:14,970
Like, I need more food. But Angelo's garden.

472
00:57:15,180 --> 00:57:18,450
I heard the pictures were awesome. Diamonds? Yeah.

473
00:57:18,450 --> 00:57:23,040
Zingerman's. All right. With that, we are done with G.

474
00:57:24,420 --> 00:57:28,590
One day we're going to start with work and we'll go from there.

475
00:57:29,730 --> 00:57:32,220
He said, I'll be around as long as folks are in the room.

