1
00:00:07,800 --> 00:00:14,440
Just like a last minute question book versus wondering, just like for like this.

2
00:00:14,800 --> 00:00:33,850
Yeah. Mhm. Like I had this question so like are our contacts that and which well like whichever you feel based on your understanding,

3
00:00:33,850 --> 00:00:36,069
you can't, you can write it. Um, yeah.

4
00:00:36,070 --> 00:00:43,870
Basically is which parameter that really connects the observed classes and the, you know, the length of the process.

5
00:00:43,910 --> 00:00:49,120
Right. So yeah. So probably ah she likes to say yeah, yeah.

6
00:00:49,150 --> 00:01:05,990
Okay. Gotcha. That's correct. So but this is exactly the right thing to do.

7
00:01:07,370 --> 00:01:10,660
Yeah, right, right, right.

8
00:01:13,680 --> 00:01:31,150
Sure. I would like you to go back to the last one for that.

9
00:01:31,540 --> 00:01:37,150
Okay. Thanks for the second homework. Finally, I have the chance to see your second homework.

10
00:01:39,340 --> 00:01:49,570
Actually, I've been looking forward to this for a while and hopefully you had your good fourth break.

11
00:01:51,490 --> 00:01:55,420
So I think that's. It's good that I have a break then.

12
00:01:55,660 --> 00:02:01,209
So you can prepare midterms, solve the homework problems.

13
00:02:01,210 --> 00:02:09,970
And Homework two is a little bit lengthy in the in the sense that you spend quite a bit of time pulling up data, doing data integration.

14
00:02:11,080 --> 00:02:14,830
But I think that's what I did when I was a student, of course, that, you know,

15
00:02:14,830 --> 00:02:20,470
there's but you learn something after you get your teaching degree or whatever that

16
00:02:21,640 --> 00:02:26,110
degree you have from the probably you will surprise other people to do the work.

17
00:02:26,110 --> 00:02:31,269
I mean, we you become older and and you start to show by people to do this work,

18
00:02:31,270 --> 00:02:37,509
you don't need to really physically, you know, do do things to yourself that you need to understand.

19
00:02:37,510 --> 00:02:39,130
What are the key steps,

20
00:02:39,790 --> 00:02:49,269
how like which steps that people are likely to make some errors or some inconsistencies in data cleaning data integration at will you will

21
00:02:49,270 --> 00:02:56,860
you should write someone to do you know exactly where you are to pay attention to because they have pages of things right in practice.

22
00:02:57,430 --> 00:02:59,919
Then you need to turn to the right page,

23
00:02:59,920 --> 00:03:10,639
the right place to do it to to to identify issues because you have experience of such similar data cleaning and data preparation.

24
00:03:10,640 --> 00:03:15,430
Well, then you know where you should really pay attention to.

25
00:03:16,120 --> 00:03:26,589
And I would say that, I mean, of course, from your GSR work or other you or data analysis work, you have some experience.

26
00:03:26,590 --> 00:03:32,350
But having a little bit more experience when we are a student is really something good.

27
00:03:32,350 --> 00:03:40,940
I know that it's very time consuming. You feel sometimes it's wasting your time, but when you come to a practical work, either you work on Facebook,

28
00:03:40,950 --> 00:03:47,260
Google or a pharmaceutical company, you have to deal with data kind of data process all the time.

29
00:03:47,590 --> 00:03:53,260
You don't have much time really talking about I mean, you'll have time to do like Cox Regression or,

30
00:03:53,590 --> 00:04:00,399
you know, the minor regression, but most of time, you know, your work is the data are data processing.

31
00:04:00,400 --> 00:04:04,660
So having some experience is very important to me.

32
00:04:04,660 --> 00:04:05,920
I do that quite a bit.

33
00:04:05,920 --> 00:04:17,770
I want to so as a student and of course I don't have time to do it, but I'd like to I'd like you to do it and I will pay attention to those places.

34
00:04:17,770 --> 00:04:21,399
I feel that you're likely to have some issues.

35
00:04:21,400 --> 00:04:35,200
But anyway, so if something any questions or concerns and we we'll have midterm next week I will look at the your homework and to

36
00:04:35,200 --> 00:04:46,419
what stage of data processing and grouping you have done so that I can sort of make some midterm questions next week.

37
00:04:46,420 --> 00:04:50,020
So so I will talk about the midterm on Tuesday next week.

38
00:04:50,530 --> 00:04:57,610
Yeah. So in the last US you said that we can have like an open mind at the start of this class to discuss like what procedures we use.

39
00:04:57,670 --> 00:05:03,670
Oh yeah. Yeah. Cool. Can we hear from various groups about.

40
00:05:03,910 --> 00:05:08,670
Yeah, do one come from two to. Penetration where you want just verbally.

41
00:05:08,970 --> 00:05:13,440
I don't know. Okay, cool. That's very nice.

42
00:05:13,770 --> 00:05:32,240
I like you guys to share your experience. Before season begins.

43
00:05:32,780 --> 00:05:39,770
Okay, so this is just the pdf of mine marked down that I'll slightly go for and explain what we did.

44
00:05:40,700 --> 00:05:44,509
So as most of you know, we were given three data sets.

45
00:05:44,510 --> 00:05:48,739
The first one was the weekly death count in the US.

46
00:05:48,740 --> 00:05:55,219
They were subdivided by different age and sex groups and one of the first steps we had to do is figure out the

47
00:05:55,220 --> 00:06:01,010
best way into like getting our age groups because the ones you enjoy that before you started protection came.

48
00:06:01,010 --> 00:06:15,200
Introduce yourself. Yes, my name is Dan. And then and so the age groups we wanted were 0 to 17, 18 to 64 and 65 plus.

49
00:06:15,500 --> 00:06:19,240
So for some age groups, it was very easy under one one, two, four,

50
00:06:19,250 --> 00:06:30,110
5 to 14 easily fit into the 0 to 17 group while 25, 34, 25 to 34, 35 to 44, 45 to 54 and 55 to 64.

51
00:06:30,320 --> 00:06:33,500
Nicely fit into the 1864 category.

52
00:06:33,650 --> 00:06:41,240
And then for a 65 plus age category was also simple with three age categories that neatly fit into those categories we wanted.

53
00:06:41,630 --> 00:06:53,480
Now the problem category was this age group, 15 to 24 age, and here you have teenagers going from 15 to 24 where three of those ages, specifically 15,

54
00:06:53,480 --> 00:07:03,530
16 and 17 year olds make up part of the first group, one 0 to 17, while the rest, seven of those age groups make up the second part of the age group.

55
00:07:03,530 --> 00:07:07,730
We one going from 18 to 64, specifically 18 to 24.

56
00:07:08,120 --> 00:07:13,909
So the way we kind of thought of doing it was using a binomial sampling approach where we have,

57
00:07:13,910 --> 00:07:19,130
let's say, ten people in this age, category 15 to 24 that died in a specific week.

58
00:07:19,580 --> 00:07:26,840
We give a probability that there are 15 to 17 with 3/10 probability, as that's how many years are within that interval.

59
00:07:26,840 --> 00:07:32,330
And then we randomly sample how many people we're putting into that 15 to 17 assumption of uniform.

60
00:07:32,700 --> 00:07:38,870
Yes, we're assuming that's the number. Probabilities is just over 50.

61
00:07:39,650 --> 00:07:46,280
And when you look at the data, you might disagree with that assumption specifically on problem two A,

62
00:07:46,280 --> 00:07:54,559
when we're looking at weekly death counts by age, you see that the 0 to 18 group, after you do a lot of manipulations, has very few deaths usually.

63
00:07:54,560 --> 00:07:57,620
So maybe you'd assume that the top half has more.

64
00:07:57,620 --> 00:08:01,370
But it's difficult just from looking at the data to make that assumption.

65
00:08:01,370 --> 00:08:07,309
So we thought that that discriminant doesn't matter because that's now that the critical group of individuals.

66
00:08:07,310 --> 00:08:15,350
Yes. Associated with death. Right. Is older people that are more, you know, under risk and higher risk of death.

67
00:08:15,550 --> 00:08:20,900
So. Yes, exactly. So if you run sensitivity analysis using different distributions,

68
00:08:21,560 --> 00:08:28,070
I think that you probably get the same result because that's not really a most important group in terms of risk of death.

69
00:08:28,160 --> 00:08:36,410
Right. And so let's say you had ten people in this age category for this week with binomial sampling,

70
00:08:36,410 --> 00:08:41,720
you might put four into 5 to 17 and then we just put the other six into the 18 to 24 group

71
00:08:41,990 --> 00:08:48,350
and then we basically grouped by these age categories and count it up for that specific week.

72
00:08:48,350 --> 00:08:51,469
How many were in all of these age categories you put together?

73
00:08:51,470 --> 00:08:54,800
402, 17, 1864 and 65 plus.

74
00:08:55,640 --> 00:09:01,820
That is just with sample how the other groups do with this you probably face the same issues.

75
00:09:02,270 --> 00:09:05,960
I didn't run binomial, I just multiply it by a point three.

76
00:09:06,290 --> 00:09:09,949
And did you round because that was the cause you want to hold number. I did.

77
00:09:09,950 --> 00:09:14,840
I did run around it. You do something about okay.

78
00:09:15,380 --> 00:09:23,480
Yeah. You just want to make sure you have it. Yeah. I'm going to say that it is still or not binomial.

79
00:09:24,200 --> 00:09:26,430
Okay. It signifies the difference of the.

80
00:09:26,570 --> 00:09:36,170
Although anyway, I probably think that this may not matter because this risk group is really lowest the risk.

81
00:09:36,620 --> 00:09:42,019
Yes, that that and so once you calculated that we did one additional thing and this

82
00:09:42,020 --> 00:09:46,880
was mostly dictated by the way the Michigan data set up that was given to us.

83
00:09:47,090 --> 00:09:50,600
It was given to us by county probable versus confirmed.

84
00:09:50,840 --> 00:09:55,310
That's per a time frame, but wasn't giving us a specific age or sex category.

85
00:09:55,610 --> 00:09:57,280
So what we did with the U.S., though,

86
00:09:57,320 --> 00:10:03,889
is we specifically went in and with the that we calculate the proportion of each of those six distinct categories.

87
00:10:03,890 --> 00:10:07,190
So male 0 to 17, female 0 to 17.

88
00:10:07,370 --> 00:10:11,690
And then the same thing for the other age categories 18 to 64 and 65 plus.

89
00:10:12,080 --> 00:10:18,500
So basically we're looking at the proportion of the total deaths in that week across all age sex categories.

90
00:10:18,710 --> 00:10:22,550
And can you what's the proportion in each of those subcategories?

91
00:10:23,330 --> 00:10:27,800
And once we did that, you could then go to the Michigan data and.

92
00:10:30,060 --> 00:10:33,660
A few specific things you have to do before you actually did any type of merging.

93
00:10:33,870 --> 00:10:45,570
The date in the Michigan one was not aggregated by week and was actually reported daily, so you wanted to aggregate that by week.

94
00:10:45,580 --> 00:10:49,890
And since the U.S. specific that data was reported only on Saturdays.

95
00:10:50,160 --> 00:10:59,130
What we did is we took the previous Sunday and aggregated up to that Saturday to get that week's weekly death count in Michigan.

96
00:10:59,190 --> 00:11:06,060
That makes sense. But that's for the question that there's another date problem later on.

97
00:11:06,570 --> 00:11:10,880
But so what? We basically did a lot of problems about CBC data.

98
00:11:10,900 --> 00:11:15,190
Yes, there are one additional assumption,

99
00:11:15,190 --> 00:11:19,290
and I'm curious what other people did is we decided to just take the confirmed

100
00:11:19,290 --> 00:11:23,249
that data so we eliminate that all rows in the Michigan that that were probable.

101
00:11:23,250 --> 00:11:26,690
I don't know what other people in the class did confirm.

102
00:11:26,880 --> 00:11:30,150
You know, we heard.

103
00:11:30,150 --> 00:11:36,170
Yes. From other groups. Now I think using firm death as.

104
00:11:38,640 --> 00:11:44,910
And so again we work by date and also across county since each of it was reported for different counties.

105
00:11:45,780 --> 00:11:50,219
One thing that we weren't very sure. Some counties were also like Department of Corrections.

106
00:11:50,220 --> 00:11:53,790
So we assume that that also just fed into the overall Michigan data.

107
00:11:53,790 --> 00:11:56,399
So we didn't really care what the county name was.

108
00:11:56,400 --> 00:12:03,090
If it was a county for specific Department of Michigan, we just kind of aggregated over all of that information.

109
00:12:03,630 --> 00:12:08,580
And the next step was to review the data because we're going to second part of the course will

110
00:12:08,730 --> 00:12:15,520
focus on specialty analysis and we'll look at the Arrow data at the level of the county rate.

111
00:12:16,380 --> 00:12:23,880
We will look at their heterogeneity across different counties and yeah, the risk factor and so on.

112
00:12:25,140 --> 00:12:35,070
But we will the data because now we would just aggregate all the data at the county level together to give the overall one perfect.

113
00:12:35,070 --> 00:12:40,350
And then what you have to do is now join this Michigan, the newest day that you had by date.

114
00:12:40,590 --> 00:12:47,040
And from there, we had to figure out the proportion, the number of deaths in each age and sex category.

115
00:12:47,050 --> 00:12:52,830
So you have the proportions from the U.S. data. You have the total amount of deaths in the Michigan data for a specific week.

116
00:12:53,070 --> 00:12:57,809
And really all we did was we multiply the total that's in that week by the specific proportion

117
00:12:57,810 --> 00:13:01,830
we got in the Michigan data and then we rounded to the nearest integer as we want.

118
00:13:01,920 --> 00:13:05,750
The good news for you guys I did receive then I use the CDC.

119
00:13:05,770 --> 00:13:12,090
You may want to say of your factor and the work force and look at discrepancies.

120
00:13:12,540 --> 00:13:26,990
I am very curious whether or not this sort of the imputation or predicting or whatever that that really matched with the actual data observed.

121
00:13:27,000 --> 00:13:33,020
I hope so. And I will say, okay, but this is reverse engineering.

122
00:13:33,030 --> 00:13:36,690
The let's let, let's see what that reverse engineer works.

123
00:13:37,320 --> 00:13:41,549
Yeah. And then from there we just kind of gathered and spread it until the data was in

124
00:13:41,550 --> 00:13:46,830
the right format where each row was giving you them the sex of the category,

125
00:13:46,830 --> 00:13:52,830
the age of the category, the date, and the number of deaths for that specific combination.

126
00:13:53,190 --> 00:13:58,259
And that's kind of all we really did. You get the death data in Michigan.

127
00:13:58,260 --> 00:14:01,829
I don't know if anyone has any questions or what they did differently or some

128
00:14:01,830 --> 00:14:06,600
other thing they realized was inconsistent in the data that they corrected for.

129
00:14:09,620 --> 00:14:13,459
Same. Same. Okay. Then tell us a little bit different.

130
00:14:13,460 --> 00:14:16,760
But the procedure was. Yes, pretty the same, right?

131
00:14:17,450 --> 00:14:24,710
Yes or no? No. Yeah, I think so.

132
00:14:24,790 --> 00:14:29,339
We don't have gold digging out, right? Cold. Oh, but yeah.

133
00:14:29,340 --> 00:14:32,730
This is the procedure that. Yeah. Yep.

134
00:14:33,150 --> 00:14:35,969
And so the next thing that we have is this vaccine data,

135
00:14:35,970 --> 00:14:42,000
which is for the entire U.S. But then you there's a specific column you can just subdivide to the Michigan data that you get.

136
00:14:42,420 --> 00:14:47,520
And the problem here was the give us specific columns that were the percent of the population,

137
00:14:47,520 --> 00:14:53,220
65 plus vaccinated and the the percent of the population 18 plus vaccinated.

138
00:14:53,430 --> 00:14:57,180
But what we really want in order to kind of correlate the death in the vaccine,

139
00:14:57,180 --> 00:15:02,580
they said, is to have the percent of people, 18 to 64 that are vaccinated.

140
00:15:02,820 --> 00:15:06,660
So there's actually another variable in the data set that allowed us to do this.

141
00:15:06,960 --> 00:15:07,830
Specifically,

142
00:15:08,070 --> 00:15:16,320
you were told the number of doses administered to individuals 65 plus and also the number of doses administered per 100 came the visuals.

143
00:15:16,620 --> 00:15:24,510
So using this equation right here, you could kind of like backtrack and say, so what's the number of people that are 65 plus in this population?

144
00:15:24,930 --> 00:15:32,850
And you can actually go even further because now you have that for 18 plus and you can find how many people 18 cluster in the population,

145
00:15:32,850 --> 00:15:40,710
subtract the number of people, 65 plus from the population, and now you have the number of people, 18 to 64 that are in your population.

146
00:15:41,970 --> 00:15:44,820
So then you can backtrack with your vaccine data.

147
00:15:45,030 --> 00:15:53,850
You know how many people 18 plus have at least one dose of the vaccine or full vaccination of the vaccine?

148
00:15:54,000 --> 00:15:58,890
And then now you have your new denominator, which is specific to that age category.

149
00:15:59,100 --> 00:16:03,810
But you can get for just that age category what the vaccine percentage is.

150
00:16:05,160 --> 00:16:08,550
Did anyone do something different or have any questions?

151
00:16:09,030 --> 00:16:18,540
I used a variable that was like the proportion or percentage or something for each of the age categories.

152
00:16:20,280 --> 00:16:25,830
But now I'm wondering if that would give me a different result than per 100 K.

153
00:16:28,000 --> 00:16:32,049
I'm like, underscore. I didn't see that column.

154
00:16:32,050 --> 00:16:39,820
It might have been something very similar. I was like, I think I think we did see that column and I think there was some issues that were there.

155
00:16:40,300 --> 00:16:45,530
Yeah, I think the denominator was sort of a different system, which is what you see is that, you know.

156
00:16:48,780 --> 00:16:54,870
103 variable refers to the total number of days.

157
00:16:54,960 --> 00:17:01,620
Yes. Yes. So that you don't use that, you get the number of people the just to get the population.

158
00:17:01,950 --> 00:17:07,830
Because if you know that 10,000 doses were given and then how many doses per population,

159
00:17:08,160 --> 00:17:11,309
you can get it just for the population, not the actual vaccine.

160
00:17:11,310 --> 00:17:17,520
You have to use a different variable because that's the denominator in the numerator like are the same in terms of or whatever.

161
00:17:17,760 --> 00:17:24,720
You know, they can sort of take part. And now I don't know if people noticed this because it was very confusing to me.

162
00:17:25,110 --> 00:17:34,950
So the CDC, starting in about February of 2010, I want released the proportion of people 18 plus with the vaccine, but not 65 plus.

163
00:17:34,950 --> 00:17:42,090
That only began in March. So you actually only have vaccine data with this method starting in March of 2021.

164
00:17:42,300 --> 00:17:44,370
Before that, you really have an ease.

165
00:17:44,370 --> 00:17:57,569
And I think everybody noticed that right now problems about CDC data and so you basically you that you got your percentage

166
00:17:57,570 --> 00:18:04,410
for all of these different age categories but now you want to correlate with the death data and you realize again,

167
00:18:04,410 --> 00:18:13,380
the vaccine data from Michigan is in an interesting format up until about June 11, 2022, it is reported daily.

168
00:18:13,890 --> 00:18:20,280
So for those dates we just picked the Saturday that corresponds to the Saturday and our death data and kept those Saturdays.

169
00:18:20,940 --> 00:18:23,970
But afterwards it is reported every Wednesday.

170
00:18:24,330 --> 00:18:29,250
Now you have to correlate that one thing to a specific Saturday if you really want to use that.

171
00:18:29,250 --> 00:18:33,209
So we just picked the nearest Saturday, which was next Saturday coming up.

172
00:18:33,210 --> 00:18:41,130
So that's how we kind of correlate that Wednesday to of the next Saturday so we could get our vaccine and they are our graduates.

173
00:18:41,130 --> 00:18:46,250
The word is CDC and Michigan because that works.

174
00:18:46,770 --> 00:18:53,099
Yeah. And they're at the same I who in this session what is the supposed to put on the data.

175
00:18:53,100 --> 00:19:00,360
Oh my God. Yeah. And we decided it was okay just reporting the SATs because the vaccine percentage is cumulative.

176
00:19:00,660 --> 00:19:06,989
Technically, your population size can change over time, so it could fluctuate, but it's mostly cumulative.

177
00:19:06,990 --> 00:19:10,560
So you can just take the Saturday date for this.

178
00:19:10,980 --> 00:19:15,629
So you just put you just said like, oh, this is Saturday set of Wednesday.

179
00:19:15,630 --> 00:19:18,450
Yeah, I just do that. I found this post Saturday.

180
00:19:19,290 --> 00:19:28,649
There's this sealing date function where you give it the date and you say, well, week start is six, which is a Saturday, find me the Saturday.

181
00:19:28,650 --> 00:19:37,680
And one package has this function I think it's lubricate the veneer is yeah it finds like new year Saturday this function.

182
00:19:38,250 --> 00:19:41,920
Yeah. The nearest or next I'm not yeah yeah.

183
00:19:42,070 --> 00:19:47,640
And so the what it is doing and what even if it's the nearest etc. because yeah.

184
00:19:48,360 --> 00:19:51,990
Yeah. Wednesday so but there's a sealing date and there's also a floor date.

185
00:19:52,200 --> 00:19:59,370
Okay. So it depends which function use is the function called cart and it can directly give you the.

186
00:20:04,370 --> 00:20:07,780
Yeah. Perfect. And what libraries that functioning.

187
00:20:08,030 --> 00:20:14,870
Just base. Okay. I was very clear cut function.

188
00:20:14,870 --> 00:20:18,500
I said this is not a cut function.

189
00:20:19,370 --> 00:20:23,000
Okay. A city that sounds better than cut.

190
00:20:27,980 --> 00:20:31,640
And that is basically how you get the vaccine data.

191
00:20:31,850 --> 00:20:39,710
Now, one additional thing that is not on here that we made as an assumption and the homework actually is when you join this data,

192
00:20:39,920 --> 00:20:47,990
you have no vaccine data prior to March 2021, but you have that data prior to March 2021.

193
00:20:48,140 --> 00:20:53,570
So you had to make the choice whether or not you truncate the data in your model or what we decide to do,

194
00:20:53,570 --> 00:20:57,950
as we want you to include all the parts that were absent. The question we just made it zero.

195
00:20:58,400 --> 00:21:03,440
Anything before our train train one, the the vaccine proportion was just set to zero.

196
00:21:04,160 --> 00:21:09,110
I don't know if other people chose something else or had a different opinion on what would be most appropriate.

197
00:21:12,370 --> 00:21:12,580
Yeah.

198
00:21:12,590 --> 00:21:23,950
I think that of course, vaccination started in December 2020, but only the very small proportion of people are eligible eligible for vaccination.

199
00:21:24,700 --> 00:21:31,800
Like virtually probably from population. So I think that the zero is reasonable.

200
00:21:32,260 --> 00:21:42,460
I think the only caveat is if you look at our plots for problem two, B and C for vaccination starts at like 60% for individuals, 65 plus.

201
00:21:42,580 --> 00:21:48,160
So it kind of goes from zero than 65. So like so you can do this interpolation?

202
00:21:48,520 --> 00:21:55,209
Yes, you could. You can also connect those two points by a linear lie and, you know,

203
00:21:55,210 --> 00:22:01,810
sort of using this sort of whatever the imputed value of on that as sort of estimate.

204
00:22:02,380 --> 00:22:05,920
Yeah. The first dose coverage or something like that.

205
00:22:07,000 --> 00:22:11,860
And but we did see it was exponential. So you could start started like when, you know, when the vaccine started.

206
00:22:11,870 --> 00:22:18,580
So you could like maybe start out the day the vaccine was given to the general public and make like a straight line up to your next point.

207
00:22:19,240 --> 00:22:23,680
But it does rise exponentially. So I wouldn't be surprised if it kind of just goes like this.

208
00:22:24,040 --> 00:22:32,870
So it's the is that they probably had the no infrastructure in place to collect the data until they said, oh my gosh, we need to collect the data.

209
00:22:32,880 --> 00:22:37,600
And finally started collecting data in large sums like that.

210
00:22:37,840 --> 00:22:45,999
And probably the infrastructure was not ready at that moment because after the the presidential election,

211
00:22:46,000 --> 00:22:52,209
there was mass, you know, so a lot of controversial things going on about this.

212
00:22:52,210 --> 00:23:01,020
And so, anyway, I think that it's a very interesting period of history to look out into vaccination and yeah yeah.

213
00:23:02,890 --> 00:23:10,030
And that questions for the midterm before you use your can you send me the data I'm sure I can use the one data.

214
00:23:10,030 --> 00:23:15,579
If you agree to use the data, we can use one data and your data.

215
00:23:15,580 --> 00:23:19,719
Yes, I'll send the appendix I have. Yes, send me that.

216
00:23:19,720 --> 00:23:25,810
Probably you just missed one thing, but it's interesting that you have done I consider the data center Celsius.

217
00:23:25,810 --> 00:23:33,100
Well, just so the data set itself not appendix. I need the data I will do everybody use the same thing.

218
00:23:33,140 --> 00:23:36,790
Yeah. For the midterm. Right. So if you agree. Yeah, I appreciate it.

219
00:23:36,790 --> 00:23:43,610
Okay. But we need other variables than the ones we think.

220
00:23:43,630 --> 00:23:57,700
If we have those three results, we don't care. So we appreciate your effort and sounds like you had a lot of fun and torture at the same time.

221
00:23:57,700 --> 00:24:00,790
And yes, there was more drugs. Right.

222
00:24:00,790 --> 00:24:06,600
This is part of your job interview when some of our interviews say, what are you well,

223
00:24:06,610 --> 00:24:11,040
what is the most interesting things you have done when you're in your master program?

224
00:24:11,410 --> 00:24:16,510
Bring that to you or impress you or the person who you interview.

225
00:24:16,510 --> 00:24:22,930
For sure, your knowledge is really something that is not it's not trivial, right?

226
00:24:22,930 --> 00:24:30,220
So but you have to use a lot of the intelligence moving here or there, and mostly you need the patients to do that.

227
00:24:30,250 --> 00:24:33,670
Right. So anyway, I appreciate it a lot.

228
00:24:34,060 --> 00:24:47,320
This name is just come back to this lecture so homework three is coming after midterm your practice

229
00:24:47,500 --> 00:24:51,640
come a field or come a future will not be in the midterm but you will practice come after.

230
00:24:51,910 --> 00:25:00,700
The reason I want you to practice computer is that this is very essential idea for reinforcement learning and online learning.

231
00:25:00,880 --> 00:25:08,590
Okay so so that we need to read from the slides for midterm guess what is in the syllabus I will talk about on Tuesday.

232
00:25:08,750 --> 00:25:16,690
Okay. So, so so you'll have it for the reinforce learning, right?

233
00:25:16,690 --> 00:25:21,489
So you have you basically replace your theta t by action, right?

234
00:25:21,490 --> 00:25:29,290
So so I gave you this for the later process.

235
00:25:29,620 --> 00:25:36,229
Okay. So, so you need reinforcement learning what you have here is really the thing that you will be action, right?

236
00:25:36,230 --> 00:25:40,390
The action could be is a categorical variable.

237
00:25:40,450 --> 00:25:43,450
It could be taking action or not taking action.

238
00:25:43,450 --> 00:25:52,390
You have binary decision making or you have a continuous decision space.

239
00:25:52,480 --> 00:25:59,920
So in the early version of if reinforce learning, reinforcement learning is called Markov decision process.

240
00:25:59,920 --> 00:26:10,010
Right? That that's. So later on this,

241
00:26:10,010 --> 00:26:19,340
I said basically the thing that people work on in the business or commerce or economics or later on when computer scientists come into,

242
00:26:19,350 --> 00:26:28,960
they change in them to reinforce very essentially is just like you need to make a decision in a sequential way and then you can improve your decision.

243
00:26:29,430 --> 00:26:38,559
Okay. So, so of course, you don't know how to do it. You need to optimize the decision making work of optimize decision making according

244
00:26:38,560 --> 00:26:45,370
to what the data you make a decision on T minus one that would generate the outcome.

245
00:26:45,640 --> 00:26:54,970
If you decide to advertise this vaccine vaccine where you you make a decision to put this drug on the market,

246
00:26:55,050 --> 00:27:01,660
that there will be a consequence of that. So so that's your y t minus one.

247
00:27:02,380 --> 00:27:08,830
So you make a decision. The decision will give you a new data, whatever you want.

248
00:27:08,870 --> 00:27:15,840
Observe is a decision and business contacts or investment contact or is the treatment contact for it.

249
00:27:15,850 --> 00:27:19,790
So whatever there will be outcome generated from your action, right?

250
00:27:19,810 --> 00:27:27,850
This is actions of course that what this observed and this then then one additional

251
00:27:27,850 --> 00:27:33,460
things in the reinforcement is really that you have the environmental variable,

252
00:27:33,610 --> 00:27:40,950
right. So that they add this sort of additional variable here is called E key.

253
00:27:41,530 --> 00:27:46,800
So when you make a decision, the decision can change the environment to change your infrastructure,

254
00:27:46,810 --> 00:27:55,170
you can change the the team or it will change some human manpower,

255
00:27:55,180 --> 00:28:01,810
whatever that the environment has been changed and the change of improvement would affect next decision.

256
00:28:02,230 --> 00:28:07,690
So maybe the group of people who are evolving the decision making is different from previous one.

257
00:28:08,380 --> 00:28:15,730
So there is a feedback from from your y t observe you if you invested this,

258
00:28:15,730 --> 00:28:22,330
but you find out over this bad investment like this Prime Minister of the UK was,

259
00:28:22,720 --> 00:28:30,340
you know, she was selected, there was the action and the outcome was so bad and then you know that she resigned, right.

260
00:28:30,370 --> 00:28:31,449
So, you know,

261
00:28:31,450 --> 00:28:45,100
did these process just resign of being 45 days of premier of UK the very next person they are going to do election next week right for this UK,

262
00:28:46,090 --> 00:28:54,040
you know the leader of Conservative Party that the decision making group will be different and there will be new, different decision making.

263
00:28:54,040 --> 00:29:03,460
And then you have your way to observe as a consequence of that and decision making and causes the change of the environment.

264
00:29:04,630 --> 00:29:15,460
Right. Environment change. And then then your thoughts, of course, so you can of you can formulate the environment of chance as you're at.

265
00:29:15,640 --> 00:29:25,400
Like our process of we observe vaccination, we can observe some of the political variables and so on and so forth.

266
00:29:25,450 --> 00:29:33,040
Right. So, so, so they this is basically the formulation of reinforced learning.

267
00:29:33,040 --> 00:29:40,510
What you are really interesting here is you want to find optimal decision over time.

268
00:29:40,750 --> 00:29:44,739
So you're trying to figure out see that t isn't exactly confused.

269
00:29:44,740 --> 00:29:48,069
Right? Right. Like what you are trying to do. Okay.

270
00:29:48,070 --> 00:29:56,290
So you want to figure out the decision making sequence of what decision will be your optimal decision to make next time,

271
00:29:56,980 --> 00:30:08,469
given what you have observed so far, you have observed data up to t from t zero up to y t based on all the revenues,

272
00:30:08,470 --> 00:30:13,290
all the incomes or the benefits, whatever you have observed at the time.

273
00:30:13,300 --> 00:30:17,770
T What is my best decision to make?

274
00:30:18,250 --> 00:30:27,400
Okay, given the current invert say so that that's basically a latent process of estimation optimization problem.

275
00:30:27,910 --> 00:30:35,410
So there in in that context there are something called the balance equation.

276
00:30:39,340 --> 00:30:43,390
It reinforces the right balance equation.

277
00:30:43,840 --> 00:30:51,010
This is very much similar to Common Field. If you look carefully, it's really a generalization of common field.

278
00:30:51,340 --> 00:31:00,510
Okay, so everything from cover fielder that's engineer origin to motivated from an engineer problem the rocket science a rocket.

279
00:31:00,520 --> 00:31:05,470
You know people want to figure out what's the position of a flying object in space.

280
00:31:05,800 --> 00:31:14,290
But the idea has been later on sort of transplanted or implanted into business decision making.

281
00:31:14,320 --> 00:31:21,130
Now you do computer science. Yeah. So so that's why I want to give you a little bit more time to study common filter.

282
00:31:21,790 --> 00:31:26,200
This is very, very fundamental way of doing sequential decision making.

283
00:31:26,350 --> 00:31:29,560
I mean, estimation the leader variable in sequential way.

284
00:31:31,540 --> 00:31:38,170
Our optimization is based on destiny down by some prediction.

285
00:31:38,320 --> 00:31:46,270
But of course that in your in decision making, your proceed on T is a likely a discrete variable.

286
00:31:46,390 --> 00:31:57,520
You take action or active viewer I can see right you want to make optimal policy out of this structure and to make a decision.

287
00:31:57,640 --> 00:32:10,140
Okay. So I think there are two things that that people have not considered why is how to bring in more converts in the model of enforcement.

288
00:32:10,670 --> 00:32:21,750
I know people take this environment variable in a very generic abstract way, not really tied to wall of the firm.

289
00:32:22,290 --> 00:32:26,520
Okay. So in other words, people don't really have this.

290
00:32:26,700 --> 00:32:35,610
The first method to see which environment factors are really are significant factors to drive the decision making.

291
00:32:36,180 --> 00:32:40,650
People say, I have five factors. Let's put them all to this reinforced learning framework.

292
00:32:41,100 --> 00:32:51,929
But there is no a statistical method that can help people to assess which human variables should be the important ones.

293
00:32:51,930 --> 00:32:57,270
To be included in this framework. And this importance may change over time, right?

294
00:32:57,660 --> 00:33:04,710
Maybe. And this time, you know, this factor is important, but two years later, this factor is no longer an important one.

295
00:33:04,890 --> 00:33:11,220
Okay, who knows? So there should be something like what do we learn from this inference part, right?

296
00:33:11,610 --> 00:33:17,550
To help people to identify important environment factors in the decision making.

297
00:33:17,850 --> 00:33:26,820
Okay. This is one number one is that people have never thought about this as a special people process.

298
00:33:26,970 --> 00:33:31,920
People always think about this is only a time driven process psychology.

299
00:33:32,610 --> 00:33:42,240
But if you have or might right this chance thoughts in different locations in one state or even in the country,

300
00:33:42,750 --> 00:33:46,260
there is a special feature across this.

301
00:33:46,710 --> 00:33:51,660
Why you want just make one decision for all the stores in the country.

302
00:33:51,660 --> 00:33:58,860
Why not this mole them as a process with a spatial history technology?

303
00:33:58,860 --> 00:34:07,350
In this reinforcement and in the principle, of course we make decisions using different part over brand.

304
00:34:07,590 --> 00:34:11,520
So functionality is a special recorded process.

305
00:34:12,120 --> 00:34:23,780
So, so there is a big room to advance the sort of reinforced memory out of here primarily now in the version of Temple

306
00:34:23,790 --> 00:34:32,840
sequential decision making process or Markov decision about the process in time into a special temple version.

307
00:34:32,850 --> 00:34:36,720
I think that's something to learn from. This course can be extended to that.

308
00:34:36,900 --> 00:34:41,910
Yeah, of course. Now people talk about online learning and common filter.

309
00:34:41,910 --> 00:34:48,120
I want to A Common Future is a online learning sort of framework.

310
00:34:48,690 --> 00:34:59,700
So you see how people do this. Like about 60 years ago by engineers in the aerospace engineering.

311
00:34:59,990 --> 00:35:09,180
Okay. So it's quite interesting and lot of important ideas in statistics are coming from the field.

312
00:35:09,180 --> 00:35:13,560
Also like M algorithm, right, is coming from genetics.

313
00:35:14,010 --> 00:35:21,120
You do know that that was originally e-mail was the most proposed by geneticists, the now by statistician.

314
00:35:21,150 --> 00:35:30,270
Right and so some important method like and some say it's coming from physics a physicist.

315
00:35:30,270 --> 00:35:36,509
Right. So those guys who are working on this are nuclear physics, right?

316
00:35:36,510 --> 00:35:40,440
So high energy physics, this story is the most hollow.

317
00:35:40,440 --> 00:35:46,670
Right. You'll remember this famous story, how this Monte Carlo method has been named as Monte Carlo.

318
00:35:46,680 --> 00:35:50,010
Right solstice by physicist. Not now.

319
00:35:50,040 --> 00:35:58,499
Not by statistician. Yeah. By the way, we are good learners and we learn from our refuse computers.

320
00:35:58,500 --> 00:36:08,730
One example so when I was a student while for my dissertation members is from Faculty of Commerce

321
00:36:08,760 --> 00:36:17,540
at UBC and he wrote a book about Martin Pullman and he wrote a book about Markov decision process.

322
00:36:18,170 --> 00:36:23,790
He large encouraged me to study this because I work on a lot of common fields,

323
00:36:23,790 --> 00:36:29,490
realized continuously that t right looking at how you know, this latent process of fast,

324
00:36:29,940 --> 00:36:35,969
you know the prevalence of the disease and he said oh in our field that really

325
00:36:35,970 --> 00:36:40,410
we're we're interested in more a discrete type of late in process related

326
00:36:40,410 --> 00:36:47,819
decision making and you know your life sometimes you run into a very important

327
00:36:47,820 --> 00:36:52,860
person but you did not realize if I started working on Markov decision process,

328
00:36:52,860 --> 00:36:56,280
I probably will be one of the pioneer. Also a reinforcement, Ernie.

329
00:36:56,880 --> 00:37:08,610
But I do now choose the. Direction to do it. But I work on mostly the problem of motivation from Public House or you know, this are, you know,

330
00:37:09,540 --> 00:37:19,380
longitudinal data rather than from decisionmaking point of view or a fact that because this is the early version of reinforced memory,

331
00:37:20,400 --> 00:37:25,350
I mean, Markov decision process is version of that. Okay.

332
00:37:25,620 --> 00:37:30,509
That's why I want to really talk a little bit more about some of future is smoother and I want to

333
00:37:30,510 --> 00:37:37,559
you I share this code article I wrote for simple model like possible common model in the canvas.

334
00:37:37,560 --> 00:37:43,790
I will go over bit today to see how this procedure can be coded and why this is so fast.

335
00:37:43,800 --> 00:37:48,240
And I will introduce also an advert and a new way of doing bootstrap.

336
00:37:48,600 --> 00:37:52,510
Okay. This random weighting method. Okay.

337
00:37:53,220 --> 00:37:59,700
Anyway, so so I will talk about drive common future smoother and the possible gamma states based model.

338
00:38:01,110 --> 00:38:08,950
And I will talk about at this point estimating the association parameters using common estimate in question.

339
00:38:09,540 --> 00:38:12,540
Oh, that's coming from my dissertation actually.

340
00:38:13,980 --> 00:38:21,180
So this is inference on social parameter and that that allows you to identify important environmental factors.

341
00:38:21,660 --> 00:38:28,200
That will be a factors sort of Markov property work this of you know,

342
00:38:28,200 --> 00:38:37,260
also observe the process and I will show you a little bit to our scripts after implementing of this common estimating function,

343
00:38:38,610 --> 00:38:46,979
including the piece for Common Filter. Okay. To quickly remind you about this, plus on gamo states space model,

344
00:38:46,980 --> 00:38:57,250
you have this comp structure right as I to draw there and then you are in our case, we're interested in a number of counts, right?

345
00:38:57,300 --> 00:39:02,820
The counts were number of deaths, number of hospitalization or a number of the infections.

346
00:39:02,820 --> 00:39:15,330
So so that's count. And we assume that there is a latent process that represents the population level dynamics of infection.

347
00:39:16,350 --> 00:39:24,989
Then we try to explain the variability of your observe the counts by a plus distribution.

348
00:39:24,990 --> 00:39:34,980
And the problem distribution is takes that to mean of the distribution or the incidence rate takes the

349
00:39:35,770 --> 00:39:45,060
multiple care form where 80 is the factor that accommodates some of observed process for for example,

350
00:39:45,540 --> 00:39:56,729
seasonality time between and the vaccination whichever the you very specific burden factors for you know some some coverage factors

351
00:39:56,730 --> 00:40:09,570
that factors that you'd like to use to explain possibly the variability of y t plus something that cannot be explained by this a s t,

352
00:40:09,990 --> 00:40:19,800
you know, so also that is the underlying process that you cannot easily capture by the observe the culverts that you really see,

353
00:40:20,340 --> 00:40:29,860
what are the residual, what are the remaining things going on in the population in addition to what we observed white here next.

354
00:40:30,060 --> 00:40:33,060
Right. So that's something you trying to figure out using this.

355
00:40:35,430 --> 00:40:45,600
Okay. So you have multiple choices about how you model see that in this particular case because that is part of the possible mean or the,

356
00:40:45,960 --> 00:40:54,180
you know, the average incidence rate. We want to model this as a positive, continuous random variable.

357
00:40:54,180 --> 00:41:02,740
And because this is time dependent, we model this as a stochastic process using this simply this stationary error.

358
00:41:02,790 --> 00:41:07,349
One time our process, you were to make sure that this theory is positive.

359
00:41:07,350 --> 00:41:18,590
I mean, you can choose order. You can choose this as law, but normal as chair and not an adult or people or if you know, you know, you can use it.

360
00:41:18,900 --> 00:41:24,630
For me, I think that's probably very straightforward. Just model this as a common process.

361
00:41:25,560 --> 00:41:31,500
And then because your model is something that you do not actually observe.

362
00:41:32,490 --> 00:41:39,180
So that is late in process, you'll want a simple structure to capture the key features of the process.

363
00:41:39,420 --> 00:41:44,950
What's the key feature of live in? Process number one is a markov process, right?

364
00:41:44,970 --> 00:41:49,110
Is time time dependent? You have the time.

365
00:41:49,110 --> 00:41:54,570
Sorry. So you have a time varying time evolution process.

366
00:41:54,780 --> 00:41:58,110
The latent process evolves.

367
00:41:58,220 --> 00:42:07,490
Over time. And the situation today depends on situation yesterday, so that you need to model this as a mark of process.

368
00:42:08,270 --> 00:42:17,360
Secondly, that you need to make this process to be meeting one so that you can identify all the parameters in the eight part.

369
00:42:17,360 --> 00:42:25,850
You can estimate the intercept term in this RFA parameter to make this month.

370
00:42:26,060 --> 00:42:31,400
Thirdly, you can work on some non stationary process.

371
00:42:32,090 --> 00:42:41,150
Part of my dissertation published a paper by March of 1989 was I modeled this as a noun station or process,

372
00:42:41,810 --> 00:42:50,990
and but for the simplicity, you can start with a station or a process and just try to capture something.

373
00:42:51,680 --> 00:42:59,690
Of course, this this narrative may not be really a fit into the ongoing pandemic because a lot of things are changing over time.

374
00:43:00,260 --> 00:43:14,959
So but you can use this model as a sort of sitcom to model one segment of this process so that something is relatively stable,

375
00:43:14,960 --> 00:43:19,550
particularly for a according to the dominance of the event.

376
00:43:20,150 --> 00:43:29,750
When you switch it from one variant, like from RFA to Delta, there are dramatic change of underlying infection stories.

377
00:43:30,320 --> 00:43:36,020
And when you change from Delta to Omicron, that there's not a dramatic change of things,

378
00:43:36,500 --> 00:43:46,520
so that you cannot really think that this is a very justifiable assumption

379
00:43:46,520 --> 00:43:53,720
to model this transition from one variant of the coronavirus to another one.

380
00:43:54,020 --> 00:43:58,790
But if you think folks are only in the period of our favorite, the Delta variant,

381
00:43:58,790 --> 00:44:07,729
then you you can think that presumably this whole process is relatively stable or stationary anyway.

382
00:44:07,730 --> 00:44:17,510
So this is the gamma process that's that has to satisfy this sort of convolution of closure,

383
00:44:18,410 --> 00:44:27,920
namely that this variable plus this variable has to follow a gamma distribution because satiety is gamma,

384
00:44:27,920 --> 00:44:35,300
that you have this sticky narrative that requires that to see that T is commonly distributed and set a T minus twice,

385
00:44:35,990 --> 00:44:40,890
commonly distribute it with the same gamma distribution because you have this

386
00:44:40,910 --> 00:44:47,360
nerdy the parameter of of gamma distribution doesn't change of time because the

387
00:44:47,360 --> 00:44:51,830
requirement of this narrative and the same time that you need make sure that the

388
00:44:51,950 --> 00:44:58,820
the two terms in the equation were to sum up to follow a gamma distribution.

389
00:44:59,090 --> 00:45:01,530
Okay. So so you I mean, Lewis,

390
00:45:01,730 --> 00:45:14,030
this a British statistician figured out a way to do this decomposition and using this thinning operator defined by beta distribution.

391
00:45:14,720 --> 00:45:24,830
And you know that the role has to be positive here because beta parameters must be negative.

392
00:45:24,830 --> 00:45:28,640
So so the role has to be positive from 0 to 1.

393
00:45:28,910 --> 00:45:33,370
Okay. So anyway, the role bar is one month.

394
00:45:33,860 --> 00:45:41,719
Okay. So this is actually the of the gamma process of gulp gamma Markov process of order.

395
00:45:41,720 --> 00:45:46,610
One is stationary process.

396
00:45:46,820 --> 00:45:55,160
Okay. And then to the initial value of this late in process human to be degenerated at this mean because

397
00:45:55,460 --> 00:46:02,450
the thing that you will have mean so the gamma star one lambda denotes gamma distribution.

398
00:46:02,450 --> 00:46:09,620
This means one and index of parameter lambda, namely the dispersion parameter is the one over lambda.

399
00:46:10,700 --> 00:46:13,930
So that's the definition of this notation, gamma stock.

400
00:46:14,540 --> 00:46:18,509
So the first parameter in this is the meaning of the gamma process.

401
00:46:18,510 --> 00:46:26,630
The second parameter in this is the the index parameter essentially while over lambda is the dispersion parameter.

402
00:46:26,750 --> 00:46:32,870
Okay. So then you have your initial position and according to Lewis,

403
00:46:34,340 --> 00:46:42,530
the this process will generate a markov station or a markov process of order, one for positive random variables.

404
00:46:43,070 --> 00:46:51,800
And this autocorrelation function which is defined in this is really the road to the absolute that of the H.

405
00:46:52,100 --> 00:46:56,509
Okay, so this is exponential decayed function.

406
00:46:56,510 --> 00:47:06,060
So. When you look at the correlation of see that is theta, namely h equal to zero, then your correlation is one.

407
00:47:06,320 --> 00:47:11,940
Of course the correction of Peter and Peter himself would be one.

408
00:47:12,180 --> 00:47:15,660
So there is no longer so perfect correlation.

409
00:47:16,140 --> 00:47:25,200
And so the, the, the verb of Peter and yesterday of Peter for either yesterday it will be corrected by role.

410
00:47:25,380 --> 00:47:34,530
Okay. And so anyway, so the h could be positive looking, but looking at some forward or looking at negative or backward.

411
00:47:34,530 --> 00:47:42,300
But anyway, the correlation autocorrelation function is not to depend on actually time.

412
00:47:42,750 --> 00:47:47,100
It doesn't matter. You look at the course in today or you look at caution tomorrow.

413
00:47:47,100 --> 00:47:52,620
It only depends on the time lag.

414
00:47:53,970 --> 00:47:57,450
Doesn't matter which specify they they're looking at the correlation.

415
00:47:57,450 --> 00:48:02,670
You look at what matters is like the separation of the two variables.

416
00:48:02,970 --> 00:48:07,670
The two time lag or time duration matters now specific time.

417
00:48:07,680 --> 00:48:11,100
So this is the essential property of station narrative.

418
00:48:12,030 --> 00:48:17,220
So you can see that this all the important things and doesn't depend on t but depend on only a bit like

419
00:48:17,460 --> 00:48:24,780
okay how much what's the duration between this two variable and the lack of two of the two time points?

420
00:48:24,810 --> 00:48:30,330
Not especially the time to. Okay, so I de la the derivation.

421
00:48:30,330 --> 00:48:38,580
You trust my derivation you can check it is too late to the fair to fill my the degree, but I did long time ago.

422
00:48:38,580 --> 00:48:54,299
But you know you can do it and and so so to verify if I did my derivation correctly but anyway so so I just did this calculation and if you trust me,

423
00:48:54,300 --> 00:49:03,000
just take them or you can I can put this as qualitative exam if people can do it next year and or I suggested them.

424
00:49:03,000 --> 00:49:07,049
I'm not in the candidacy committee, but I can suggest this as a question.

425
00:49:07,050 --> 00:49:12,540
You make a lot of them anyway. So. So you calculate this conditional expectation.

426
00:49:12,540 --> 00:49:14,700
I'll see that here I've seen a T minus one.

427
00:49:15,090 --> 00:49:27,629
This gives you another function of the value of yesterday that due to this error one process and to do the conditional variance of zero,

428
00:49:27,630 --> 00:49:34,680
the giving the value yesterday is the given by F C, the times plus the constant.

429
00:49:35,370 --> 00:49:44,459
And so the expectation of C, the T minus one is row row bar over lambda y.

430
00:49:44,460 --> 00:49:48,120
They happen to be this old, so it happens to be this.

431
00:49:48,120 --> 00:49:57,660
It's okay. Okay. So for the possible process you calculate a conditional sort of this expectation were expectation of emission.

432
00:49:58,230 --> 00:50:02,550
So given say that he y to false positive distribution.

433
00:50:03,060 --> 00:50:08,690
So conditional expectation will be the mean of the possible distribution and

434
00:50:08,690 --> 00:50:12,540
awareness of the possible distribution equals the name of the possible distribution.

435
00:50:12,540 --> 00:50:21,870
That's the unique characterization of percent distribution meaning the variance this in in this case we have this condition moment for that.

436
00:50:23,400 --> 00:50:27,750
So given this things you can easily derive the marginal things.

437
00:50:27,990 --> 00:50:32,850
Okay, so this is a conditional moments and here is the marginal moments.

438
00:50:32,850 --> 00:50:40,590
And so if you calculate here, right, so, so you use double expectation argument, just do a little bit.

439
00:50:45,450 --> 00:50:52,260
So if you add another expectation, this is frequency signal to you, right?

440
00:50:52,410 --> 00:50:55,950
So this is the property and all this from six or two or six so on.

441
00:50:55,950 --> 00:51:01,370
And, and this, this is one this is a how to see that he's constructed to right.

442
00:51:01,380 --> 00:51:04,620
You have a mean one comma process. Okay.

443
00:51:05,010 --> 00:51:15,209
So you can work out the variance of play that the good marginal variance of this and then you know,

444
00:51:15,210 --> 00:51:24,090
this follows comma distribution and the comma distribution is what the dispersion parameter of times do.

445
00:51:24,090 --> 00:51:28,200
You can use words you learn from. 651 Right?

446
00:51:28,560 --> 00:51:33,510
So this comma distribution has dispersion parameter times variance, you mean one,

447
00:51:33,510 --> 00:51:43,530
that's the meaning variance relationship for a comma distribution and the dispersion parameters while the indexed parameter of lambda and new is one.

448
00:51:45,510 --> 00:51:49,830
So that's why you get whatever lambda. You can derive this from here if you like.

449
00:51:51,750 --> 00:51:57,330
Right? So you already have the appearance of theta t, t minus one.

450
00:51:58,520 --> 00:52:08,480
And then take expectation. And then you have various aspects of expectation of being a team in a team minus one.

451
00:52:09,470 --> 00:52:14,050
So this gives you the marginal variance of the right.

452
00:52:14,060 --> 00:52:20,510
This is famous formula and the variance of expectation of this is already given here.

453
00:52:21,770 --> 00:52:26,930
Right. So expectation of F expecting f is.

454
00:52:27,680 --> 00:52:39,560
So. So you apply here. The expectation of F is row, row, bar overlap and plus the constant right rover lambda.

455
00:52:40,910 --> 00:53:02,350
And the expectation of this would be your what do you have here is rho variance of role theta minus one plus a rule bar.

456
00:53:03,770 --> 00:53:06,830
So that's that's already caved in the previous of right here.

457
00:53:07,940 --> 00:53:13,190
Right. So so you can kind of the variance of this this would be very simple.

458
00:53:13,190 --> 00:53:18,360
That would be rule square and the years of this.

459
00:53:20,590 --> 00:53:26,740
And then the parents of this constant, of course, a00 consonant has no variability.

460
00:53:27,700 --> 00:53:32,170
Okay, so now you have this and you just find it because you have it.

461
00:53:32,180 --> 00:53:40,270
Naturally, the variance will be the same. Do you have stationary steering system that produces that appears to be the same?

462
00:53:40,270 --> 00:53:45,190
Then you solve this and you get get this as the result.

463
00:53:45,280 --> 00:53:53,409
Okay. So if you want derive, you can still derive is quite straightforward, some 600 level derivation.

464
00:53:53,410 --> 00:53:58,090
But if you want to practice, you consider in practice,

465
00:53:58,090 --> 00:54:08,620
then you get margin moment to why you get 80 because expectation of C that is what and then

466
00:54:08,620 --> 00:54:17,290
you have the margin of variance of Y so that also you you have you use the same formula.

467
00:54:17,720 --> 00:54:23,000
You can give this dispersion part that you have done already.

468
00:54:23,530 --> 00:54:30,700
The first question of form two. So now you can do a little bit more derivation on the covered structure.

469
00:54:32,380 --> 00:54:39,940
You can see that, okay, if, if whatever lambda is zero, then let dispersion parameters zero,

470
00:54:40,570 --> 00:54:44,200
then this will be really same as you are passing distribution, right?

471
00:54:46,210 --> 00:54:59,410
So that the, the first two moments of y t are completely determined by 18 has nothing to do with any parameters from the item process.

472
00:54:59,820 --> 00:55:04,750
Okay. So here is cover and structure.

473
00:55:04,750 --> 00:55:07,560
You can have the covers between Y and C that.

474
00:55:07,840 --> 00:55:22,020
You can have the covariance between the latent process and you can have the, you know, the covariance of y, y and the Y.

475
00:55:22,030 --> 00:55:26,740
So anyway, those are the the all the, the moment condition you can derive.

476
00:55:27,790 --> 00:55:32,380
I think I try to. Okay, let's just try one of the.

477
00:55:33,420 --> 00:55:39,820
So I tried to and see how to prove I just should show you exactly the same formula repeatedly used.

478
00:55:39,910 --> 00:55:43,600
Okay, so it's quite easy to figure out how to do this.

479
00:55:44,890 --> 00:55:53,710
Just show you the derivation in case you want to confirm if I did, you write the revision.

480
00:55:54,820 --> 00:56:01,600
Okay, let's prove the first one. Basically looking at plus the covariance of y.

481
00:56:01,600 --> 00:56:04,750
You can see that t waiting to see that to you.

482
00:56:04,930 --> 00:56:10,550
How how how we can see the T, of course, that's what you're trying to do.

483
00:56:10,580 --> 00:56:14,380
Right. Okay.

484
00:56:15,490 --> 00:56:21,309
So because the incorrect model.

485
00:56:21,310 --> 00:56:24,790
Right, everything is defined by conditioning something.

486
00:56:24,910 --> 00:56:29,530
So of course that you need to do this conditioning.

487
00:56:32,380 --> 00:56:38,260
So the the driven, the driven factors.

488
00:56:38,260 --> 00:56:43,060
Sidoti Because you have the bottom part sort of state arise constructor,

489
00:56:43,720 --> 00:56:53,170
the default in process is your city of course you want conditional say that if you were to apply the modeling assumptions the structure.

490
00:57:01,910 --> 00:57:09,800
Okay. This is formulary, right? You bring a conditioning into this and then you have expectation of complete covariance.

491
00:57:10,220 --> 00:57:16,550
Covariance plus triggering. So conditioned me that that's that's the formula you'll run on from six or two.

492
00:57:17,960 --> 00:57:21,260
Okay. So we see that T is fixed.

493
00:57:21,560 --> 00:57:30,730
We see that in a fixed rate condition. I'll say that he's see that he's fixed C that he cannot move gives you conditional 7570 is fixed.

494
00:57:31,580 --> 00:57:40,250
Then this becomes a constant. Well 17 fixed condition also 70 is fixed and this becomes a constant chorus of of anything or

495
00:57:40,250 --> 00:57:45,910
constant will be zero would say that he is causing the condition of variance a little bit.

496
00:57:46,050 --> 00:57:51,380
Sally is not moving. You have no covariance. I mean, you have convinced that the question is zero.

497
00:57:52,460 --> 00:57:56,990
And for this one you have ATC that T right.

498
00:57:59,120 --> 00:58:02,990
Right. So this is the definition of possible distribution. What's the meaning of your problem?

499
00:58:03,350 --> 00:58:10,670
Why do you follow distribution base meaning 80, 70 and then so.

500
00:58:11,000 --> 00:58:18,530
So what does the exclusion of say that you give to say that to you that's in a T right the you know this property

501
00:58:18,800 --> 00:58:28,820
right now so you get 80 and the covariance of say that you have say that you will be what variance of the right.

502
00:58:31,140 --> 00:58:36,090
So it is a new fact, a rise out of convenience here.

503
00:58:36,090 --> 00:58:47,580
And this one is something that just rolled. So that's 80 over number because this is the appearance of your vetting process that's gonna process.

504
00:58:47,730 --> 00:58:56,310
Karma distributed this meme one and this person params a Walmart number that's exactly at first solution.

505
00:58:56,520 --> 00:58:59,730
Right. Okay, so use it to drive. Okay.

506
00:59:00,960 --> 00:59:05,310
You're going to drive everything in development patients, how to drive them.

507
00:59:07,080 --> 00:59:13,610
Okay. I certainly think this knowledge of derivation should be tested using for example.

508
00:59:13,620 --> 00:59:19,620
Sure, y t and y t plus h.

509
00:59:20,340 --> 00:59:29,190
Okay. So you consider what's the covers of white t white here in the white T plus h, right?

510
00:59:30,000 --> 00:59:33,030
So you have another one white sheet plus H.

511
00:59:33,750 --> 00:59:39,480
So this tool Y variables are connected through this latent process, right?

512
00:59:40,470 --> 00:59:46,050
So if you want to make something happen, either calculation, you have to condition that under making process.

513
00:59:46,620 --> 01:00:03,330
So that's what you're trying to do. So expectation of progress white t white given see that he and see that t plus plus

514
01:00:04,380 --> 01:00:19,260
covariance of y condition and y you see that you see the key h and the second term,

515
01:00:19,320 --> 01:00:26,280
which is expectation. White house aides say that you see that he was h.

516
01:00:27,990 --> 01:00:29,940
Right. So this is the standard form.

517
01:00:29,940 --> 01:00:41,220
You know, where are you using tool set of values to create your sigma edge right to create a condition in part so.

518
01:00:45,390 --> 01:00:51,330
So we'll see that and see that we are given this is equal to zero.

519
01:00:52,140 --> 01:01:01,140
Why is that? Because I said that in the comp structure you have conditional independence.

520
01:01:01,470 --> 01:01:10,500
When this is fixed. This white and white who push Asia are completely disconnected.

521
01:01:11,730 --> 01:01:17,910
So you'll win your fix there. The bottom part, this two variables are completely disconnected.

522
01:01:18,360 --> 01:01:27,120
That's the independence we showed for this. Right. So that this first term is zero because they've come this Independence Day.

523
01:01:27,180 --> 01:01:30,360
How the way you specify your heart rate for them. All right.

524
01:01:31,440 --> 01:01:35,490
And then for the second term. So we see that t is fixed.

525
01:01:36,060 --> 01:01:41,880
Why do you only is only depend on wife to say that he has nothing to be said on T plus h.

526
01:01:42,960 --> 01:01:51,000
Right. So when this is fixed, then they say that she has no way to in fact y okay.

527
01:01:51,630 --> 01:01:58,260
So you have this only a single connectivity B to white t from y you can see that there is

528
01:01:58,260 --> 01:02:04,140
no way that obviously that would be a fact into y t would say that she is controlled.

529
01:02:05,040 --> 01:02:15,390
So so this one that you can for this one same argument only say that t plus h of the factors of white t plus h according to her outcome model.

530
01:02:15,840 --> 01:02:21,479
So now you have your 80 C, the T, right?

531
01:02:21,480 --> 01:02:25,080
That's a problem meaning of your conditional mean.

532
01:02:25,080 --> 01:02:30,900
And so you have two plus H on a two plus two percentage.

533
01:02:32,190 --> 01:02:47,260
So now you have your 83 that ups 80 plus eight and covariance of t and see that he was h right that that so what is the

534
01:02:47,280 --> 01:02:59,340
say that he say that you plus h well that's easy to calculate y because this covariance is equal to what the variance.

535
01:03:00,570 --> 01:03:11,639
So this is correlation correlation obviously that you see that he possibly okay times the variance

536
01:03:11,640 --> 01:03:16,770
of C that he and the variance of Sidoti percentage right that's the definition of covariance

537
01:03:17,250 --> 01:03:22,420
but you have to station or process the variance of C that variance of students of procedure

538
01:03:22,420 --> 01:03:28,290
are the same because of the stationary policies they're having the same margin distribution.

539
01:03:28,590 --> 01:03:32,760
That's how the stationary process. So that's why the square.

540
01:03:34,990 --> 01:03:40,270
One coming from the parents of serial number one coming from the parents will see that passage.

541
01:03:40,810 --> 01:03:46,630
So what is the correlation of that? You have our you know, our process and news already proved to you, right?

542
01:03:47,080 --> 01:03:48,620
This one is from TV.

543
01:03:50,890 --> 01:03:59,240
This is a if all the corporation function that our news already proved that this is how they construct this leak and process this role to the edge.

544
01:04:00,040 --> 01:04:04,249
Okay. So that's it, right?

545
01:04:04,250 --> 01:04:10,970
Right, right, right here. So this one will be ready equal to roll over long or short.

546
01:04:12,590 --> 01:04:21,590
That's what the revision. Oh, sorry.

547
01:04:21,740 --> 01:04:25,030
You have a square appearance. Okay.

548
01:04:25,100 --> 01:04:28,670
I feel my quality exam. Okay.

549
01:04:29,240 --> 01:04:32,690
Is a correct covariance. Is equal to correlation times.

550
01:04:32,690 --> 01:04:35,830
The square root of verse. See that here and there.

551
01:04:35,840 --> 01:04:40,730
So see that t plus h and and each one gives you lambda.

552
01:04:41,090 --> 01:04:51,380
Then get the square root of lambda squared. That's what I'm saying. Okay, so this is what I did right here.

553
01:04:51,440 --> 01:04:55,000
Okay. Okay. Okay.

554
01:04:55,160 --> 01:05:02,330
You just do this kind of funny, repetitive derivation and prepare all the moment conditions.

555
01:05:02,330 --> 01:05:10,399
Because I told you that common field outcomes moves are based on the first two moments and we're doing this best.

556
01:05:10,400 --> 01:05:13,790
Anita, on this prediction. Only use the first two moments.

557
01:05:13,850 --> 01:05:17,640
Okay. Now, we made all the preparations.

558
01:05:17,660 --> 01:05:20,719
Oh, the moment conditions you see here, here.

559
01:05:20,720 --> 01:05:28,280
And now we move to this stage, applying the formula of carbon filter and how much smoother.

560
01:05:30,170 --> 01:05:34,240
Okay, so that's something that we learned before.

561
01:05:34,250 --> 01:05:39,740
And so now, first of all, you as I said, that this is recursive formula.

562
01:05:39,890 --> 01:05:44,990
Right. So so you do this in the recursion, right?

563
01:05:45,000 --> 01:05:48,800
So suppose that you complete the iteratively, right?

564
01:05:49,040 --> 01:05:58,280
So suppose you completed the calculation of your block and the variance of block based on the data up to T minus one.

565
01:05:58,970 --> 01:06:05,090
What do you want to achieve here is to get moved to the next step.

566
01:06:05,480 --> 01:06:11,540
So what you want to be is here. Okay. Suppose you have done the job up to t minus one.

567
01:06:11,990 --> 01:06:17,450
Now you want to do the update out of of of the, you know,

568
01:06:17,450 --> 01:06:27,520
the blocks the best in any prediction to the next next data point so that you can sort of do this calculation recursively or iteratively.

569
01:06:28,100 --> 01:06:31,220
You know, this online version, one by one. Okay.

570
01:06:32,810 --> 01:06:39,140
So when a new data point come in, you can just use whatever the result you already have to make an update.

571
01:06:39,360 --> 01:06:50,309
Okay. So in this sense, is the online version. So so you have a the first step, let's say for the time being, ignore the first one.

572
01:06:50,310 --> 01:06:56,610
Just suppose that at the time t we already have done everything up to t minus one.

573
01:06:56,610 --> 01:07:05,790
Now we are interested in finding the the T plus, you know, this one update.

574
01:07:06,950 --> 01:07:08,190
So how do you do that?

575
01:07:08,370 --> 01:07:22,530
Well, the first thing is that you do this prediction y, t and your particular y t of course, you know, you have the data of up to T minus one.

576
01:07:23,130 --> 01:07:29,310
So this is block prediction and this is the variance of the prediction.

577
01:07:29,880 --> 01:07:33,690
Okay, so how do you calculate this and you happens to be this.

578
01:07:36,000 --> 01:07:41,790
So so this seems to be quite a easy two to follow, right?

579
01:07:42,360 --> 01:07:46,920
So how do you predict? I mean, let me try this.

580
01:07:47,010 --> 01:08:02,650
This is very, very intuitive. So the common filter is consist two steps.

581
01:08:02,660 --> 01:08:06,680
First, you predict the y t, then you predict your theta t.

582
01:08:06,800 --> 01:08:20,620
Okay, so. So now I have everything up to my team waiting for the lights.

583
01:08:21,720 --> 01:08:30,840
So coming from status 03.1..3.1 is a team.

584
01:08:34,050 --> 01:08:42,310
I don't care about anything after that. I only need to care about how I'm going to do my job from way she lives 1 to 1.

585
01:08:42,810 --> 01:08:57,540
Of course this one will continue. But. But for the time being, for the computer, I only need to know from data here to to make a prediction of this.

586
01:08:58,260 --> 01:09:06,040
Okay. So first of all, you say that, well, how wages generated.

587
01:09:06,460 --> 01:09:09,550
Wages generated from a parcel distribution.

588
01:09:10,150 --> 01:09:17,500
Right. So so this one give and see that he has the meaning of 83, that he's.

589
01:09:20,000 --> 01:09:24,650
Right. So, of course, I don't know about you, but I know that.

590
01:09:24,770 --> 01:09:38,300
I mean, I've seen a t if I say that T minus y is the role of the theater, T minus one plus rule bar.

591
01:09:39,440 --> 01:09:42,460
I think so. Right.

592
01:09:44,210 --> 01:09:51,540
What is my conditional meaning of that? Rose said she might have one role.

593
01:09:51,990 --> 01:09:57,130
Okay. Right here. Right here. That. Okay.

594
01:09:57,850 --> 01:10:04,450
So how am I going to do this? So, of course, I want to replace this thing by the best.

595
01:10:05,290 --> 01:10:14,410
The best. Yes. I don't say that because I want to predict this guy, but I don't know.

596
01:10:14,420 --> 01:10:16,520
I don't know this guy. Right.

597
01:10:17,450 --> 01:10:27,620
The best prediction would be something like 80 times my best to get the ball fair if I can, but I know that city will be by this.

598
01:10:28,220 --> 01:10:36,560
Then my best guess of t minus one will be my minus one because that's the block for this one.

599
01:10:36,740 --> 01:10:42,980
If you replace theta T minus one by m, t minus one, that's the best, the best you can have.

600
01:10:44,240 --> 01:10:47,540
Okay. So that if you plug in here. Right.

601
01:10:51,450 --> 01:11:00,560
Sort of the best guess. Then you have Raul and T minus one plus your plus.

602
01:11:00,760 --> 01:11:05,790
That's the best the case of your theta t given what you have done in the previous day.

603
01:11:06,300 --> 01:11:10,920
So if you open this up, right, that's exactly what you get do.

604
01:11:11,250 --> 01:11:19,170
So so some of it is very, very intuitive because every scintilla of this so-called minor press, the near unbiased prediction.

605
01:11:19,180 --> 01:11:22,110
So this is very intuitive. We will kind of formula. Oh, my God.

606
01:11:22,140 --> 01:11:33,090
This is exactly what you supposed to have if you believe that your this one is the best prediction you get from previous update.

607
01:11:33,690 --> 01:11:37,950
The next one you want to do is, of course, this is just intuition.

608
01:11:38,400 --> 01:11:43,250
You need to prove it mathematically. Right. But this is the sample.

609
01:11:43,260 --> 01:11:50,730
You've set up a target that you want. I mean, it's something very, very close to what you're supposed to do.

610
01:11:50,880 --> 01:11:54,800
Okay. Then you do this calculation of this. That's the advanced.

611
01:11:55,130 --> 01:12:00,110
There's a formula for this. It's a little complicated. This is a point of prediction.

612
01:12:00,120 --> 01:12:07,500
This is a prediction. Uncertainty of what? The variance of prediction, which you can derive it from the formula.

613
01:12:07,950 --> 01:12:15,750
Okay. So now you you have the prediction m.y t now you move down here.

614
01:12:15,900 --> 01:12:25,410
Okay. So if you look at a formula here in a prediction of your y t, you only use the common field or from previous step.

615
01:12:26,100 --> 01:12:32,880
Right. You do not use anything the Y, t and a that at this time.

616
01:12:33,190 --> 01:12:39,240
Right. So now we want to predict the empty.

617
01:12:39,300 --> 01:12:50,160
So now you're looking at a prediction. They'll see that Y is of course, the first part is coming from the the mean part.

618
01:12:51,510 --> 01:12:59,010
So this is the part that you to use to predict your next state of T but in the

619
01:12:59,010 --> 01:13:03,570
same time that you know that there is a little bit deviation from the mean.

620
01:13:04,680 --> 01:13:09,120
So the deviation will be characterized by something called innovation.

621
01:13:09,690 --> 01:13:14,610
So what's what's the difference between your observed the white t okay.

622
01:13:14,910 --> 01:13:23,390
What you observe here, which is available versus what you have predicted using the previous comment field.

623
01:13:24,180 --> 01:13:32,520
So this will be additional knowledge that you want to use to correct your prediction of the T.

624
01:13:33,210 --> 01:13:39,630
Okay. Because safety is supposed to be the one, right?

625
01:13:39,960 --> 01:13:53,910
That is beyond what the model can capture so that this result you read it tells you what is the additional sense beyond what the model can capture.

626
01:13:54,300 --> 01:14:01,440
Okay. So this is the combination of a simple prediction from the mean relationship,

627
01:14:01,800 --> 01:14:07,830
and it's a prediction, Carol, that describes something beyond what the the model can capture.

628
01:14:08,040 --> 01:14:11,100
That's should be part of your latent process.

629
01:14:11,580 --> 01:14:14,640
That's what they believe in. Process is designed supposed to be.

630
01:14:15,060 --> 01:14:19,780
So that's quite straightforward. And the city is the appearance of empty.

631
01:14:19,800 --> 01:14:23,610
So what we can derive that. It's quite intuitive to do that.

632
01:14:23,820 --> 01:14:32,650
Okay. So after we get the common filter, then you do this backward prediction to get a smoother.

633
01:14:33,390 --> 01:14:38,850
So that. So you essentially do the best in your eyes.

634
01:14:38,850 --> 01:14:47,640
Prediction. And so so so for the for the the you can process using the entire data.

635
01:14:48,180 --> 01:14:57,110
Okay. So that's before you only use the data up to your common field or only use data for up to time.

636
01:14:57,120 --> 01:15:03,970
T Now you want to do the prediction using all data available up to time.

637
01:15:04,410 --> 01:15:07,650
Right. So, so that's kind of smoother.

638
01:15:09,090 --> 01:15:14,430
So so you, you get this formula and you're not quite as true for it.

639
01:15:14,430 --> 01:15:14,910
Actually,

640
01:15:15,510 --> 01:15:27,870
they the calculations are based on the common filters do not really require the gamma distribution of possible distribution in the common filter.

641
01:15:27,870 --> 01:15:33,600
And most are completed completely operated on the first two moments of the system.

642
01:15:34,320 --> 01:15:37,740
But in order to derive the first two moments,

643
01:15:37,740 --> 01:15:43,650
we use this cost and gamma distribution to derive the first to moment after you

644
01:15:44,040 --> 01:15:49,169
figure out all the first two moments to establish common filter and smoother,

645
01:15:49,170 --> 01:15:55,350
you only need to use. The first thing you do now need the higher order moments from the two institutions.

646
01:15:55,770 --> 01:16:05,910
That's something. Okay, so maybe I can bring up the, um, I put this arc quote here.

647
01:16:06,630 --> 01:16:11,160
So this is the article that I wrote for the part of data analysis.

648
01:16:13,140 --> 01:16:16,950
So here is something. Initial values.

649
01:16:17,880 --> 01:16:22,170
And so here. Here you can see initialization.

650
01:16:22,290 --> 01:16:26,570
Oops. Let me just put the bigger control.

651
01:16:29,850 --> 01:16:42,670
Okay. Good people. So here I don't know how.

652
01:16:59,200 --> 01:17:01,269
I don't know how to change a story. Okay.

653
01:17:01,270 --> 01:17:09,190
So, so here you have the Internet initialization of computer, essentially your set up something to begin with,

654
01:17:09,340 --> 01:17:21,640
your comment filter basically become a filter is calculated empty and say, right, you need to calculate queries to my emptiness.

655
01:17:21,700 --> 01:17:26,200
80. Okay.

656
01:17:26,350 --> 01:17:29,560
So you want to get empty and a seat in the common theater.

657
01:17:30,370 --> 01:17:34,239
So here is the utilization.

658
01:17:34,240 --> 01:17:42,430
You're doing the Ford calculation. You need the first position, then you have the forward calculation for calculation.

659
01:17:42,880 --> 01:17:48,700
This is basically the way Sigma Square is one overlap, basically.

660
01:17:48,730 --> 01:17:56,860
Okay. So you do this recursion the first step, then recursion until up to end, you do this recursion.

661
01:17:57,040 --> 01:18:02,060
The formula I just showed you has been called in here to do that.

662
01:18:04,300 --> 01:18:09,580
Here I have additional things to calculate, like dambatta or something like that.

663
01:18:09,610 --> 01:18:12,760
This is calculate the official information or patient.

664
01:18:13,990 --> 01:18:23,010
Know if you want to do this. Now consider greedy.

665
01:18:23,770 --> 01:18:29,679
Greedy. So you can you can calculate this done in the online or into the greedy learning.

666
01:18:29,680 --> 01:18:33,909
But I will tell you that you may not necessary to to do that.

667
01:18:33,910 --> 01:18:39,640
But anyway, here I have additional quantities for for the quantity we use.

668
01:18:40,000 --> 01:18:44,730
Here you have f you have you you have C, okay?

669
01:18:44,800 --> 01:18:51,660
This is the essential the things that M and C are the things that we want to calculate.

670
01:18:51,670 --> 01:19:00,450
This is common filter. Our C is common the variance of common filter and the F is the prediction and Q is the prediction error.

671
01:19:01,080 --> 01:19:04,420
Right. So I just shoot here. Right.

672
01:19:05,140 --> 01:19:13,060
But I have additional time. The S and data you can you can work that for full time being.

673
01:19:13,060 --> 01:19:20,930
And so, so now you have the comma smoother here next to that you do backward things.

674
01:19:20,980 --> 01:19:24,940
So here you do your m star and see star.

675
01:19:25,120 --> 01:19:30,519
Those are the common filter and the variance outcome and filter. So, so.

676
01:19:30,520 --> 01:19:36,550
So so. Very simple. It you can use my formula to do the calculation.

677
01:19:36,880 --> 01:19:46,420
You just pick up the m essence c s and above you pick up F, F, Q, C,

678
01:19:46,420 --> 01:19:53,230
and those are things that they are the common filter that you need to do to implement this.

679
01:19:56,960 --> 01:20:05,960
That's just my PDF file, so that's easy to implement here.

680
01:20:05,960 --> 01:20:09,110
I basically try to prove this.

681
01:20:09,620 --> 01:20:15,660
You get this formula, we can skip this part. Okay. And so what do you have?

682
01:20:15,680 --> 01:20:19,520
And the very end is this.

683
01:20:19,700 --> 01:20:22,420
This is common, smoother. Okay.

684
01:20:22,430 --> 01:20:30,650
So you will have the predicted late in process this process that can now be captured between the system, reality and everything.

685
01:20:31,160 --> 01:20:34,310
So that's these data. See that empty star?

686
01:20:34,790 --> 01:20:43,460
So you can pull that. Look at that. What additional variability that you can now really captured by the cover to cover your process.

687
01:20:43,880 --> 01:20:47,110
So. So after we implement that, you can get that. Okay.

688
01:20:48,310 --> 01:20:55,910
This this is very nice to interpret what's going on. And for G and M algorithm, you were not able to get this kind of estimate.

689
01:20:56,270 --> 01:21:05,600
So that's why I like coming forward. And I didn't now realize that this is really something strongly to negative reinforcement learning nowadays.

690
01:21:05,690 --> 01:21:16,090
So popular. Right. And become a filter and smoother is basically the original idea of Bell's equation which is essential in the reforms.

691
01:21:17,120 --> 01:21:27,739
So I hope that you learned that lecture from this lecture, and I can give you a, well, access to reinforcement and something in the future.

692
01:21:27,740 --> 01:21:35,960
You can say, Oh my God, I have learned this before, but this is just different for different ways to make a decision making process.

693
01:21:37,250 --> 01:21:42,560
And okay, Tuesday, I will talk to midterm and also talk about the remaining of the slides.

694
01:21:42,980 --> 01:21:50,420
How do you actually incorporate a smoother common filter into estimation to get estimate of the parameter after?

695
01:21:50,950 --> 01:21:54,320
Okay. Okay. That's all I want to see.

696
01:21:54,710 --> 01:22:02,030
If you all send me your colorful, beautiful figures and by email, I look forward to receive them.

697
01:22:06,300 --> 01:22:06,600
Okay.

