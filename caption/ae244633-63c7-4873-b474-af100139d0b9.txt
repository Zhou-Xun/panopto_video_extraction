1
00:00:00,030 --> 00:00:08,729
2016. Was that good? I think the sports fortunes are all right.

2
00:00:08,730 --> 00:00:13,070
As of 3:00 through 2 p.m., there are 49 homework assignments turned in.

3
00:00:13,090 --> 00:00:16,550
So I think there are 56 people signed up.

4
00:00:16,560 --> 00:00:22,200
I guess I have a few delays, so it's pretty close to target statistics.

5
00:00:22,200 --> 00:00:29,129
Everything is approximate, right? So we have approximately all the homework assignments in which I again,

6
00:00:29,130 --> 00:00:37,530
my goal is to have some grade these within a week so that by the time we come together next Wednesday at 3:00,

7
00:00:37,980 --> 00:00:42,420
you will have a grade inside of canvas telling you how you did on the homework assignment.

8
00:00:43,320 --> 00:00:47,340
And at that point, I'll have an idea of what what's going well, what isn't.

9
00:00:48,240 --> 00:00:53,340
Some assignments that maybe I'll be asking you. I could share with the class to show you what you guys came up with.

10
00:00:54,660 --> 00:00:57,330
So hopefully in a week, it's my goal.

11
00:01:01,170 --> 00:01:08,370
The way this class is working is I'm realizing I'm asking you now to do homework assignment too, and you have maybe no idea how to do it.

12
00:01:09,000 --> 00:01:13,489
So that's our goal. And I don't mean for everything to be a surprise.

13
00:01:13,490 --> 00:01:19,580
And you certainly are not expected in this class to go on the Internet and try and find libraries and so forth.

14
00:01:20,330 --> 00:01:21,920
That is not the point of this class.

15
00:01:22,490 --> 00:01:29,540
So if there's anything you need to do and ah, I'm going to give it to you in class at some point, and if you want it sooner, let me know.

16
00:01:30,200 --> 00:01:34,280
And I'm happy to talk about it. So homework number two.

17
00:01:34,290 --> 00:01:37,430
Now we're going to start doing stuff that you haven't done before.

18
00:01:38,510 --> 00:01:41,360
A little bit of what you have done before and a little bit of this stuff.

19
00:01:42,280 --> 00:01:45,650
So Walmart number two, you're going to use the same two data sets you had last time.

20
00:01:45,860 --> 00:01:50,060
So you don't have to make the long and the white and all that that should already be ready to go.

21
00:01:51,300 --> 00:01:53,870
I want you to build upon what you did for homework, number one.

22
00:01:54,470 --> 00:02:06,350
So for your two continuous datasets, the first thing you're going to do is set a model that includes group time and the interaction of group of time.

23
00:02:07,400 --> 00:02:14,120
Someone came up to me and said that their data set had some other variables, maybe gender or sex or age or something else.

24
00:02:15,240 --> 00:02:18,889
So we're going to ignore those right now. We're not going to talk about adjusting for other things.

25
00:02:18,890 --> 00:02:23,840
We want to get the concepts of these models down before we worry about confounders and so forth.

26
00:02:24,410 --> 00:02:30,560
So a model that has time group and the interaction presets to fit a linear model.

27
00:02:31,070 --> 00:02:34,750
So ignore the correlation. You can do this right now. This is 650.

28
00:02:34,760 --> 00:02:43,819
This is outlined in our parts B and C, then ask you to fit a linear, linear model with different correlation structures.

29
00:02:43,820 --> 00:02:49,250
And that's what we're going to learn this week. And then I ask you to do things with these models.

30
00:02:49,340 --> 00:02:54,979
I ask you to come up with a sandwich variance estimate to a we haven't gone over that yet and I ask you to

31
00:02:54,980 --> 00:03:05,840
compute our squared AC and by C did they cover AC in back in your regression in class B and C plus the AC?

32
00:03:05,870 --> 00:03:09,830
Yes. Okay. So at least you have a familiarity with this this thing.

33
00:03:09,980 --> 00:03:18,830
We're going to use those again to compare models to each other. Hopefully get to that by next Wednesday.

34
00:03:22,130 --> 00:03:25,310
So you're going to fit three separate models to your first data set.

35
00:03:25,940 --> 00:03:30,260
You're going to tell me which of those three models you like the best and write up a little report on it.

36
00:03:31,130 --> 00:03:37,610
You're going to set those same three models to the second data set, and you're going to tell me which of those three models you like the best.

37
00:03:38,150 --> 00:03:47,360
And that's the homework. If you want to get going now, Friday, I'm not going to go through our code for the homework.

38
00:03:47,450 --> 00:03:51,770
I'm going to go through some other code panel things go.

39
00:03:52,430 --> 00:04:01,159
But in our studio cloud, if you want to know the libraries and the functions, you need to do homework.

40
00:04:01,160 --> 00:04:04,970
Number two. If we start back at project.

41
00:04:05,750 --> 00:04:14,299
There are two things you can do. You can go to my homework two folder and I have already done homework two for one of the data sets,

42
00:04:14,300 --> 00:04:19,130
this PIB dataset that I covered with you last week. So there is a file called Homework two.

43
00:04:20,270 --> 00:04:31,240
Again, I'm not asking you to do verbatim what I did, but what I wanted to point out was, is that in this homework, number two, whoops.

44
00:04:31,280 --> 00:04:35,780
There we go. There's a section where I say about the needed libraries that I need for this homework assignment.

45
00:04:36,140 --> 00:04:41,600
So and LME is the function we need in order to fit generalized least squares.

46
00:04:42,020 --> 00:04:45,920
So linear regression with a non independence correlation matrix.

47
00:04:47,110 --> 00:04:56,530
And club sandwich. Someone's very funny. This has to do with how to compute a sandwich variance estimate that we're going to get to in class.

48
00:04:57,100 --> 00:05:02,559
So you will need these two libraries in the NL M E Library.

49
00:05:02,560 --> 00:05:08,500
The function to fit a generalized squares model is, not surprisingly, g l s.

50
00:05:09,290 --> 00:05:16,779
Lots of the syntax looks just like l m. So there's very little new stuff in each line except for fitting the correlation structure.

51
00:05:16,780 --> 00:05:21,940
So I'll show you how to set an exchangeable correlation, which may or may not mean anything to you right now.

52
00:05:22,720 --> 00:05:28,780
Once you fit the model, l still want you to get a sandwich variance estimate or the function for that is called Z Cab.

53
00:05:28,780 --> 00:05:33,700
See our capital C, r and there's some other arguments in there that would be needed.

54
00:05:33,700 --> 00:05:41,559
Right? So again, if you want to get a head start on this, you have some code in front of you that you can start to play with again.

55
00:05:41,560 --> 00:05:45,940
I give you the data. You can run this code. It should run in our studio phone.

56
00:05:45,940 --> 00:05:49,780
I think I have the right working directory set up and you can see what comes out.

57
00:05:50,650 --> 00:05:57,790
Another thing you can do is everything I do with the labor pain dataset that I talked to it showed you earlier in class.

58
00:05:58,330 --> 00:06:02,920
I have in our file there called Fitting Correlation structures and in that file.

59
00:06:06,700 --> 00:06:10,419
You'll also see me lowering the two libraries and I fit some other correlation structures

60
00:06:10,420 --> 00:06:15,090
and just it's more of a playing around set of our code than what I would do for 100%.

61
00:06:15,100 --> 00:06:26,990
But so you have our code to help you with number two if you want to start getting going on that now some homework questions issues but.

62
00:06:29,800 --> 00:06:33,180
Easy. Right.

63
00:06:34,410 --> 00:06:38,310
We're going to do a poll everywhere. But I want to make sure I get through the lecture notes.

64
00:06:38,320 --> 00:06:43,889
I want to have fun, but I also want to get you guys going on the stuff you need to know for the homework.

65
00:06:43,890 --> 00:06:48,540
So I want to do a poll of work, but I want to make sure I get through my lectures first today instead of the other way around.

66
00:06:52,970 --> 00:06:56,420
Oh, yes. Before I do that. So that's done.

67
00:06:57,590 --> 00:07:00,830
I sent you guys all an announcement if you check the announcements in canvas.

68
00:07:01,880 --> 00:07:11,160
I want to make sure that when we do this, Paul, everywhere, as you actually realize that there is a purpose to the website reloaded.

69
00:07:11,600 --> 00:07:17,299
So in the canvas I sent you an announcement that basically said, here are your responses.

70
00:07:17,300 --> 00:07:26,570
So they're all downloaded to a CSP file. And I took a look at them and I would tell you all to do the same if you want to try something different.

71
00:07:27,170 --> 00:07:33,200
So I got lots of responses on musical artists. 50% I heard of, 50% I've never heard of.

72
00:07:34,960 --> 00:07:42,230
I think again, I think the beauty of this is I get to check out some artists that I wouldn't even know exists because they don't sing in my language.

73
00:07:43,820 --> 00:07:50,000
So there are some Xs I put down here. So these are groups that I tried out last Friday that I had never heard of before.

74
00:07:50,000 --> 00:07:53,809
I don't want to imply that I haven't heard of some of these folks. I don't need to listen to Justin Bieber.

75
00:07:53,810 --> 00:07:57,500
I know what he sounds like. He's really cool.

76
00:07:57,500 --> 00:08:03,200
But does anyone know the podcast song Exploder?

77
00:08:04,220 --> 00:08:15,890
Never heard of this. So Hrishikesh Hirway, who's an amazing human being, interviews an artist about a song they created and he almost speaks nothing.

78
00:08:15,890 --> 00:08:20,690
It's all the artists explaining from the very beginning how they developed the song,

79
00:08:21,260 --> 00:08:24,980
where it came from, how they mixed the sounds and the lyrics and so forth.

80
00:08:25,280 --> 00:08:29,509
Anyway, there's a great interview with Rick Astley and his son.

81
00:08:29,510 --> 00:08:34,340
Never going to give you up from when I was in high school, but it's fascinating.

82
00:08:35,600 --> 00:08:41,390
Where did he go and why did he have this hit that is so strong anyway?

83
00:08:41,510 --> 00:08:44,900
And I know he's got some cultural references, too.

84
00:08:45,320 --> 00:08:49,610
Motionless and white. I tried out again, but can't say that I'm going to listen to any of these again.

85
00:08:51,080 --> 00:08:56,090
But it's fascinating. How does that work? I listen to both the chicken.

86
00:08:56,750 --> 00:09:01,760
I learned that there is a there are two trumps in Spotify.

87
00:09:01,790 --> 00:09:10,610
One is more electronic music. One is I don't know what the right term is, the hip hop, gangster, rap kind of music.

88
00:09:11,860 --> 00:09:15,530
It's a sex party. Does a great cover of Tears for Fears.

89
00:09:16,340 --> 00:09:19,940
No, if you know who Tears for Fears is, I hope you do.

90
00:09:21,000 --> 00:09:24,709
And Solomon, I think of all of these. The person I will listen to again is Solomon.

91
00:09:24,710 --> 00:09:28,850
Was that what they you? So anyways, don't need to listen to Taylor Swift.

92
00:09:28,850 --> 00:09:32,930
My daughter plays that all the time. Anyway, check some of those out.

93
00:09:32,960 --> 00:09:36,160
I think it's really cool to see what what some of you listen to.

94
00:09:36,170 --> 00:09:43,010
It's just the music is a fascinating thing. It's just what makes us happy and doesn't make us happy anyway.

95
00:09:43,280 --> 00:09:47,480
So check those out. It was a good exercise for me.

96
00:09:48,800 --> 00:09:53,390
All right. We'll do a poll everywhere. But like I said, let's talk about my statistics first.

97
00:09:54,830 --> 00:09:59,120
So I updated the lecture slides about 230 today.

98
00:09:59,120 --> 00:10:04,190
So if you've downloaded them, the first couple will have some changes to them.

99
00:10:04,430 --> 00:10:09,580
So just be aware of that points.

100
00:10:11,790 --> 00:10:18,200
All right. We're still reviewing a lot, which is really important because I know that works.

101
00:10:18,860 --> 00:10:24,340
Can I just pick one there? So reviewing is an important thing.

102
00:10:24,360 --> 00:10:32,630
So I want to talk about what you covered last year, the concepts that I think are really important to carry over in this class.

103
00:10:32,660 --> 00:10:38,600
I know you probably learned betas and variance inflation factors and residual plots.

104
00:10:39,100 --> 00:10:42,650
A lot of that's going to go away at least this semester.

105
00:10:43,100 --> 00:10:47,690
But a lot of the concepts on estimation and inference are going to continue using this class.

106
00:10:49,250 --> 00:10:55,430
All right. So linear regression, one observation per person, no correlation here, no longitudinal anything.

107
00:10:56,720 --> 00:11:03,980
We got capital Y denoting an outcome or one of the outcomes in our dataset.

108
00:11:04,700 --> 00:11:16,790
And then we have covariates, age, gender, ethnicity, so forth, income a set of quadrants x one through XP minus one for each individual.

109
00:11:19,170 --> 00:11:26,389
And we assume that each of the outcomes has a normal distribution that's conditional on

110
00:11:26,390 --> 00:11:31,379
these covariance in the mean we assume that they all have the same variance sigma squared.

111
00:11:31,380 --> 00:11:39,300
That's the constant variance assumption. So you can write it as I have up on top there, that each Y has its own distribution.

112
00:11:39,300 --> 00:11:41,820
So they're not I.D., they're I.

113
00:11:42,060 --> 00:11:47,670
They're independent, but they're not identically distributed because they each have a different mean depending on what their covariance are.

114
00:11:49,470 --> 00:11:57,000
Typically, however, write it in the second form, which says that Y is a set of linear combination of covariates plus an error.

115
00:11:57,540 --> 00:12:03,090
And then we see that the error does mean zero and they all have the same central square, right?

116
00:12:03,990 --> 00:12:07,890
So the variance doesn't change among the individuals in our dataset.

117
00:12:08,160 --> 00:12:11,670
It doesn't change with covariance. It is constant is the same number.

118
00:12:13,160 --> 00:12:19,340
We make a very strong assumption again that these covariates explained the mean in a very specific way.

119
00:12:20,270 --> 00:12:24,590
We multiply them by a number and we add them together. Linearity assumption.

120
00:12:26,300 --> 00:12:33,030
We also make the strong assumption that each of these errors for each of the Y's is independent of all the others.

121
00:12:33,050 --> 00:12:37,760
So my outcome has no association whatsoever with anybody else's outcomes.

122
00:12:39,150 --> 00:12:43,000
In the question at the bottom. That is why. Why do we care about these assumptions?

123
00:12:43,010 --> 00:12:46,880
What are they doing for us when we fit our models?

124
00:12:48,840 --> 00:12:54,840
So again, if you don't like matrix algebra, we're going to help you make it a little bit more this semester.

125
00:12:55,800 --> 00:12:59,820
Everything gets even a layer right instead of a matrix for the data.

126
00:13:00,060 --> 00:13:04,050
Each person is going to have a matrix. Then we're going to put all these matrices together.

127
00:13:05,040 --> 00:13:09,810
So we're going to use a lot of matrix algebra. So the data, again, I put all of the outcomes in a single vector.

128
00:13:11,010 --> 00:13:19,920
Each person has a vector of covariates, X and Y, and then I put all the covariance vectors on top of each other into a design matrix capital x.

129
00:13:20,910 --> 00:13:28,440
The model says that there are a vector of coefficients for those in a vector, and we the vector means one for each person.

130
00:13:29,520 --> 00:13:37,630
And the errors are put into a vector. And so we can say that all of the outcomes of a multivariate normal distribution.

131
00:13:37,720 --> 00:13:42,400
It just happens that there have no correlation. And so capital Y,

132
00:13:42,400 --> 00:13:50,559
all of the ways have a mean new view as supposed to look boldfaced and turn out here x data and have a variance

133
00:13:50,560 --> 00:13:57,100
covariance matrix that is simply a diagonal matrix with sigma along the diagonal and zeros everywhere else.

134
00:13:57,670 --> 00:13:59,890
It's sigma squared times and identity features.

135
00:14:02,530 --> 00:14:09,700
And again, the purpose of this last line is because we're going to expand that to change the sigma matrix.

136
00:14:09,700 --> 00:14:14,710
The whole point of this is to get to a sigma matrix that isn't just sigma squared times that identity.

137
00:14:15,040 --> 00:14:18,610
We want things off the diagonal. That's correlation.

138
00:14:20,170 --> 00:14:28,300
However, in linear regression, we estimate the Russian coefficients using maximum likelihood, which is the same as least squares.

139
00:14:30,130 --> 00:14:33,820
Again, there is the joint distribution of all of the YS.

140
00:14:34,360 --> 00:14:39,160
It is proportional to this normal col rate, the exponential of something squared.

141
00:14:39,520 --> 00:14:45,399
Here we're in matrix land so we take something times the inverse of the variance multiplied by that.

142
00:14:45,400 --> 00:14:46,090
Think again.

143
00:14:47,200 --> 00:14:54,280
So if we take the log of that, we end up with some function of sigma that we don't care about because we're interested in estimating beta.

144
00:14:55,180 --> 00:15:00,910
And then we have these other terms involving the axes in the whys and the matters which hopefully you are all familiar with.

145
00:15:01,840 --> 00:15:04,180
We take the derivative of that set of equal to zero.

146
00:15:04,210 --> 00:15:11,590
The derivative has this very nice form and if we solve we get that beta hat is what it looks like there.

147
00:15:12,400 --> 00:15:22,300
It's sigma, it's transpose sigma inverse x, all inverse x, transpose sigma inverse y in 650 that sigma disappeared.

148
00:15:22,390 --> 00:15:25,630
Right. It wasn't there, it was x transpose x inverse x transpose y.

149
00:15:26,800 --> 00:15:32,020
I just had professor who said you should have a few formulas tattooed on your wrist for the rest of your life.

150
00:15:32,620 --> 00:15:38,020
That's one of them. Beta had is x transpose x in vertex transpose y except in the correlated data.

151
00:15:38,530 --> 00:15:45,939
But that one should just stick in your head. We take this formula here and we know what the sigma is.

152
00:15:45,940 --> 00:15:50,200
Sigma is sigma squared times an identity matrix.

153
00:15:50,200 --> 00:15:58,780
And so the inverse of that is really easy. And so beta hat, if we stick in what we know, sigma is the sigma's cancel out.

154
00:16:00,170 --> 00:16:07,040
And that's why we're left with X transpose December six, transpose Y in the case of independent constant variance data.

155
00:16:08,990 --> 00:16:14,360
We know that this estimate that we've come up with is an unbiased estimate of data,

156
00:16:14,540 --> 00:16:20,779
because if you take the expected value, the only random part is Y in the expected value of Y as x beta.

157
00:16:20,780 --> 00:16:24,990
So you plug that in, everything cancels out and you're left with data, right?

158
00:16:25,730 --> 00:16:29,040
So on average. We get the right thing.

159
00:16:31,030 --> 00:16:37,690
And one of the interesting things about our profession is this doesn't mean I get the right thing right.

160
00:16:38,080 --> 00:16:42,030
It means on average, we all end up getting something on average.

161
00:16:42,040 --> 00:16:46,660
That's right. But you may be way off and you may be right. I'm right on average.

162
00:16:47,560 --> 00:16:54,280
So your coefficient estimates may be lousy, but you're consoled in the fact that on average they're pretty good.

163
00:16:55,300 --> 00:17:06,190
And what's the variance covariance of is these regression coefficients and the variance of beta hat in a quadratic form.

164
00:17:06,640 --> 00:17:11,530
Right. If you go up to the top part of my slide there, you take the variance of beta hat, you get x,

165
00:17:11,530 --> 00:17:17,910
transpose x and vertex transpose variance of y, and then you have to multiply by the transfers of that stuff, right?

166
00:17:17,920 --> 00:17:21,790
The variance of something a number of times something is that number squared.

167
00:17:21,870 --> 00:17:23,710
And that's what we do with matrices there.

168
00:17:25,660 --> 00:17:33,820
So again, we've got the top line there that the variance of beta hat are these two x transpose x transpose x pieces and the variance of Y.

169
00:17:34,390 --> 00:17:41,250
And because we believe the variance of y is sigma squared times an identity matrix, we get lots of things to cancel out.

170
00:17:41,260 --> 00:17:45,670
Lots of the x, there's one x transpose x that cancels out with its inverse.

171
00:17:46,570 --> 00:17:54,250
And we're left with the thing that you saw in your regression class that the variance of your regression parameters is the variance of the outcomes.

172
00:17:54,250 --> 00:17:57,250
Y sigma squared james x transpose x inverse.

173
00:18:02,170 --> 00:18:07,810
Now we don't know what sigma squared is, we have to estimate that thing.

174
00:18:08,530 --> 00:18:13,000
And so we can use maximum likelihood to estimate sigma squared as well.

175
00:18:13,880 --> 00:18:15,490
Again, I'm not going to go through the algebra there,

176
00:18:15,490 --> 00:18:19,390
but you get the likelihood you take that you know the log and you take the variables that equal equals zero.

177
00:18:20,260 --> 00:18:25,300
You get that familiar form there. It's the sum of the squared residuals divided by it.

178
00:18:26,500 --> 00:18:30,700
And of course, we've told you and we tell folks you don't use you don't divide by N,

179
00:18:31,420 --> 00:18:36,219
you divide by N minus P because you're estimating P parameters, the mean parameters.

180
00:18:36,220 --> 00:18:41,320
Right. There's some bias induced because you're putting in estimates rather than the true values for data.

181
00:18:42,280 --> 00:18:46,750
And so that second one, they're dividing by N minus P instead of and gives you an unbiased estimate.

182
00:18:47,860 --> 00:18:51,550
And again, I work with individuals who take a regression class.

183
00:18:52,480 --> 00:18:58,570
I don't really care if you use an R, N minus P, if you've got five regression coefficients and 200 people,

184
00:18:59,860 --> 00:19:05,470
whether you use 200 or 200 minus five, it's the same number for all intents and purposes.

185
00:19:05,980 --> 00:19:12,880
So again, theoretically, the latter is unbiased, but the other one is probably pretty good.

186
00:19:12,890 --> 00:19:18,130
If you have enough data and you don't have this crazy regression model that has 35,000 parameters in it, right?

187
00:19:18,610 --> 00:19:23,860
If P is relatively small and then is relatively big, it don't really care which one you use.

188
00:19:24,160 --> 00:19:29,800
My answer is going to be pretty much the same. But all right.

189
00:19:30,490 --> 00:19:35,830
As I was talking with everybody in class or in my office hours about homework, number one,

190
00:19:36,850 --> 00:19:41,380
one of the challenging things about modeling longitudinal data is the time affect.

191
00:19:42,400 --> 00:19:46,299
How exactly should I enter time into the model? Should it be continuous?

192
00:19:46,300 --> 00:19:50,020
Should it be a factor? Should it be a quadratic? Should it be a swain?

193
00:19:51,040 --> 00:19:55,090
And again, there's no right answer here. I'm going to tell you, it really depends on what you want to do.

194
00:19:55,210 --> 00:19:56,380
What do you think you should do?

195
00:19:58,240 --> 00:20:10,330
So if you were part of a longitudinal study where you actually know the measures of time week one, day seven, day 14, day 31, month six, month 12.

196
00:20:11,380 --> 00:20:16,390
If you know those numbers, then why not use those numbers? Don't call it one, two, three and four.

197
00:20:16,540 --> 00:20:19,840
Call it six, 12, 18, whatever. Right. Continuous.

198
00:20:21,700 --> 00:20:26,760
I think that's the easiest approach. One, because it only requires one parameter, right.

199
00:20:26,770 --> 00:20:35,860
You're going to fit this slope through time. That's just one beta coefficient that makes my model a lot easier to interpret if they're equally spaced.

200
00:20:36,250 --> 00:20:45,520
Right. Six, 12, 18 seems pretty good, but do remember that you're making the very strong assumption you should plug the data first.

201
00:20:46,240 --> 00:20:52,050
Do you see this fairly linear increase over time across across your study age?

202
00:20:52,360 --> 00:20:55,989
You should look at that before you set the model so many times.

203
00:20:55,990 --> 00:20:58,600
I think you'll just be fitting one coefficient times time.

204
00:20:59,350 --> 00:21:07,420
Remember then that if you're doing an interaction like you are in homework, number two, you have an additional parameter for every group.

205
00:21:08,440 --> 00:21:15,640
So if you want to fit a cool cubic spine in time, you got three parameters and then you have three groups.

206
00:21:16,450 --> 00:21:20,259
That's going to add six more parameters to your model and it gets really hard to explain.

207
00:21:20,260 --> 00:21:25,030
So remember, accuracy has to be balanced with parsimony.

208
00:21:25,600 --> 00:21:29,350
Simple models are often better and get at the same answer good enough.

209
00:21:31,110 --> 00:21:37,169
If your outcome doesn't appear to change linearly over time, like the one I showed you last week with this PIB data that I did for homework.

210
00:21:37,170 --> 00:21:41,400
Number one, you could get a polynomial, right? You get your time and time squared.

211
00:21:42,360 --> 00:21:46,740
If you've worked with some lines, you could fit a spine to try and put some curvature.

212
00:21:47,280 --> 00:21:55,710
But as I said, the number of parameters grows and more importantly, it's really hard to interpret, especially the use of spline.

213
00:21:56,790 --> 00:22:03,660
We teach you that being a one is, you know, per unit, times year per unit change in x average y changes by this much.

214
00:22:04,350 --> 00:22:11,429
So the spine coefficients aren't that interpretation because the things that go in the model are not covariance,

215
00:22:11,430 --> 00:22:14,100
they're basis functions of covariance.

216
00:22:14,100 --> 00:22:21,840
So it gets really hard to tell an individual who has a lot of statistics that you fit a cubic spine and you see a difference between the groups.

217
00:22:22,620 --> 00:22:27,390
So just keep all that in mind. I'm not telling you not to do it, and you're welcome to do it in this class.

218
00:22:28,710 --> 00:22:33,080
Just keep in mind you have to explain this stuff to if you want.

219
00:22:33,090 --> 00:22:37,020
You could treat time as a factor, especially if you don't know the exact timings, right?

220
00:22:37,020 --> 00:22:40,530
If you come up to a study, they say, well, this was visit one, visit two and visit three.

221
00:22:42,960 --> 00:22:45,690
We don't really want to call it time one, time two, time three,

222
00:22:45,690 --> 00:22:49,530
because you're making a very strong assumption that they're equally spaced time one, two, three.

223
00:22:49,530 --> 00:22:55,319
Right. So you might just want to make them factors so that you're setting a mean at each point in time.

224
00:22:55,320 --> 00:23:05,930
So they might go up and down. It's also useful as I serious visits are spread fire and time if they have month three and then a year later.

225
00:23:06,740 --> 00:23:12,680
Do you really want to assume linearity through that whole period of time that there was nothing going on in the observations that you saw?

226
00:23:14,180 --> 00:23:19,570
So you can sit as a time as a factor. But again, they're using dummy variables.

227
00:23:19,910 --> 00:23:25,390
You pick a reference time point, usually time one. And then you have dummy variables for the different cities, which are time point.

228
00:23:26,020 --> 00:23:28,450
If you've got a lot of time points, you've got a lot of dummy variables.

229
00:23:28,450 --> 00:23:32,590
If you don't have interactions with with group, you've got a really crazy model.

230
00:23:32,770 --> 00:23:39,410
So how are you going to explain how time is in this model and whether it differs across the groups?

231
00:23:39,430 --> 00:23:47,200
That's what we're trying to do this semester. So as I showed you last week with the data, this is like m this was a perfect I didn't even plan this.

232
00:23:47,200 --> 00:23:52,600
This was a perfect example for the situation. Here are three groups of individuals.

233
00:23:53,530 --> 00:23:56,680
How do I fit time in this model? What makes sense here?

234
00:23:56,710 --> 00:24:02,110
Probably not a straight line. But okay, so I said that.

235
00:24:03,130 --> 00:24:07,510
So I have model one here again. I just said, well, I'm going to set an intercept and a slope for time.

236
00:24:08,080 --> 00:24:13,090
Right? That's a linear. Right. So there is there is a change over time, but it isn't linear.

237
00:24:13,090 --> 00:24:20,440
So this probably isn't a good choice. I could fit a quadratic term, so x and that is so x is time again.

238
00:24:20,440 --> 00:24:22,240
We've got time and time squared.

239
00:24:24,580 --> 00:24:32,920
I don't, I don't like quadratic models again trying to explain to folks, you know, what does beta one mean versus beta two.

240
00:24:32,950 --> 00:24:39,579
It's the whole curve and what do you do if the beta one term isn't significant and the beta two term is right?

241
00:24:39,580 --> 00:24:43,750
There's all kinds of fun stuff. Model three I have.

242
00:24:44,200 --> 00:24:46,300
So again, these data were collected at time zero.

243
00:24:46,810 --> 00:24:52,210
Then half an hour, hour and a half are two, three, four, five, six, five, one, two, three, four, five, six, seven, eight.

244
00:24:53,080 --> 00:24:57,970
So I could fit a bunch of factors, so I've got a bunch of dummy variables.

245
00:24:58,420 --> 00:25:01,420
So again, data not tells me the average at time zero.

246
00:25:01,660 --> 00:25:14,110
This is the difference at time, half an hour or one or two or 3 hours or over half an hour, one hour, one and a half hours.

247
00:25:14,110 --> 00:25:20,980
Two, three, four, five. There we go. So but again, that's a lot of coefficients and I don't even have interaction with group here.

248
00:25:21,440 --> 00:25:25,720
Right. Do this means that each time point change would between the groups and they do early on.

249
00:25:25,870 --> 00:25:32,770
Right. So you could fit that model you could fit a piecewise linear model.

250
00:25:33,070 --> 00:25:37,660
I don't know if this is covered in 650 anymore. Anybody.

251
00:25:37,930 --> 00:25:42,310
Anybody. Take 650. Any thing like this?

252
00:25:43,180 --> 00:25:53,860
Yes. No. Okay. Maybe. So these data, I think, strongly suggests to me that there is a linear trend for about tilde or two,

253
00:25:54,430 --> 00:25:56,980
and then there's another linear trend beyond that or two.

254
00:25:57,130 --> 00:26:05,860
So I'm going to fit two lines that meet up at time to in time to what's called the not in spines and in piecewise linear models.

255
00:26:05,860 --> 00:26:09,630
That's called a not. And so there is the model there.

256
00:26:09,640 --> 00:26:16,570
So I have a slope for everybody. And then there's an additional subcomponent for the time points that are beyond our two.

257
00:26:17,260 --> 00:26:24,309
So down there ends up getting these there's a set of data yesterday, there's a negative slope here and then there's a positive slope here.

258
00:26:24,310 --> 00:26:29,530
And you can see interactions with Group C and the fact the same, the groups have the same values.

259
00:26:30,820 --> 00:26:35,500
Again, this isn't perfect. Is that the rate? Not right.

260
00:26:35,590 --> 00:26:41,410
You're you're making a very strong assumption that in any of these measurements for anybody else out there in the world,

261
00:26:42,070 --> 00:26:49,150
there is this distinct change at our two. And, you know, that's random two that's that's all variable a little bit.

262
00:26:49,720 --> 00:26:53,740
But so be careful when you pick nuts you might say you know what time?

263
00:26:53,740 --> 00:27:00,610
I think that there is a steeper slope for the first half hour and then there's another slope and then there's the slope and then there's the slope.

264
00:27:01,150 --> 00:27:07,300
Right? You can start to let the data drive your model a little bit too much and you start overfitting and so forth.

265
00:27:07,570 --> 00:27:16,750
But again, in these homework assignments, you may set categorical time, you may set a slope, you may get something else with the same data set.

266
00:27:17,230 --> 00:27:20,920
Right? As long as you can explain to me what you're doing, I don't care what you do.

267
00:27:21,770 --> 00:27:27,850
Be smart. All I want you to do is be smart. So that's the first set of slides.

268
00:27:27,880 --> 00:27:35,620
Again, basic introduction to simple linear regression and then focusing on out how you might think about time in your very own analyzes.

269
00:27:40,210 --> 00:27:47,650
Let's move a little bit further weight at least squares a slide show from the beginning.

270
00:27:48,370 --> 00:27:58,300
So we want to move beyond now the idea of independent observations and somehow account for the fact that they have different variability.

271
00:27:59,950 --> 00:28:08,110
So again, we have always assumed in linear regression that each outcome has the same variance sigma squared, right?

272
00:28:08,650 --> 00:28:11,500
And so therefore, if all the outcomes are independent,

273
00:28:11,500 --> 00:28:18,160
we have this very nice structure of a variance matrix that just simply as sigma squared on the diagonal and.

274
00:28:20,700 --> 00:28:23,370
Without these assumptions if we get rid of those assumptions.

275
00:28:23,610 --> 00:28:29,070
These are the two equations that I had in my previous slides that regardless of what Sigma looks like,

276
00:28:29,340 --> 00:28:34,410
there is my estimate for my regression coefficients as well as its sampling variability.

277
00:28:35,010 --> 00:28:42,770
It's the typical form, that typical form, and it's Sigma Square is in fact a diagonal with Sigma squared on it.

278
00:28:43,040 --> 00:28:55,739
I think that's. If we have a different sigma matrix and if we know what it is, if it isn't sigma squared times an identity,

279
00:28:55,740 --> 00:29:01,860
if it's something else, it can very easily fit in to the estimation and the inference.

280
00:29:01,930 --> 00:29:06,419
It becomes a weight matrix, essentially. And so we give it the term weight,

281
00:29:06,420 --> 00:29:13,080
at least where you are weighting each of the observations by the variance and covariance that they have with each other.

282
00:29:15,060 --> 00:29:20,520
So I'm going to remove the assumption of constant variance right now, but everything is still independent.

283
00:29:21,900 --> 00:29:29,070
So zeros along the diagonal and a different sigma squared, a different variance for every observation.

284
00:29:30,240 --> 00:29:34,710
Now this is impossible because you can't estimate someone's variance.

285
00:29:35,400 --> 00:29:45,270
If you only have one observation for them. Right? But the inverse of each person's variance we see is a weight in of the regression model,

286
00:29:46,050 --> 00:29:51,120
because variance is a measure of how much information or precision someone's data gives to me.

287
00:29:51,690 --> 00:29:58,920
So if we use the inverse of variance as a weight, that means more weight is given to those with smaller variance.

288
00:29:59,280 --> 00:30:06,690
We're more confident about that observation. It's going to give me a little bit more information as to what the regression coefficients look like.

289
00:30:07,050 --> 00:30:13,860
And people who have observations that are very variable, we're going to have less import into the model setting.

290
00:30:15,750 --> 00:30:20,820
That's only if you know it should be Sigma Square. It's minus two, and he has his five and hers is three and so forth.

291
00:30:20,830 --> 00:30:28,160
Right. But that never happens. So we can't estimate the weights because we have, like I said, one observation for each person,

292
00:30:28,170 --> 00:30:31,920
I can't say what how much variability there is for my Y because I only have one.

293
00:30:33,300 --> 00:30:38,520
This is where you learn to use residual plots and one of the utilities of residual plots was

294
00:30:38,520 --> 00:30:45,060
to see how in fact the fitted values looked relative to what was left over of the residuals.

295
00:30:45,600 --> 00:30:56,100
And if we saw a pattern there, which you said that with the covariance, if you set the residuals versus one of the covariance,

296
00:30:56,100 --> 00:31:01,500
you might see that that there's still some variability tied up with that X variable H or something.

297
00:31:02,550 --> 00:31:05,880
And so there are numerous weighted, least squares methods.

298
00:31:06,030 --> 00:31:09,600
Again, I'm not sure that's covered in 650 because we don't do it very often.

299
00:31:10,410 --> 00:31:18,600
But if you think that everybody's variance is proportional to their age, there's a way for you to account for that in weight of these squares.

300
00:31:19,620 --> 00:31:21,780
We're not going to do that in this class. Right?

301
00:31:21,870 --> 00:31:28,830
So we're not going to explore the idea of non constant variance in the context of independent observations

302
00:31:29,970 --> 00:31:35,580
which are going to allow for non constant variance when we have correlated or repeated measures.

303
00:31:37,740 --> 00:31:40,920
But regardless of what you do with Sigma,

304
00:31:42,150 --> 00:31:51,570
whether you assume independence or you decide you know better and you have some cool variance covariance matrix for all of the data, what happens?

305
00:31:51,750 --> 00:31:58,770
What happens if your assumed model is again, let's be clear, where'd my Twitter go?

306
00:32:00,480 --> 00:32:06,780
This is my assumed model for how the the outcomes are related to each other in terms of covariance and variance.

307
00:32:07,470 --> 00:32:10,530
This is the truth. Whatever it is out there, what if I get it wrong?

308
00:32:11,950 --> 00:32:14,980
What does that do to my estimation of my interests?

309
00:32:15,460 --> 00:32:19,150
So what happens to the expected value of my regression parameters?

310
00:32:19,480 --> 00:32:27,160
You remember, you take the expected value of this thing. The only thing that is random is why the expected value?

311
00:32:27,220 --> 00:32:31,270
Why is x data? Everything cancels out nicely and you're left with data.

312
00:32:32,080 --> 00:32:37,180
So getting correlation wrong doesn't affect estimation so much.

313
00:32:37,660 --> 00:32:44,590
It's still unbiased. So if you have correlated data, this is really important because this is going to carry through the semester.

314
00:32:45,220 --> 00:32:49,810
If you have correlated data in another way.

315
00:32:49,990 --> 00:32:58,600
If you fit a simple linear regression model and you find out later that there's correlation in the data, your regression parameters are fine.

316
00:33:00,080 --> 00:33:04,490
What's wrong is the insurance deciding whether or not they're significant from zero,

317
00:33:05,000 --> 00:33:08,390
deciding what the confidence intervals look like because of this fact right here,

318
00:33:09,080 --> 00:33:14,240
the actual correlation structure of the outcomes does not affect estimation in terms of being unbiased.

319
00:33:14,870 --> 00:33:28,939
However, still and based on what is the variance of Theta Hat again each they have the

320
00:33:28,940 --> 00:33:33,770
formula right and going to compute the variance of this thing right here.

321
00:33:35,000 --> 00:33:44,420
So that's all of this times, the variance of y times, the transpose of all of that, again, that quadratic form.

322
00:33:48,270 --> 00:33:51,840
Getting your answer ready. It's only 22 for. My God.

323
00:33:52,650 --> 00:33:56,490
All right, let's figure out a solution here. So that's what this says right here.

324
00:33:58,110 --> 00:34:02,850
If I got it right, if the variance of the data of the outcomes truly is sigma.

325
00:34:03,990 --> 00:34:07,470
Then I get this. That's what the computer gives you.

326
00:34:07,860 --> 00:34:10,860
That's what we call the model based variance.

327
00:34:11,850 --> 00:34:16,560
If my model for Sigma matches the truth, then this is what I should get.

328
00:34:16,600 --> 00:34:22,020
But it doesn't cancel out. There's no sigma to cancel out here if in fact, the variance of y is something else.

329
00:34:22,710 --> 00:34:25,710
So we have a problem. The inference is wrong.

330
00:34:26,040 --> 00:34:30,690
So if there's correlation in your data and you don't account for it, your inference is off.

331
00:34:30,750 --> 00:34:36,540
He's got the wrong sampling distribution. It's centered around data, but it's got the wrong variance.

332
00:34:38,950 --> 00:34:44,560
And again, this is what the computer is giving to you, because you've told us that everything is a dependent or just some structure.

333
00:34:46,420 --> 00:34:50,260
So if we don't trust our model for the correlation of the outcomes,

334
00:34:50,590 --> 00:34:59,110
if we don't think that what we're fitting is right, then we have to fill this in somehow with something else.

335
00:34:59,320 --> 00:35:06,970
It isn't our segment. And the best estimate for a variance comes from the residuals, right?

336
00:35:07,120 --> 00:35:10,120
The sum of the squared. The residuals have mean zero.

337
00:35:10,810 --> 00:35:14,110
So the sum of the square residuals is an estimate of the variance.

338
00:35:14,920 --> 00:35:23,560
But again, we're talking about matrix matrices here. Right? So this is the again, this is the residuals times their transpose.

339
00:35:23,950 --> 00:35:32,729
So I'm getting an end by end matrix. Getting the variance along the diagonal of each observation and the correlation of each residual with each other.

340
00:35:32,730 --> 00:35:41,780
Residual. That's what I'm trying to get up here. This resulting estimator where I take again, all of this is already fitted.

341
00:35:41,840 --> 00:35:49,160
This is already known. This is an estimate. And this is what is known as what we call a sandwich estimate.

342
00:35:50,510 --> 00:35:54,080
Because, again, we're so clever in statistics.

343
00:35:54,770 --> 00:35:58,610
We think of this as a sandwich. We've got the bread on both sides.

344
00:36:00,210 --> 00:36:05,190
And then for you, Non-Vegetarians, we've got the meat inside of the sandwich.

345
00:36:06,360 --> 00:36:13,079
This concept pervades regression. This is a really useful trick, a really interesting insight.

346
00:36:13,080 --> 00:36:19,229
It's a tool that is very useful. Right. So again, the bread is the model based.

347
00:36:19,230 --> 00:36:23,820
This is the model based. This is what comes out of our when you said linear regression.

348
00:36:24,510 --> 00:36:31,800
So we have to take that thing. And we have to multiply inside by this other piece here.

349
00:36:33,840 --> 00:36:38,190
Yeah. Ten years ago. Five years ago, we all had to program this ourselves in class.

350
00:36:38,640 --> 00:36:45,660
You have all of this. You would have had to get the residuals and multiplied the transpose and done some matrix algebra an hour.

351
00:36:46,050 --> 00:36:50,880
But of course, somebody not made a library for us. And so we're going to use that library to compute all this.

352
00:36:51,810 --> 00:36:55,200
Yes, no question about the Big Sigma.

353
00:36:55,290 --> 00:36:59,520
Big Sigma does not have to be diagonal. Right. Like you can have code.

354
00:37:00,150 --> 00:37:03,960
Like you mean diagonal in the fact that it can't have zero.

355
00:37:03,990 --> 00:37:09,240
Yeah. Yes. And most of the time it will be the off diagonal terms will not be zero.

356
00:37:09,270 --> 00:37:13,280
So basically you're saying that our variance of Y does not equal to Big Sigma.

357
00:37:13,290 --> 00:37:17,220
It just means like we are unsure the number of you got the numbers wrong.

358
00:37:17,280 --> 00:37:24,910
You've just got the whole structure wrong, right? It's harder to see when there's one observation per person also to become obvious in a second.

359
00:37:24,970 --> 00:37:35,080
What I'm talking about here. Oh, yeah. So I bring this up to you now because sandwich estimates are used a lot in something

360
00:37:35,080 --> 00:37:40,540
called generalized estimating equations that we will learn soon in this class,

361
00:37:40,540 --> 00:37:48,250
probably in October. Sandwich variants estimated, however, can be used with any regression model, not just T.

362
00:37:48,280 --> 00:37:51,790
So when I came out of grad school, I thought sandwich variants estimates were affected.

363
00:37:51,790 --> 00:37:57,700
To think of G, they just got more attention and G for reasons that we will we will see later.

364
00:37:58,180 --> 00:38:05,950
But if you don't make your correlation structure or you're not sure, you can always fit to the sandwich variants estimated to again.

365
00:38:05,950 --> 00:38:07,750
What does the sandwich variance estimate are doing?

366
00:38:08,350 --> 00:38:14,650
It's accounting for all the correlation and variance that you didn't think was in Sigma that you missed when you picked Sigma.

367
00:38:15,340 --> 00:38:20,950
The residuals are telling. You know, there's more that you didn't account for and it's getting fixed.

368
00:38:20,950 --> 00:38:24,310
So this is a way to fix for unmodified correlation.

369
00:38:25,160 --> 00:38:28,300
So it's not again, there's always a tradeoff.

370
00:38:30,250 --> 00:38:35,620
If you get the model right, if you get Sigma right, you should go with it, right?

371
00:38:35,620 --> 00:38:40,099
You should fit the model based. Because you lose again.

372
00:38:40,100 --> 00:38:46,670
You lose efficiency if you start doing too much stuff. The sandwich variants are smarter.

373
00:38:47,510 --> 00:38:50,750
You will also hear about you will probably hear me call it this as well.

374
00:38:50,780 --> 00:38:56,540
It's often referred to as a robust variance estimate or if you've read any longitudinal papers or done much analysis already.

375
00:38:57,860 --> 00:39:03,470
I had a professor who really detracted us from saying that robust.

376
00:39:03,480 --> 00:39:07,970
What does it mean to be robust? No one knows, but it sure sounds good, doesn't it?

377
00:39:08,990 --> 00:39:14,930
Robust against what tornadoes write? What is what is the problem you're trying to save yourself from?

378
00:39:15,500 --> 00:39:25,430
So robust comes from an asymptotic concept that the residuals give us a really nice estimate for the actual variability in life.

379
00:39:27,410 --> 00:39:31,309
It's an asymptotic argument and you need a lot of data.

380
00:39:31,310 --> 00:39:35,090
You need a lot of data to estimate variability. Right.

381
00:39:35,450 --> 00:39:41,510
So when I say large samples, I mean much more than 20 a t test.

382
00:39:41,840 --> 00:39:45,890
But how do you get to 20 observations? You're in a large sample world for a T test.

383
00:39:47,590 --> 00:39:53,170
That's not the case here. So, again, it's not always as perfect as it's sold.

384
00:39:53,920 --> 00:39:57,970
The Asymptomatiques requires a pretty big sample size for things to work well.

385
00:39:58,720 --> 00:40:02,140
And as I said, the sandwich variants estimated will show up a lot with GDP.

386
00:40:03,070 --> 00:40:06,370
But we'll get to that later. But it is useful and you will use it in homework.

387
00:40:06,370 --> 00:40:11,650
Number two before you even search. She is G. So that many, many times.

388
00:40:14,200 --> 00:40:18,339
Right. So I've told you what a sandwich variance estimate here is.

389
00:40:18,340 --> 00:40:19,850
So hopefully you know why you're doing it.

390
00:40:19,870 --> 00:40:24,250
Homework number two, because in homework number two, I'm going to ask you to set the wrong correlation structure.

391
00:40:25,430 --> 00:40:30,110
I'm giving you correlated data and I'm going to ask you to fit linear regression to it.

392
00:40:30,110 --> 00:40:38,810
It's wrong. So wrong correlation structure, but you can throw on a variant sandwich variance estimate and you're done.

393
00:40:39,260 --> 00:40:46,280
You've accounted for the correlation not by modeling sigma, by using the residuals to help figure out what it is.

394
00:40:48,660 --> 00:40:52,680
It doesn't mean you should just always fit linear regression and use the sandwich variants as demanded.

395
00:40:52,680 --> 00:40:56,159
But that is one approach. Is one approach.

396
00:40:56,160 --> 00:41:04,070
And I do that sometimes. To see, right?

397
00:41:04,270 --> 00:41:08,170
I'm losing my train of thought. Just like you. I am having.

398
00:41:09,080 --> 00:41:14,030
A movement of brainwaves, not floating rates.

399
00:41:16,220 --> 00:41:23,870
So linear regression review moved on to when we have weights because of the sigma matrix that doesn't necessarily have zero.

400
00:41:23,870 --> 00:41:31,190
So I'm off the diagonal. Let's talk about this in the context of the actual correlation that we've been doing already.

401
00:41:33,560 --> 00:41:40,370
You can't repeat these things enough. We got those two assumptions there that we have independence and constant variance,

402
00:41:41,030 --> 00:41:45,530
and they gave us a very specific structure for what the correlation and variance look like.

403
00:41:45,980 --> 00:41:50,750
And you get the same science as with the previous set of lectures.

404
00:41:51,540 --> 00:42:00,750
Last time I got rid of the constant variance. It's more often that we keep the constant variance, but we throw away independence.

405
00:42:00,870 --> 00:42:05,430
That's the whole point of this class. So again, I still have one observation per person.

406
00:42:07,920 --> 00:42:14,040
There's a constant variance. So there's a sigma squared now simply multiplied by the entire applied to the entire matrix.

407
00:42:14,630 --> 00:42:17,980
And that gives us ones along the diagonal and then we have correlations.

408
00:42:18,000 --> 00:42:22,470
So again, row one two is the correlation of the first observation with the second.

409
00:42:22,980 --> 00:42:26,130
And of course, that's equal to the correlation of the second with the first.

410
00:42:27,000 --> 00:42:30,540
So now we've got all these off diagonal terms, as was just asked a second ago.

411
00:42:31,770 --> 00:42:37,710
Problem is we have a lot of parameters there. Right. We've got a very complicated structure here.

412
00:42:37,950 --> 00:42:42,810
I've got enzymes and minus enzymes and minus one or two correlation parameters to figure out.

413
00:42:44,460 --> 00:42:50,130
That's too complicated. So I need to come up with a simpler model depending upon the setting in which I'm in.

414
00:42:50,910 --> 00:42:55,680
One of which we've already done in this class. And that's in the setting of pairs.

415
00:42:56,220 --> 00:43:02,310
So imagine that I had y and observations, but it turns out that there are pairs of them that go together.

416
00:43:02,730 --> 00:43:06,540
So there's actually am pairs of observations.

417
00:43:07,730 --> 00:43:14,840
And so again, I still have a vector of outcomes. It's just that now I know that there's a natural connection between some of them.

418
00:43:16,040 --> 00:43:20,600
And so for each person, they have two observations.

419
00:43:22,060 --> 00:43:26,080
Those two observations have a variance covariance structure that says they

420
00:43:26,080 --> 00:43:30,370
both have the same variance and they're correlated with each other appeared.

421
00:43:31,560 --> 00:43:32,790
That's for one person.

422
00:43:34,190 --> 00:43:42,050
If I then apply that to all the people in my dataset, I've got the first pair and the second pair and the third pair and so forth.

423
00:43:42,620 --> 00:43:48,090
Each of those pairs has the same correlation. But then everything else is independent.

424
00:43:48,110 --> 00:43:51,950
The first person is independent of the second person and so forth.

425
00:43:52,640 --> 00:43:59,690
So now we have a very specific correlation matrix in which there is a one along the diagonal and a band of roll on either side.

426
00:44:01,050 --> 00:44:08,000
All right. So we assume each pair has the same variance and the correlation is the same across all individuals in the dataset.

427
00:44:09,080 --> 00:44:14,390
So now I have two parameters I have to estimate in the variance part of my data sigma squared.

428
00:44:15,380 --> 00:44:20,790
And what is ROE with regard to modeling means?

429
00:44:20,810 --> 00:44:26,040
Again, we'll go back to our model here. Each person has two observations.

430
00:44:26,060 --> 00:44:30,500
They each have a mean that changes with covariates.

431
00:44:30,500 --> 00:44:34,550
Depending on what those covariates are. Might be time. Wait.

432
00:44:34,690 --> 00:44:40,200
The mean is bad at times. There are two. There's an interception and a beta one coefficient.

433
00:44:41,440 --> 00:44:47,320
The X variable in this example, here is an intercept and then a group indicator.

434
00:44:49,000 --> 00:44:53,170
It's zero. If it's if J is one and it's one, if it's two.

435
00:44:53,200 --> 00:44:56,290
So you can think of group as time to. This is time one.

436
00:44:56,590 --> 00:45:04,120
This is time to. So I have a regression model that says there is an intercept and then there is a time of that.

437
00:45:05,820 --> 00:45:08,970
So what message do we currently have for estimation inference in the setting?

438
00:45:17,280 --> 00:45:23,590
Any thoughts? Comparing two time points, which can be two groups.

439
00:45:24,610 --> 00:45:28,920
And there's two observations that are paired one for each group, each person.

440
00:45:31,250 --> 00:45:35,620
Anything we've seen so far. Somebody.

441
00:45:36,950 --> 00:45:41,569
Yes. It's a pretty test. As I said last week.

442
00:45:41,570 --> 00:45:48,830
Right. All tests are regression. And so here we are fitting a pair t test without calling it that.

443
00:45:48,830 --> 00:45:56,420
It looks like a longitudinal model. I've got a correlation structure that has a correlation parameter and I have a regression parameter,

444
00:45:56,420 --> 00:46:04,100
a regression model that says I've got the mean for the first time point and then I mean a difference for the means of the two time points.

445
00:46:05,150 --> 00:46:08,420
So this is a different way of writing a pair of t test.

446
00:46:10,280 --> 00:46:14,600
And so in this setting, I see estimation by a maximum likelihood of straight forward.

447
00:46:15,620 --> 00:46:23,690
And it's with some algebra, the intercept estimate is simply the mean of the first group or the first time point,

448
00:46:24,710 --> 00:46:27,980
and that the extra parameter is simply the difference in the mean.

449
00:46:28,250 --> 00:46:33,950
This is what we're interested in. The paired t test. Is there a difference between the two groups in a paired setting?

450
00:46:35,180 --> 00:46:38,840
Sigma squared is the usual deviations from the means.

451
00:46:40,480 --> 00:46:44,889
And there is an estimate. And again, hopefully this formula is not surprising to you.

452
00:46:44,890 --> 00:46:51,770
This is just Pearson's correlation coefficient. It's the covariance divided by the variance in both time,

453
00:46:51,800 --> 00:46:55,690
both y one and which you have the same variance or the two divided by the square root of the variance.

454
00:46:56,260 --> 00:47:06,490
You multiply as two of those so you get sigma squared out. Which then gives us again.

455
00:47:07,000 --> 00:47:10,780
Remember my ys have a sigma matrix that's now estimated.

456
00:47:11,380 --> 00:47:18,730
It's estimated by whatever sigma squared is. And then there's a band of rosettes where rosette is defined in the previous page.

457
00:47:21,200 --> 00:47:27,680
If you plug sigma hat into the formula for the variance of beta it.

458
00:47:29,550 --> 00:47:35,370
You get the tune variance formulas right here for the intercept and the slope.

459
00:47:35,610 --> 00:47:42,710
This is the variance of the difference in the means. Again, with substantial algebra, this I don't mean to say that this takes two steps.

460
00:47:42,720 --> 00:47:48,060
This takes a lot of steps, but I don't want to spend time in class on that. This should look familiar.

461
00:47:49,680 --> 00:47:57,720
This is what I talked about with the parent t test. If you ignore the pairing, you're not accounting for the correlation.

462
00:47:57,870 --> 00:48:03,120
And the difference of two means is two sigma squared over m has a this is variance.

463
00:48:04,170 --> 00:48:10,350
The correlation brings that variance down. If the pairing has positive correlation, which it usually does.

464
00:48:12,220 --> 00:48:16,150
The variance of the differences is smaller than when Roman zero.

465
00:48:17,500 --> 00:48:23,110
By a factor. That's one minus sigma that. This might be on the test.

466
00:48:25,250 --> 00:48:35,920
That's why I. So in the parents setting, when we have a pre and post measure, again, what we're interested in is comparing individuals to each other.

467
00:48:37,810 --> 00:48:41,110
How am I in one condition relative to how I am in that other condition?

468
00:48:41,560 --> 00:48:44,090
I'm not comparing different individuals to each other.

469
00:48:44,200 --> 00:48:50,500
So within subjects comparison or within their comparison, usually the pairs are the same person.

470
00:48:52,360 --> 00:48:56,380
But we're in a longitudinal data class.

471
00:48:56,920 --> 00:49:03,520
We're not going to have pairs of observations. We're going to have multiple observations per person, possibly a different time,

472
00:49:03,520 --> 00:49:07,240
points with different covariates that we might want to adjust for and so forth.

473
00:49:07,810 --> 00:49:16,210
So what do we do in our regression models where each individual has more than two repeated measures and maybe not all the same number?

474
00:49:17,590 --> 00:49:22,770
Not every individual has the same number of repeated measures, either by design or due to the same data.

475
00:49:22,780 --> 00:49:26,230
Right. And you may say some people are going to be observed more often than others.

476
00:49:26,680 --> 00:49:30,249
And of course, we know that over time, there's usually attrition in longitudinal studies.

477
00:49:30,250 --> 00:49:33,880
There's just naturally fewer observations as time goes on.

478
00:49:34,430 --> 00:49:39,750
If we don't keep up with folks and monitor them, what do we do when time is continuous?

479
00:49:39,760 --> 00:49:43,630
I told you earlier we probably want to set time as a continuous variable in the models.

480
00:49:44,080 --> 00:49:51,430
You can't do that. In the previous setting we had two timepoints, but if we have multiple time points and we want time to be continuous,

481
00:49:51,430 --> 00:49:53,260
we've got to come up with a different approach.

482
00:49:54,280 --> 00:49:59,229
If individuals are measured at different points of time, and as I said, maybe you want to adjust for age,

483
00:49:59,230 --> 00:50:03,790
maybe you want to adjust for some blood pressure or some other covariate that you think is important.

484
00:50:03,850 --> 00:50:05,500
The only way to do that is regression.

485
00:50:06,670 --> 00:50:17,110
So again, in the non longitudinal setting, we had a two sample t test, which then if we adjust for covariates, moves into linear regression.

486
00:50:18,670 --> 00:50:25,090
If we had a paired T test and we wanted to adjust for covariates, that becomes a longitudinal type model.

487
00:50:25,960 --> 00:50:33,460
And then if we have more than two time points, etc., etc. So we are doing nothing more than at least squares.

488
00:50:34,690 --> 00:50:38,380
But with that sigma matrix as a weight inside of estimation and inference.

489
00:50:39,280 --> 00:50:42,580
So that's why it's called generalized squares. I don't know why it generalized.

490
00:50:43,300 --> 00:50:47,500
It seems like it is like squares of glass generalized.

491
00:50:47,500 --> 00:50:53,590
These squares is nothing more than linear regression with a correlation matrix accounted for.

492
00:50:55,240 --> 00:51:00,010
And so we're going to take a whole homework assignment just to do that, because it's not that trivial,

493
00:51:00,220 --> 00:51:05,260
especially in estimation where we got, Oh, good, I'm going to make my plan here.

494
00:51:09,750 --> 00:51:17,550
And I have one more satisfied service and this I'll do the rest next week because there are more.

495
00:51:17,940 --> 00:51:21,880
There is always more. The slide show from beginning.

496
00:51:24,100 --> 00:51:26,740
All right. So they just said generalize these squares.

497
00:51:26,890 --> 00:51:36,470
Is least squares with a correlation covariance matrix to deal with the potential of correlation in the data that's relative.

498
00:51:40,470 --> 00:51:45,240
I think of generalized these squares as a type of linear, mixed model.

499
00:51:45,600 --> 00:51:50,280
And again, just to make that clear, not everyone agrees with that nomenclature.

500
00:51:50,640 --> 00:51:55,860
But essentially, there are two ways to deal with correlated data or longitudinally data.

501
00:51:56,640 --> 00:51:59,340
There is what we're going to do today, and that says, well,

502
00:51:59,340 --> 00:52:05,880
I know Sigma has to account for correlation wherever it exists, and I will do that directly.

503
00:52:07,320 --> 00:52:09,480
Or I will use random effects.

504
00:52:10,560 --> 00:52:22,050
And again, random effects as we move into later in the course are a way to model correlation without explicitly specifying a correlation matrix.

505
00:52:23,760 --> 00:52:28,470
And again, hopefully that will become clearer as we start looking at what random effects are and what they do.

506
00:52:28,920 --> 00:52:33,290
So we're going to focus what's and I want to make it clear to her.

507
00:52:33,390 --> 00:52:36,870
In the old stars days, there were two.

508
00:52:36,870 --> 00:52:41,520
And if you you still you sense there's a repeated statement and a random statement

509
00:52:42,780 --> 00:52:46,830
and I only remember which is which one does a number one and one does number two?

510
00:52:48,520 --> 00:52:53,740
And many times folks will try to do both at the same time, and that's possible.

511
00:52:54,070 --> 00:52:59,980
So you can't put a correlation on the errors and also put random effects into the model.

512
00:53:01,140 --> 00:53:09,060
You're going to cause yourself more problem than it's probably worth because one, sometimes these two approaches will do the same thing.

513
00:53:10,020 --> 00:53:16,290
So you're going to learn a correlation matrix today. They can also be created through random effects directly.

514
00:53:17,430 --> 00:53:20,940
So if you do both at the same time, the computer is going to go, Well,

515
00:53:20,940 --> 00:53:24,600
I can't do both because they're both the same thing and one of them won't fit right.

516
00:53:24,720 --> 00:53:28,500
So and it's just hard to estimate variation.

517
00:53:29,370 --> 00:53:38,190
We do a good job of estimating the mean. It is really hard to estimate correlation and covariance as especially when we get to binary data.

518
00:53:38,730 --> 00:53:42,570
How do you quantify variability when everybody has only zero or one?

519
00:53:43,860 --> 00:53:48,300
It's really hard to estimate. So just keep keep that in mind.

520
00:53:48,750 --> 00:53:53,940
We're going to learn two approaches, but they can both be done at the same time and they can be done at the same time in R.

521
00:53:54,360 --> 00:53:58,800
And I'm going to show you how to do it. I don't know how to do it because I never bother to do both.

522
00:54:00,210 --> 00:54:03,270
And so number one is commonly called generalize these squares.

523
00:54:05,770 --> 00:54:13,600
So let's get into the notation here. So now we have two subscripts, one for person and one for the time points.

524
00:54:15,220 --> 00:54:19,570
Every person can have a different number of time points. So I say m survive.

525
00:54:19,840 --> 00:54:27,730
So I could have three observations. Someone else could have five. Every person has a vector of lies.

526
00:54:29,980 --> 00:54:41,820
One for each Timepoint Each person has a regression model that has their covariance linear combination of covariance with parameters plus error rate.

527
00:54:41,890 --> 00:54:49,780
This is the mean structure. This is where the correlation is going to lie in a vector of regression parameters.

528
00:54:50,440 --> 00:54:55,450
Every person has a matrix of covariance, right?

529
00:54:55,960 --> 00:54:56,980
And sibling regression.

530
00:54:56,980 --> 00:55:05,470
They had a vector of covariance, but we have the covariance vector for the first time point and we have the vector for the second point and so forth.

531
00:55:07,590 --> 00:55:14,400
Many times if the covariates are baseline, many times in longitudinal data we have baseline covariates.

532
00:55:14,940 --> 00:55:21,870
We assume age at entry of the study is just their age without changing over time someone's sex or someone's gender.

533
00:55:22,890 --> 00:55:28,380
Those are going to be the same in each row of the matrix. We can have time, varying covariance.

534
00:55:29,010 --> 00:55:33,660
This is why it's important in this general framework here. One of the axes could also be changed over time.

535
00:55:38,610 --> 00:55:42,330
For instance, group membership. It might be in a different group at different time points.

536
00:55:45,520 --> 00:55:51,970
And then we have, again, a vector of residuals. These are the residuals for a person.

537
00:55:54,000 --> 00:55:55,820
And here's where the correlation might lie.

538
00:55:55,830 --> 00:56:02,400
We don't assume that everybody has residuals that are independent of each other because they came from the same person.

539
00:56:03,120 --> 00:56:07,170
It must be correlated for some reason, for some unmeasured factors.

540
00:56:09,600 --> 00:56:14,350
So we put some structure on the variance covariance of a person's residuals.

541
00:56:14,350 --> 00:56:19,230
So now we've got sigma sub by everybody has a variance covariance matrix.

542
00:56:20,760 --> 00:56:24,750
The subscript is only there because of the dimensionality.

543
00:56:25,440 --> 00:56:31,590
I might have a three by three matrix. Someone else might have a five by five matrix because they have more observations.

544
00:56:31,890 --> 00:56:37,620
That's where the eye comes into play. So it doesn't have to do with anything else.

545
00:56:37,650 --> 00:56:44,490
So, for instance, even though we have a different dimension for each person, the parameters are the same across all individuals.

546
00:56:44,820 --> 00:56:49,320
So the correlation of anyone's observation first time with the second time is real.

547
00:56:49,320 --> 00:56:52,680
One, two, four. Everybody gets the same number across people.

548
00:56:52,950 --> 00:56:59,790
Right. And we believe that the first observation has a variance and that number is the same for everybody on the first observation.

549
00:57:00,540 --> 00:57:03,210
So there's no eye, there's no eyes or anything here.

550
00:57:04,290 --> 00:57:10,040
The only reason we have an eye here is because this is specific to a dimension of of am I by am I.

551
00:57:12,150 --> 00:57:18,990
Lots of parameters there. This is what we call an unstructured covariance matrix.

552
00:57:19,020 --> 00:57:23,760
There is no structure. Every pair of observations has a different correlation.

553
00:57:24,240 --> 00:57:29,790
Every observation has its own variance. So a lot of parameters there.

554
00:57:31,500 --> 00:57:35,370
This can't be fit in our. Can I show you the code to do it?

555
00:57:36,240 --> 00:57:39,820
And again, I work with a lot of students who think, well, gee, Tom,

556
00:57:39,850 --> 00:57:45,600
if I don't want to worry about structure on Sigma, I'm just going to fit unstructured and I'm a winner.

557
00:57:45,810 --> 00:57:50,810
I got everything solved right. It is so hard to estimate these parameters.

558
00:57:50,820 --> 00:57:57,120
They're they're really noisy. So unstructured is not the solution to worrying about structure.

559
00:57:57,600 --> 00:58:01,700
Structure is your friend because it requires fewer parameters.

560
00:58:01,710 --> 00:58:05,010
And often we don't have enough data to be unstructured. Right.

561
00:58:05,160 --> 00:58:10,230
So keep that in mind. We often need to reduce the number of parameters to produce stable estimates.

562
00:58:10,260 --> 00:58:14,250
Remember, this is going into estimation of beta hat and its variance.

563
00:58:14,460 --> 00:58:17,700
There's that sigma inverse in the formulas.

564
00:58:18,780 --> 00:58:22,230
If this is really unstable, then you get unstable estimation.

565
00:58:22,350 --> 00:58:28,260
So we need to think a little more smartly and try to make something with a little bit of structure.

566
00:58:28,650 --> 00:58:34,830
Again, structure makes things simpler and hopefully it doesn't hurt in terms of accuracy.

567
00:58:37,330 --> 00:58:40,330
All right. Let's get to Rome in a second here.

568
00:58:40,750 --> 00:58:47,200
Across all individuals, again. Across all individuals, we have a vector of vectors.

569
00:58:47,830 --> 00:58:53,800
I have my observations. And then the next person's observations, the next person's all in big, long vector.

570
00:58:54,700 --> 00:58:58,719
Is that a regression model with a bunch of regression coefficients?

571
00:58:58,720 --> 00:59:06,879
Again, that looks just like it does in simple linear regression. Everybody has a vector of covariates and then we matrix of covariates.

572
00:59:06,880 --> 00:59:15,490
We put those on top of each other. And we again, everybody has a vector of residuals and they'll form one big one vector.

573
00:59:16,600 --> 00:59:20,620
The variance of all of the residuals is what we're going to call Sigma.

574
00:59:21,880 --> 00:59:23,890
And this is a block diagonal matrix.

575
00:59:24,190 --> 00:59:30,640
I've got the variance covariance of the residuals for the first person, the second person all the way through the last person.

576
00:59:31,360 --> 00:59:37,810
This might be three by three, this might be five by five, but then we have zeros everywhere else.

577
00:59:38,260 --> 00:59:44,350
My residuals, although correlated with each other, have no correlation with anybody else's residuals.

578
00:59:44,350 --> 00:59:54,540
They're independent. So it's a it's an identity matrix with, with the diagonals having these correlation matrices.

579
00:59:54,540 --> 00:59:58,450
So. So I'm trying to figure out what this is going to be.

580
00:59:59,680 --> 01:00:00,670
What's it going to look like?

581
01:00:03,810 --> 01:00:12,030
The simplest correlation structure that you're going to learn is one in which every observation that I give has the same correlation with each other.

582
01:00:12,330 --> 01:00:15,990
So my second one is correlated with the first one is correlated with the third one.

583
01:00:16,590 --> 01:00:20,249
Nothing is changing. And so that is a two parameter model.

584
01:00:20,250 --> 01:00:29,850
I have got the variance and I've got a correlation, and the correlation doesn't change depending upon how far apart observations are from each other.

585
01:00:31,020 --> 01:00:35,490
Constant variance. So again, we call that compound symmetric.

586
01:00:36,570 --> 01:00:40,890
I don't even know why we use this term anymore. It has some some utility 20 years ago.

587
01:00:41,460 --> 01:00:45,040
So, again, this is a symmetric matrix, right?

588
01:00:45,540 --> 01:00:56,969
There's a sigma along the off diagonal. We call it exchangeable because what that means is that if I took my observations and I shuffled them

589
01:00:56,970 --> 01:01:01,800
up in a different order and I looked for the correlation matrix of that shuffled vector of wise,

590
01:01:02,640 --> 01:01:05,850
it's that right? Does it matter what order the wise come in?

591
01:01:05,910 --> 01:01:09,720
The correlation structure is the standard. So we call that exchangeable.

592
01:01:11,250 --> 01:01:18,140
So we've got to sigma squared B that estimate rho residuals that's exchangeable, compound symmetric.

593
01:01:18,570 --> 01:01:26,280
There's something called heterogeneous exchangeable. Again, it has one correlation parameter, but then it says,

594
01:01:26,280 --> 01:01:33,030
I'm going to let every observation over time have a different variance if we think variance is changing over time to do that.

595
01:01:34,860 --> 01:01:43,560
But again, you've got to estimate a lot more parameters. I typically never fit a heterogeneous model, and you shouldn't just means I don't.

596
01:01:44,790 --> 01:01:50,670
The other important structure is what's known as auto regressive order one,

597
01:01:51,150 --> 01:01:55,470
and this is what I was telling you in class is most commonly used with longitudinal data.

598
01:01:55,770 --> 01:02:03,510
We think that as time goes on to offset observations that move further forward in time or less correlated than observations close in time.

599
01:02:04,260 --> 01:02:10,710
Okay. If I want to predict what I look like in the future, the closer I am to the future, the better I can predict things.

600
01:02:11,010 --> 01:02:19,820
They're just more related to each other. So using again for a time points as an example here this person has a variance parameter,

601
01:02:19,830 --> 01:02:26,490
but then we have a correlation parameter again to the power one. So this is the first observation versus the second.

602
01:02:27,090 --> 01:02:30,840
The first observation relative to the third is rho squared.

603
01:02:31,770 --> 01:02:39,500
And the last one. Is real cute.

604
01:02:41,330 --> 01:02:46,400
A very simple structure. All I need is a real parameter and a sigma squared parameter.

605
01:02:46,760 --> 01:02:50,000
And then there's a natural decay, right?

606
01:02:50,030 --> 01:02:54,750
There's a model here. The correlation of YRI with why J is simply a row raised to the power.

607
01:02:54,770 --> 01:02:58,850
That is the absolute difference between the two to the two subscripts.

608
01:02:59,990 --> 01:03:03,260
Very common structure. Again, it assumes a balanced design.

609
01:03:03,570 --> 01:03:08,650
I'm using the difference between the visit number is basically one, two, three, four, five.

610
01:03:08,660 --> 01:03:14,890
It's not the actual time. So it does make this implicit assumption that the visits are equally spaced here.

611
01:03:14,900 --> 01:03:18,890
Right. It also makes the very strong assumption of how it decays.

612
01:03:18,980 --> 01:03:26,630
Right. If row is point to row squares .04, it's almost gone already in row cubed is .08.

613
01:03:27,260 --> 01:03:33,680
So again, it is a decay. It's a very specific kind of decay used very commonly.

614
01:03:34,070 --> 01:03:35,510
It's very easy for estimation.

615
01:03:36,920 --> 01:03:45,350
You can have a heterogeneous error, one model by simply using an error, one model and allowing the variances to change among the observations.

616
01:03:51,060 --> 01:03:58,080
Uh, 99% of the time I had to used compound symmetric or one in my 23 years.

617
01:03:58,950 --> 01:04:04,049
I've never used any of these others, but they exist and you can use them.

618
01:04:04,050 --> 01:04:10,070
I'm saying they're bad, but they are. One is a specific form of what they call a toeplitz structure.

619
01:04:10,080 --> 01:04:15,240
So Mr. Toeplitz lives in infamy by getting his name on a correlation structure.

620
01:04:16,410 --> 01:04:20,700
It simply means that there's a band, right? There should be a real one there.

621
01:04:21,090 --> 01:04:24,870
Yikes. So there's a band of real ones and there are two and three.

622
01:04:24,870 --> 01:04:33,540
And so we move off the diagonal. So Aruwan had a specific form for a row one, two and three, but that can be loose and to be anything you want, right?

623
01:04:34,260 --> 01:04:39,450
So that's again, that's what's called the total structure. Again, most of this is that way.

624
01:04:39,450 --> 01:04:48,330
I know. If you don't have individuals that are measured right at specifically nice spaced up point in time, you know,

625
01:04:48,360 --> 01:04:55,950
maybe outcomes are measured at specific discrete points in time like every 30 minutes or every three months that go to space.

626
01:04:57,030 --> 01:05:04,800
And individuals can only be measured at those time points. Everybody is measured at time, one at a time to what if time is changing?

627
01:05:05,280 --> 01:05:09,120
I came in at three months, someone else came in at four months and so forth.

628
01:05:09,600 --> 01:05:15,060
So we can have correlation changed with the specific distance in actual time, calendar time.

629
01:05:16,700 --> 01:05:23,820
And so it's the now it's the absolute difference, not of the visit numbers, but the actual times in which those visits occurred.

630
01:05:24,450 --> 01:05:32,580
And we can define the correlation to simply be some sort of model with a parameter that changes with this distance.

631
01:05:32,910 --> 01:05:38,069
And some folks have come up with some ideas. There's something called a continuous error.

632
01:05:38,070 --> 01:05:44,190
One month looks looks like an error one model, except now the exponent is not one, two, three, four.

633
01:05:44,520 --> 01:05:51,160
It's the actual span of calendar time. There's something called exponential correlation.

634
01:05:51,370 --> 01:05:55,500
Right? And it looks like this. It's a correlation parameter.

635
01:05:55,550 --> 01:06:02,650
And again, it decays as D gets larger. And there's something called Gaussian because it looks like a Gaussian kernel.

636
01:06:02,650 --> 01:06:05,920
This looks like the kernel of a normal distribution. Well, same distribution.

637
01:06:06,880 --> 01:06:15,790
Again, a different way to have things decay over time. How do you decide if you should use any of these over just a typical air one model?

638
01:06:16,210 --> 01:06:19,270
We'll talk about that. Very complicated.

639
01:06:19,510 --> 01:06:23,470
Again, this we use Rho as the parameter in some of these models.

640
01:06:24,220 --> 01:06:31,840
But this isn't row isn't a correlation. It's just a number that goes into the computation of correlation.

641
01:06:31,870 --> 01:06:35,530
So again, probably should use a different parameter, but we stick row in there.

642
01:06:36,790 --> 01:06:40,330
Again, you can have non constant variance applied to these models.

643
01:06:41,320 --> 01:06:48,970
I'm not sure these exist even in the GLC function in our know that they're out there.

644
01:06:49,030 --> 01:06:54,430
There's lots of ways. Again, if you like the model and you think it's right, then you should use it.

645
01:06:56,650 --> 01:07:01,390
If you don't like the model, then you can always fix it later with a sandwich variance estimate.

646
01:07:03,830 --> 01:07:09,650
All right. So we've now talked about what GLC is.

647
01:07:09,680 --> 01:07:17,810
It is nothing more than a regression with picking variance covariance matrix for the repeated measures on an observation.

648
01:07:18,590 --> 01:07:21,680
So I'm going to ask you to set some jealous models in the next homework assignment.

649
01:07:23,030 --> 01:07:30,049
Right. And then next Wednesday, we'll come back and talk about the remainder of the slides that have to do with model comparisons.

650
01:07:30,050 --> 01:07:34,790
Because I do ask you in homework to just set three models and then I ask you to pick one.

651
01:07:34,820 --> 01:07:36,410
Which one do you think is best and why?

652
01:07:37,250 --> 01:07:45,200
So there are some techniques for that, which I think, you know, already they're not very different from from simple linear regression.

653
01:07:46,700 --> 01:07:50,840
All right. I pull everywhere.

654
01:07:50,870 --> 01:07:55,580
Okay, let's get our phones out. Let's learn some more about each other.

655
01:07:56,360 --> 01:08:04,240
You're going to remember how to do this. You can't leave until you answer mine.

656
01:08:05,010 --> 01:08:10,680
I have a special trap set up that if you walk out without responding, then things will happen to you.

657
01:08:12,010 --> 01:08:15,940
Just kidding. Let's do this one.

658
01:08:25,380 --> 01:08:29,040
Someone gave you $100,000. Right now I'm going to give you a hundred thousand bucks.

659
01:08:30,540 --> 01:08:36,690
What are you going to do with it? It's not a lot of money, but it is a lot of money.

660
01:08:40,140 --> 01:08:49,040
Nightclubs in Texas. Doesn't have to be grand.

661
01:08:49,050 --> 01:08:52,200
It doesn't have to be. You can be greedy.

662
01:08:52,710 --> 01:09:51,470
You can not be greedy. Oh, well, the second.

663
01:10:32,070 --> 01:10:46,960
You. Three.

664
01:10:49,000 --> 01:10:52,480
Have any of you heard of the show alone on the History Channel?

665
01:10:53,200 --> 01:10:56,650
You ever watched it? I see anybody, but I'd say no.

666
01:10:58,750 --> 01:11:05,260
Again, a bunch of very highly trained survivalists are thrown into a part of the world.

667
01:11:06,970 --> 01:11:11,770
They're dumped out by themselves, and they're all far enough apart that they'll never find each other.

668
01:11:13,090 --> 01:11:18,280
In there to videotape every moment of their life, and they hang on as long as they can.

669
01:11:18,370 --> 01:11:25,410
They don't know when someone else has quit. And as soon as you're the last person standing, you get half a million dollars.

670
01:11:28,860 --> 01:11:32,790
It's amazing what happens to you after about ten days of being by yourself all the time.

671
01:11:33,090 --> 01:11:35,340
Even if you know how to catch food and build a house.

672
01:11:36,540 --> 01:11:45,060
But I'm always intrigued by its half a million dollars, which again is as you move along in your life, half a million dollars doesn't last that long.

673
01:11:45,390 --> 01:11:54,030
If you want a house and a car and trips, I am used to a world in which I have that luxury for these folks.

674
01:11:54,060 --> 01:11:59,970
Half a million dollars changes their lives. These are people who make $20,000 a year anyways.

675
01:12:00,480 --> 01:12:07,920
So I'm always intrigued what a lot of money sounds like to people because 100,000 to me doesn't sound like a lot of money.

676
01:12:07,950 --> 01:12:12,510
It is a lot of money. I'm not saying it isn't a life.

677
01:12:12,660 --> 01:12:21,900
It isn't necessarily life changing for me. But for some people, hundred thousand dollars is life changing in ways, a change in how people answer this.

678
01:12:23,070 --> 01:12:27,210
So let's see it again. I try to give them a classic because they haven't figured out how to speed this up.

679
01:12:28,350 --> 01:12:43,260
Right. Yes.

680
01:12:57,340 --> 01:13:17,530
Stars are there, buy a house, invest interesting features and spend any of it.

681
01:13:21,640 --> 01:13:40,670
James, your global man, what's the hardest thing?

682
01:13:44,440 --> 01:14:04,120
Look inside. Someone wants to make more, even to leave the house and see a lot of places with $1,000.

683
01:14:10,600 --> 01:14:22,479
Very specific. Oh, and then what they say.

684
01:14:22,480 --> 01:14:30,440
Spend a third. Save a third. Donate a third. Right. Okay.

685
01:14:30,970 --> 01:14:34,810
Hopefully. I don't know right now. Yeah.

686
01:14:42,060 --> 01:14:45,820
I think we missed.

687
01:14:45,820 --> 01:14:53,960
That bodes well for a lot of us.

688
01:14:54,280 --> 01:15:00,590
It's. It's.

689
01:15:14,890 --> 01:15:20,350
Know, houses, house, cars of listing.

690
01:15:26,440 --> 01:15:48,760
I have. And that -1000 and something that's like a genius.

691
01:15:49,000 --> 01:15:53,110
Yes. All right. That's it. Thank you. It is time to go.

692
01:15:53,110 --> 01:15:57,150
Perfect. So nice to see you guys.

693
01:15:57,160 --> 01:16:00,720
You guys appreciate the time.

