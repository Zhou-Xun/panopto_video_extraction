1
00:00:00,810 --> 00:00:06,480
Yeah. I thought you were prioritization. You should forget about the previous system that moves forward.

2
00:00:06,480 --> 00:00:10,090
Give us a few days to direct them. All the music. You that. Okay.

3
00:00:10,110 --> 00:00:14,309
It's a new plus new Skype case appears so so so so.

4
00:00:14,310 --> 00:00:20,280
So what I'm supposed to have this view will one this class give users equal skills?

5
00:00:21,240 --> 00:00:24,510
So this is the business model. Yeah.

6
00:00:24,510 --> 00:00:32,060
Yeah. So you have hundreds. I see this one is going to be right way to like in this problem in the hybrid is

7
00:00:32,070 --> 00:00:37,760
just like better linked to the other side of the test or you just describe it okay.

8
00:00:37,800 --> 00:00:43,950
Yeah. Do not give me a test so you don't have data, no studies that you can use up here so that I feel like in your business,

9
00:00:43,950 --> 00:00:48,239
something like that, you kind of like it like a racial test then because this is right on boundary.

10
00:00:48,240 --> 00:00:54,479
So we sort of. Yeah, I mean, you just told me think that you don't want to develop a test statistic for this.

11
00:00:54,480 --> 00:01:02,490
Like what is the meaning of is that important to me this. So sometimes people say I do now because these interests this city was not right.

12
00:01:02,490 --> 00:01:06,360
So so you which situation that late in process is not needed.

13
00:01:07,170 --> 00:01:10,470
Yeah, right. It's okay. I mean, this will not be good.

14
00:01:10,740 --> 00:01:17,520
Okay. Yeah, essentially. That's a big question, right? Yeah, it's.

15
00:01:23,980 --> 00:01:29,690
Someone else is doing it for you.

16
00:01:37,610 --> 00:01:52,850
Be. Because.

17
00:02:03,600 --> 00:02:09,540
Okay. How's your homework? Number two going? And so do you.

18
00:02:09,540 --> 00:02:17,350
Take some legwork? Divide and conquer approach. The problem is different part of the data and then combine together or something like that.

19
00:02:18,300 --> 00:02:26,670
I've seen some of data processed during my office hours, so yeah, it's a little bit of time consuming,

20
00:02:27,930 --> 00:02:39,720
but we need to work on the data and the midterm will use some of the data you'll process so you're not really wasting your time.

21
00:02:39,930 --> 00:02:44,579
And so that that's part is essential.

22
00:02:44,580 --> 00:02:47,820
You need to make sure that you process your data properly. Yes.

23
00:02:48,480 --> 00:02:56,760
What should we expect from the exam given the hallmarks, like I mean, similar to the homework survey?

24
00:02:58,650 --> 00:03:07,110
Yeah, I, I covered several aspects and it's very similar to homework but put it that way.

25
00:03:07,110 --> 00:03:15,500
Yeah. That, I mean it's not exactly the same but that there are similar type of question.

26
00:03:15,570 --> 00:03:20,220
Okay. Yeah, have a little bit sort of understanding of the modeling.

27
00:03:20,520 --> 00:03:29,850
Yeah. And also do live the data analysis. But we were not doing some fake data and also we're doing the analysis that you processed.

28
00:03:32,100 --> 00:03:38,460
Yeah. Okay. Okay. So you are planning for your fall break?

29
00:03:38,640 --> 00:03:46,140
Going somewhere or staying in the classroom or office to do or prepare exams?

30
00:03:46,800 --> 00:03:52,680
Okay. Anyway, just enjoy whatever you are doing.

31
00:03:52,770 --> 00:03:56,819
Yeah. So, yeah, like you said, it's time consuming and you're trying to do it by next Thursday.

32
00:03:56,820 --> 00:04:02,930
But if like if there's some hearing that. What? Because yesterday we found that we were processing it incorrectly.

33
00:04:03,420 --> 00:04:07,680
So if there like some metric and we maybe do stuff together.

34
00:04:07,770 --> 00:04:14,520
Oh, yeah. Okay. Okay. I mean, you know, I certainly hold my office or to test it next week.

35
00:04:14,550 --> 00:04:18,120
No problem. Or do you email me and I can take a look?

36
00:04:18,120 --> 00:04:21,660
Or you want the in-person meeting? We can always meet in person.

37
00:04:21,720 --> 00:04:21,960
Yeah.

38
00:04:23,430 --> 00:04:31,510
No, I was saying, like, if we don't finish, we finish it by next Thursday and we think you have to you're going to write midterm and a week after.

39
00:04:31,540 --> 00:04:38,129
Yeah. No, I was saying before I will try to help you any way that you can finish up the homework number to

40
00:04:38,130 --> 00:04:44,880
align our data processing to make sure everybody doesn't feel like you have to do this sometime.

41
00:04:45,270 --> 00:04:56,010
Yeah, we should probably. Okay. What we could do is the we have, um, let's see, we have class starting next week, right?

42
00:04:56,130 --> 00:05:02,130
Make sure that we have maybe each group will come like I think there are several versions and

43
00:05:02,130 --> 00:05:07,260
come to have five points presentation or someone want to present to other groups or say,

44
00:05:07,260 --> 00:05:10,290
oh, we did something slightly different or something like that.

45
00:05:11,010 --> 00:05:19,169
Yeah. Oh, okay. Yeah. Yeah. Or some of the assumptions we took on right where you want to have a Zoom meeting on Monday or.

46
00:05:19,170 --> 00:05:22,320
I don't know if you are going somewhere for a holiday break.

47
00:05:23,120 --> 00:05:26,820
Yeah, I think so. You will be here. No, no, you're not.

48
00:05:26,880 --> 00:05:31,830
So that does make sense. I don't think I'm in class, but Thursday because I was doing something.

49
00:05:32,290 --> 00:05:36,090
Oh, well, at least for the exam. On the exam, at least the full exam.

50
00:05:36,120 --> 00:05:45,750
Yeah. Right. We have sort of the reconciled or consensus, um, data processing which, you know,

51
00:05:45,780 --> 00:05:52,620
we can have like the open mike for 20 minutes here in front and people can jump up and say,

52
00:05:52,620 --> 00:05:58,170
Hey, here's the way I process that and let's hear our experience of the process here.

53
00:05:58,980 --> 00:06:11,550
Yeah. Okay. That's terrific. Yeah. So I have to let you know that I have a dental appointment for my implant surgery at 230 today.

54
00:06:13,170 --> 00:06:16,800
Anyway, I need to lecture a little bit short today.

55
00:06:16,890 --> 00:06:24,510
Maybe 10 to 2, because I have to drive to my clinic from has been scheduled for a while.

56
00:06:24,510 --> 00:06:30,700
So, um, anyway, so that's half changed my schedule for that surgery.

57
00:06:30,720 --> 00:06:37,860
But anyway, hopefully I'll stand up and have a shorter lecture before you have your break.

58
00:06:40,360 --> 00:06:44,520
I sort of have. For the midterm exam.

59
00:06:44,920 --> 00:06:51,590
Hmm. We'll have to just some question on the canon filter and smooth.

60
00:06:51,840 --> 00:06:58,979
Just no. Okay. So your program, your will practice a filter.

61
00:06:58,980 --> 00:07:02,010
Next homework. The three. Now the in the midterm.

62
00:07:02,230 --> 00:07:07,600
Okay. Oh, you are very afraid I'll come a filter on.

63
00:07:07,770 --> 00:07:10,810
And so you know what? Yes. Yeah. Yeah.

64
00:07:10,830 --> 00:07:14,580
No, no, no. We can talk about the exam on Thursday next week.

65
00:07:14,790 --> 00:07:18,450
Okay. So. So you have a practice my filter yet?

66
00:07:19,110 --> 00:07:24,389
Okay, I will. Homework three. I'll give you a couple of problems to practice.

67
00:07:24,390 --> 00:07:27,620
Come at filter and comfortable. Right.

68
00:07:27,630 --> 00:07:37,070
So I have the Oracle Aggregate Group pass that to you and then you can practice and do some number crunching on that.

69
00:07:37,110 --> 00:07:41,250
And so. Mm hmm. Yes.

70
00:07:42,510 --> 00:07:45,600
Any other questions? Okay.

71
00:07:47,160 --> 00:07:53,220
So I want to talk about the the IBM montecarlo IBM method.

72
00:07:53,250 --> 00:08:01,440
So so as I said, that the G method presented by Gal Ziegler,

73
00:08:01,440 --> 00:08:07,710
based on the first two assumption of the the assumption of the first two moments of data distribution,

74
00:08:08,370 --> 00:08:17,320
even though you have very clear hierarchical model of a very pragmatic structure of how the data are generated.

75
00:08:17,820 --> 00:08:26,940
But Zegers suggests that we just have a simpler estimation procedure taking only the, you know,

76
00:08:26,940 --> 00:08:33,330
the first two moment structures in the constructing of estimation procedure ignoring.

77
00:08:33,380 --> 00:08:39,230
A lot of details of the dynamics of, you know, set up in this hierarchy model.

78
00:08:40,490 --> 00:08:49,760
So it's a quick and dirty method. But on the other hand, we lose some of the, you know, efficiencies of US estimation,

79
00:08:49,880 --> 00:08:56,180
particularly G.M. will not allow you to really make a prediction without, you know,

80
00:08:56,420 --> 00:09:01,040
would not allow you to understand the dynamics for that late in process.

81
00:09:01,550 --> 00:09:07,730
So so that's why people trying to find some method that can deliver better analysis.

82
00:09:08,540 --> 00:09:17,960
Monte Carlo GM algorithm is one of the the methods that people proposed in that kind of modeling framework.

83
00:09:18,530 --> 00:09:23,330
You work to have a little bit more efficient estimation than okay.

84
00:09:23,840 --> 00:09:28,999
So the basic idea of Monte Carlo Yama algorithm is really treating the latent

85
00:09:29,000 --> 00:09:34,070
process as missing data because you do not observe through that depleted process.

86
00:09:34,550 --> 00:09:42,120
Then you just say, Let's treat this latent variable so late, impulsive as me, some data that we can, you know,

87
00:09:42,140 --> 00:09:55,970
use this GM algorithm to evaluate the likelihood and do the estimation, you know, using this sort of step and step m step up approach.

88
00:09:56,480 --> 00:10:00,770
Overall, of course, the algorithm is known for its slow conversions.

89
00:10:00,830 --> 00:10:07,580
It has only linear convergence rate and and also somewhat sensitive to initial values.

90
00:10:08,090 --> 00:10:15,170
So people are always looking for a better method. That's basically one of my dissertation research topics.

91
00:10:15,620 --> 00:10:24,130
I tried to find a common filter that will sort of on the same line of the thinking of GM,

92
00:10:24,140 --> 00:10:34,880
but it's a little bit faster because you can use the recursive relationship in the calculation of the latent process rather than doing integration.

93
00:10:35,330 --> 00:10:45,020
Because in the IM algorithm, we have to use a deep sampling method like the mock up of Temple Carlo type of,

94
00:10:45,260 --> 00:10:48,980
you know, method to implement more column method.

95
00:10:49,040 --> 00:10:56,420
You were to evaluate each step which is very computationally expensive to do that.

96
00:10:56,900 --> 00:11:05,420
So I think so far, in my view of the key common estimation function,

97
00:11:05,420 --> 00:11:14,510
the method I proposed so far probably is the one that has the best balance of estimation, efficiency and a computational efficiency.

98
00:11:16,160 --> 00:11:20,030
Wow. A student idea. Now realize that having the ah package.

99
00:11:20,150 --> 00:11:32,030
Well of course there was no our software existed at my time and I did not know that I would write a C++ or C of or Python.

100
00:11:32,150 --> 00:11:35,470
Python did not exist that I my time has suggested.

101
00:11:35,810 --> 00:11:40,340
But anyway, so I would prepare a package of time.

102
00:11:40,340 --> 00:11:45,140
We did not really realize the importance of this computational needs.

103
00:11:45,500 --> 00:11:51,860
So I was a student. Is that a problem that people think, okay, publishing in Card Journal is the goal?

104
00:11:51,860 --> 00:12:01,339
But nowadays I think having your method available, accessible by practitioners as, as more important published in top generally my view.

105
00:12:01,340 --> 00:12:13,430
And so you know now if you any other you want to implement this method I will have this basic coding card available that would be very useful.

106
00:12:13,430 --> 00:12:19,819
Ah. Package given now the importance of studying the you facet disease.

107
00:12:19,820 --> 00:12:30,020
Right. So because this stays to be smaller hierarchy model is very natural modeling sort of

108
00:12:30,620 --> 00:12:35,449
approach people would take to really study the relationship between the outcomes,

109
00:12:35,450 --> 00:12:39,490
time search outcome and some cohorts like vaccination or so.

110
00:12:40,250 --> 00:12:45,140
So I think that the CIA model, the remote compiler model is great,

111
00:12:45,590 --> 00:12:51,440
but if you want to go into the detail to look at the association between the outcome and the career,

112
00:12:52,010 --> 00:12:55,580
you need something better than the assignment model, more comparable model.

113
00:12:56,630 --> 00:13:03,170
And this hierarchy model, using this states, this small framework is I think very useful one.

114
00:13:03,200 --> 00:13:12,380
And unfortunately we don't have very good our package, a very comprehensive our package that has been available for practitioners to use.

115
00:13:12,980 --> 00:13:23,059
Otherwise this, this sort of methodology will be better cited referenced in the literature.

116
00:13:23,060 --> 00:13:30,050
But anyway, that, that's what I like to talk about in the next lecture.

117
00:13:30,530 --> 00:13:36,969
The comma estimating function that. Swamped the dissertation topics.

118
00:13:36,970 --> 00:13:45,640
I work for my dissertation. Okay, so let's talk about m m start with this augmented data augmentation.

119
00:13:45,640 --> 00:13:51,910
Right? So basically you have your observed data, which is let me just draw down the process here.

120
00:13:53,680 --> 00:14:02,200
Remember that you have laid the process represents the underlying infection dynamics.

121
00:14:02,410 --> 00:14:11,020
Okay, so you don't want it, right? Observe the severity of infection or dynamics of infection in the population.

122
00:14:11,560 --> 00:14:18,790
What you observed just some snapshots right from this system through this surveillance.

123
00:14:19,480 --> 00:14:29,950
So that collection mechanism typically funded by Google, CVC or something like that, this this gives you a wide end which is,

124
00:14:29,950 --> 00:14:38,170
you know, basically the whole time series observe that this could be vector and could be out of scale at time series.

125
00:14:38,560 --> 00:14:49,750
Now you have your data which collects the sort of the latent information you don't deserve, but it is happening in the population.

126
00:14:50,260 --> 00:14:54,910
And then you have something observe, something to observe.

127
00:14:55,240 --> 00:15:01,420
And so if you work on augmentation, basically in a recent data literature,

128
00:15:01,420 --> 00:15:08,800
you treated this latent process of using data that you put when you put the particular right is called argument data.

129
00:15:09,010 --> 00:15:18,249
Okay. So you have so there you write the likelihood function or modulate the function of this outcome in the data.

130
00:15:18,250 --> 00:15:22,810
Then you have something called augment likelihood, right?

131
00:15:23,080 --> 00:15:31,660
So in this context, because you have this column structure, right, it's called structured, this condition independence,

132
00:15:32,110 --> 00:15:40,600
it's pretty easy to write out as log like to function, right, because with joint distribution of this thing.

133
00:15:40,750 --> 00:15:51,870
Right, can be the joint distribution can be written as the condition of distribution of this and you know, of conditional life.

134
00:15:51,890 --> 00:16:03,390
Right. So, so this is to this to the joint distribution or, you know, if you take, you know, this as density, basically you have your likelihood,

135
00:16:03,400 --> 00:16:09,580
right argument like you have your region density and I this one you have the cost structure

136
00:16:09,580 --> 00:16:15,070
right so given see that he white he only depends on see that he has independent of

137
00:16:15,160 --> 00:16:21,810
any other negative so that you can really write this very easily using this condition

138
00:16:21,820 --> 00:16:27,219
dependance and for this part right the later process or later data needs to do the part.

139
00:16:27,220 --> 00:16:31,330
You basically have this, first of all, remarkable process, right?

140
00:16:31,330 --> 00:16:36,489
You can write this. I see that he t minus one.

141
00:16:36,490 --> 00:16:40,660
Right? So you have this, you know, the product, right?

142
00:16:40,660 --> 00:16:47,200
Because you have, first of all, remarkable process. You can continue to use this conditional document.

143
00:16:47,950 --> 00:16:57,100
Right? So, for example, you can see the end at minus one and theta zero.

144
00:16:57,490 --> 00:17:00,580
For example, you put this, but you have first of all,

145
00:17:00,580 --> 00:17:07,690
are macro passes and the other part will be canceled because the curve that only depends on what happens yesterday.

146
00:17:08,140 --> 00:17:16,360
One lives so so it can cost and use that kind of first order model of property to get this, you know, product of density.

147
00:17:16,360 --> 00:17:21,160
And this one is coming from to essentially this condition of independence.

148
00:17:21,400 --> 00:17:29,890
This is a nice part of the hard rock model you set up so that you come up with a likely function that is really computable and,

149
00:17:30,520 --> 00:17:39,970
you know, interpretable. So if you take a log of that, then you end up with the the outcome in that log likelihood,

150
00:17:40,240 --> 00:17:47,290
you have your partner coming from you have your parameter coming from the observed process of this are fault, right.

151
00:17:47,500 --> 00:17:55,780
That for a transfer right from the bottom ball then you have the primary coming from your market forces here.

152
00:17:55,780 --> 00:18:08,530
I use to think okay that that that will be the parameter that you will use to describe the deprived was involved in this latent process.

153
00:18:08,860 --> 00:18:18,460
Okay so that's the that you know, the two sets of parameters evolving this dynamics system or this hierarchy model.

154
00:18:19,060 --> 00:18:22,240
And then so the first part of because of the independence,

155
00:18:22,240 --> 00:18:32,970
you can see that you have log density of y ticket and see that T and only depends on parameter alpha and this is possible.

156
00:18:33,030 --> 00:18:37,079
Density or if you like, you can use other type of density.

157
00:18:37,080 --> 00:18:47,040
For example, if you work on this widely as proposal, if you work on an incidence rate, run it or work on a number of cases,

158
00:18:47,640 --> 00:18:55,110
then you can use either payload bay the density or the simplex density if you work on proportions.

159
00:18:55,440 --> 00:19:00,110
But now for simplicity, let's focus on just number of cases, right?

160
00:19:00,120 --> 00:19:05,310
So number of deaths, for example, that this would be possible distribution density.

161
00:19:06,270 --> 00:19:12,989
And now you have this first order Markov process and plus the initial thing, right?

162
00:19:12,990 --> 00:19:22,590
So there is a set of zero that stars everything, but you don't have the Y zero to capture this zero zero.

163
00:19:22,980 --> 00:19:33,690
So before the CDC or some some civilian system is able to react, there is a process already started.

164
00:19:33,960 --> 00:19:34,340
Right. Okay.

165
00:19:34,410 --> 00:19:44,969
There has to be a starting position after that that, you know, this surveillance data collection system start to, you know, react to to collect data.

166
00:19:44,970 --> 00:19:55,260
So see that zero will have. Why zero? Because the whole government or this whole surveillance system is not aware of that situation yet.

167
00:19:55,560 --> 00:19:58,830
But it's not often when you move to theater one.

168
00:19:58,830 --> 00:20:05,100
That's probably the first time the new surveillance system tries to collect data.

169
00:20:05,700 --> 00:20:13,110
So you have the extra term here. That's really the distribution of your see that zero, which is not estimated.

170
00:20:13,760 --> 00:20:22,770
This is something that is that beginning of the year not this in process but after

171
00:20:22,770 --> 00:20:27,450
you get that the you have this Markov process that will be reading in this form.

172
00:20:27,900 --> 00:20:37,770
And then the purpose here, of course, is to estimate the your our parameter and to take the parameter from it in process.

173
00:20:38,040 --> 00:20:42,030
Okay. Now what I want to do here is of course use the IM algorithm, right?

174
00:20:43,380 --> 00:20:47,370
So where these are TS are the they can process our recent data.

175
00:20:48,000 --> 00:20:49,960
So as I say that, you know, yeah,

176
00:20:49,980 --> 00:21:01,890
I'm aware there are two steps you would need to do is that the expectation step and the the end step maximization step.

177
00:21:02,340 --> 00:21:09,450
So what is the easy to establish like you do the average of your likelihood condition on the procedural missing data.

178
00:21:09,870 --> 00:21:13,560
So you have you log the likelihood let me put this way.

179
00:21:16,200 --> 00:21:29,729
So your effort, why stick to this algorithm like the augmented likelihoods, given your on our fault?

180
00:21:29,730 --> 00:21:36,360
And see now, of course, that this is not computable because you don't measure data from data.

181
00:21:36,510 --> 00:21:41,309
So this but you need to average this out. I mean, name mutation, right?

182
00:21:41,310 --> 00:21:46,440
In the multiple computation, you only impute a few values to that, right?

183
00:21:46,990 --> 00:21:56,550
So so the assumption is multiple imputation is a discounted version of EMR algorithm because you they don't you put in quotation marks,

184
00:21:56,760 --> 00:22:07,590
you're simply just clocking some imputation value, right, to replace your missing data to write your likelihood.

185
00:22:08,040 --> 00:22:12,239
You if you use five, you retain, you use 100 whatever.

186
00:22:12,240 --> 00:22:20,879
You just use discrete finite number of the values of theta to replace something you don't know like they are missing.

187
00:22:20,880 --> 00:22:24,970
You don't know the value of the missing values, right? The missing that little.

188
00:22:25,020 --> 00:22:30,570
But you want to compute anyway. You want to compute. It's just like you replace them.

189
00:22:30,660 --> 00:22:37,380
Missing the values by some imputed that you can have one replacement or five replacements,

190
00:22:37,740 --> 00:22:49,530
but you'll do this sort of value insertion in here just to ensure that space but you have algorithm is better if

191
00:22:49,530 --> 00:22:57,300
you are able to do this because rather than using the finite number of imputed value to replace the decent value,

192
00:22:57,660 --> 00:23:02,460
you try to use the distribution of this.

193
00:23:03,000 --> 00:23:07,920
Right? So what does distribution of that? That's the posterior distribution you see.

194
00:23:08,280 --> 00:23:13,770
What is the distribution of my recent data given what I know?

195
00:23:14,130 --> 00:23:17,760
Right. But you don't see that. Oh, my God.

196
00:23:18,610 --> 00:23:27,740
I need to rewrite this whole thing. That's why it is an art form.

197
00:23:28,100 --> 00:23:31,340
This is my argument.

198
00:23:31,880 --> 00:23:42,230
It likelihoods. I want to integrate out the recent data using the best knowledge I have.

199
00:23:46,390 --> 00:23:51,660
So what is my best knowledge of that? So this, um, algorithm is iterative algorithm, right?

200
00:23:52,120 --> 00:24:01,360
So here you have that iteration finish up on our calculation, you have the iterative algorithm.

201
00:24:01,810 --> 00:24:06,540
Our four parameters are flattened to the happy property,

202
00:24:07,060 --> 00:24:19,480
upped up to step R So you have some value about the transfer so that you if you're able to work all this density and then you say,

203
00:24:19,480 --> 00:24:29,320
I'm not going to just draw all five data point from this density, I'm going to use this as the weight to average out.

204
00:24:30,490 --> 00:24:35,300
Right? In this case, I should have.

205
00:24:36,230 --> 00:24:40,790
Ah, so theta is positive, this latent process.

206
00:24:41,810 --> 00:24:45,170
So n dimensional integral on the DC that.

207
00:24:46,730 --> 00:24:55,580
And so I'm going to say that I'm going to use the, the posterior word my best knowledge up to this.

208
00:24:56,540 --> 00:25:00,830
After I finish iterating ah this is my best knowledge.

209
00:25:01,730 --> 00:25:07,930
I have a bold lead to process this because distributions whatever dynamics.

210
00:25:07,940 --> 00:25:15,050
Right so, so, so. Okay, here's the difference between probability and statistics.

211
00:25:15,500 --> 00:25:21,680
Probability is trying to say here's the distribution studied the probability and the properties of the distribution.

212
00:25:22,040 --> 00:25:26,299
Statistically, say I, I know there is a distribution. I don't know what distribution is.

213
00:25:26,300 --> 00:25:29,720
I want to figure that out. Right. So that's the basic difference.

214
00:25:29,960 --> 00:25:39,140
Now, after I finish our iteration, I want to see, okay, this is my best knowledge about this late in process.

215
00:25:39,140 --> 00:25:49,370
I want to average out this whole thing that this is my queue function, it becomes my object function here.

216
00:25:49,370 --> 00:25:52,640
The parameter is still here. So this is parameter.

217
00:25:54,170 --> 00:26:03,049
What, what is the queue function Q function is a average log log of likelihood under

218
00:26:03,050 --> 00:26:07,430
the best knowledge of might be something that's really that queue function.

219
00:26:07,950 --> 00:26:16,910
Okay so you take because you're taking the integration on the basis of the function, this basis is expectation, right?

220
00:26:17,300 --> 00:26:26,780
So I'm taking the expected about of my local likelihood under my best knowledge about my recent data after I finish my integration.

221
00:26:27,200 --> 00:26:30,499
Okay. That's really the, the Q function.

222
00:26:30,500 --> 00:26:41,629
That Q function has the two set parameters are for a prime of the to the prime are up to the values that you already obtained from our situation.

223
00:26:41,630 --> 00:26:49,730
Now you have the parameters that you like to estimate to want to maximize this Q function

224
00:26:49,880 --> 00:26:56,210
or maximize the average path like likelihood function after you do the iteration.

225
00:26:56,480 --> 00:27:06,110
After we do integration, this recent data is marginalized or is gone because and this resulting function is only function depend

226
00:27:06,110 --> 00:27:13,730
on y and parameter because this data has been integrated out using this best knowledge you have.

227
00:27:14,750 --> 00:27:19,820
So this becomes a function of our to that of course, also a function of your data.

228
00:27:20,450 --> 00:27:26,170
This Q function does not involve any theta, any beaten variable or recent data anymore.

229
00:27:26,480 --> 00:27:33,530
No, that's that's you set up a new object function and you try to update your parameter

230
00:27:33,530 --> 00:27:38,960
estimate from this object function gives an M step to M step is trying to.

231
00:27:41,740 --> 00:27:44,799
So maximize this. Okay, maximize this.

232
00:27:44,800 --> 00:27:57,070
That's your misstep. The the the resulting solution that gives you the maximum of this Q function will be the update.

233
00:27:57,130 --> 00:28:02,440
Right? So then you just keep doing this, iterate iteratively until you get convergence.

234
00:28:03,520 --> 00:28:14,140
So I already mentioned that the original paper of algorithm doesn't give the wrong mathematic proof of convergence for this iteration until 1982,

235
00:28:14,470 --> 00:28:22,600
if you prove it using a corrected mathematical math method.

236
00:28:22,900 --> 00:28:25,959
Okay, so here's a lot of detail I skip.

237
00:28:25,960 --> 00:28:41,470
So the analytic analytic objective here is trying to maximize the skew function, right, with respect to our focus and time, right?

238
00:28:41,470 --> 00:28:44,200
So so you can get up to the value of your parameters,

239
00:28:44,560 --> 00:28:56,800
but this is equivalent to you turn this maximization prop into a, a problem of finding routes from this to equations.

240
00:28:57,190 --> 00:29:02,350
Okay. So this is pretty much like the way you did for Emily, right?

241
00:29:03,340 --> 00:29:07,150
This is pretty much same as the way you do for Emily.

242
00:29:07,390 --> 00:29:11,650
You'll have your log likelihoods. You want maximize log, like in order to get Emily.

243
00:29:11,980 --> 00:29:17,470
But actually the way you get Emily is to solve the find the root of your score equation.

244
00:29:17,830 --> 00:29:23,980
Exactly. Same procedure. You want to maximize the Q function by the end of the solving the are.

245
00:29:24,280 --> 00:29:33,969
There's two equations to find the root of this two equations to update your prompt, but you just need a little bit extra to derive that.

246
00:29:33,970 --> 00:29:38,590
But it's very straightforward. It's not a statistical problem.

247
00:29:38,590 --> 00:29:46,809
It's a really a mathematical problem. How do you take a derivative of that to set up a scoring question and solve from that?

248
00:29:46,810 --> 00:29:51,220
But anyway, you get this, okay?

249
00:29:51,400 --> 00:29:55,690
This is nothing that's strange to us.

250
00:29:55,840 --> 00:30:00,370
We're all know this. This is just a the the function.

251
00:30:00,470 --> 00:30:13,870
We we know the form of this and what different here it is right in the pricing model if you run crossing regression.

252
00:30:13,880 --> 00:30:30,980
Right. So if you run profile regression, then you have to transpose the white and 1817 people to zero.

253
00:30:32,300 --> 00:30:36,260
If you see that he's known, you see that he is no.

254
00:30:38,690 --> 00:30:42,740
And this one will be offset in the possible regression.

255
00:30:43,160 --> 00:30:46,850
And this will be the score equation that you used to find.

256
00:30:48,470 --> 00:30:56,540
This will be the r squared equation that you will derive from E to find America's our find here.

257
00:30:57,320 --> 00:31:06,140
So our face. Right? So you have used your X key to transpose our focus, right?

258
00:31:08,000 --> 00:31:11,870
So, so this is nothing new is is really just a possible regression.

259
00:31:11,870 --> 00:31:18,740
This officer said, okay, if this is basically offset by you can tell.

260
00:31:18,950 --> 00:31:22,370
But now, in reality, this is unknown.

261
00:31:23,300 --> 00:31:31,850
So according to M, so what is the best way to, you know, the MLA that you were supposed to solve this problem?

262
00:31:32,960 --> 00:31:36,830
Okay. Right on to this simple imputation. Right.

263
00:31:37,730 --> 00:31:51,980
You replace this by conditional expectation university under the best the knowledge you have about this thing that t after you finish the iteration.

264
00:31:52,430 --> 00:31:58,250
Okay. So our from prime to the prime are the update values from present iteration.

265
00:31:59,150 --> 00:32:07,430
And that's the best knowledge about your knowledge you have about the parameters that you try to calculate that the expectation,

266
00:32:07,850 --> 00:32:14,150
the average value of the T given the best the knowledge you have about the distribution of theta T.

267
00:32:14,830 --> 00:32:21,980
Okay. So here, if you know, so that he then of course you just treat this as offset right in the proximity question.

268
00:32:22,520 --> 00:32:28,130
But now you don't know this. What? It would be the best thing that you can deal with.

269
00:32:28,460 --> 00:32:33,710
Best described as you can give if this student he. Well, according to the algorithm.

270
00:32:34,430 --> 00:32:37,639
Okay. This very very famous.

271
00:32:37,640 --> 00:32:44,750
Yeah. Model that you should replace that t by the average value of the conditional, the best knowledge you have.

272
00:32:45,530 --> 00:32:47,890
Okay. Of course you do this iteratively.

273
00:32:47,900 --> 00:32:57,440
You cannot solve this one step because this average changes after you have the update values from previous iterations.

274
00:32:57,830 --> 00:33:01,750
So. So you keep updating this. Okay.

275
00:33:01,760 --> 00:33:04,430
So, so this can be easily solved.

276
00:33:04,970 --> 00:33:14,410
If you are able to compute the expectation of this, then you can just run custom regression with the opposite of this average value.

277
00:33:15,200 --> 00:33:21,410
If you are able to analytically get the expression of the expectation of this,

278
00:33:21,860 --> 00:33:31,010
you just tweak this expectation as your offset to run a possible regression to get your estimate alpha right.

279
00:33:31,040 --> 00:33:37,670
That's easy to solve. You can use that here in our function.

280
00:33:39,580 --> 00:33:49,480
To solve this to to get your to offer estimate very straightforward where you just did the extra step to figure out what's the average value of that.

281
00:33:50,680 --> 00:34:02,159
Okay. And if you can figure out that for the part of this Markov process, right, then it becomes the ten part parameter.

282
00:34:02,160 --> 00:34:05,500
It becomes very involved. It looks very, awfully.

283
00:34:05,920 --> 00:34:11,050
It's very hard to see involved.

284
00:34:11,270 --> 00:34:21,040
It's not easy to solve numerically. Okay. And people have the different ways to specify the latent process, the distribution theory.

285
00:34:21,050 --> 00:34:26,110
Some people use log no more, some people use com or some people use our distributions.

286
00:34:26,470 --> 00:34:35,740
The key can be different. Different choice of do the distribution for rate and prices would be associated with this different type,

287
00:34:35,770 --> 00:34:41,100
a different level of difficulty or complicity in solving the parameter with the setup.

288
00:34:42,280 --> 00:34:52,960
So okay, I tried the when I was a student to try to work out different type of distribution and to figure out the estimates I found in general,

289
00:34:53,380 --> 00:35:03,820
you are estimating the parameter from the the process from missing data because if you look at this right, you'll have a lot of missing data, right?

290
00:35:03,820 --> 00:35:06,280
So you have n dimensional missing data, right.

291
00:35:06,280 --> 00:35:15,040
So they don't want to say that and or missing do I not just one basically half of the 5% of data are missing in this case.

292
00:35:15,040 --> 00:35:16,600
You will observe half of it.

293
00:35:17,350 --> 00:35:26,290
And now you are trying to estimate parameter from the recent data process, which is this latent process, of course, condition Y.

294
00:35:26,710 --> 00:35:37,050
But sometimes if the information from the observe process is not strong enough, you get a lot of wacky or unstable estimate of the total.

295
00:35:37,840 --> 00:35:53,440
So of course MLP is great, but when MLP is formulated in the case that, uh, information is limited, you estimate it could be very unstable.

296
00:35:54,850 --> 00:36:00,280
So that's the, I mean, in principle, you can go to solve, find the root from this question.

297
00:36:00,730 --> 00:36:09,940
But what I end up is doing here is that runner and solving this problem directly from the, you know, algorithm to like to function.

298
00:36:10,720 --> 00:36:26,080
I just simply suggest that we use a simple method moment estimate for the take time to, you know, to get the the estimate of of the typewriter.

299
00:36:26,290 --> 00:36:36,769
So that's basically what I, I think that's the the compromise we should take where you found that this are sort of most efficient.

300
00:36:36,770 --> 00:36:42,489
The method is efficient. The method of Emili is numerically unstable.

301
00:36:42,490 --> 00:36:50,020
You certainly trying to find something more robust, simpler to to find this because what you are really interested here,

302
00:36:50,320 --> 00:36:57,879
this idea of grzegorz g right what you are really interesting here is your problem in 80 which

303
00:36:57,880 --> 00:37:02,950
is are from parameter that describe the association between you whitey and your covered.

304
00:37:03,550 --> 00:37:11,530
This is a latent process that describes the general belief and dynamics of evolution of the infection.

305
00:37:11,530 --> 00:37:20,740
But when the Taylor parameter cannot be estimate very stable numerically, then you're trying to find of some per method then this equation to do that.

306
00:37:20,830 --> 00:37:22,629
Okay, so an end of day,

307
00:37:22,630 --> 00:37:35,260
of course you that you need you lose a little bit efficiency but you can numerical stability which is quite a quite a purely in practice.

308
00:37:35,980 --> 00:37:40,480
So that's basically the idea is trying to to to to solve it.

309
00:37:40,960 --> 00:37:50,890
So where is the Monte Carlo come seeing? Well, the Monte Carlo comes to you here is that you need to evaluate the exploitation.

310
00:37:52,060 --> 00:37:57,610
So it turns out that evaluating this expectation is not that simple.

311
00:37:58,330 --> 00:38:12,010
Okay. So that you can use Monte Carlo methods. Okay, so you you'll have this distribution and then you you simulate this sidoti.

312
00:38:12,670 --> 00:38:19,720
Okay, many maybe we see that and and then you use Monte Carlo method.

313
00:38:19,930 --> 00:38:26,079
Okay, so, so this expectation can be numerically valid.

314
00:38:26,080 --> 00:38:38,260
Use the Monte Carlo method. Now you have another expectation right here that also requires some kind of work that you need to do for the expectation.

315
00:38:38,290 --> 00:38:41,410
Operation, which can be done by one column. Okay.

316
00:38:41,710 --> 00:38:47,230
So that's the more the method jumps in to help you to evaluate those expectations.

317
00:38:47,650 --> 00:38:57,370
Okay. So what I did in my key is that, okay, I want to have something faster than Monte Carlo.

318
00:38:58,330 --> 00:38:59,889
So later on next week,

319
00:38:59,890 --> 00:39:13,090
I introduce that because you calculate this conditional expectation and then this conditional expectation can be calculated by comma,

320
00:39:13,090 --> 00:39:16,930
filter or comments from or in very, very fast a way.

321
00:39:17,830 --> 00:39:23,889
So instead of doing this Monte Carlo simulation, which is numerically intensive,

322
00:39:23,890 --> 00:39:36,130
I just want to have very simple fast you calculations based on common theorem small so that it turns out to be very effective to solve this problem.

323
00:39:36,380 --> 00:39:40,000
Okay. That's basically the idea of comma as the mini function.

324
00:39:40,300 --> 00:39:46,090
Okay. So I still do a estimating function, but I do it faster only to compute that.

325
00:39:48,730 --> 00:40:00,730
So. So the the Monte Carlo M algorithm has no R package because you have to really, you know, drive your.

326
00:40:01,750 --> 00:40:09,220
You need to really work out this distribution, the sampling distribution from this post to,

327
00:40:09,240 --> 00:40:15,010
you know, you have to work out the the sampling scheme for from a specific posterior.

328
00:40:15,190 --> 00:40:20,379
Right. Sometimes you can just give sampling because this one can be sampled directly.

329
00:40:20,380 --> 00:40:30,610
Some times you need to rejection by symptom. The sort of the skim to sample from this sometimes maybe is not even possible to sample.

330
00:40:30,610 --> 00:40:33,570
You have to use important sampling to sample from here.

331
00:40:33,910 --> 00:40:40,300
So basically you you do a lot of sampling from here, then you use a lot of large number to apartment estimation.

332
00:40:40,930 --> 00:40:49,329
So what I did that is I just want avoid the sampling or the difficulty sort of the sampling and try to,

333
00:40:49,330 --> 00:41:00,010
you know, use Copperfield here to have a very elegant analytic solution for this problem and then solve.

334
00:41:00,950 --> 00:41:04,839
So what I did here, I think probably up to date,

335
00:41:04,840 --> 00:41:16,900
I think this is probably the most of the optimal balance between estimation efficiency and the the computation efficiency.

336
00:41:17,080 --> 00:41:24,850
Okay. So what I try to do here is that. Okay, I plotting the to to use the method moment estimate.

337
00:41:25,240 --> 00:41:35,200
Okay. Very simple. Right. And likelihood I use your method moment estimate which is not inconsistent and replace this one here.

338
00:41:35,530 --> 00:41:48,640
Okay. To top which is good and consistent that I try to solve this equation with respect to the, uh, this RFA that's a parameter of, of interest.

339
00:41:49,060 --> 00:41:56,200
So of course I have this iteration going on. I'm still following the EMR algorithm to do iteration, given iteration.

340
00:41:56,200 --> 00:42:00,240
So I first get my estimate out there. Mm hmm.

341
00:42:01,070 --> 00:42:05,210
The uses parameter from the written process and fixed that.

342
00:42:05,230 --> 00:42:08,920
I'm doing iteration on the the parameter of interest.

343
00:42:09,480 --> 00:42:18,970
Okay. So this parameter is interesting then I every time I wouldn't I have opted that after I do come a filter.

344
00:42:19,420 --> 00:42:23,379
So I opted to come in smoother. So here I use come a smoother.

345
00:42:23,380 --> 00:42:30,790
I find the common smoother is more stable than common future because we are essentially use the entire time

346
00:42:30,790 --> 00:42:37,870
series to make a prediction or make the estimation of meaning rather than using the forward method to estimate.

347
00:42:38,350 --> 00:42:44,770
So you have more data involved in the calculation of that expectation.

348
00:42:44,780 --> 00:42:48,669
So I found the common smoother is numerically more stable.

349
00:42:48,670 --> 00:42:58,959
So I would come and a smoother to estimate that conditional expecting of thing that he and I do iterations solving this

350
00:42:58,960 --> 00:43:05,890
is true for is that you're going to but you need to figure out to become a smoother when you have opted out of RFA.

351
00:43:06,400 --> 00:43:13,230
Okay. So you do come a few other comments. Smooth, smoother then and after AGM you have to observe out of RFA.

352
00:43:13,260 --> 00:43:18,790
I think back to the the common filter and coming smoother and you you try to resolve it until that

353
00:43:19,540 --> 00:43:25,360
this whole thing get converged and will converge because this is essentially an idea of algorithm.

354
00:43:25,990 --> 00:43:34,120
So this is a m algorithm and this is a sort of equation from Poisson regression, which is the opposite.

355
00:43:35,260 --> 00:43:39,460
So it's very, very fast to solve this whole problem.

356
00:43:39,490 --> 00:43:48,060
And so so that's basically the idea of doing this common estimate equation.

357
00:43:48,070 --> 00:43:56,770
This is estimated equation. But if I use the common filter, a common smoother technique to replace the offset term.

358
00:43:57,160 --> 00:44:01,930
Okay. So, so here is the performance of that.

359
00:44:02,200 --> 00:44:06,390
Okay. So I already show that this is g estimate.

360
00:44:06,400 --> 00:44:10,910
This is from the dollar to the use M algorithm.

361
00:44:10,930 --> 00:44:16,060
Right? So Montecarlo M, it takes quite a bit of computing power to complete that.

362
00:44:16,420 --> 00:44:24,160
This is the match that I proposed that coming from my dissertation, the comment estimating function.

363
00:44:24,670 --> 00:44:34,120
You can see that if if we have this time between because some people us time train habitually is just

364
00:44:34,120 --> 00:44:42,040
a linear train right so so whether or not you have a decline of incidence read the full polish.

365
00:44:42,280 --> 00:44:47,470
Right so. So this is something people like to know.

366
00:44:47,580 --> 00:44:56,610
Because an early 1970s there was a national wide interim policy intervention for polio vaccination.

367
00:44:57,000 --> 00:45:08,190
So people will see that if that policy is effective over time to mostly use this incident, rates of polio will decrease.

368
00:45:08,250 --> 00:45:11,910
So this year, twins the parameter of interest.

369
00:45:12,420 --> 00:45:21,510
You want to see whether or not the policy is effective so that you have a steady decreasing of this incidence rate over time.

370
00:45:21,510 --> 00:45:24,050
So that's of course, you have this novelty.

371
00:45:24,510 --> 00:45:36,750
Okay, so in the spring or fall, it tends to be less in fact infectious and the winter and you know, and anyway versus a the facts.

372
00:45:38,490 --> 00:45:47,510
And here you can see that the G does not give you a gives your estimate of the operation but does not give you an inference.

373
00:45:48,170 --> 00:45:54,149
Okay. Because t only gives you estimate of the temple dependance.

374
00:45:54,150 --> 00:45:57,870
It does not make any inference fully.

375
00:45:58,050 --> 00:46:05,129
So you do not have the standard URL here. And anyway, so.

376
00:46:05,130 --> 00:46:12,720
So for this common oncology MRI algorithm, you do see a statistical significance.

377
00:46:12,720 --> 00:46:20,700
There is a decrease in Twin over time in terms of incidence of polio in the country.

378
00:46:21,120 --> 00:46:30,239
And if is key, you can say that is also give you a statistic and you can see that this ratio is bigger than 2.0.

379
00:46:30,240 --> 00:46:41,140
Right? So both this melancholy and key streak is speaking is idea of the beam operator.

380
00:46:41,580 --> 00:46:45,990
But right now I use modern column the key use common filter.

381
00:46:46,380 --> 00:46:50,490
It has close form expression to make a calculation expectation.

382
00:46:51,090 --> 00:47:00,510
So it has a lot of the efficiency loss. But you know, if this runs like 1/2, then this will run hours to get results.

383
00:47:00,810 --> 00:47:07,200
Okay. You know, so while I was writing my dissertation,

384
00:47:08,010 --> 00:47:17,160
I can easily use S plus this very baby versus bar at the time like 20 years ago to to improve this this very fast.

385
00:47:17,700 --> 00:47:20,790
But I can now use as to implement this method.

386
00:47:20,790 --> 00:47:24,210
I had to write C++ to do that.

387
00:47:24,330 --> 00:47:32,220
So that forced me to pick up C++ as a student in order to implement this to, you know, like so.

388
00:47:32,970 --> 00:47:39,870
But it's good opportunity for me to revisit C plus and that that by the C program I shall

389
00:47:39,870 --> 00:47:46,620
see C positive was very new at time I did I learned C and full was a graduate student.

390
00:47:47,130 --> 00:47:51,660
So I implement the C for this one and it takes quite a bit.

391
00:47:51,670 --> 00:48:04,220
It just cannot be handled by S plus software and both T and you can be implemented using the very basic programing language.

392
00:48:04,770 --> 00:48:16,280
Now R is even faster to improve I think probably will take like 1/10 of a second you were to finish all the calculation was quite fast the way I.

393
00:48:16,940 --> 00:48:23,640
I regret I did not put a very nice package out for from this method I that's something.

394
00:48:26,530 --> 00:48:36,240
Okay. So now you can say that from here because you have the this common filter smoother than you can really estimate this data.

395
00:48:36,250 --> 00:48:45,970
T g does not provide this estimate of your safety because you basically marginalize it from this.

396
00:48:46,060 --> 00:48:52,210
This is a nice output from the the key method of where you can really estimate the late in

397
00:48:52,220 --> 00:49:00,700
process using this sort of for the glob best in the unbiased prediction of your process.

398
00:49:02,200 --> 00:49:08,780
And so. So let me just quickly go over this.

399
00:49:08,880 --> 00:49:18,860
And so if the way to the county level modeling of daily number infections, you can also go through the process model.

400
00:49:18,860 --> 00:49:21,680
For example, you have 83 counties in Michigan.

401
00:49:22,340 --> 00:49:31,760
You can model the R, the number of infections or number of deaths using the plotter model where I is index for county.

402
00:49:31,940 --> 00:49:45,900
Right. So that you can do this analysis, your homework tool, you start with the case with a working independent structure.

403
00:49:45,920 --> 00:49:53,600
It's not open process. But when time goes on that we will learn how to analyze the Michigan based related process where you can.

404
00:49:55,490 --> 00:50:06,890
Okay, not only just get estimate RFA, also get this little undermine in fascist dynamics from coming here.

405
00:50:07,170 --> 00:50:12,350
Okay, so that's additional things. They can really understand the competitiveness model.

406
00:50:13,490 --> 00:50:23,000
I have to stop here. I have to run to get my surgery. So have a nice full break and I see you next week, then Thursday.

407
00:50:23,130 --> 00:50:32,160
Okay. If you want to send me your slides for data processing, I can put on the, you know,

408
00:50:32,750 --> 00:50:41,030
here and you can share your experience of the data processing this or a classmate.

