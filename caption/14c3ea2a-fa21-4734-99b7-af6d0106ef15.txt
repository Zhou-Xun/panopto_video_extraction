1
00:00:08,200 --> 00:00:18,600
Well, as far as I know, that's how you feel.

2
00:00:19,440 --> 00:00:55,330
Is that because she doesn't know how to tell us exactly.

3
00:00:55,710 --> 00:00:59,440
What are you saying?

4
00:00:59,610 --> 00:01:03,870
Happy birthday. What a way to celebrate, huh?

5
00:01:08,850 --> 00:01:12,900
That's a full house. All right.

6
00:01:13,440 --> 00:01:17,070
Good morning, everybody. Good morning, everybody.

7
00:01:17,610 --> 00:01:20,040
Welcome back. Welcome back to the semester.

8
00:01:21,400 --> 00:01:29,670
Uh, I do my best to shout through my mask, and typically, I won't be wearing a mask, but I'm getting over a cold, and I knew this room would be full.

9
00:01:29,790 --> 00:01:35,250
So come on in. Um, but you're welcome to wear masks.

10
00:01:35,700 --> 00:01:45,000
This room, historically used to be really, really hot. But at some point, people are uncomfortable with the crowd.

11
00:01:46,080 --> 00:01:48,060
You don't know what things are going to look like this winter.

12
00:01:48,560 --> 00:01:58,050
The windows still open and the university's policy is, you know, wear masks as you feel appropriate.

13
00:01:59,120 --> 00:02:06,660
They're probably going to say this class has a waitlist.

14
00:02:08,250 --> 00:02:13,110
If you enrolled and you're in the class already, that's great. If you're on the waitlist and you're here today.

15
00:02:14,430 --> 00:02:20,009
That's okay. But the seats are reserved for those who have a reservation in this class.

16
00:02:20,010 --> 00:02:24,450
Right. So it's going to be packed if you want to come because you're on the waiting list.

17
00:02:25,530 --> 00:02:30,390
I know some of you are going to drop this class after you see how today goes or how the first week goes.

18
00:02:31,830 --> 00:02:35,850
But hopefully we'll have seats for everybody as the semester goes on.

19
00:02:37,830 --> 00:02:41,850
All right. My name is Tom Ryan. Some of you know me.

20
00:02:41,850 --> 00:02:49,230
I know some of you from last semester, from the mindset department. I've been in this department for 23 years.

21
00:02:50,760 --> 00:02:55,680
This was the class I taught my very first semester here at the University of Michigan 23 years ago.

22
00:02:56,910 --> 00:03:00,420
I designed the curriculum. It's changed a lot over 23 years.

23
00:03:00,960 --> 00:03:09,120
I have studied in about seven or eight years, so it's been a while since I've looked at this material and I'm excited to cover it again.

24
00:03:09,120 --> 00:03:18,200
Some of it's, I think, something unusual and different from what you see in most of your master classes in my understandable suits.

25
00:03:19,020 --> 00:03:21,479
So you're going to want to have a syllabus.

26
00:03:21,480 --> 00:03:28,980
I'm not going to read through everything in here, but I do want to just, you know, get the nuts and bolts down on what this class is going to work,

27
00:03:29,760 --> 00:03:34,770
the schedule and so forth, so that everybody is on top of this throughout the semester.

28
00:03:35,700 --> 00:03:40,919
One of my office hours p m Thursday is here.

29
00:03:40,920 --> 00:03:45,930
That's how this is going to go this semester, probably. Anybody see any open seats?

30
00:03:47,760 --> 00:03:52,590
We can put that table anyway. So let's figure. Let me put that table somewhere for you guys.

31
00:03:53,190 --> 00:03:59,969
You want to pull it off to the side? I don't want you to have to sit behind that whiteboard or move it over here.

32
00:03:59,970 --> 00:04:02,380
If you want to grab a chair and sit over here today, you can do that.

33
00:04:02,400 --> 00:04:10,650
The last hours are going to be on Thursday mornings, I think at least 830 for those of you who have a 9:00 class.

34
00:04:10,650 --> 00:04:14,100
So if you do want to come and see me, you do have a chance.

35
00:04:15,030 --> 00:04:19,920
You know, for 10:00 on Thursdays in my office, I don't run over there.

36
00:04:20,340 --> 00:04:24,480
I get there unless you do not appear to know that.

37
00:04:26,040 --> 00:04:32,579
There's my email address. I do answer emails. However, I'm not going to see an email at 10:00 at night.

38
00:04:32,580 --> 00:04:37,350
If you're freaking out about a homework or a project, we'll get to until the next day.

39
00:04:37,350 --> 00:04:41,909
But I do try to stay on top of the email as best as I can, but I am not on call.

40
00:04:41,910 --> 00:04:45,239
24 seven. I have one GSI.

41
00:04:45,240 --> 00:04:49,350
Her name is Montreat Dunbar. She is a second year bio start student.

42
00:04:50,070 --> 00:04:55,980
So she is at the same point for a lot of you. So treat her with some respect.

43
00:04:57,120 --> 00:05:00,390
She is your GSI, but she is a second year master. Students.

44
00:05:00,890 --> 00:05:06,740
And she's going to help you as much as she can. Sometimes she'll she'll have you might have questions for her that she won't

45
00:05:06,740 --> 00:05:11,120
immediately be able to get to because she's learning this material to welcome.

46
00:05:13,400 --> 00:05:16,459
Her office hours will be on Wednesday afternoons.

47
00:05:16,460 --> 00:05:21,140
Again, we're trying to cover a later in the day slot for folks who need it later in the day.

48
00:05:21,140 --> 00:05:26,910
So on assistance Wednesdays, we don't know where yet.

49
00:05:26,930 --> 00:05:33,170
I've asked the admin to find a room. I probably be over in a speech to how to keep you posted on that.

50
00:05:34,730 --> 00:05:43,020
We are going to have a lot of stories this week. I don't know how in demand we will be this week, but we will be available starting this one.

51
00:05:43,340 --> 00:05:52,790
Things that I would like to keep track of all rides and location.

52
00:05:52,790 --> 00:05:58,160
Everybody's figured that out. 10 to 1120 in this room, there is a class at 1130.

53
00:05:58,730 --> 00:06:05,010
So we do have to clear out of here by the time class ends. I'm happy to talk to folks here tonight from the area out here.

54
00:06:05,010 --> 00:06:10,370
And if you have questions for me after class, then I will certainly do my very best.

55
00:06:10,370 --> 00:06:14,300
There is no class, so we want you to be done at 1120.

56
00:06:14,420 --> 00:06:17,990
Today I am recording the lectures.

57
00:06:19,010 --> 00:06:25,860
I will be available again. I don't know when the powers that be take a recording and make it to make me available on campus.

58
00:06:25,880 --> 00:06:31,860
But I believe by 1:00 today and so forth.

59
00:06:32,210 --> 00:06:43,760
So they are available in the first half of this course is the historic rank based testing that you have talked about wilcoxon rank,

60
00:06:43,760 --> 00:06:47,990
some test or faction assigned rank tests in any classes you've taken.

61
00:06:48,950 --> 00:06:56,180
That's what we're talking about here. And then we're going to do things like swines, vocal regression smoothing, classification of regression trees,

62
00:06:56,990 --> 00:07:02,030
things that are complements to the linear regression concepts that you've seen in your other classes.

63
00:07:03,200 --> 00:07:07,790
So there's five areas there. I don't need to go through them all the bootstrap.

64
00:07:07,990 --> 00:07:12,530
See, in the bootstrap we're going to talk about how to use the bootstrap for inference permutation, tests in general.

65
00:07:13,070 --> 00:07:17,750
Again, other concepts that you might have seen a little bit of data.

66
00:07:17,910 --> 00:07:24,410
And so these five, I guess like all the modules here,

67
00:07:25,130 --> 00:07:29,480
we're going to learn about these things through the lectures that we all go through in class together,

68
00:07:30,650 --> 00:07:35,660
examples of our code that I will present in lecture and sometimes we'll do on the fly in class.

69
00:07:37,040 --> 00:07:42,350
There are six homework assignments, six, and there are three in depth projects.

70
00:07:43,320 --> 00:07:44,880
There are no tests in this class.

71
00:07:46,320 --> 00:07:53,520
I just don't feel it's worthwhile to have you do a written exam on something that we want you actually doing and data analysis.

72
00:07:54,240 --> 00:07:55,380
And we'll get to that in a second.

73
00:07:56,160 --> 00:08:05,180
You are expected, if you're a bio set student, to have taken 601 and 602 and at least 650, which is a basic regression.

74
00:08:05,670 --> 00:08:08,790
If you're not in the bio said department, again, you should have two semesters of theory.

75
00:08:08,790 --> 00:08:14,160
You should know a maximum likelihood is you should know all of the basic theory behind regression.

76
00:08:14,460 --> 00:08:20,280
And this isn't a theory class. You're talking about something from linear regression that you haven't seen.

77
00:08:20,280 --> 00:08:25,890
It's a little less easy to understand what we're doing. So those are the prerequisites.

78
00:08:26,820 --> 00:08:30,630
Then you should feel comfortable with parameter, estimation and regression and so forth.

79
00:08:31,530 --> 00:08:35,340
And the site. There is, of course, a camera search for this class.

80
00:08:36,390 --> 00:08:43,940
It looks like that there is the syllabus for the calendar.

81
00:08:43,960 --> 00:08:47,070
And I have a tentative plan for how this thing's going to work this semester.

82
00:08:47,370 --> 00:08:52,800
It probably will go off the rails in about two weeks, but I'll do my best to stay on track for the semester.

83
00:08:53,540 --> 00:08:57,900
So each topic has a link to it. Each of the five topics that I just told you about,

84
00:08:58,560 --> 00:09:05,100
and in there you'll find the slides like the introduction and the two simple rank tests we're going to go through today as best we can.

85
00:09:06,270 --> 00:09:09,360
And then a few more things as we move along assignments.

86
00:09:10,230 --> 00:09:14,370
So assignment number one that you have to do on Friday, this Friday,

87
00:09:16,320 --> 00:09:21,660
and then data sets that either are in the homework assignments or that I show you in my

88
00:09:21,660 --> 00:09:27,900
lecture notes if you want to try and replicate what I did and then sensibilisation.

89
00:09:29,910 --> 00:09:32,940
Oh, you're going to have trouble finding a seat on the table.

90
00:09:33,500 --> 00:09:37,980
All right. Any seats?

91
00:09:38,130 --> 00:09:46,050
Even if you can find a chair over and somebody can write it down on the corner. All right.

92
00:09:47,190 --> 00:09:53,180
So, again, that's the outline of the class. There is a link down here called the survey.

93
00:09:53,190 --> 00:09:59,550
This is going to become important for you every day you come to class and I'll get into the second.

94
00:09:59,560 --> 00:10:06,240
There are different ways to get to that Qualtrics survey, but everything you need for the course should be on the website.

95
00:10:06,240 --> 00:10:12,719
Everything's going to be electronic homework assignments, the end up projects, everything submitted through canvas, no paper, nothing.

96
00:10:12,720 --> 00:10:23,400
The guy that you have to hand in to me physically. We're going to use our exclusively in this class that 20 years ago was a big ask for students.

97
00:10:23,400 --> 00:10:31,980
But now you will know more about her than I do. So I think by now all of you are comfortable enough with the things that we do in our.

98
00:10:32,730 --> 00:10:36,209
Sometimes I will do things in our in class.

99
00:10:36,210 --> 00:10:45,390
I'm going to use opposite how it used to be our studio cloud lecture called with you that way and run through that code through the internet.

100
00:10:46,380 --> 00:10:52,690
You know that as the semester goes on, then the objectives, you're going to learn how to do a lot of stuff, right?

101
00:10:53,770 --> 00:11:00,120
That's my goal and that's okay. So let's talk about attendance.

102
00:11:01,360 --> 00:11:07,080
So, um, I'm, I guess I'm requiring attendance in a certain fashion here.

103
00:11:07,960 --> 00:11:13,160
Um, I was very disappointed with the amount of attendance I got in my class last semester.

104
00:11:13,170 --> 00:11:16,230
Those of you who were in my class know that already.

105
00:11:17,160 --> 00:11:27,420
Um, we're trying to figure out as instructors and in the world of academia, how we can get everybody more on stage with coming back to class.

106
00:11:28,170 --> 00:11:37,139
And so this is my plan. So there's a little trick survey I'm going to have you fill out a survey at one says what?

107
00:11:37,140 --> 00:11:43,680
Tell me your name and to another question. And a question you can only answer correctly if you're in class.

108
00:11:44,980 --> 00:11:50,040
Right. So we're going to go through in a second here. I'm not trying to be a jerk.

109
00:11:50,460 --> 00:11:56,460
I truly do believe that coming to class can help you navigate life.

110
00:11:57,540 --> 00:12:03,040
We're not going to talk about life necessarily, but I want you to come to class.

111
00:12:03,060 --> 00:12:07,260
I want you to engage with other human beings when you need it.

112
00:12:08,640 --> 00:12:12,120
If you are sick, please don't come to class.

113
00:12:12,780 --> 00:12:18,090
I don't want you here. We don't want you here. We want you to take care of yourself and take care of us.

114
00:12:18,900 --> 00:12:23,490
So if you're sick, please let me know ahead of time. I'm just asking you to be proactive.

115
00:12:24,780 --> 00:12:27,930
I don't need a doctor's note. I don't need a picture of your protein test.

116
00:12:28,980 --> 00:12:34,340
Right. Unless you have COVID every day. Then there's the problem we have to resolve.

117
00:12:34,350 --> 00:12:40,740
Okay. But if you wake up feeling crummy, like I did last week, and I don't want you to come with us,

118
00:12:41,490 --> 00:12:44,490
you tell me that that doesn't count towards your attendance demands.

119
00:12:44,490 --> 00:12:51,140
That just goes away. All right, I'm giving you six class periods in which you can just say, you know what?

120
00:12:51,150 --> 00:12:54,870
I don't feel like going to class today and tired. I'm hung over.

121
00:12:55,350 --> 00:12:59,730
I just don't want to get out of bed. Six of those be my guest.

122
00:12:59,940 --> 00:13:03,180
Right. Beyond that, you're going to start being penalized.

123
00:13:04,540 --> 00:13:08,020
Now. I think that's immature, but because I think that's enough to get everybody coming.

124
00:13:08,860 --> 00:13:13,690
It's much more fun for me to teach to all of you than a handful of you.

125
00:13:14,050 --> 00:13:17,370
Although I get to know that handful very well. But I want to get to know everybody.

126
00:13:17,380 --> 00:13:20,950
So I'm going to take this survey as a way to measure attendance.

127
00:13:23,290 --> 00:13:28,320
I know that you all can text your friends and tell them that the questions are online right now and what the answer is.

128
00:13:28,330 --> 00:13:35,649
I know those tricks. If I look around the room and I see 40 people, but there were 70 responses to the survey.

129
00:13:35,650 --> 00:13:41,470
Then again, there's a problem. Is hard to write, but please come to class.

130
00:13:43,930 --> 00:13:49,720
If you start coming to class less often than desired, I'm going to start knocking off a great category.

131
00:13:50,170 --> 00:13:56,790
So those are less than 75% of attendance outside of illness, outside of death, in the family, etc.

132
00:13:56,800 --> 00:14:01,000
Those don't count. I'm going to drop your grade from an A-minus to a B-plus so far.

133
00:14:01,060 --> 00:14:06,190
Right. And I don't have this percentage. Hopefully, I don't have to use this.

134
00:14:06,940 --> 00:14:13,250
This is just to keep you honest and coming and seeing the dynamic lectures that I have in store for you.

135
00:14:13,330 --> 00:14:21,370
All right. So but again, if you can't attend a lecture for serious matters, just please tell me.

136
00:14:22,600 --> 00:14:30,370
If you were having a horrible time with life, if things are getting too much for you to bear, tell me that.

137
00:14:30,370 --> 00:14:35,140
Don't just not come to class. I'm here to help you navigate that as well.

138
00:14:35,790 --> 00:14:41,080
So unless your life is really, really unbearable.

139
00:14:41,890 --> 00:14:45,520
Dr. Braun, I have a test. Today is not an excuse to miss my class.

140
00:14:46,480 --> 00:14:51,549
Okay. I know you have tests, but you can still win the White House.

141
00:14:51,550 --> 00:14:54,879
I think they were enough to get everything else right.

142
00:14:54,880 --> 00:14:58,209
So that kind of stuff is not an excuse if you have a conference.

143
00:14:58,210 --> 00:15:01,600
I know some of you are getting close to graduating. You might have a job interview.

144
00:15:02,260 --> 00:15:11,740
Just tell me I like it, but I'm a nice guy once, you know, if you had a job interview, when you're in this class period, you're going to come late.

145
00:15:12,010 --> 00:15:15,880
So tell me, be proactive. They'll tell me afterward. And that nice guy afterwards.

146
00:15:16,390 --> 00:15:25,690
I'm a nice guy beforehand. So there is on there on the syllabus there is a link to this canvas survey.

147
00:15:26,920 --> 00:15:32,590
There is a QR code that you can scan with your phone. I want you all to do that right now.

148
00:15:34,180 --> 00:15:38,740
So go to canvas and hopefully that will transmit to a phone.

149
00:15:41,260 --> 00:15:47,030
Okay. And there is a survey on they're going to do it to be open.

150
00:15:50,170 --> 00:15:54,040
And you know how we do it.

151
00:15:55,180 --> 00:16:01,870
I'm try to do this at random times during my lectures so that everyone doesn't know that at 11:00.

152
00:16:02,740 --> 00:16:08,410
The survey is coming up. I know we're all going to get a free pass today, but this is how it's going to work.

153
00:16:10,640 --> 00:16:14,390
So enter your first and last name. Hopefully that isn't a hard question.

154
00:16:16,770 --> 00:16:20,310
Its next question is in honor of the disaster that was last night.

155
00:16:21,430 --> 00:16:29,040
Dr. Braun is a fan of which NFL team that would be the Green Bay Packers who suck anyway.

156
00:16:29,190 --> 00:16:32,819
They used to be good. So. Packers, Green Bay.

157
00:16:32,820 --> 00:16:39,940
Packers, Green Bay. I don't care. There's no grading here. I just want to see an answer from that in an answer there.

158
00:16:42,190 --> 00:16:46,589
He said, it's a girl and you're done.

159
00:16:46,590 --> 00:16:53,840
And you have told me that you came to class. So, again, this is not working for you today.

160
00:16:53,850 --> 00:16:57,180
Not a big deal. We get a free pass today. I am sorry.

161
00:17:00,510 --> 00:17:05,570
There is a table but was filled with exceptions.

162
00:17:08,000 --> 00:17:15,000
What was one of the great meals in sports? This was.

163
00:17:21,950 --> 00:17:31,560
You are.

164
00:17:32,260 --> 00:17:43,660
You're there. You're sitting there, as you can see.

165
00:17:46,480 --> 00:17:55,550
All right. This is 40 or 40 minutes.

166
00:17:57,740 --> 00:18:00,820
Anyway, there's two more shooters. Oh, my. We got three more folks.

167
00:18:02,410 --> 00:18:09,490
All right. Again, this room supposedly holds 72 people.

168
00:18:10,440 --> 00:18:18,100
I think it's a big wish from the School of Health. But again, if you're on the wait list, you don't technically have a seat in this class.

169
00:18:19,480 --> 00:18:25,370
Somebody else drops a cannon, right.

170
00:18:25,390 --> 00:18:31,510
So that's attendance. Maybe we can navigate who doesn't come on Sundays.

171
00:18:31,510 --> 00:18:35,770
So their seats for everybody, please come to class.

172
00:18:36,490 --> 00:18:40,639
And if it gets too difficult for you, please let me know. I really never get that.

173
00:18:40,640 --> 00:18:44,540
Who have never taught in a mess before? Huh?

174
00:18:44,720 --> 00:18:49,040
All right. Homework assignments. 4 to 6 assignments in this class.

175
00:18:49,340 --> 00:18:56,180
I don't think that's too much work. Well, I really wish I could get even more practice for you guys, because this is a data analysis class.

176
00:18:56,690 --> 00:19:02,150
More than a theory class. We'll do six homework assignments, whether it's worth 80%.

177
00:19:03,660 --> 00:19:08,880
Each assignment will be due on a Friday at noon. So we meet on Monday and Wednesday.

178
00:19:09,120 --> 00:19:16,080
If you want an extra day to work on the homework, you've got Thursday and Friday morning turned in electronically and canvas.

179
00:19:17,970 --> 00:19:22,260
After the deadline, I take 20% off for every day late.

180
00:19:23,710 --> 00:19:32,920
Yeah. And I'm a really nice guy. If you reach out to me ahead of time, I am happy to make exceptions to any deadline that I have.

181
00:19:33,940 --> 00:19:39,099
I'm less happy to make accommodations for a deadline like an email on Saturday saying,

182
00:19:39,100 --> 00:19:45,720
Dr. Brown, I just couldn't turn that homework assignment on that time. Just tell me beforehand.

183
00:19:45,930 --> 00:19:50,640
Just tell me. I have a daughter who's a senior in high school, and this is a life skill.

184
00:19:51,090 --> 00:19:54,150
Okay. Talk to your teachers and your professors.

185
00:19:54,510 --> 00:19:59,280
Most of us are really nice. Not all of us, but most of us are really nice and accommodating.

186
00:19:59,280 --> 00:20:07,790
Right. So if you can't make the deadline, absolutely no extension because things are just building up.

187
00:20:07,800 --> 00:20:14,580
I get it. Just let me know. You're more than welcome to work with each other on the homework assignments.

188
00:20:15,790 --> 00:20:22,630
I encourage it. It's the only way to get through grad school, to work with your friends and your peers, to understand and learn.

189
00:20:25,690 --> 00:20:32,890
The first homework assignment. There's really only one way to do it. So a lot of your art code is going to look the same.

190
00:20:33,040 --> 00:20:38,500
Your answers are going to look the same. I told my friends the GSA again to be really lenient here.

191
00:20:40,120 --> 00:20:43,900
It's to your own benefit to do the art code and things yourself.

192
00:20:44,830 --> 00:20:47,260
But if you want to work with your friends, that's more than fine.

193
00:20:47,410 --> 00:20:53,680
Just try to make your homework assignments look a little different and they'll become more important as the semester goes down.

194
00:20:53,680 --> 00:20:57,050
There will be more creativity as a break.

195
00:20:57,430 --> 00:21:03,700
So put together, I'm not going to we're not going to go through our code if I see who is cheating and who's not or whatever,

196
00:21:04,060 --> 00:21:11,440
you know, we're fine for that. But work together and do your own work as much as you can for your own learning and benefit.

197
00:21:12,040 --> 00:21:15,400
But as I said, there are no tests in this class.

198
00:21:16,870 --> 00:21:20,110
However, there will be what I'm calling three in depth projects.

199
00:21:20,410 --> 00:21:23,830
So after a couple of homework assignments, after we finish rank base testing.

200
00:21:25,160 --> 00:21:33,650
There will be what I call an in-depth project that will open on a Wednesday, Wednesday at 5 p.m., and it's open until Sunday.

201
00:21:34,190 --> 00:21:41,900
You have as much time as you want between Wednesday on Sunday to download the data set to analyze and answer the questions that pop up.

202
00:21:42,730 --> 00:21:47,760
Right, so you can go in. I believe this occurred in such a way that you can come in and come out great.

203
00:21:47,930 --> 00:21:56,170
It isn't like a test where it opens and then 2 hours later it's one. I don't really want to grade 72 reports on the same rank based test.

204
00:21:57,410 --> 00:22:05,620
So I'm going to try and program this so that you have to do a lot of analysis, but in the end you're going to answer lots of questions on canvas.

205
00:22:05,960 --> 00:22:09,470
So that will be short answer to ask you to actually explain something.

206
00:22:09,890 --> 00:22:15,630
Some of them will be computation. Those of you who took my class last semester are familiar with how I did that on tests.

207
00:22:15,650 --> 00:22:21,979
All right. So there are three projects, but one a month, one in February, one in March,

208
00:22:21,980 --> 00:22:29,870
and then one at the end of the semester if everything goes as planned, that Sunday is the Sunday before the last day of class.

209
00:22:30,680 --> 00:22:35,840
So we stay on track here. You won't have the last day of class with me and the class will be done.

210
00:22:40,850 --> 00:22:45,620
You are to work on these projects by yourself. These are not true projects.

211
00:22:47,420 --> 00:22:51,370
Again, they're on the internet, so you're welcome to use my notes.

212
00:22:51,380 --> 00:22:55,640
You're welcome to use books. You're welcome to use the Internet to answer the question.

213
00:22:55,660 --> 00:23:02,060
So I'm not going to I can't even check all that. So now you're welcome to use material except for your friends.

214
00:23:04,090 --> 00:23:10,879
Yeah, I don't want to have to monitor cheating, but if I see three projects that all opened at the same time and questions were

215
00:23:10,880 --> 00:23:15,530
answered at the same time and both at the same time and have the same wrong answers,

216
00:23:17,390 --> 00:23:23,660
then we have to have a conversation. So you're in your best behavior here in my classroom that you're working on this on your own.

217
00:23:23,930 --> 00:23:32,150
All right. So three projects, there they are. They're each worth 17 and a third percent of your grade so that we get everything add up to 100%.

218
00:23:32,720 --> 00:23:37,600
So each project is worth two assignments, essentially each.

219
00:23:40,280 --> 00:23:46,820
And questions on all that stuff. What you are expected to do this semester.

220
00:23:49,770 --> 00:23:56,010
All right. Lots of other stuff here. Again, I don't need to go through all of this.

221
00:23:58,410 --> 00:24:01,440
We're here to learn from each other. I'm here to teach you what I know.

222
00:24:02,490 --> 00:24:04,720
I always learn our code from students.

223
00:24:05,280 --> 00:24:11,890
When I look at homework assignments, those within my class know that I just learned our mark down about a month ago.

224
00:24:11,910 --> 00:24:20,160
I'm so proud of myself, but I always learn from teaching folks who are learning from me.

225
00:24:20,220 --> 00:24:24,000
So again, please come on in. I'm here to help you.

226
00:24:24,060 --> 00:24:27,540
I am not an impediment to your success. I am a bridge to your success.

227
00:24:27,810 --> 00:24:36,060
I want to help you on whatever journey you have next, whether it's getting a job, going to a Ph.D. program, etc. there for you.

228
00:24:36,880 --> 00:24:40,080
I am open to feedback by going too fast.

229
00:24:40,080 --> 00:24:43,590
If I'm going too slow, if you don't understand what the heck I'm saying,

230
00:24:44,250 --> 00:24:51,930
the only way it's going to change is if you tell me you really have deep feelings about people telling me they don't like my lecture.

231
00:24:52,380 --> 00:24:55,590
I can always fix that. All right. So please tell me now.

232
00:24:55,590 --> 00:24:59,430
Don't tell me on the e-mails in April when it isn't of any benefit to you.

233
00:24:59,850 --> 00:25:04,980
And as I said, emails. I'll do my best to respond to emails.

234
00:25:05,280 --> 00:25:09,100
If they come late at night, if they come on the weekends, I won't be as responsive.

235
00:25:09,120 --> 00:25:12,450
I do have a life outside of my that. I hope you love you as well.

236
00:25:13,260 --> 00:25:20,920
And statistics. Sorry. I'm so used to teaching my staff folks, but I was nervous to respond to emails because the country.

237
00:25:23,420 --> 00:25:31,969
I do ask that you leave Montreat alone with emails unless you know her and you can have a conversation with her.

238
00:25:31,970 --> 00:25:36,680
But please don't inundate her with emails. Please don't inundate me with emails.

239
00:25:38,130 --> 00:25:43,380
If you're struggling with a homework assignment, wait for an hour and see how many questions you have.

240
00:25:43,650 --> 00:25:48,000
Rather than sending me three emails. I'd rather see one email with three questions.

241
00:25:48,420 --> 00:25:53,700
All right. So think about all that. Again, if you need accommodations.

242
00:25:53,700 --> 00:26:00,660
If you need any help. Any assistance in being successful in this class had beer for that.

243
00:26:02,100 --> 00:26:07,830
You know, if you experience sexual misconduct, there are resources for that here at the university.

244
00:26:08,910 --> 00:26:15,660
And I am a mandatory reporter if you come to me with issues such as issues.

245
00:26:16,170 --> 00:26:21,389
But yeah, please, please know that we do not tolerate that behavior.

246
00:26:21,390 --> 00:26:27,150
At least I don't. And we can help you with that. What I expect from you again, come to class.

247
00:26:27,780 --> 00:26:30,960
Come to class, be timely, ask me questions.

248
00:26:31,290 --> 00:26:32,820
Please ask me questions.

249
00:26:33,750 --> 00:26:44,130
When a class this big, I know that your question is shared by more than just, you know, be proactive, do your assignments, and again, seek help again.

250
00:26:44,160 --> 00:26:49,320
Life is going to crash down on at least one or two of you. This class is big enough and they're doing this long enough.

251
00:26:50,640 --> 00:26:57,930
It might come crashing down on me, but I'm here to help you navigate whatever you need this semester.

252
00:26:57,930 --> 00:27:03,889
So please keep all of that in mind. Again, this is a speech.

253
00:27:03,890 --> 00:27:08,750
Some of you are in school like all students. But again, there are things like emergency fund requests.

254
00:27:08,750 --> 00:27:15,950
If you have to travel home for a funeral and you can't afford a plane ticket, there are ways around this to help you get what you need.

255
00:27:15,950 --> 00:27:23,909
So please keep all. In integrity here.

256
00:27:23,910 --> 00:27:34,850
You're you're on an honor code here, assuming that you're going to do as I ask, I'm going to be honest, trustworthy, and and do the work as we should.

257
00:27:36,030 --> 00:27:44,100
Please don't make me do it. I have to do some stuff on that, on untoward behaviors.

258
00:27:44,760 --> 00:27:48,450
Um, again, things about diversity, equity and inclusion.

259
00:27:50,190 --> 00:27:53,280
Again, we all come from different departments, different perspectives.

260
00:27:53,280 --> 00:28:00,840
We're all different parts in life. We should be respectful of all of those, those differing opinions and experiences.

261
00:28:03,720 --> 00:28:08,470
Once again, I want to keep this clear here.

262
00:28:09,480 --> 00:28:13,350
There's been some really egregious behavior by professors at the University of Michigan.

263
00:28:14,640 --> 00:28:17,910
Some you've been here, some that occurred before you were ever a student here.

264
00:28:19,530 --> 00:28:26,879
There are some guidelines here. My office hours. I keep my door open unless you explicitly request that it be closed because you want to

265
00:28:26,880 --> 00:28:30,570
discuss something that you don't want to be heard by anybody passing in the hallways.

266
00:28:33,000 --> 00:28:36,270
I will document the meetings there in my calendar.

267
00:28:36,300 --> 00:28:39,690
It was just I put in my calendar because I'm going to forget otherwise.

268
00:28:40,260 --> 00:28:47,120
But everything is documented. Again, we don't have any really, really reason to meet outside of the university,

269
00:28:47,130 --> 00:28:52,710
but I'm not going to propose any meetings or places where it isn't safe, especially where alcohol is served.

270
00:28:53,340 --> 00:28:56,790
And I'm only going to contact you through email, through canvas.

271
00:28:57,240 --> 00:29:02,280
We're not going to slack in this class unless it becomes very useful for everybody.

272
00:29:04,090 --> 00:29:07,680
Keep all that in mind. All right.

273
00:29:08,040 --> 00:29:14,100
Of course. Calendar. Yes, that's right.

274
00:29:14,430 --> 00:29:19,180
All right. Here we are in January. This is in the syllabus.

275
00:29:19,200 --> 00:29:28,770
It's also on the campus website as a PDF. To give you an idea of where everything is due during the semester, it probably will change.

276
00:29:30,030 --> 00:29:35,640
But this is my plan here. So, you know, boxes for every Monday and Wednesday that we're meeting here in class.

277
00:29:36,740 --> 00:29:40,040
We already don't have class a week from today because of Martin Luther King Day.

278
00:29:40,490 --> 00:29:43,910
So we have today and Wednesday and then we have no class on Monday.

279
00:29:44,750 --> 00:29:46,880
Blue indicates when you have a homework assignment, do.

280
00:29:47,180 --> 00:29:53,030
So you can see that you already have one due on Friday, which you're like, How the heck can I do any homework?

281
00:29:53,030 --> 00:29:56,930
We haven't even talked about material yet, but most of it is stuff you already know.

282
00:29:59,270 --> 00:30:02,480
And if it becomes problematic again, we can always be flexible.

283
00:30:03,200 --> 00:30:08,989
The next assignment will be due on the 27th. Again, that assignment is already on canvas.

284
00:30:08,990 --> 00:30:13,729
I'm going to try and be as ahead of time, ahead of schedule as I can,

285
00:30:13,730 --> 00:30:18,980
so you can see another homework assignment in case you have lights coming up at the end of January.

286
00:30:18,980 --> 00:30:28,130
You want to get a head start on this assignment. And evidence orange is a project so nothing in January.

287
00:30:29,180 --> 00:30:37,040
So learning in February. So we'll meet in for a lecture on Wednesday the first and then the first project will open at 5:00.

288
00:30:37,850 --> 00:30:42,880
And so by that time we've covered all of the rank based testing and permutation tests and so forth.

289
00:30:43,250 --> 00:30:51,500
If there will be a project that opens Wednesday night and you're able to do as much work on it as you can until Sunday, at which point it closes.

290
00:30:51,920 --> 00:30:55,010
And then I have to read your work and then we come back together on Monday.

291
00:30:56,450 --> 00:31:01,130
These projects should not take you four continuous days.

292
00:31:02,030 --> 00:31:07,790
That is not my goal. My goal for four days is for you to say, You know what, I've got a lot of classes on Thursday.

293
00:31:07,790 --> 00:31:13,910
I'm not going to start until Friday. It is not so that you spend 5 hours every day in this class.

294
00:31:14,700 --> 00:31:19,500
No, it should not be that demanding and it will not be that demanding.

295
00:31:20,040 --> 00:31:26,340
So there's the first project that we've got another couple of homework assignments, which a break comes at the end of February.

296
00:31:26,790 --> 00:31:34,049
So we have to know first and then you're going to come back refreshed and ready for

297
00:31:34,050 --> 00:31:38,760
more work and we'll have a couple of lectures and then there will be another project.

298
00:31:39,980 --> 00:31:49,310
And so far this week here, the end of March, is the Ann Arbor Public Schools spring break, which of course, never overlaps with the university.

299
00:31:50,640 --> 00:31:54,410
And so I have some family travel on this date, so there will be no class on the 27th.

300
00:31:57,500 --> 00:32:07,520
And then we go to April. So another homework assignment, like I said, the Wednesday, the second, the last day of class.

301
00:32:07,700 --> 00:32:10,760
We'll have a wrap up of TBA there.

302
00:32:11,420 --> 00:32:14,420
I know we're going to fall behind. I know things will will go off track.

303
00:32:16,040 --> 00:32:20,330
Your final project to do that week and then the final day of classes the 17.

304
00:32:21,110 --> 00:32:24,350
And we don't need to have class if we're all having a great time.

305
00:32:25,100 --> 00:32:28,639
We want to see each other again. We want to little party. I'm happy to do that.

306
00:32:28,640 --> 00:32:33,540
But there's no official lecture on that day. You should be done with all your responsibilities by the day.

307
00:32:35,240 --> 00:32:42,350
All right. So would you like to experience?

308
00:32:44,100 --> 00:32:49,770
Changed this calendar three times. I hope there are no typos in first.

309
00:32:52,130 --> 00:32:56,120
Homework, number one. So let's to do this.

310
00:32:56,120 --> 00:32:59,540
We're going to see how it goes. So you do have a homework assignment do on Friday.

311
00:33:02,620 --> 00:33:08,710
What I want you to do is do a quick comparison of things you already know with a rank based approach.

312
00:33:10,060 --> 00:33:17,460
And in the spirit of the NFL football playoffs coming up over the holiday break,

313
00:33:17,470 --> 00:33:23,080
I downloaded a dataset that has statistics information on every NFL player.

314
00:33:23,860 --> 00:33:27,400
Somebody has a fantasy football league and has a really nice set of data out there on the Internet.

315
00:33:28,270 --> 00:33:32,580
Every football player, every game and ad nauseum amount of information.

316
00:33:33,010 --> 00:33:36,550
The dataset I have collected is on the quarterbacks in the NFL.

317
00:33:37,640 --> 00:33:41,330
If you don't like football, if you don't know football, American football.

318
00:33:41,330 --> 00:33:47,120
Excuse me, you don't need to do this analysis.

319
00:33:47,420 --> 00:33:53,600
So, again, I'm trying to make this approachable for everybody. You don't have to be a football fanatic to analyze this data.

320
00:33:55,070 --> 00:33:59,030
There is a dataset on campus called, I think, quarterback data.

321
00:34:00,140 --> 00:34:08,810
I'm going to ask you to make histograms of a couple of the outcomes on the computer, some means of the mediums, some use of T tests, which, again,

322
00:34:08,810 --> 00:34:13,520
I'm assuming you already know how to do an R and then there's a new function here called Wilcox test,

323
00:34:14,120 --> 00:34:17,960
which I guarantee you you could step out of this class in half an hour and know what to do.

324
00:34:19,250 --> 00:34:26,900
This is just is the beginning of the class. It's the same syntax as t test, except you put the word wilcoxon to test.

325
00:34:27,980 --> 00:34:30,350
There's two outcomes total passing yards.

326
00:34:31,010 --> 00:34:39,650
So again, in any one game, how many yards to the quarterback accumulate in his throws and then to the completed pass percentage?

327
00:34:39,740 --> 00:34:48,500
So in the dataset you have the number of completed passes and you have the number of attempted passes and ones the percentage.

328
00:34:48,500 --> 00:34:51,740
So one step, you get a computer ratio there beforehand.

329
00:34:52,250 --> 00:34:58,670
All right. But again, a lot of data. Every game played through 4000, 19 through right before Christmas.

330
00:34:59,420 --> 00:35:07,010
We have another one. So I want to ask you again to make some histograms, to do some T tests, and then to do those wilcoxon rank film test.

331
00:35:07,580 --> 00:35:11,900
So I think that's all doable. But again, if it's not, please let me know.

332
00:35:13,330 --> 00:35:17,990
Then I have one question at the end. In a class like this, I don't want to be just computational.

333
00:35:19,510 --> 00:35:23,680
I do want you to maybe have some thought about what you're doing and what I'm telling you to do.

334
00:35:24,350 --> 00:35:26,500
And so there's a question here at the end as open ended.

335
00:35:27,400 --> 00:35:34,660
I'm going to ask you to do wilcoxon like some tests t test, then i'm going to ask you, is there maybe some assumption that's being violated here?

336
00:35:35,350 --> 00:35:38,830
And what do you think it's doing to your results? Right.

337
00:35:39,310 --> 00:35:43,240
So there it is. Have it done by Friday at noon.

338
00:35:45,400 --> 00:35:52,240
Questions there. If he knows what's ahead of them, he should not have.

339
00:35:52,270 --> 00:35:57,790
Well, I shouldn't say that you are going to need a couple of libraries in our we only the small library and a couple of things.

340
00:35:57,820 --> 00:36:01,330
Oh, yes. Do you have any recommended reading for this?

341
00:36:01,990 --> 00:36:06,850
I have no recommended readings. I will post at times things from textbooks.

342
00:36:07,750 --> 00:36:12,520
We cover topics that are in and there's no textbook that has all of this information.

343
00:36:13,270 --> 00:36:17,850
So I will post some PDFs on and I close. Sorry.

344
00:36:18,450 --> 00:36:22,530
There on the home page of Canvas, it says Miscellaneous documents near the bottom.

345
00:36:23,130 --> 00:36:33,270
That's where I'm going to post things like that. Hopefully the rank based things are pretty easy to absorb.

346
00:36:33,720 --> 00:36:42,550
But I will post things as necessary. But there is no textbook. Reading Arizona Civil Rights.

347
00:36:42,810 --> 00:36:51,270
It's already 1030. Look at that. So today I want to go through an introductory lecture, sort of the picture of the first semester.

348
00:36:51,870 --> 00:36:56,370
And then I do want to get to rank base testing at least a little bit so that you have some sort

349
00:36:56,370 --> 00:37:00,090
of discussion with me about rank based tests before I just have you analyze data used just.

350
00:37:01,340 --> 00:37:09,080
So again, all of the lecture notes will be made available to you before class on canvas there in pdf format.

351
00:37:12,060 --> 00:37:17,340
You probably don't want to. Well, I'm going to make changes sometimes up until half an hour before class.

352
00:37:17,940 --> 00:37:24,840
They're not serious content changes, but they're always typos. And I hate typos and I just have to change them the minute I see them.

353
00:37:25,080 --> 00:37:28,370
So they will be on canvas.

354
00:37:28,380 --> 00:37:39,270
But I would highly recommend perhaps you download a new version right before class so that we're all on the same page to make this full screen.

355
00:37:48,070 --> 00:37:54,830
So that's. The definition is PDF presentation.

356
00:37:55,550 --> 00:38:00,140
All right? Oh, no. Here we go. Why are you here?

357
00:38:00,320 --> 00:38:05,540
Besides needing three credits toward your master's degree or your Ph.D.?

358
00:38:06,950 --> 00:38:13,729
Well, parametric statistics is sort of a misnomer. You know, we all know we've been working on parameters.

359
00:38:13,730 --> 00:38:19,460
We talk about means and standard deviations and medians and other properties of a population or distribution.

360
00:38:20,720 --> 00:38:24,730
So we really when we say parametric statistics, we really mean a distribution.

361
00:38:24,740 --> 00:38:30,470
We assume that the population has a known distribution because that just makes our lives easier.

362
00:38:30,500 --> 00:38:39,200
If we can collapse the idea of a distribution down to one or two parameters, that really helps us to focus on describing the entire population.

363
00:38:39,890 --> 00:38:46,580
So when we talk about a parametric model, we're really talking about probability distributions that have a fixed number of parameters.

364
00:38:47,720 --> 00:38:51,500
If you're taking a survival analysis this semester through our department,

365
00:38:51,500 --> 00:38:57,649
my department is that survival analysis is well as you move along in that class is considered

366
00:38:57,650 --> 00:39:03,590
semi parametric parametric aspects of survival analysis and non parametric estimates.

367
00:39:03,890 --> 00:39:07,310
So there's non parametric. There's parametric or semi parametric as well.

368
00:39:09,070 --> 00:39:15,309
And you've been doing a lot of parametric statistics most often with the normal distribution for some distribution,

369
00:39:15,310 --> 00:39:20,680
the binomial distribution and genomes, and there's the exponential distribution rate and then all the exponential

370
00:39:20,680 --> 00:39:25,060
families and there's a beta distribution that's useful in Bayesian analysis.

371
00:39:25,900 --> 00:39:27,400
When you're looking at binary data,

372
00:39:29,260 --> 00:39:37,030
we also talk about non parametric in the regression setting where you're modeling the mean and the mean is described by a fixed number parameter.

373
00:39:37,040 --> 00:39:38,690
So that's parametric, right?

374
00:39:38,710 --> 00:39:45,850
We talk about the expected value of Y is a function of covariance and these parameters, these betas that go by multiply by the parameters.

375
00:39:45,850 --> 00:39:54,060
But again, a fixed number is a matter of a sample size zero P versus problem probability number parameters and number of observations.

376
00:39:55,660 --> 00:40:01,690
So what is the opposite of parametric statistics that's being non parametric?

377
00:40:01,840 --> 00:40:06,780
And as it says here, it's difficult to give you a definition of what we're going to do here.

378
00:40:08,290 --> 00:40:12,940
But geometric statistics is basically anything that wasn't in your parametric statistics class,

379
00:40:14,810 --> 00:40:18,730
just when there is a fairly a parametric component to what you're doing.

380
00:40:20,690 --> 00:40:21,040
Again,

381
00:40:21,040 --> 00:40:27,070
a more practical definition is that we try to reduce all the assumptions or remove some of the assumptions that you've made in that are classes.

382
00:40:27,370 --> 00:40:35,620
We don't assume normality. For example, to compare two populations we may not and assume normality of the errors we know.

383
00:40:35,620 --> 00:40:40,720
To assume a linear function of rates, we can assume a smooth function of coherence.

384
00:40:42,790 --> 00:40:46,570
Oh, there's a typo right there. Foremost. All right.

385
00:40:46,990 --> 00:40:50,590
Um. Oh, yes.

386
00:40:51,620 --> 00:40:55,700
These are some of these methods will be scrapped during this course first.

387
00:40:56,420 --> 00:41:01,219
So, again, five topics roughly. The first is non parametric testing.

388
00:41:01,220 --> 00:41:07,610
So that is the classic rate based testing methods that were started by Wilcox and others.

389
00:41:08,510 --> 00:41:15,710
And they're used um, inappropriately I think in a lot of settings.

390
00:41:16,640 --> 00:41:20,330
Then I mostly what I'm going to teach you in this class,

391
00:41:20,330 --> 00:41:26,960
in this non parametric subjective statistics class, is that parametric assumptions are not that bad.

392
00:41:28,160 --> 00:41:32,000
Folks really get excited when I say we have to make any assumptions on anything.

393
00:41:33,620 --> 00:41:37,130
But then it's really hard to explain what's going on, as you'll see in this class.

394
00:41:37,710 --> 00:41:43,370
So the parametric methods are great, but parametric methods are not that shabby for a lot of the things we want to do.

395
00:41:44,090 --> 00:41:50,570
But we're going to learn rigorous testing as an alternative to the hypothesis testing you've learned in other classes.

396
00:41:50,810 --> 00:41:57,560
There's a Chi square test for categorical data. Mr. Fischer, if you've heard of Mr. Fisher, he's going to Fisher's exact test.

397
00:41:58,340 --> 00:42:01,490
That's a rank based test. If you hadn't seen it presented in that way.

398
00:42:02,450 --> 00:42:06,740
And then we're generally rank based tests belong to what are known as permutation tests.

399
00:42:07,310 --> 00:42:14,270
So again, if the null hypothesis is true, is there something we can compute about the data and redo our test?

400
00:42:14,420 --> 00:42:22,880
So for example, in a two simple T test, you have a group membership rate of a continuous outcome in one column and a group one group to the other.

401
00:42:23,810 --> 00:42:30,350
If the null is true, if the two groups have the same distribution, then I can compute the group membership variable.

402
00:42:30,440 --> 00:42:34,280
I can just shuffle those numbers and I can reduce my analysis.

403
00:42:35,240 --> 00:42:37,370
I do that many, many times as a permutation test.

404
00:42:38,600 --> 00:42:45,230
Then we'll move on to estimating non parametric quantities, essentially an alternative to a histogram.

405
00:42:46,310 --> 00:42:51,560
So a histogram is a non parametric method. We made a histogram of the data.

406
00:42:51,570 --> 00:42:55,010
We didn't have to assume a population distribution to make a histogram.

407
00:42:56,600 --> 00:43:04,130
But we don't like histograms because they're very non continuous, very pretty and they, we don't really know how many pens to use and so forth.

408
00:43:04,610 --> 00:43:13,669
So we're going to learn more smooth versions of the histogram through the estimate will move on to non parametric confidence intervals.

409
00:43:13,670 --> 00:43:17,270
So again, inference, we're gonna do something called the bootstrap.

410
00:43:17,870 --> 00:43:21,720
I might cover the jackknife. That makes you have to.

411
00:43:23,780 --> 00:43:27,260
It's an alternative to the bootstrap that existed before the bootstrap.

412
00:43:29,180 --> 00:43:34,400
And the bootstrap when I was in graduate school only 20 some years ago was just

413
00:43:34,460 --> 00:43:38,540
being talked about because we finally had computers that could do the bootstrap.

414
00:43:40,370 --> 00:43:44,629
Yeah, I sound like that old guy who walked barefoot to school uphill both ways.

415
00:43:44,630 --> 00:43:48,950
Right? Back in my day, we didn't have computers to do what you did now.

416
00:43:49,640 --> 00:43:54,710
But the Japanese existed for a long time simply because we didn't have computers to do the bootstrap.

417
00:43:55,310 --> 00:43:59,630
But now the bootstrap is a trivial, for the most part, computational method.

418
00:43:59,960 --> 00:44:07,670
It is a really nice way of computing confidence intervals without having to assume that typical something plus or -1.96 times the standard error.

419
00:44:08,080 --> 00:44:11,840
Right? That's a normal that's a esoteric or normal assumption.

420
00:44:11,840 --> 00:44:18,020
We don't need that anymore to do confidence intervals. And we'll talk about regression again.

421
00:44:18,530 --> 00:44:22,880
How do you fit the relationship of an X variable to a y variable?

422
00:44:24,200 --> 00:44:27,350
If you don't want to assume that a line has to describe that relationship,

423
00:44:27,680 --> 00:44:33,200
if there's some sort of up and down pattern in the data, how can we see the regression model through those data?

424
00:44:33,650 --> 00:44:35,270
So we'll talk about kernel smoothing.

425
00:44:36,140 --> 00:44:43,880
In this context, this is different than the kernel density estimate or in part three lines and then local regression as well.

426
00:44:45,440 --> 00:44:47,990
And then we'll finish off the semester as best we can.

427
00:44:48,020 --> 00:44:57,440
Talking about classification and regression trees, which again are a really cool method that primarily would be done when I was in graduate school.

428
00:44:58,160 --> 00:45:01,220
But I now use a lot of machine learning in a very,

429
00:45:01,610 --> 00:45:09,770
very non parametric way to explain how a series of covariates relate to a binary outcome in classification trees,

430
00:45:10,250 --> 00:45:15,140
you know, regression trees, but also get to ensemble methods in this class.

431
00:45:15,380 --> 00:45:22,610
But we'll see again how our calendar results non parametric versus parametric.

432
00:45:23,060 --> 00:45:28,340
So again, I'm going to go through a little example of each of the five sections five that we just talked about.

433
00:45:29,500 --> 00:45:32,550
So we're in the typical to group comparison.

434
00:45:32,560 --> 00:45:42,160
We've got a continuous outcome, perhaps group one as independent, identical, identical and identical and independently distributed x one through x.

435
00:45:42,160 --> 00:45:47,440
And we come from some distribution that's the x, right?

436
00:45:48,130 --> 00:45:58,060
So it's the definition of the CDF and then the group two outcomes are a sample size m independent observations from another distribution function.

437
00:45:58,330 --> 00:46:04,270
We'll call that X and they have again. So what are we interested in here?

438
00:46:05,620 --> 00:46:09,310
Again, if you think of the context around a clinical trial, perhaps we have two treatments.

439
00:46:09,520 --> 00:46:16,150
We want to compare them and see how people do on each of the treatments. We want to know if the two sides are different from each other.

440
00:46:17,890 --> 00:46:23,950
So we'll save that in some very vague hypothesis language. Here is the null says that the two distribution functions are the same.

441
00:46:24,670 --> 00:46:32,200
Now remember, that's not that's of X really. It's just a mix of an argument of T, what's the probability of being less than or equal to some value?

442
00:46:33,040 --> 00:46:42,730
The very strong null says that at any value the two sides are the same at the exact same distribution, and the alternative says they're not the same.

443
00:46:43,430 --> 00:46:47,380
That's a pretty general alternative. There are lots of the ways they can't be the same.

444
00:46:48,410 --> 00:46:51,650
It could be a shift. It could be just different at certain cut points.

445
00:46:53,060 --> 00:47:01,150
It's a really hard problem to answer. And so what we do is we say, well, let's think about a specific distribution.

446
00:47:01,160 --> 00:47:09,500
Let's compare it to distributions to each other. The first the most common assumption is that both X and Y are normal distributions.

447
00:47:09,680 --> 00:47:14,570
Because if we do that, we don't have to estimate the entire distribution of each group.

448
00:47:15,020 --> 00:47:17,120
We just need the mean and the standard deviation.

449
00:47:17,870 --> 00:47:23,260
And then we made another assumption that the two groups have the same standard deviations, make our life even more easy.

450
00:47:23,970 --> 00:47:29,220
But there is a t test for unequal variances. But for the most part we've got three parameters.

451
00:47:29,230 --> 00:47:34,630
Now. We've taken an infinitely dimensional problem here, and we've planted into three parameters.

452
00:47:35,800 --> 00:47:40,480
And so now because we've assumed normality and they have the same variants.

453
00:47:40,600 --> 00:47:44,650
The only thing that's different between the two distributions are their means.

454
00:47:45,730 --> 00:47:49,830
And so we do a t test. We can say the means are equal versus they're not equal on a two sided test.

455
00:47:49,980 --> 00:47:53,140
And then there's the two. The standard T statistic.

456
00:47:54,070 --> 00:48:01,840
One of the nice things about parametric tests is that often the test statistic is based upon the statistics we're interested in.

457
00:48:03,310 --> 00:48:06,910
And I like to look at what is the mean of one group, what is the meaning of the other?

458
00:48:07,180 --> 00:48:15,350
Those are numbers I typically look at before I do a T test. Those parametric tests don't really have those because we're not estimating parameters.

459
00:48:15,350 --> 00:48:23,230
Remember, X bar is an estimate of a parameter use, but if we get rid of parameters, what is this test statistic useful for?

460
00:48:23,470 --> 00:48:27,610
So that is one of the challenges that we'll get into in this quest. But again, hopefully, you know,

461
00:48:27,610 --> 00:48:33,129
all of us by now there's the classic to sample t test when we don't have the same variance in

462
00:48:33,130 --> 00:48:40,930
both groups as a T distribution with the total sample size of M plus and minus two parameters.

463
00:48:41,470 --> 00:48:48,490
And it looks like a normal distribution, except that the tails are heavier as the sample sizes go off to infinity,

464
00:48:48,850 --> 00:48:57,219
a p distribution becomes a normal distribution so many times if the sample sizes are large enough, our critical value is 1.96.

465
00:48:57,220 --> 00:49:04,090
We don't need to know the degrees of freedom. The degrees of freedom are only important when I want to use the T distribution.

466
00:49:06,970 --> 00:49:12,100
So just again, notice that the no distribution of my test statistic one requires a parameter assumption

467
00:49:12,340 --> 00:49:17,020
that both means of a normal distribution because the original population's good.

468
00:49:18,220 --> 00:49:24,190
Again, that's exact. If my data are normally distributed and if NW and are large enough.

469
00:49:24,610 --> 00:49:26,889
Again, if we didn't have the central limit theorem,

470
00:49:26,890 --> 00:49:32,080
there's a whole lot of stuff we couldn't do research on zero that says that I don't need normally distributed data.

471
00:49:32,770 --> 00:49:38,930
I just need sample sizes that are large enough. For me to use my morality and my hypothesis testing.

472
00:49:39,860 --> 00:49:45,379
But normality also plays an implicit role in the formulation of a hypothesis test because

473
00:49:45,380 --> 00:49:49,940
a difference in the distributions is fully described by the difference in the means.

474
00:49:50,450 --> 00:49:56,990
We don't care about any other feature of those distributions. So we need to look at so normality kind of comes up twice here.

475
00:49:57,860 --> 00:50:02,600
So we're going to do away with normality when we want to compare two groups. That's what we're going to do first in this class.

476
00:50:04,020 --> 00:50:08,490
So you say that the parametric tests are often distribution free.

477
00:50:09,420 --> 00:50:13,710
You don't need to make assumptions about the distributions of the underlying data.

478
00:50:14,700 --> 00:50:21,120
And this is why when I work with individuals who don't have a statistical background, that sounds really awesome.

479
00:50:22,380 --> 00:50:26,460
You mean I can do this any time I want? I don't need to worry about normality.

480
00:50:26,790 --> 00:50:30,630
We spend a lot of time on statistics, and I like to preach sometimes.

481
00:50:31,500 --> 00:50:32,970
Some of my colleagues will disagree with me.

482
00:50:33,420 --> 00:50:37,980
We spend a lot of time teaching people introductory statistics, and we talk about the assumption of normality.

483
00:50:40,050 --> 00:50:46,080
And the minute someone worries about that, they think they can't do all the stuff we taught them.

484
00:50:47,100 --> 00:50:50,970
As I just said, the central limit theorem is incredibly powerful.

485
00:50:51,870 --> 00:50:55,380
You don't need normally distributed data to do a lot of the things you've learned already.

486
00:50:55,920 --> 00:51:00,510
You just need enough data for the convergence of the central limit to apply.

487
00:51:01,380 --> 00:51:07,860
So again, people will look at histograms of data, right?

488
00:51:07,890 --> 00:51:11,940
You get a histogram of 20 people and they say, well, that doesn't look normal, Tom.

489
00:51:11,940 --> 00:51:16,140
It doesn't look symmetric. I don't want to use the T test. And I'm like, Why not?

490
00:51:17,010 --> 00:51:20,370
Because we were told we have to have normal distributed data in order to do a T test.

491
00:51:21,830 --> 00:51:26,450
How do you know that histogram didn't come from normally distributed data? It never gets a sample.

492
00:51:28,140 --> 00:51:33,540
It's really hard to prove that data did not come from a normal distribution.

493
00:51:33,780 --> 00:51:37,380
There is a test. I know there's an the of normality. Some of you have probably learned it.

494
00:51:38,640 --> 00:51:45,790
It's horribly non powerful. And so you'll get a p value of like point six.

495
00:51:46,720 --> 00:51:53,840
It wasn't in the nor was true. It just means you don't have enough data to figure out whether the alternative is right.

496
00:51:54,410 --> 00:51:57,700
So, again, a lot of folks will do a test of normality on a set of data,

497
00:51:57,710 --> 00:52:03,230
get a big P value or a small P value on a cell to do a non parametric test and so forth.

498
00:52:04,700 --> 00:52:07,840
Even though I'm teaching you in this class how to do a non parametric test,

499
00:52:07,850 --> 00:52:13,640
I don't use them that often because normality is often fine in large samples of data.

500
00:52:13,760 --> 00:52:19,370
All right. But if we do deal wilcoxon tests, they're based on the ranks.

501
00:52:20,750 --> 00:52:25,700
So again, we're going to rank order the data related to order statistics, which you've probably seen in your theory classes.

502
00:52:26,210 --> 00:52:33,140
So we can rank the data under the ANOVA hypothesis that the two distributions are the same.

503
00:52:33,740 --> 00:52:41,930
So again, conceptually, I think it's very easy to understand if I combine both groups into one set of data and I rank them from smallest to largest,

504
00:52:43,370 --> 00:52:46,980
I shouldn't see higher ranks in one group versus the other.

505
00:52:47,030 --> 00:52:49,490
If they're the same, they should be equally distributed.

506
00:52:49,850 --> 00:52:56,380
If I see one group with lots of high ranks or low ranks and the other group not, that tells me there's a difference right here.

507
00:52:56,420 --> 00:53:03,560
Conceptually, I think very easy. We'll talk about the continuous need for the distributions as well.

508
00:53:04,850 --> 00:53:11,510
And again, the statements of hypothesis tests for non frame rates are just don't talk about any parametric assumptions.

509
00:53:12,110 --> 00:53:18,860
But again, what do we mean when this CDF is greater than or equal to this CDF?

510
00:53:19,670 --> 00:53:22,970
The mean at every possible value at some values.

511
00:53:23,450 --> 00:53:28,190
Again, the hypothesis hypotheses get a little bit fuzzy here.

512
00:53:30,750 --> 00:53:35,820
Although non parametric tests give us the ability to stay away from parametric assumptions.

513
00:53:36,100 --> 00:53:45,660
There is no free lunch in statistics. If you want to give up the assumption of a parametric distribution, you might lose some statistical power.

514
00:53:46,810 --> 00:53:51,170
And if the data are truly normally distributed, you should be using a T test.

515
00:53:52,530 --> 00:53:55,830
If you use a wilcoxon rank some test, you're going to lose some statistical power.

516
00:53:58,490 --> 00:54:00,890
We're going to examine how much power we're talking about here.

517
00:54:02,450 --> 00:54:05,900
Again, if parametric assumptions are not correct, this is what people worry about more.

518
00:54:06,500 --> 00:54:11,510
Well, if I don't have normally distributed data and I use a T test, what goes wrong?

519
00:54:11,990 --> 00:54:15,310
That's usually what happens is you're type one error is inflated.

520
00:54:16,750 --> 00:54:21,750
You might reject and all that, but this is when you shouldn't be moral and you might, might, right?

521
00:54:23,070 --> 00:54:28,400
So those are the tests we're going to talk about now.

522
00:54:28,440 --> 00:54:34,200
Parametric estimation of a distribution. The title should be a little more specific.

523
00:54:34,710 --> 00:54:37,050
So again, our with one group, we just have one group of data.

524
00:54:38,030 --> 00:54:42,829
And we have an observations which again we assume to be independent and identically distributed.

525
00:54:42,830 --> 00:54:48,860
They all have the same distribution function F of X, and now we want to estimate the entire distribution,

526
00:54:49,070 --> 00:54:53,210
not just the mean or the median, but, you know, what does it look like?

527
00:54:53,690 --> 00:55:01,549
This again, it's a smooth version of a histogram. So in a parametric approach to estimating the distribution, again, we make an assumption,

528
00:55:01,550 --> 00:55:05,629
Oh, it's normal distributed to all I need to know is the mean and the variance.

529
00:55:05,630 --> 00:55:08,690
Once I know that, I can describe the whole L-shaped curve.

530
00:55:10,250 --> 00:55:18,290
And other distributions the same aspects. And so that's in a non parametric approach.

531
00:55:18,290 --> 00:55:22,490
Second bullet point here, we want to characterize the entire distribution function.

532
00:55:22,970 --> 00:55:28,100
So we have to estimate everything about that distribution that the specific quintiles or the mean or whatever.

533
00:55:29,210 --> 00:55:34,940
And so the most common approach and you have seen this, even though you may not have thought about it,

534
00:55:35,870 --> 00:55:42,110
but the simplest estimation for the CDF is the empirical distribution function.

535
00:55:42,800 --> 00:55:47,510
And it simply says for every observation. Coming up, how not?

536
00:55:47,540 --> 00:55:51,320
How many are values in the data or less than any given value?

537
00:55:52,760 --> 00:56:01,750
Historically, zero. And it goes up to one as you, as s t as T becomes every observation in the data set.

538
00:56:01,760 --> 00:56:06,410
The thing was up by a step, and that's what all that language says there.

539
00:56:08,320 --> 00:56:13,510
So again, the empirical distribution function is a discrete distribution function and it could be thought

540
00:56:13,510 --> 00:56:18,430
of as an estimate having the parameters every time we have an observation of the dataset.

541
00:56:18,640 --> 00:56:25,420
This thing changes in parameters, so the number of parameters changes with the number of observations in the dataset.

542
00:56:26,110 --> 00:56:30,640
That's not usually something that's that ideal. Again, it's a step function.

543
00:56:30,880 --> 00:56:38,020
It goes up every time we have a new observation and the height of each step is proportional to how many observations equal that value.

544
00:56:38,950 --> 00:56:46,269
And here's the CDF, right? So I have a dataset of ages security zero.

545
00:56:46,270 --> 00:56:55,000
We go to age 90. You can see that there are no observations in the dataset until we get to about age 17 and that goes up by one over ten.

546
00:56:56,020 --> 00:56:59,260
Somebody has a set of values and so forth.

547
00:56:59,720 --> 00:57:07,060
Sometimes, sometimes the steps are one over N, sometimes they're two over in, depending upon if two people have the same age.

548
00:57:07,740 --> 00:57:12,110
Okay, let's go that way. This is the cumulative distribution function.

549
00:57:12,130 --> 00:57:16,510
This is not the density. If the zero goes to one.

550
00:57:19,250 --> 00:57:30,370
But do you like school functions? I'd like to see something that takes all of these jumps and makes it look visually like it's just one of.

551
00:57:31,520 --> 00:57:36,290
And that's what, Colonel, this estimation is for. Here's the formula.

552
00:57:36,380 --> 00:57:40,550
We'll get more comfortable with later in the semester. But it looks like the histogram.

553
00:57:42,710 --> 00:57:45,770
What I'm saying is I want to get to the density.

554
00:57:45,770 --> 00:57:49,669
Now we're estimating the density that the CDF. At any value.

555
00:57:49,670 --> 00:57:58,819
T I'm going to average across a neighborhood of values around t How many of my observations are close to ten?

556
00:57:58,820 --> 00:58:03,200
In order for me to estimate the density at ten, I want to see how much data there is at ten.

557
00:58:04,260 --> 00:58:08,820
And the further an observation is from ten, the less weight it plays in my computation.

558
00:58:10,110 --> 00:58:13,500
Excuse me. Each event is called the bandwidth.

559
00:58:15,940 --> 00:58:21,730
James called the kernel function. So we're gonna look at different kernel functions at different bandwidth.

560
00:58:21,940 --> 00:58:25,040
Again, a colonel is nothing more than a distribution.

561
00:58:25,060 --> 00:58:28,540
So think of a normal distribution. That's the colonel shape.

562
00:58:30,340 --> 00:58:35,140
And that tells me again at the middle of the middle of the normal distribution.

563
00:58:36,190 --> 00:58:40,720
Any observations close to the middle should supply lots of information about the density.

564
00:58:41,290 --> 00:58:47,710
As that estimations move further away from TI, they get less weight, the value of the normal distribution.

565
00:58:48,730 --> 00:58:53,260
And again, eventually, if the kernel is finite, there are some observations that aren't included at all.

566
00:58:53,980 --> 00:59:02,740
So again, very hand way, the explanation of an entire section of this class, but it attempts to produce a continuous estimate of the density.

567
00:59:03,730 --> 00:59:08,680
Of course, once we get a continuous estimate of the density, we can get a continuous estimate of the KD as well.

568
00:59:08,980 --> 00:59:14,470
But here's an example again of those ages. So I need a histogram of the ages in the dataset.

569
00:59:16,120 --> 00:59:23,290
And then I did a kernel density estimator of it. And again you can see it generally follows the pattern of the histogram and it says,

570
00:59:23,290 --> 00:59:27,370
well, maybe it's bimodal, maybe there's another volunteer later on in the data.

571
00:59:28,240 --> 00:59:33,959
I could make a parametric assumption. And I say, well, ages are positive, they can't be negative.

572
00:59:33,960 --> 00:59:40,740
So I need a distribution that only takes positive values when I'm in parametric approaches the low level distribution.

573
00:59:41,710 --> 00:59:46,510
Once more, I assumed above normal distribution. All I need is the mean and the variance of that thing.

574
00:59:47,260 --> 00:59:51,790
So I use the data to estimate those two parameters what goes in.

575
00:59:51,970 --> 00:59:58,240
And that's what the yellow is. Down below normal is a unit model distribution.

576
00:59:58,660 --> 01:00:06,310
So by design, by assuming law of normality, this bump here is going to get taken out by that parametric assumption.

577
01:00:08,220 --> 01:00:12,060
And do I want to believe this? Is this over?

578
01:00:12,060 --> 01:00:15,270
Smooth. I could smooth this bump out to Col.

579
01:00:15,270 --> 01:00:17,850
Does this make sense? We'll get into that as well.

580
01:00:20,850 --> 01:00:28,860
And again, once I have the PDF density, I can get the CDF and so corresponding to the the curves I just showed you a second ago.

581
01:00:30,120 --> 01:00:36,590
Here's the empirical distribution function. I showed you earlier steps and here's the kernel density estimate.

582
01:00:36,600 --> 01:00:41,790
You can see it's just trying to fit a smooth curve for the histogram result essentially.

583
01:00:42,780 --> 01:00:45,150
And then a lot of normal looks so.

584
01:00:46,280 --> 01:00:54,200
And here's where that bump is in the distribution as a lot of normal goes above because of that smoothing out that bump.

585
01:00:57,060 --> 01:01:04,799
That's what women do there. Confidence intervals. And we're trying to create a world in which P values become less prevalent and

586
01:01:04,800 --> 01:01:09,320
perhaps confidence intervals speak louder than P values for some of their.

587
01:01:09,990 --> 01:01:13,740
But we often want to know about an estimate.

588
01:01:15,000 --> 01:01:20,440
So now we are kind of back in a parametric world. Got a parameter here.

589
01:01:20,480 --> 01:01:26,680
Now we've got an estimate of it and we say, you know, perhaps that is in its sampling distribution.

590
01:01:26,680 --> 01:01:32,860
It's centered around the right thing, but it's got some spread around that depending upon how much data I have,

591
01:01:33,490 --> 01:01:39,639
some some variance estimate is a function of n square root of the variance.

592
01:01:39,640 --> 01:01:46,280
We get the standard error. As you move out in the world, this is going to be one of the most problematic terms in your life.

593
01:01:47,470 --> 01:01:51,400
People don't understand what standard error is relative to standard deviations.

594
01:01:52,090 --> 01:01:55,220
Know who came up with this word, these two words. Right.

595
01:01:56,120 --> 01:01:59,960
The folks who ask me, how should we report the standard deviation or the standard error?

596
01:02:01,550 --> 01:02:06,330
Well, what do you want to talk about? Standard deviation describes the population.

597
01:02:07,550 --> 01:02:10,690
The standard error describes the sampling distributions.

598
01:02:11,430 --> 01:02:17,760
Right. Two very different things. And standard bearer is the standard deviation of the sampling distribution.

599
01:02:18,000 --> 01:02:21,470
Isn't that fun? Typically we assume normality.

600
01:02:21,480 --> 01:02:27,740
We get a symmetric confidence interval to take the estimate and we add or subtract about two standard errors.

601
01:02:28,670 --> 01:02:32,420
And that's what happens when we everything is normal, even with the central limit there.

602
01:02:34,540 --> 01:02:40,180
So maybe we can get away from assuming any sort of normality when we get this confidence interval.

603
01:02:41,670 --> 01:02:49,140
But again, right. If our parameter estimate, if a parameter was the mean, a parameter estimate is the sample mean.

604
01:02:49,590 --> 01:02:56,430
And so the central limit theorem applies when the data aren't normally distributed, things are exact.

605
01:02:56,430 --> 01:03:03,299
When the data are normal. The variance of my parameters some the index bar is simply the population of variance divided by the sample

606
01:03:03,300 --> 01:03:09,450
size and the standard error is simply right square with that with an estimate of sigma plugged into a.

607
01:03:11,170 --> 01:03:14,410
You've learned a lot about maximum likelihood estimation. Right.

608
01:03:15,130 --> 01:03:22,330
So of is that any parameter that's and Emily also asymptotically is a normal distribution around the right value.

609
01:03:22,810 --> 01:03:29,500
Right. And the variance is proportional to the information contained in the data, the Fisher information.

610
01:03:30,820 --> 01:03:40,630
Right. But again, that either requires assumption of theory, assumption or esoteric closely.

611
01:03:42,120 --> 01:03:46,589
Or even requires you to know what the sampling distribution might be.

612
01:03:46,590 --> 01:03:47,400
Asymptotically.

613
01:03:47,820 --> 01:03:55,799
There are lots of parameter estimates out there for whom the living distribution is not normal or doesn't have the typical normal root,

614
01:03:55,800 --> 01:03:59,190
square root and convergence rate.

615
01:03:59,760 --> 01:04:03,300
So we could use equation two for a confidence interval,

616
01:04:04,080 --> 01:04:11,220
but figure out what the standard error is can be computationally challenging if you want to work it out on a piece of paper.

617
01:04:13,560 --> 01:04:21,780
And the problem with the bootstrap is it has allowed folks to not have to go through that horrible theory because also always a great experience.

618
01:04:21,780 --> 01:04:30,690
Right. But especially those of you doing a Ph.D., you may end up in a world in which getting standard error formulas proves very, very difficult.

619
01:04:31,620 --> 01:04:34,830
And the bootstrap can give us a solution by using the data.

620
01:04:35,520 --> 01:04:39,610
It doesn't require some of X or any sort of formulas. It's not perfect.

621
01:04:40,320 --> 01:04:50,020
The bootstrap does have its limitations. But it does not rely on any parametric assumptions and as it says here, can be applied in almost any context.

622
01:04:51,160 --> 01:04:54,580
Almost any context. It is not perfect.

623
01:04:54,610 --> 01:05:00,180
There are situations in which the bootstrap will fail to give appropriate coverage of the confidence level.

624
01:05:01,230 --> 01:05:08,610
But what does the bootstrap do? And hopefully some of you have seen this or a general flavor of the bootstrap, but here are the original data.

625
01:05:09,120 --> 01:05:12,480
Five Observations. Right. And let's take the mean.

626
01:05:12,570 --> 01:05:22,460
The mean is the parameter estimate. That's theta hat. The Bootstrap says, remember, what is the concept of a confidence interval?

627
01:05:22,470 --> 01:05:25,920
It's based upon repeated samples from the population.

628
01:05:27,760 --> 01:05:30,870
And this is what throws off a lot of people who don't take a lot of classes.

629
01:05:30,880 --> 01:05:39,450
What do you mean? I got my data. I don't understand what you mean by repeated sampling, but that's what we mean by sampling distribution.

630
01:05:39,960 --> 01:05:46,680
If many, many, many of us get samples from the same population in the same way, all of our estimates form a distribution.

631
01:05:47,940 --> 01:05:56,160
The bootstrap says if my data are a valid representation of the population, albeit small.

632
01:05:57,640 --> 01:06:04,990
Then why can't they just do multiple sampling? Repeated sampling from my data in parameter estimates every time.

633
01:06:05,710 --> 01:06:09,360
And this will form a sampling distribution on my original estimate or.

634
01:06:10,610 --> 01:06:17,030
That is the concept of a bootstrap. The important part is that the sampling is done with replacement.

635
01:06:17,570 --> 01:06:25,430
So each of these observations I pull one out. It appears in my bootstrap sample, but then it goes back in and can be drawn again.

636
01:06:27,470 --> 01:06:31,730
So again, there are some very difficult theoretical concepts in there with strip.

637
01:06:33,520 --> 01:06:39,130
Why can't I have an observation as the same observation appear multiple times in a data set?

638
01:06:39,790 --> 01:06:43,330
Yet I can still treat them as an idea. Sample from the population.

639
01:06:45,650 --> 01:06:51,210
That should make you scratch your head a little bit. That is a very difficult theoretic problem.

640
01:06:51,240 --> 01:06:52,700
There's a whole textbook on the bootstrap.

641
01:06:53,120 --> 01:07:02,210
So the question is a powerful and wonderful idea whose theory is very, very difficult but has solved a lot of computational problems over the years.

642
01:07:03,170 --> 01:07:11,989
So again, like I said in here, each of these is a bootstrap sample from the original data from each of the bootstrap samples I computer.

643
01:07:11,990 --> 01:07:19,700
But those four values form a bootstrap distribution for my parameter estimate.

644
01:07:20,420 --> 01:07:30,980
And the standard error for my or my my parameter estimate is simply the observed standard deviation of these four observations.

645
01:07:32,390 --> 01:07:38,440
So now I have a parameter estimate 25. I have a standard error of .116.

646
01:07:38,450 --> 01:07:41,840
I needed no theory and he didn't know asymptotes.

647
01:07:43,550 --> 01:07:50,000
And now I have a confidence interval. I can say I'm going to take my parameter estimate plus or minus two senators.

648
01:07:52,780 --> 01:07:54,580
No again this assume normality.

649
01:07:55,680 --> 01:08:04,090
The future of distribution is usually quite symmetric so that this is a valid approach we would never do for bootstrap samples.

650
01:08:05,560 --> 01:08:11,470
We would do many bootstrap samples and in fact the number of bootstrap replications is two N minus one choose n,

651
01:08:12,670 --> 01:08:15,850
which is not a small number for even like an equals ten.

652
01:08:17,170 --> 01:08:24,069
Bootstrap replication is massive, so we don't typically try and get all of the observations in the sample.

653
01:08:24,070 --> 01:08:26,080
We get a monte Carlo sample of them.

654
01:08:27,700 --> 01:08:36,640
The cool thing about the bootstrap, however, is remember that only do these four observation, these four values here help me compute a standard error.

655
01:08:37,630 --> 01:08:38,920
They give me a distribution.

656
01:08:40,990 --> 01:08:49,750
And so across many, many bootstrap samples, I don't just use the standard deviation from this distribution to get a confidence interval.

657
01:08:50,110 --> 01:08:58,420
I can look at the sampling distribution itself itself and say 0.2. 5% in 97.5.

658
01:08:59,470 --> 01:09:07,000
There are my two endpoints for the confidence interval from the bootstrap distribution, not necessarily plus or -1.96.

659
01:09:08,290 --> 01:09:13,660
But again, you can see the bootstrap distribution is pretty, pretty symmetric around the parameter.

660
01:09:15,070 --> 01:09:17,500
So here's an example of his inability to get a confidence interval.

661
01:09:17,500 --> 01:09:23,110
So the confidence interval would be this number and this number and the bootstrap values.

662
01:09:23,800 --> 01:09:29,890
And in the distribution that we computed, I will do a lot more of that in the class.

663
01:09:31,700 --> 01:09:41,330
Uh, not parametric regression. So linear regression is a really, really useful tool except when linearity starts to be a problem.

664
01:09:42,140 --> 01:09:46,490
So again, we have one outcome, one for it.

665
01:09:47,660 --> 01:09:53,960
So we're back in simple linear regression world, and we think that these pairs of observations have some association with each other.

666
01:09:54,850 --> 01:10:01,460
And so linear regression again says, Well, they're related to each other and they're related to each other very specifically.

667
01:10:02,670 --> 01:10:10,160
Right. And the observation Y is some fraction of my X value plus an intercept.

668
01:10:10,490 --> 01:10:12,710
And then, of course, there's there's noise around the line.

669
01:10:12,980 --> 01:10:18,080
Every observation isn't exactly equal to better now plus better times of X squared later on.

670
01:10:18,890 --> 01:10:24,950
And so we make a strong assumption here, and we make another assumption to make our lives easier.

671
01:10:24,950 --> 01:10:31,730
We say that the errors are normally distributed. That helps us with inference because it helps us with finding the least squares estimate.

672
01:10:32,870 --> 01:10:39,260
But I take on the question of ah x and y related to each other, and I collapse it into three parameters.

673
01:10:41,190 --> 01:10:46,020
Right. And doesn't matter how many pairs I have, no parameter stays fixed within.

674
01:10:47,580 --> 01:10:55,560
So here's an example. Again, contrived because I took some data and purposely made Y change in a cubic fashion with x.

675
01:10:56,610 --> 01:11:01,629
And so you can see that a line is a horrible. But you can already see visually you probably.

676
01:11:01,630 --> 01:11:07,510
Well, I think actually there's a line here maybe and then maybe it goes up again and then maybe there's some changes here.

677
01:11:08,140 --> 01:11:16,240
There isn't an overall slope. The change in x affecting y maybe is less here and it is here.

678
01:11:16,240 --> 01:11:22,340
Here we can see a much deeper slope. So a line here maybe isn't a good way to go.

679
01:11:22,970 --> 01:11:27,710
So the non parametric counterpart again that in two different ways.

680
01:11:28,520 --> 01:11:35,599
It says the relationship of Y with X is no longer explained through a line except through a smooth function.

681
01:11:35,600 --> 01:11:42,500
M And then we have the errors around M And it also have to be normally distributed.

682
01:11:44,490 --> 01:11:51,990
Again and can be described as a finite number of parameters and there's lots of ways of estimating them.

683
01:11:52,710 --> 01:12:00,210
And as assessor, again, some of these slides are not mine, I would say popular, but it's commonly used.

684
01:12:00,900 --> 01:12:04,800
I put it on a smoothing sports. We're going to talk about some other ways.

685
01:12:05,070 --> 01:12:11,190
Local linear regression, what spines are probably something that you probably have maybe even done in your

686
01:12:11,190 --> 01:12:15,659
classes already and don't really understand where they came from that possible.

687
01:12:15,660 --> 01:12:19,229
Right. So what is the smoothing spine? Cool things.

688
01:12:19,230 --> 01:12:23,430
Fine again says that I'm going to add up.

689
01:12:25,150 --> 01:12:28,450
Functions of X multiplied by parameters theta.

690
01:12:29,080 --> 01:12:33,360
So it looks like a linear regression model, right? We had better times x.

691
01:12:33,660 --> 01:12:45,470
The function of X was x itself. But here we can have many functions as one to MN and a collection of what we call basis functions.

692
01:12:47,450 --> 01:12:58,010
But because we have so many parameters here to estimate email versus the data number, we like the balance between complex models and parsimony.

693
01:12:58,820 --> 01:13:03,230
We don't want to overfed the data. We also want to get a good representation of the data.

694
01:13:04,130 --> 01:13:11,420
And so if you've seen some other penalized regression methods, why am I blanking?

695
01:13:11,450 --> 01:13:18,860
Oh, my God. Which regression? If you've seen any coverage, average regression retrogression is a penalized regression method, as is the less so.

696
01:13:20,760 --> 01:13:25,350
This is also using the same concept of my saying this is the lasso and saying to use the same concept.

697
01:13:25,920 --> 01:13:31,890
And again, what does it say? It says to find these parameters. Again, I want to minimize the sum of squared errors.

698
01:13:32,490 --> 01:13:36,950
This is the observed value. This is what the model says it should be. I want this to be small.

699
01:13:38,680 --> 01:13:42,520
But there's going to be a penalty term. I don't want models that are too complex.

700
01:13:43,420 --> 01:13:50,829
So there is a value lambda multiplied by the sums of all of these functions, of these parameters.

701
01:13:50,830 --> 01:13:54,010
And again, this omega is the curvature in these functions.

702
01:13:54,700 --> 01:13:58,510
Point is, is we want to fit the data well, but we don't want to go crazy.

703
01:13:58,900 --> 01:14:04,180
We don't want it bumping up and down. A perfect function is one that would connect to every X.

704
01:14:05,700 --> 01:14:08,970
That is not useful. But we also don't like our lines.

705
01:14:09,660 --> 01:14:14,190
We want something that is kind of bumpy, but not too bumpy. And that's what this is doing.

706
01:14:16,400 --> 01:14:18,380
The problem is, I don't know what land is.

707
01:14:19,400 --> 01:14:26,990
Lambdas are too many tuning parameter and if lambda is zero we're back to something, something that just looks at the sum of squared errors.

708
01:14:28,010 --> 01:14:33,440
If Lambda is infinity, I think that's putting a lot of weight on this over here.

709
01:14:33,590 --> 01:14:38,959
We're going to get a very flat function. So we'll talk about all of those aspects.

710
01:14:38,960 --> 01:14:40,610
How do we figure out where land is?

711
01:14:41,810 --> 01:14:52,220
There are ways we can not just to pick it out of thin air, but then once we solve that equation, we get parameter estimates and we get this.

712
01:14:52,430 --> 01:15:01,249
Again, this. We add up these small functions and every observation is mean.

713
01:15:01,250 --> 01:15:04,490
That is as a some of these functions added together.

714
01:15:05,840 --> 01:15:12,950
So it looks like a regression type of approach, but one is that it's going to change with the sample size.

715
01:15:13,950 --> 01:15:18,100
Right. So fitting a very complicated spline function with ten observations.

716
01:15:19,250 --> 01:15:22,490
It's going to be different than fitting us one function with a thousand observations.

717
01:15:22,610 --> 01:15:26,060
We can be a little more crazy with a thousand observations.

718
01:15:28,470 --> 01:15:32,940
But the point of all this. Yeah, well, that's a theory that we're not going to cover in this class.

719
01:15:33,570 --> 01:15:38,580
That's for for an entry level class, but for a sufficiently large number of basis functions,

720
01:15:39,600 --> 01:15:43,020
we should be able to estimate the true curve quite accurately.

721
01:15:44,340 --> 01:15:53,130
Again, we're estimating a non parametric function as a sum of a bunch of these known functions.

722
01:15:54,870 --> 01:15:58,440
And it's a really cool concept. How do we how does that work?

723
01:16:00,030 --> 01:16:05,310
But here is a fitted suit explained through the set of data that I showed you earlier.

724
01:16:06,060 --> 01:16:11,670
Instead of a flat line, it figures out that, yeah, things are going up with X,

725
01:16:11,670 --> 01:16:17,490
but then maybe they flatten out a little bit and then they start to increase at a much, much faster rate as we go further.

726
01:16:17,610 --> 01:16:23,460
X of that is. Yeah, that's really nice.

727
01:16:25,960 --> 01:16:31,560
How do you explain this to investigators? I'm not interested in these better heads.

728
01:16:33,450 --> 01:16:39,740
She's better. Hands are simply numbers I multiply by these basic functions in order to get this curve right.

729
01:16:39,750 --> 01:16:45,930
I don't say well beta hat three indicates that there's not a significant beta three.

730
01:16:46,620 --> 01:16:53,660
It doesn't mean anything. So like I said earlier, this is really nice.

731
01:16:54,830 --> 01:16:58,190
But when an investigator comes to you who is familiar with linear regression and says,

732
01:16:58,190 --> 01:17:04,280
Well, can you give me a table of coefficient estimates and p values? No, they don't exist anymore.

733
01:17:05,360 --> 01:17:11,000
We can talk about statistical significance in different ways, but although this is very nice and very useful,

734
01:17:11,840 --> 01:17:16,310
all the things we teach in linear regression classes, some of those go out the window in terms of interest.

735
01:17:17,380 --> 01:17:21,110
You want to deal with that and then classification and regression trees.

736
01:17:22,250 --> 01:17:26,030
It is not 1117. Oh, crap.

737
01:17:26,090 --> 01:17:28,230
Okay. We're going to be done today with this.

738
01:17:29,840 --> 01:17:34,880
The last part of this class is going to cover something that I think is really fun and it's a really cool tool.

739
01:17:35,930 --> 01:17:42,800
It's practicality, yet I think has to be figured out even in these big data kind of applications.

740
01:17:43,370 --> 01:17:49,640
But again, now we have pairs of outcomes, but now again, we're going to look at multiple covariates.

741
01:17:49,910 --> 01:17:56,100
The smoothest one will only smooth over one covariate. Now we're going to get multiple covariates.

742
01:17:56,100 --> 01:18:05,360
So now we're in a multiple regression. And regression trees are used as a way to predict somebodies Y value based upon their covariance.

743
01:18:07,430 --> 01:18:11,240
And so here the regression function is a decision tree rather than a curve.

744
01:18:11,930 --> 01:18:17,329
So again, all the things we teach are collaborators in the introductory classes, all that goes out the window.

745
01:18:17,330 --> 01:18:22,670
There are no parameter estimates at all in terms of classification or regression trees.

746
01:18:23,570 --> 01:18:30,430
So the decision tree, a final prediction from a covered vector, is usually obtained by answering a sequence of yes or no questions.

747
01:18:30,440 --> 01:18:33,680
It is not just usually it is. So here's an example.

748
01:18:35,570 --> 01:18:38,690
Then there is some outcome. There's some outcome in the data set.

749
01:18:39,920 --> 01:18:46,700
But what the data told me through this recursion algorithm is that the outcomes

750
01:18:47,360 --> 01:18:51,050
first are discriminated between each other depending on which arm they're in.

751
01:18:51,950 --> 01:18:56,000
Think of this as a term trial. So they're either in arm or they're not.

752
01:18:56,570 --> 01:18:58,370
This is probably arm. ABLES Yes.

753
01:18:58,910 --> 01:19:12,170
And Matt, for those in arm A, we see a difference in outcomes between those who are under 63 and a half years old and those 63 and a half or higher.

754
01:19:13,220 --> 01:19:18,980
And we see there are 136 people in this group. Their average outcome is 13.95.

755
01:19:19,940 --> 01:19:23,240
And then we have average outcomes for. Again, there's three groups.

756
01:19:23,570 --> 01:19:32,450
The data have told me that individuals have outcomes that can be described by three groups, not in army in arm.

757
01:19:32,450 --> 01:19:35,810
A younger. Arm a older.

758
01:19:36,970 --> 01:19:41,350
That's it. And here's a more complicated tree.

759
01:19:41,980 --> 01:19:46,330
And we're going to talk about how many branches should we have on our tree.

760
01:19:46,690 --> 01:19:50,710
That's called pruning. We can make the tree more complex or less complex.

761
01:19:51,400 --> 01:19:57,610
So here's an example of a variable start that turned out to be the best discriminator between two groups.

762
01:19:59,020 --> 01:19:59,228
And that.

