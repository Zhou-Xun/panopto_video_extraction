1
00:00:00,420 --> 00:00:03,810
There we go. All right.

2
00:00:04,940 --> 00:00:09,270
You have a homework assignment. Do on Wednesday at the beginning of class.

3
00:00:10,800 --> 00:00:15,870
You then have another homework assignment, do two weeks from this coming Wednesday.

4
00:00:16,860 --> 00:00:20,160
And then you have an exam that following Friday.

5
00:00:22,280 --> 00:00:25,849
Um, no class on that Friday.

6
00:00:25,850 --> 00:00:34,430
If, again, if you want to use this time to take the test, that's perfectly up to you because that's what we would normally do with an in-class exam.

7
00:00:35,870 --> 00:00:39,680
I am programing it right now. It is all on canvas.

8
00:00:40,760 --> 00:00:45,560
It is a lot of multiple answer type questions.

9
00:00:46,460 --> 00:00:50,690
So it is not multiple choice. It isn't. Pick one of the following as the right answer.

10
00:00:50,690 --> 00:00:53,810
It is pick any of the following. That might be the right answer.

11
00:00:57,470 --> 00:01:01,790
Canada's programs this very poorly. If you've ever taken these sorts of things,

12
00:01:02,660 --> 00:01:11,990
it either gives you all or nothing or it will give you credit for a right answer and penalize you for the wrong answer,

13
00:01:13,370 --> 00:01:22,610
which both make people unhappy. So I have programed it to give you all or nothing, and then we will go back in and figure out the personal credit.

14
00:01:23,090 --> 00:01:26,419
So I like to do the multiple answer, multiple choice questions,

15
00:01:26,420 --> 00:01:31,190
because it's easier to give you personal credit than it is to one answer, right or wrong.

16
00:01:31,370 --> 00:01:39,530
So I'm trying to do this to help you get points, not to try and take as many points away from me as I possibly can.

17
00:01:41,090 --> 00:01:44,450
The exam. I don't even like to call it an exam.

18
00:01:45,320 --> 00:01:50,330
It is 90 minutes. It is conceptual. There might be one computation question on it.

19
00:01:51,020 --> 00:01:54,050
I don't give you a devastating ask you to analyze and answer questions.

20
00:01:54,620 --> 00:02:01,670
It is concepts that we cover in the lectures that you can figure out the answers to in a handful of minutes.

21
00:02:01,700 --> 00:02:07,370
There should be about 15 questions. So again, in 90 minutes, that's about 6 minutes of questions.

22
00:02:07,390 --> 00:02:12,020
Some of the questions will take you a second or two. Others might take you a few minutes to think about.

23
00:02:12,590 --> 00:02:17,000
But it is not meant to be this intensive computational nightmare.

24
00:02:17,630 --> 00:02:20,960
So go into that with that that frame of mind.

25
00:02:22,220 --> 00:02:26,720
I don't know if I'm going to have any example questions for you, because I've never done this before in this class.

26
00:02:27,200 --> 00:02:33,230
In any good question, I come up with, I'm going to put on the test. So we'll see what we can do.

27
00:02:33,680 --> 00:02:38,210
But again, it's going to come primarily from what we talk about in class through the PowerPoint lectures.

28
00:02:39,340 --> 00:02:43,990
I'm not going to ask you questions about our, you know, how to use this function and that sort of thing.

29
00:02:44,000 --> 00:02:48,110
It's not a test on our it's a test on conceptual alleged tunnel.

30
00:02:48,290 --> 00:02:54,889
It will only cover the lecture science from this week and next week, which is derived variable sort of concepts,

31
00:02:54,890 --> 00:03:00,350
the introduction and generalized squares, which has to do with correlation matrices.

32
00:03:00,860 --> 00:03:04,009
And then the next exam will move on to new topics. Nothing is cumulative.

33
00:03:04,010 --> 00:03:13,940
There are three of them, three of them. Again, they are meant to be horrifying, but I know what exams to do.

34
00:03:13,940 --> 00:03:18,680
Everybody. They're like I said, it's 90 minutes.

35
00:03:18,890 --> 00:03:24,290
Again, I told you, it's open from Friday at noon until the following Tuesday.

36
00:03:24,590 --> 00:03:31,040
So you have 99, 90 minute block in those days to take the exam, whether it's on Friday, Saturday,

37
00:03:31,040 --> 00:03:38,750
Sunday or Monday or part of Tuesday, should give everybody an opportunity to take it in this open book.

38
00:03:39,090 --> 00:03:48,230
I'm not going to mind. I don't know how I would ever monitor that. Shouldn't need open book attention in class and have the slides available.

39
00:03:50,540 --> 00:03:57,830
And don't cheat. Don't take it with other people and will check time frames and sort of those sorts of things.

40
00:04:00,200 --> 00:04:03,970
So anyways, be smart when you check these exams.

41
00:04:03,980 --> 00:04:08,390
I don't want to have to babysit false questions.

42
00:04:09,130 --> 00:04:13,220
Yes. We can't get here on Friday reading your piece.

43
00:04:13,220 --> 00:04:26,209
We like asking questions. How would folks like to be here to take it with you in your office?

44
00:04:26,210 --> 00:04:30,020
Then we could drop by and ask. Sure. Why don't we go with that deal?

45
00:04:30,350 --> 00:04:34,429
I will be here awaiting your arrival.

46
00:04:34,430 --> 00:04:39,320
So if you take it at that time, what does that do for everybody else?

47
00:04:41,100 --> 00:04:44,160
How are you doing? Let's try that and see what happens.

48
00:04:46,060 --> 00:04:52,470
Again, hopefully you wanted me, but. If there are staff who will figure that out.

49
00:04:53,610 --> 00:04:56,760
I've done this for other classes and nothing has really, really gone badly.

50
00:04:58,210 --> 00:05:05,610
So, yeah, let's go with that. I will be in my office that Friday, even though we're not here in class.

51
00:05:06,450 --> 00:05:12,000
Hopefully it's a rainy day on a Friday afternoon in the rain.

52
00:05:12,060 --> 00:05:15,450
All right. My other questions about the test are the homework assignments.

53
00:05:18,560 --> 00:05:24,980
You're going to do the second homework assignment. You're going to turn it in and you won't have an answer back by the time of the test.

54
00:05:28,210 --> 00:05:33,980
But again, the homework is supposed to have you using are the concepts we cover in class.

55
00:05:34,000 --> 00:05:38,379
So hopefully there isn't a burning issue in the homework that's going to mess up the

56
00:05:38,380 --> 00:05:43,330
questions and we'll try and structure the exam that way so that the homework is not crucial.

57
00:05:43,810 --> 00:05:49,210
But if you want to know something before the exam, I'm happy to come in and straight answer it for you.

58
00:05:49,450 --> 00:05:53,130
You know, I did X, Y and Z. Is that right?

59
00:05:53,140 --> 00:05:59,240
Is that wrong? So. So again, you won't have the second homework assignment before the test.

60
00:06:00,920 --> 00:06:04,550
If that turns out to be a really bad plan, then I'll have to readjust.

61
00:06:05,210 --> 00:06:13,580
But that's how it's going to work. Again, Friday is going to be mostly R.

62
00:06:13,610 --> 00:06:22,909
Whenever possible. Except when I spend a lot of time doing pull over questions or something else and oh damn,

63
00:06:22,910 --> 00:06:29,400
I meant to send you guys the all the songs that you guys said you told me about and suggested, huh?

64
00:06:30,500 --> 00:06:39,650
So let's finish all the lecture slides that I meant to finish on Wednesday and we'll show a little bit as far as well.

65
00:06:40,400 --> 00:06:55,270
So just to review your beginning, so lecture slides, one C talked about this labor pain.

66
00:06:55,400 --> 00:07:00,590
They're the examples that I will use a lot in my slides that no one in this class is analyzing.

67
00:07:02,630 --> 00:07:06,080
The first point about this lecture was, again, to get you comfortable with the data,

68
00:07:06,080 --> 00:07:12,379
but also to show you how I attempted to visually and numerically explore these data with the

69
00:07:12,380 --> 00:07:18,740
hope that perhaps you would take some of that when you analyze your two datasets for the class.

70
00:07:20,080 --> 00:07:23,840
So it's so for instance, spaghetti plots that we talked about last time,

71
00:07:24,410 --> 00:07:37,399
I'm tabulating how much missing data there are over time plots of means across time in the two groups were box spots and then some sort

72
00:07:37,400 --> 00:07:43,100
of measure of what do you think is the correlation of observations on the same subject of basic Pearson's correlation coefficient,

73
00:07:44,420 --> 00:07:51,110
right? So we talked about that, um, I put, and I've been putting the announcements in canvas.

74
00:07:51,110 --> 00:07:57,380
I don't know if you're paying attention to those or know that they're out there trying out to fill up your email inboxes.

75
00:08:02,990 --> 00:08:09,990
So there are a set of announcements there already. I did a crappy job of explaining the probability of a circle.

76
00:08:11,090 --> 00:08:15,630
I handwrote it last night or the other night and sent in an announcement.

77
00:08:15,650 --> 00:08:23,030
My solution, if you're interested. I think I talked about our code.

78
00:08:23,030 --> 00:08:29,360
So again, in our studio cloud, I gave you a link to this activity site, signed me out.

79
00:08:30,540 --> 00:08:35,790
So you should be able to get to it. There is no code or any sort of password you need to get there.

80
00:08:36,420 --> 00:08:40,170
So in my workspace is a bio set 653 folder.

81
00:08:40,800 --> 00:08:45,630
And if you're familiar with our our studio or studio cloud looks exactly the same.

82
00:08:46,740 --> 00:08:53,379
I'm going to have a folder for every homework assignment. And I also have a folder, for example.

83
00:08:53,380 --> 00:09:01,630
So in the labor pane example, I have one file called Explorer which corresponds to the slides with all the exploratory data analysis.

84
00:09:02,260 --> 00:09:05,560
So let's look at that file just a little bit here.

85
00:09:08,260 --> 00:09:13,330
I am not in our expert and I am not presenting you our code as the best possible way to do this.

86
00:09:14,110 --> 00:09:17,740
I am presenting it to you as someone who learned how to use our 20 years ago.

87
00:09:19,180 --> 00:09:24,520
It works. I don't use glass. I tried a couple of times.

88
00:09:25,210 --> 00:09:28,750
It just isn't worth my time to understand all this syntax.

89
00:09:29,110 --> 00:09:35,680
To make the plot I want. All. So if you want to use juju plot and you're used to the all the syntax that is in a lot of,

90
00:09:35,950 --> 00:09:39,730
you know, deep player and all those sorts of libraries go for it.

91
00:09:41,140 --> 00:09:45,880
I can figure out what you're doing. And honestly, this is one reason I want your code and your homework assignments.

92
00:09:47,230 --> 00:09:51,880
I want to learn from you guys. I want to see how someone would do this in a more contemporary way.

93
00:09:52,750 --> 00:09:59,410
The analysis, I think, is going to be the same, but the visual stuff, you know, you folks are a lot more savvy than I am nowadays.

94
00:10:00,640 --> 00:10:04,360
But so I have code in here that shows you how I made the spaghetti plots.

95
00:10:05,710 --> 00:10:08,170
Again, there's probably a smarter way to do this by now,

96
00:10:08,800 --> 00:10:15,610
but I just do it for a loop and make one line at a time for every person and so forth, and save it as a PDF and so forth.

97
00:10:16,060 --> 00:10:20,590
So there's some code there if you want to use it. If you have a better way of doing it, go for it.

98
00:10:21,610 --> 00:10:23,740
And then today, we're going to talk about drive variables.

99
00:10:24,520 --> 00:10:30,850
So how exactly do I analyze a set of longitudinal data when in fact, I don't know how to analyze longitudinal area?

100
00:10:31,930 --> 00:10:33,150
So we're going to use drive variables.

101
00:10:33,160 --> 00:10:43,210
So again, I have a set of code there that shows you how the things I'm going to show you next in my PowerPoint slides were generated.

102
00:10:43,330 --> 00:10:46,570
So all the codes there, you're welcome to use it.

103
00:10:49,540 --> 00:10:55,149
And you have to do this, you know, the grain of salt. You're going have to change the dataset name and things like that.

104
00:10:55,150 --> 00:11:02,210
But again, go for it as you see fit in the labor playing data are there.

105
00:11:02,290 --> 00:11:06,820
So if you want to load them, let's see how this works. Again, you're welcome to do that.

106
00:11:09,830 --> 00:11:13,370
Let's go. We're going to try variables.

107
00:11:13,400 --> 00:11:18,230
This is the whole point of this first of this first set of lecture slides.

108
00:11:20,540 --> 00:11:24,230
It's. This is currently.

109
00:11:24,740 --> 00:11:27,120
Yeah. All right.

110
00:11:30,460 --> 00:11:38,590
So what can we do with longitudinal data at this point before we get into things like generalized estimating equations or random effects?

111
00:11:41,080 --> 00:11:45,010
As I told you, when you enter data, you already knew how to do a derived variables approach.

112
00:11:45,670 --> 00:11:51,190
Instead of you analyzing the distinct pairs of observations, we took the differences within person first,

113
00:11:51,520 --> 00:11:58,750
and so we analyzed the differences that led to one observation for a subject for individual, and then we could use the T test.

114
00:11:59,930 --> 00:12:04,280
All right. So. It's not perfect.

115
00:12:04,910 --> 00:12:09,950
You know, it forces the data to fit your method retail box.

116
00:12:09,950 --> 00:12:13,100
You should use the method that goes with the data and the way it works.

117
00:12:13,160 --> 00:12:20,569
They were collected, but it is a really nice way to get an idea if something is going on in the data before you track it.

118
00:12:20,570 --> 00:12:23,120
More complicated start. Always start simply.

119
00:12:23,120 --> 00:12:30,679
I don't care what class you're selling or what method you're using, you should start simply t testing chi square tests,

120
00:12:30,680 --> 00:12:36,680
although not exciting, are really useful in a lot of settings before we do anything more complicated.

121
00:12:37,610 --> 00:12:41,660
So it's a quick and dirty way to get an idea of what's going to happen if we try to be more complicated.

122
00:12:43,660 --> 00:12:49,660
So let's try and think of ways of analyzing the labor pain data to see if in fact

123
00:12:49,660 --> 00:12:53,230
there are differences over time in how these women are experience experiencing pain.

124
00:12:53,260 --> 00:12:58,059
Remember again, there was a control group, an inactive treatment group and some medication for pain,

125
00:12:58,060 --> 00:13:02,440
and they were observed every half hour, we think, until they delivered the baby.

126
00:13:03,730 --> 00:13:08,120
So the first thing I want to do is, again, we don't have the baseline measurement.

127
00:13:08,160 --> 00:13:11,800
The weird thing about these data is, I guess a baseline, they're all at zero.

128
00:13:12,010 --> 00:13:16,420
Well, they're going into labor. I'm curious why there wasn't a time zero.

129
00:13:16,600 --> 00:13:22,270
But the first measurement we have is at time 30 minutes. So we're going to treat that as sort of a starting point for every woman.

130
00:13:22,690 --> 00:13:26,350
And then I'm going take every future time point and look how different it is from where they started.

131
00:13:28,000 --> 00:13:31,990
So I have a series of differences for every woman in each of the two groups,

132
00:13:32,470 --> 00:13:40,330
and I'm going to do a series of tests that is the difference from 60 to 30, significantly different between the groups.

133
00:13:40,600 --> 00:13:46,510
Is there a difference from 90 minutes to 30 minutes? Is that different between the two groups and do that repeatedly?

134
00:13:46,750 --> 00:13:50,330
So a bunch of t test here. Two simple t test separately test.

135
00:13:50,500 --> 00:13:55,300
Just remember two groups. They get one observation per person comparing two groups.

136
00:13:55,870 --> 00:13:59,200
So that's a two sample t test. And here's what I got.

137
00:14:00,340 --> 00:14:03,340
So we've got our one, one and a half all the way to three and a half.

138
00:14:03,550 --> 00:14:09,520
I've tabulated how many people were in each group again, because there is a decent amount of dropout in the study,

139
00:14:09,520 --> 00:14:13,720
because we think the women are giving are giving birth to their children.

140
00:14:14,200 --> 00:14:21,550
I have tabulated the average change from 30 minutes to that time point in both the treatment group and the control group.

141
00:14:22,580 --> 00:14:25,050
I listed the standard error of the difference,

142
00:14:26,650 --> 00:14:33,070
and then you take the difference divided by the standard error that gives you a key statistic and we and the computer gives us a p value.

143
00:14:34,000 --> 00:14:38,680
So again, this is necessarily this is a direct variable approach.

144
00:14:39,550 --> 00:14:43,630
One of the drawbacks here, however, those are six p values. Right?

145
00:14:43,990 --> 00:14:48,790
You can talk about multiple comparisons, right? The overall type one error rate probably isn't 5%.

146
00:14:49,360 --> 00:14:55,360
And this is all exploratory. But there are strong indications here that there really are no differences

147
00:14:56,020 --> 00:15:00,710
until we get to about 2 hours and then things seem to be distinctly different.

148
00:15:00,730 --> 00:15:08,120
We can see that the average pain score in the treatment group is distinctly lower than it is for the women who are natural.

149
00:15:10,540 --> 00:15:15,400
Again, I showed you a picture last time. Hopefully the p values go with the picture.

150
00:15:16,040 --> 00:15:23,800
Right. It shouldn't have gotten an extremely small p value right away. Took a lot for those two those two averages lines to separate from each other.

151
00:15:24,190 --> 00:15:26,500
So those P values go very nicely with that plot.

152
00:15:26,800 --> 00:15:31,330
So I think there is probably a difference between these groups, but it probably doesn't start for a while.

153
00:15:34,870 --> 00:15:40,750
You didn't need my class to do that, right? This is this is just basic regression and t tests.

154
00:15:43,200 --> 00:15:45,029
Let's try and come up with one overall summary.

155
00:15:45,030 --> 00:15:51,630
However, rather than getting a bunch of summaries for every woman every time point from baseline, let's try and summarize.

156
00:15:51,660 --> 00:15:55,980
Each woman was an individual value. You could take the average.

157
00:15:56,430 --> 00:16:00,329
You could just say, you know, I've got this woman, she's got four measurements. I'm going to take her average.

158
00:16:00,330 --> 00:16:06,900
That's her contribution to the data. Usually with longitudinal data, however, we think that things might be changing over time.

159
00:16:08,010 --> 00:16:11,550
So does it really make sense to average a bunch of observations that are changing over time?

160
00:16:12,510 --> 00:16:16,470
A better summary is usually their slope. How much are they changing over time?

161
00:16:17,520 --> 00:16:21,630
But again, you could use an average if you think in fact there isn't going to be much change over the time.

162
00:16:22,320 --> 00:16:27,060
For instance, our Farkle data, I don't believe there's going to be a significant change in people's scores over time.

163
00:16:27,690 --> 00:16:32,580
One way to summarize each individual might be to take their average FARKLE score over the next 12 weeks.

164
00:16:34,290 --> 00:16:36,239
Or you could do something like, well,

165
00:16:36,240 --> 00:16:41,910
I don't I'm just going to take the last time point on the first time point and see how much she has changed in her pain score.

166
00:16:43,920 --> 00:16:46,980
The problem here, though, is that every woman has a different span of time,

167
00:16:47,850 --> 00:16:51,989
so I'm going to scale by how long she was followed and basically getting a slope for just

168
00:16:51,990 --> 00:16:55,500
the first and last points rather than fitting a regression line through all of them,

169
00:16:55,500 --> 00:16:58,680
ignoring all the middle. Does it matter?

170
00:17:00,150 --> 00:17:04,860
We're going to see in a second. It doesn't really matter which way I do this. Here's an example.

171
00:17:04,860 --> 00:17:07,950
Here's woman number 64. So her pain was pretty low.

172
00:17:08,430 --> 00:17:16,410
And then it started to go up. And then at three and a half hours, things are really happening, it looks like, and they're paying that extremely large.

173
00:17:17,520 --> 00:17:21,510
So the first summary of those six dots is to draw a line through them.

174
00:17:21,600 --> 00:17:25,980
Yeah, the computer fairly squares line and then it's up to have a slope of 21.

175
00:17:26,940 --> 00:17:33,749
So that's her value. And then my analysis, if I look at the first point minus the last the last point,

176
00:17:33,750 --> 00:17:43,200
minus the first point relative to the amount of time I get what I call Delta, it's a different slope of 24.8, pretty much the same number.

177
00:17:44,430 --> 00:17:47,880
Give or take a couple of points. And I do this for every woman.

178
00:17:49,210 --> 00:17:55,300
So now I have a series of slopes for the control group and I have a series of slopes for the intervention group or the deltas.

179
00:17:56,770 --> 00:18:01,510
I can make a back spot of all of the slopes of the control treatment group.

180
00:18:01,540 --> 00:18:10,449
I can make a back spot of the deltas the first last minutes minus the first, and then do it to sample test again.

181
00:18:10,450 --> 00:18:18,100
Comparing two groups on average do they have different slopes and both produce a p value that's wildly significant,

182
00:18:18,580 --> 00:18:22,120
which again isn't surprising given that picture we saw already.

183
00:18:24,240 --> 00:18:31,080
Again, don't see perfect normally distributed data here, but as I said last time, I don't really care.

184
00:18:31,410 --> 00:18:41,129
I use t tests all the time and t tests are fairly robust normality if you have enough data and as I said to investigators,

185
00:18:41,130 --> 00:18:44,400
if you have ten data points, I shouldn't be analyzing the data anyway.

186
00:18:45,810 --> 00:18:48,930
You don't get anything there for me anyways. I don't care about formality.

187
00:18:49,380 --> 00:18:54,150
Right. So anyways, we should have enough data here. That Central Theorem is going to help the t test.

188
00:18:54,150 --> 00:18:57,650
Give me valid results. So again, I didn't need to do both of these.

189
00:18:57,660 --> 00:19:02,040
I can pick one that I think the delta is probably easier to compute than the slope.

190
00:19:03,030 --> 00:19:11,030
It's all in a for loop. I'll show you in a second. So in your data, if you want to use one of these summary measures as a way to compare your groups.

191
00:19:11,820 --> 00:19:17,790
Be my guest. I work with someone at the Gentles, at the dental school.

192
00:19:17,790 --> 00:19:26,130
I used to work at the dental school and work with some of the nursing school, and they do a lot of studies in pain and how people experience pain.

193
00:19:29,150 --> 00:19:38,660
And they in this area, they like to summarize everybody by taking the area under their trajectory.

194
00:19:41,410 --> 00:19:45,940
Probably isn't any better or worse than a slope. But again, just to show you, if you wanted to do this,

195
00:19:46,300 --> 00:19:51,250
so we use a trapezoidal rule which hopefully you also at some point in your your mathematical lives.

196
00:19:52,150 --> 00:20:01,050
I want the area between the first two time points. Right. That's a trapezoid which is trapezoidal rule because area there, it's eight and so forth.

197
00:20:01,070 --> 00:20:04,840
For every one of the four time points that this woman has, there's an area under the curve.

198
00:20:05,380 --> 00:20:11,470
And we ended all up, right. If there's more area, that means that woman is experiencing more pain over time.

199
00:20:11,470 --> 00:20:15,160
So does have an interpretation. It's pretty flat, low.

200
00:20:15,160 --> 00:20:16,690
It's going to have a low total area.

201
00:20:18,260 --> 00:20:24,730
Again, we probably should scale a woman's is going to naturally contribute more area as she is a law observer longer and longer.

202
00:20:24,910 --> 00:20:30,750
So I'm going to scale her some by the amount of time she was followed.

203
00:20:30,760 --> 00:20:38,110
So I call that a scaled area, total area. So again, this woman's summary value in my analysis is around 29.

204
00:20:39,390 --> 00:20:42,540
And I would do this for every one of the women in the dataset.

205
00:20:42,540 --> 00:20:44,430
If I feel that this is a good summary measure,

206
00:20:45,300 --> 00:20:49,650
can takes a little more computing and a little more thought with a sort of open heart, but it can be done.

207
00:20:51,530 --> 00:20:56,720
Can I have one observation per woman? I can do it to sample test of these scaled areas.

208
00:20:57,500 --> 00:21:02,240
And there is the black spot of the areas for the control group in the areas for the treatment group.

209
00:21:02,780 --> 00:21:06,350
And the median for the treatment group seems to be lower than the control group.

210
00:21:07,250 --> 00:21:10,250
And the p value, again, is is highly, highly significant.

211
00:21:10,970 --> 00:21:13,070
So it didn't matter what analysis I did here.

212
00:21:13,100 --> 00:21:20,450
I'm essentially getting the same conclusion that there probably is something going on between these two groups,

213
00:21:21,320 --> 00:21:26,300
and I haven't had to deal with correlation. Relation is completely out of the story here.

214
00:21:28,630 --> 00:21:31,030
We could adjust for covariance, right?

215
00:21:31,030 --> 00:21:38,020
Instead of doing a to group comparison, we might want to adjust for again for other things, possible variables or confounders.

216
00:21:38,290 --> 00:21:41,830
We could do effect modification. Again, this was a randomized trial.

217
00:21:42,490 --> 00:21:49,930
Hopefully there isn't too much to worry about there, but to move from a t test to linear regression to adjust for the covariance, you can do that now.

218
00:21:49,960 --> 00:21:53,920
Right? Right. So again, that's where I answer my question.

219
00:21:53,920 --> 00:21:58,360
Yeah. Why would you consider that possible? Confounding. There might be imbalance between the two groups.

220
00:22:00,100 --> 00:22:07,780
Again, it's troubling here. We start out with a random sample of two women, but then over time, people are starting to give birth at different rates.

221
00:22:08,320 --> 00:22:12,460
I have to believe that giving birth is related to lots of other factors in a woman's life.

222
00:22:13,780 --> 00:22:19,090
So there is a little bit of concern there that we don't have a random sample by the time we get to three and a half hours.

223
00:22:20,590 --> 00:22:24,309
But here are the results I showed you before. I didn't show you as it certainly is numbers,

224
00:22:24,310 --> 00:22:32,469
but I've got the mean slope in the in the treatment group I'm sorry the difference in the means so the mean difference unadjusted.

225
00:22:32,470 --> 00:22:37,630
So for my two sample T test and then I just did for the two variables that there

226
00:22:37,640 --> 00:22:43,480
in the data set so the woman's contributed all those ancient first words.

227
00:22:43,810 --> 00:22:50,740
So I know the age of the woman and I know whether this is her first birth, first or not, for first time delivering, I should say.

228
00:22:52,590 --> 00:23:02,700
The main difference whether I don't adjust for those two variables or whether I adjust for it, you know, it changes from never -18 to -18.4.

229
00:23:02,940 --> 00:23:07,350
The standard error is so small relative to the difference, it's going to be significant.

230
00:23:07,950 --> 00:23:16,290
And it's true for those deltas and that area sort of concept that I gave you that across the board, whether I adjust or not,

231
00:23:17,030 --> 00:23:20,820
doesn't seem to be relevant to the question at hand here that the groups are different from each other.

232
00:23:21,630 --> 00:23:28,170
But again, I just want to show you that once you take longitudinal data collapse into one value per person,

233
00:23:28,770 --> 00:23:36,690
you can do everything you learned in 650 and 651. I just want to show you, though, some of the things, the problems that you're going to run into.

234
00:23:37,740 --> 00:23:43,680
So I gave you the adjusted difference in the slopes pointer.

235
00:23:44,640 --> 00:23:48,360
So again, this -18.4 is that 18.4, they're right.

236
00:23:48,360 --> 00:23:51,030
That's the difference. That's the treatment difference.

237
00:23:54,150 --> 00:24:03,270
Part of my job as a professor is to give you best practices, right as you move along and you're going to be in 699 next semester.

238
00:24:04,050 --> 00:24:07,170
I'm actually learning how to work with collaborators and so forth.

239
00:24:09,560 --> 00:24:13,360
I don't really care what these are. I don't care what these coefficients are.

240
00:24:13,370 --> 00:24:17,570
I just wanted you to adjust for them so that I could see if the treatment effect still exists.

241
00:24:18,260 --> 00:24:23,990
You don't need to give me a regression table like this if all I'm interested in is the treatment effect.

242
00:24:24,640 --> 00:24:29,210
Right. You can just say I adjusted for these variables. Here's the result you're interested in.

243
00:24:29,750 --> 00:24:35,330
Okay. So when you go into 699 next semester, don't get caught up in big tables of regression models.

244
00:24:36,320 --> 00:24:39,710
If I want to know what the interaction term is, then you should include it in your table.

245
00:24:40,310 --> 00:24:42,680
If I don't care about it because you just need to account for it.

246
00:24:43,670 --> 00:24:48,390
You don't need to give it to me so I can give you these two shaded rows when I showed you these three numbers.

247
00:24:48,400 --> 00:24:56,300
So there's there's a 16 and there's the 24. But when I adjust for age and whether the woman is having her first time giving birth or not.

248
00:24:57,020 --> 00:25:02,750
What I thought was interesting was there really there really play no role here in these two variables.

249
00:25:02,780 --> 00:25:07,220
Right. Which is p values are pretty large. But here there's something weird going on.

250
00:25:10,180 --> 00:25:17,080
So you know what? Again, the more times you analyze data, the more times you're going to get something that makes you scratch your head.

251
00:25:18,370 --> 00:25:23,949
So, again, just be careful. This isn't I'm not saying that I believe this one more than these are.

252
00:25:23,950 --> 00:25:28,450
I believe these more than this. I'm just saying their answer, asking slightly different questions.

253
00:25:29,380 --> 00:25:32,530
And you just have to be careful of of, you know, what's going on here,

254
00:25:33,940 --> 00:25:38,420
that there does appear to be an effect of whether the woman is giving first a birth or not.

255
00:25:39,280 --> 00:25:44,620
Relative to how much area was under her birth. I don't know how to explain that.

256
00:25:45,730 --> 00:25:54,580
Right. So, again, controlling for everything else. The area was about 17 units larger per unit time for women who were.

257
00:25:55,650 --> 00:25:59,280
Giving birth for the first time and may be right.

258
00:25:59,640 --> 00:26:04,410
Sing along louder. This is the woman's first time. She's like, Oh, my God, this is horrible.

259
00:26:04,500 --> 00:26:11,760
Right. But maybe after the second or third time, it's less horrible for lots of reasons, mental or physical.

260
00:26:12,060 --> 00:26:18,360
But anyway, intriguing that three different analyzes for the primary question give the same answer.

261
00:26:18,370 --> 00:26:21,660
But for these other variables, there's a different story.

262
00:26:24,850 --> 00:26:27,010
I try to stick with human data in this class.

263
00:26:27,610 --> 00:26:34,810
There's lots of fascinating data on mice and birds and dogs, but I don't need to cover those in this class or in public health.

264
00:26:35,710 --> 00:26:39,070
But this is a great example, and this is probably only time I'll show you animal data.

265
00:26:41,200 --> 00:26:48,400
These are looking at the estrus cycle of mares, female horses.

266
00:26:49,690 --> 00:26:53,230
And again, over time, there's a measurement on the Y-axis.

267
00:26:54,600 --> 00:26:58,560
This is longitudinal. What's the best summary measure here?

268
00:27:00,230 --> 00:27:03,920
For each, you know, for this for this mayor right here. What's the best summary?

269
00:27:05,000 --> 00:27:08,090
Should I take a slope? Should I take an average?

270
00:27:09,980 --> 00:27:13,520
If you fit a quadratic. Now you've got two coefficients.

271
00:27:13,520 --> 00:27:18,200
A slope and a curve. Right. Quadratic. Do I have both of those I have to analyze.

272
00:27:18,200 --> 00:27:20,990
Do I just stick with the curved? It gets a little more hairy.

273
00:27:22,670 --> 00:27:28,760
And you could try to think about fitting spines, but spines have multiple parameters and then they don't have one parameter.

274
00:27:30,470 --> 00:27:37,630
So if we tried to do a derived variable analysis here, we're probably going to spend more time than is probably worth trying to do it.

275
00:27:37,640 --> 00:27:43,940
Maybe you might say a slope. You know, this this mare tends to go up and down, but over time, maybe it's pretty flat.

276
00:27:46,480 --> 00:27:51,670
So, you know, summary variables here drive variables. Here's the investigational diet again.

277
00:27:51,940 --> 00:27:58,450
How do I say that there's a difference between this group and this group? By collapsing down to one observation for per bear.

278
00:28:00,250 --> 00:28:01,900
I'm not sure I've even attempted to ever do that.

279
00:28:02,740 --> 00:28:10,600
So when you get these wildly up and down types of patterns of data, that's when drive variables start to really become less and less useful.

280
00:28:11,500 --> 00:28:18,630
Just again, food for thought. So what you're doing today and by next Wednesday, what are the benefits here?

281
00:28:19,410 --> 00:28:23,879
It allows you to do things that you already know how to do. We don't have to worry about.

282
00:28:23,880 --> 00:28:29,850
The correlation is because we're accounting for it by using this dry variable as we're doing with them.

283
00:28:30,000 --> 00:28:34,049
Right. Getting within a person. Lots of limitations.

284
00:28:34,050 --> 00:28:37,320
However, you should think about what the right variable you want to use.

285
00:28:37,710 --> 00:28:41,610
Should I take the mean over time? Should I take the median over time? Should it be the slope over time?

286
00:28:43,620 --> 00:28:52,620
What happens is none of those work. Remember, every time one of the benefits of longitudinal data is your sample size is going up,

287
00:28:54,060 --> 00:28:57,810
but it isn't going up per unit up for observation because of the correlation.

288
00:28:58,500 --> 00:29:08,280
Right. So if I have 100 people. With 50 yards, with 100 people, with five observations, that's 500 data points.

289
00:29:08,290 --> 00:29:12,879
That is not a sample size of 500 because it's less than 500.

290
00:29:12,880 --> 00:29:17,920
You don't have 500 unique pieces of information because of the correlation, which you've got more than 100.

291
00:29:18,280 --> 00:29:22,630
Right. So longitudinal studies give you more information.

292
00:29:22,870 --> 00:29:29,710
They give you more power. So as soon as you decide to cut all that out and go back to one observation per person, you're losing power.

293
00:29:30,500 --> 00:29:35,500
Right. So simplicity at the expense of perhaps some accuracy or some efficiency.

294
00:29:37,870 --> 00:29:46,630
We have lost the ability. If you truly want to know not only how are things happening over time for the group, but what's the variability in people?

295
00:29:46,990 --> 00:29:53,700
Are people are some people going out very quickly, like in the labor pain data, you lose all that.

296
00:29:53,710 --> 00:29:58,960
You lose all that information when you're back to one observation per person. There's no variability within a person anymore.

297
00:29:59,140 --> 00:30:05,710
It's gone. So if your goal is to look at that variation, then by all means, you don't want to do this.

298
00:30:06,760 --> 00:30:14,110
And the other problem is that it worked in the labor pain data because the two covariates were baseline covariates.

299
00:30:14,710 --> 00:30:20,050
It was the same age was essentially the same over the three and a half hours from intents and purposes.

300
00:30:20,350 --> 00:30:26,290
And whether or not that was her first time giving birth was the same over time for that study.

301
00:30:26,920 --> 00:30:31,360
If you have covariates that are also changing over time with the outcome, we'll see some of that.

302
00:30:33,160 --> 00:30:37,629
Then what do you do right? You've now taken the outcome and collapsed it over time.

303
00:30:37,630 --> 00:30:42,460
What do you do with the covariate? You also summarize that and if you do, what are you now modeling?

304
00:30:42,790 --> 00:30:45,600
It becomes very difficult to figure that out.

305
00:30:45,610 --> 00:30:56,500
So again, nothing deep but hopefully useful for you is again, you're going to get a longitudinal data set of 699.

306
00:30:57,850 --> 00:31:03,130
Hopefully some of these ideas are taken out before you jump into the cool stuff we're going to see in November.

307
00:31:06,880 --> 00:31:16,900
And as I said, there is this drive variables file in our studio that shows you,

308
00:31:17,190 --> 00:31:21,280
you know, how did I do the series, appearances series, the parent tests.

309
00:31:21,360 --> 00:31:25,480
Wow. I really was kind of prepared.

310
00:31:29,520 --> 00:31:34,870
So here's a stem or she gets advanced.

311
00:31:35,940 --> 00:31:40,680
Again, if you really are, shouldn't doing that area under the trajectory for every woman.

312
00:31:41,850 --> 00:31:51,030
It should be some career. How I do it again, 000 rule is really a formula and I just did it repeatedly for everyone that was in it in a sense.

313
00:31:51,030 --> 00:31:56,030
So that's all there. It showed you how I adjusted for age and first delivery.

314
00:31:58,830 --> 00:32:05,130
Some of you in your datasets have a group that has two or has a grouping variable that has two categories.

315
00:32:05,850 --> 00:32:09,750
Some of you have more than two. So how many of you have more than two?

316
00:32:10,800 --> 00:32:16,020
You look at your data yet? I know some of them have more than two groups.

317
00:32:17,340 --> 00:32:23,130
All right. How much do you learn about a Nova and 650?

318
00:32:26,560 --> 00:32:30,610
Oh, okay. That's what I was afraid of.

319
00:32:36,460 --> 00:32:44,230
All right. And I want to go through another Homeric example. I know the is regression to sample t test is regression.

320
00:32:45,070 --> 00:32:52,550
Someone said that you. Right.

321
00:32:53,370 --> 00:32:57,400
Let's step back. Let's.

322
00:32:57,420 --> 00:33:00,810
This is what I want. I want the way it works and how to do this.

323
00:33:00,960 --> 00:33:04,210
I'm not using that chalkboard anymore. No more chocolate.

324
00:33:04,210 --> 00:33:13,310
My shirt. So this is a really important, as Ronald said.

325
00:33:14,860 --> 00:33:25,110
To me 20 years ago. Everything is a regression model. Everything we learned chi square test is logistic regression to sample t test.

326
00:33:25,120 --> 00:33:30,190
It's linear regression. It's just you haven't adjusted for other variables.

327
00:33:31,810 --> 00:33:58,200
See if I can do this. She is my age.

328
00:34:01,280 --> 00:34:04,669
This is a simple linear regression model where I have one predictor and it's binary.

329
00:34:04,670 --> 00:34:13,130
Yes or no? Everything's normally distributed.

330
00:34:13,140 --> 00:34:18,580
I'm not going to put all that kind of stuff up there. Right. That's a linear regression model.

331
00:34:18,590 --> 00:34:29,220
It's the interpretation of beta one. Anybody mean difference between those three groups?

332
00:34:29,570 --> 00:34:33,270
Okay. Beethoven quantifies the difference in the means between the two groups.

333
00:34:33,630 --> 00:34:40,080
Beat or not is the mean of the reference group, which is group one and beta one is the difference.

334
00:34:41,280 --> 00:34:46,590
You're comparing differences in means between two groups. That's a two sample test, right?

335
00:34:47,580 --> 00:34:53,700
So inference on beta one is the same p value you get from a to sample teachers with equal variances.

336
00:34:53,830 --> 00:35:14,860
Right. Everything here is the same variance. So if we expand this to more than two groups, though, that's not what I want to change.

337
00:35:27,790 --> 00:35:33,130
Hey. That's really legible.

338
00:35:33,340 --> 00:35:51,280
Not. Trying to make this simple response.

339
00:35:54,460 --> 00:36:20,810
So hopefully you have seen this. I mean, it's too many variables, right? Not only have three groups I'm trying to compare.

340
00:36:22,400 --> 00:36:28,900
So now we need to double the variables. Not supposed to be able to raise it.

341
00:36:31,240 --> 00:36:35,590
So now I have a dummy variable for group two versus group one and a dummy variable for group three.

342
00:36:36,220 --> 00:36:41,640
Right. Two comparisons going on. This is linear regression.

343
00:36:43,060 --> 00:36:49,310
This is a novel analysis of variance when I have more than two groups that I do a detest.

344
00:36:49,330 --> 00:36:57,520
I do ANOVA. But again, whether I had saved someone, I didn't know the model or I did a regression model with two dummy variables.

345
00:36:57,970 --> 00:37:06,320
You're answering the same question, right? You can do an F test for these two variables here are significantly different from zero.

346
00:37:06,340 --> 00:37:11,260
You're testing for whether or not there are group differences anywhere. That's the F test from ANOVA.

347
00:37:13,960 --> 00:37:17,260
The reason I'm saying all this is because some of you have more than two groups.

348
00:37:19,210 --> 00:37:27,050
I would rather you do a nova. Then pick a reference group and do two tests for the reference period.

349
00:37:28,610 --> 00:37:32,089
So if you're not comfortable with ANOVA, it is linear regression.

350
00:37:32,090 --> 00:37:40,760
If you're comfortable with linear regression, then that's what I want you to do. So let me show you where we have brought it to 43 Chase.

351
00:37:46,560 --> 00:37:50,970
So each of you has two datasets I add to the sentence 400, number one and so forth.

352
00:37:51,600 --> 00:37:54,780
And I have a dataset called Pepe Dot that deaths.

353
00:37:55,170 --> 00:38:02,100
And here is the text file, it says in a study of the Association of Hyperglycemia and Relative Hyperinsulinemia.

354
00:38:03,150 --> 00:38:06,540
Glucose tolerance tests were administered to three groups of individuals.

355
00:38:07,170 --> 00:38:18,239
So here's an example of a novo. And so we have, again, very small study of 13 controls, 12 individuals who are non hyper insulin,

356
00:38:18,240 --> 00:38:23,490
anemic obese and eight hyperinsulinemia obese patients.

357
00:38:24,930 --> 00:38:29,940
And again, these authors here, this is a really contemporary dataset collected when I finished high school.

358
00:38:30,320 --> 00:38:42,750
So the outcome is this plasma inorganic organic phosphate collected at baseline and then every half hour or then so half an hour,

359
00:38:42,750 --> 00:38:45,780
one hour, one and a half hours, two, three, four or 5 hours. Right.

360
00:38:46,920 --> 00:38:52,260
So now we've got three groups. We want to see how the different characteristics of these three groups.

361
00:38:52,950 --> 00:38:57,300
So we've got a control group. We've got obese patients who are non hyperinsulinemia.

362
00:38:58,050 --> 00:39:01,220
And we have these folks who are hyper insulin units.

363
00:39:03,760 --> 00:39:10,390
My data look like this. So this is the white format.

364
00:39:12,260 --> 00:39:14,870
So again, I know what the grouping variable is,

365
00:39:16,160 --> 00:39:23,240
the ID number of each individual within the group and then their first measurement of PIP and so forth through the study.

366
00:39:24,680 --> 00:39:27,860
Again, a textbook set of data here. We've got no missing data.

367
00:39:28,700 --> 00:39:33,290
Everybody was observed at all the time points. Again, I have a one group.

368
00:39:34,220 --> 00:39:37,350
I have a two group. And I have a three group.

369
00:39:38,580 --> 00:39:45,030
And again, your text file should. Your text should tell you who those three groups are.

370
00:39:45,840 --> 00:39:55,410
And it doesn't mean it's not relevant to a homework assignment, but it is relevant for interpretation.

371
00:39:56,350 --> 00:40:01,620
Hey, yo, yo, yo, yo. All right. How did I go about analyzing these data?

372
00:40:01,650 --> 00:40:05,900
This is what you should be doing, right? Some of you have it long.

373
00:40:05,910 --> 00:40:12,780
Some of you have been white for months. So I start reading some of my code here.

374
00:40:15,360 --> 00:40:20,570
Should our. That's right.

375
00:40:22,080 --> 00:40:29,490
Again, it should be careful that some data sets will re-use the ID numbers within each group.

376
00:40:31,590 --> 00:40:34,920
You need unique IDs when we start moving through this class.

377
00:40:34,950 --> 00:40:38,250
The ID number is telling the computer where the correlation is.

378
00:40:39,330 --> 00:40:45,060
So if you have subject one in one group and subject one in another group, the computer thinks that's the same person.

379
00:40:45,870 --> 00:40:51,930
So make sure when you look at the numbers that every person in the study is different in their I.D. number from everybody else,

380
00:40:52,620 --> 00:40:56,880
even if they're in different groups. Right. And unfortunately, in this dataset, that is the case.

381
00:40:57,480 --> 00:41:02,970
So I simply said, get rid of the ID numbers and just give everybody an I.D. number that is equal to their real number.

382
00:41:03,510 --> 00:41:06,930
But not really in, as I told you.

383
00:41:07,980 --> 00:41:11,820
You might want a long form of the data set. And so.

384
00:41:12,840 --> 00:41:16,080
And you're. What am I trying to do here?

385
00:41:17,010 --> 00:41:29,270
Where's the top? I've got to pull off the network.

386
00:41:29,400 --> 00:41:32,520
Oh. It was fun to do what I wanted to do.

387
00:41:32,850 --> 00:41:35,850
So I need the group, the ID, and the first observation.

388
00:41:36,360 --> 00:41:39,780
And then I need the group and ID and the next observation pasted on below.

389
00:41:40,200 --> 00:41:48,120
So I'm just taking the first two columns, pasting on a roll of observations and consecutively adding those on.

390
00:41:48,300 --> 00:41:53,330
And that's what all this code does. You need the time variable.

391
00:41:53,340 --> 00:41:55,530
So again, this is where the text fails. Really important.

392
00:41:56,400 --> 00:42:01,620
I know that the time points are zero, 30 minutes, one hour, one and a half hours and so forth.

393
00:42:01,620 --> 00:42:06,030
And that's what these numbers are here, right? Maybe yours are different.

394
00:42:06,560 --> 00:42:16,320
They're one, two, three, four or five, whatever. I haven't explicitly told you that you should treat time as continuous.

395
00:42:16,830 --> 00:42:23,490
Again, you could treat time as categorical. But if you want to set a line, then you're assuming a continuous time.

396
00:42:26,340 --> 00:42:35,610
So I've made the long form of the data set. The other important thing to know is that when we get to generalize estimating equations,

397
00:42:36,870 --> 00:42:42,480
again, I don't know why we can't reprogram things in our to not care about this.

398
00:42:43,860 --> 00:42:49,230
But Gigi wants things in order by adding over.

399
00:42:50,820 --> 00:42:56,910
So if you create your data set such that you have the first time point for everybody, the second one and so forth,

400
00:42:57,120 --> 00:43:02,429
you've got to get it back so that this person has all their data and then the next person has all of their data and so forth.

401
00:43:02,430 --> 00:43:05,280
Far, yes, sir. So everyone's measured at a different time.

402
00:43:06,270 --> 00:43:15,060
You need like in the wide format, do you need a variable for each visit that has the time there embedded and then another variable for each outcome?

403
00:43:15,150 --> 00:43:19,470
Yes. You need one column for time and one column for outcome.

404
00:43:20,340 --> 00:43:23,760
Yeah. So let's talk about what you just said here.

405
00:43:25,530 --> 00:43:32,550
If you didn't, you didn't catch it. There's no time mentioned in this dataset because everybody is measured at the same time.

406
00:43:33,930 --> 00:43:42,900
So time one for everybody is time zero. If time one is different from and for every person, you've got to pull that into the data set as well.

407
00:43:43,260 --> 00:43:49,979
So the Time column has to reflect what those individual things are, but far away.

408
00:43:49,980 --> 00:43:53,410
I think you all have data sets where everybody's being followed at the same times.

409
00:43:54,810 --> 00:44:02,870
So it's a problem. Yeah, you do need to keep track of that. And it's so hard to go through our code.

410
00:44:04,750 --> 00:44:08,500
So let's run all of that. So where do they start?

411
00:44:09,280 --> 00:44:12,420
That's the idea. Numbers.

412
00:44:12,440 --> 00:44:18,850
We create a lot of format of the data at column numbers and then sort all the data.

413
00:44:26,440 --> 00:44:31,740
Right. So now I have a data set where I've got ID number one there in the first group.

414
00:44:32,340 --> 00:44:36,110
I know what the time points are and I know what their like variables are, their p values.

415
00:44:38,490 --> 00:44:43,170
And likewise. Then there's idea number two and so forth. So I've created what I want.

416
00:44:43,920 --> 00:44:51,120
Again, very good practice to look at what you created before you go ahead and do all your fancy analysis,

417
00:44:51,120 --> 00:44:57,500
because I guarantee you the data probably aren't what you think they are sometimes and things will go wrong.

418
00:44:57,510 --> 00:45:00,540
So make sure you have created the data the way you think they should be.

419
00:45:01,620 --> 00:45:07,290
I want to look at the average VIP value in each of the three groups over time.

420
00:45:07,290 --> 00:45:15,270
One of those plots I showed you, as well as trying to figure out how many observations there are in each group at each time point.

421
00:45:17,070 --> 00:45:21,930
Again, I don't know how much our it doesn't seem like you guys get a lot of our experience your first year

422
00:45:22,830 --> 00:45:29,310
but t apply is essentially saying play a function over and over and over with the syntax here.

423
00:45:29,520 --> 00:45:34,710
Again, I don't want to make this in our teaching class so syntax doesn't make sense to you.

424
00:45:36,120 --> 00:45:39,240
I give you the data. You can run the stuff yourself and see what's going on.

425
00:45:40,140 --> 00:45:43,890
But I'm computing the means if you want to use it for a loop. That's okay too.

426
00:45:43,920 --> 00:45:49,500
We're not here to be excellent. Our programmers. We're here to program our so that it gives us what we want.

427
00:45:51,520 --> 00:45:56,580
As you're going to learn from me very quickly. Again, I don't use juju plot but you're good.

428
00:45:56,590 --> 00:46:01,870
So again I want a curve or a set of means over time for all three groups.

429
00:46:04,100 --> 00:46:08,270
And a legend and so forth. And this is what I got.

430
00:46:09,500 --> 00:46:14,390
So in these data, this is the trajectory over time for the three groups in terms of their averages.

431
00:46:15,200 --> 00:46:18,280
So again, this wasn't a randomized study.

432
00:46:18,290 --> 00:46:26,360
Remember that the differential between the groups is whether they were obese, hyperinsulinemia or high.

433
00:46:26,870 --> 00:46:33,770
There are three groups that isn't randomized. It's an observational. So it's not surprising that at that first time point they're different.

434
00:46:35,030 --> 00:46:40,310
So they've all come down and then they're basically identical data, right?

435
00:46:40,490 --> 00:46:42,890
So this is an unusual dataset.

436
00:46:44,630 --> 00:46:51,590
Hopefully some of you don't have something as strange as this because I'm going to try and figure out what's a good summary variables for everybody.

437
00:46:52,310 --> 00:46:53,800
I want to see if there are group differences.

438
00:46:53,810 --> 00:47:01,220
It looks like there are group differences early on, and then those group differences seem to vanish after about 2 hours of a glucose challenge.

439
00:47:02,620 --> 00:47:10,010
So. So again, here's the black spot.

440
00:47:11,670 --> 00:47:16,300
I try to show the same story. So.

441
00:47:25,260 --> 00:47:34,260
That's it. That's not what I want. That's over time.

442
00:47:45,040 --> 00:47:48,100
Last month at 0 hours.

443
00:47:50,060 --> 00:47:55,130
Was there. Something happened here.

444
00:47:55,250 --> 00:48:00,890
Wow. Something happened in the transference of what I did this summer and what I came up with last night.

445
00:48:00,900 --> 00:48:04,040
So let's ignore that. I'm sorry about that. I'm gonna fix that.

446
00:48:09,000 --> 00:48:15,990
So again, I now have a feel for what the data look like. It looks like people are having to shape they're coming down to a different rates,

447
00:48:16,500 --> 00:48:23,160
but then after year after they get there, their bottom point, they all tend to fill up at the same rate.

448
00:48:24,060 --> 00:48:27,480
What do people values look like within a personal correlated?

449
00:48:27,480 --> 00:48:32,040
Are they? So there's something called unstructured correlation.

450
00:48:32,070 --> 00:48:36,660
That's what I showed you in my slide last, and that's what I take every time point compared to every other time plan.

451
00:48:36,960 --> 00:48:42,900
I just get a Pearson's correlation coefficient. We call that unstructured because there's no structure.

452
00:48:43,890 --> 00:48:49,380
Again, I don't believe that correlation necessarily is that unstructured in real human data.

453
00:48:50,130 --> 00:48:56,880
We think perhaps that there is a decay over time, that there's a certain correlation when observations are a certain amount of time apart.

454
00:48:57,480 --> 00:49:03,270
And as they get further apart, that correlation decreases maybe exponentially or some other rate.

455
00:49:04,350 --> 00:49:09,000
But we can get an unstructured set of correlations just to get an eyeball of what's going on here.

456
00:49:11,640 --> 00:49:15,240
Rounds out so that maybe we can get more on screen here.

457
00:49:17,880 --> 00:49:21,150
Don't need a correlation coefficient that are six decimals long.

458
00:49:25,440 --> 00:49:30,100
Right. So again,

459
00:49:30,400 --> 00:49:40,750
if we look at the first time point and how its correlation is with future time points and we see that correlation tends to go down over time.

460
00:49:42,490 --> 00:49:46,899
If I look at the second measurement relative to all future measurements, again,

461
00:49:46,900 --> 00:49:53,350
we see this natural decay and correlation of two observations gets weaker as they go on time.

462
00:49:54,700 --> 00:50:01,390
Someone to talk to me about this after class of today and think about predicting if I use what I know about myself now or you now,

463
00:50:01,930 --> 00:50:07,900
how well does it predict what's going to be what the value will look like six months from now versus maybe two weeks from now?

464
00:50:08,890 --> 00:50:13,990
As time goes on, what I know now is less related to what's going to happen in the future.

465
00:50:14,530 --> 00:50:19,839
That's where that sort of concept comes from. However, do I think so?

466
00:50:19,840 --> 00:50:28,690
Here's Time one versus time two. It is a correlation of point of 25 APR versus the second versus the third.

467
00:50:28,840 --> 00:50:33,010
The correlation is point 77. They both have the same time lag.

468
00:50:33,400 --> 00:50:37,480
Do I really think those two numbers should be different or maybe they should have the same value?

469
00:50:38,140 --> 00:50:43,240
So that's when I did this leg. One correlation for you guys, folks, the other day.

470
00:50:44,590 --> 00:50:50,950
And so, again, all I'm doing is taking time one versus time two in a dataset.

471
00:50:52,030 --> 00:50:58,179
And then I take time to add time three and put them in the same two columns, and then time three and time four.

472
00:50:58,180 --> 00:51:05,350
So I've got a time out time one leg, two columns of data, and I just take it Pearson's correlation coefficient for that.

473
00:51:05,380 --> 00:51:11,740
Again, there's the code. What's.

474
00:51:13,870 --> 00:51:18,760
What's interesting here, though, is that it's not going to lag one correlation,

475
00:51:18,880 --> 00:51:25,870
every observation and it's next future time point have a correlation of about 0.74 regardless of when those two measurements were taken.

476
00:51:27,970 --> 00:51:31,990
For some reason I averaged over the three groups. You could do this separately for three groups, right?

477
00:51:32,230 --> 00:51:38,840
And see if they're different from each other, we might believe. And we're going to get system to get into this in this class.

478
00:51:38,870 --> 00:51:45,290
Do you think that the treatment affects the correlation? Do we have to model the correlation changing with treatment as well as the mean?

479
00:51:45,800 --> 00:51:50,180
These are the sorts of things we can do with our more sophisticated models that will later.

480
00:51:51,660 --> 00:51:53,730
A derived variable. In this situation,

481
00:51:55,080 --> 00:52:02,760
I decided that I was going to compute a dry variable that looks at the largest decrease from baseline because eventually they were all the same.

482
00:52:02,760 --> 00:52:06,750
So I thought, well, if there's a difference between the groups, it has to do with how much people change.

483
00:52:07,710 --> 00:52:13,230
So for every person I said, give me their, their largest difference from baseline.

484
00:52:13,620 --> 00:52:17,190
So I computed all the differences and then use the max function.

485
00:52:20,960 --> 00:52:28,060
And I have three groups. So I'm going to use a nova.

486
00:52:30,580 --> 00:52:35,200
First thing I want to do is visually look at those three groups. So, again, here's how the three groups look.

487
00:52:36,860 --> 00:52:45,740
And make a pdf this time year of the three groups in terms of how much change maximum change they'll have from baseline.

488
00:52:46,310 --> 00:52:52,459
And so we see that group one has a median change that's similar to group three.

489
00:52:52,460 --> 00:53:00,110
Maybe two has the lowest, has the smallest. So if there are differences somewhere, it's probably due to group two.

490
00:53:01,730 --> 00:53:06,950
And those are the folks they don't know because in my dataset it doesn't tell me who two is.

491
00:53:10,030 --> 00:53:13,400
Shoot, it was the to those rooms in that text file.

492
00:53:14,990 --> 00:53:21,440
But there are group differences possibly. Right. So before I do a statistical test, I think that there might be a difference.

493
00:53:21,770 --> 00:53:26,420
It's not going to be that strong. There's a lot of noise here. Black spots overlap a lot.

494
00:53:27,920 --> 00:53:31,040
We're going to do an over again.

495
00:53:31,040 --> 00:53:35,420
ANOVA is linear regression. So I'm going to use the L m function and R.

496
00:53:36,940 --> 00:53:45,190
When tested a linear regression that model's the maximum change with dummy variables for groups and RS.

497
00:53:45,190 --> 00:53:50,650
Default is to make the first group the reference and the other to have coefficients attached to them.

498
00:53:51,700 --> 00:54:01,200
And then I asked for the summary. So I am doing an elevator. And what I got was that based upon my summary values here.

499
00:54:06,550 --> 00:54:10,510
And what you have to ask, though. All right. It's wider.

500
00:54:17,930 --> 00:54:25,190
Here we go. All right, here is the coefficient table there.

501
00:54:25,970 --> 00:54:33,890
Right. So as I expected, there is a greater difference for two versus one.

502
00:54:34,310 --> 00:54:37,700
The second coefficient says What's the difference between the reference group and group two?

503
00:54:38,480 --> 00:54:42,770
It's not statistically significant. I just don't remember. I had one. They had eight and 13 or something.

504
00:54:42,770 --> 00:54:47,450
People per group. This is another thing for you to to keep in your mind.

505
00:54:47,450 --> 00:54:52,999
Whenever an investigator comes to you with groups that have ten, 12 people in them,

506
00:54:53,000 --> 00:54:59,120
the first thing you should tell them is it's unlikely for us to see anything, but we're going to try.

507
00:55:00,950 --> 00:55:04,940
They're always very helpful with a small amount of data, right?

508
00:55:05,360 --> 00:55:12,010
There probably are differences between these three groups. We don't have enough power to find it because we don't have enough data.

509
00:55:12,440 --> 00:55:17,210
Right. So again, we don't say that group two is not different from group one.

510
00:55:18,440 --> 00:55:21,979
That's the no hypothesis. Right. So please don't tell investigators.

511
00:55:21,980 --> 00:55:28,070
Yeah. These groups are different from each other. I don't have any evidence that they're different.

512
00:55:29,400 --> 00:55:34,020
I don't have enough data here. So anyways, the third group isn't wildly different.

513
00:55:34,230 --> 00:55:38,300
Again, the challenge with a nova is what's the right reference group that I used?

514
00:55:38,310 --> 00:55:44,220
One is the reference group should have been two and then I've got to recode things. But again, a nova is regression.

515
00:55:44,670 --> 00:55:48,360
So this is how I would analyze this for the homework. I would have had a plot.

516
00:55:48,660 --> 00:55:58,140
I would have defined my summary variable, sort of explain why I think it's valid a plot and a summary of the analysis.

517
00:55:59,010 --> 00:56:03,390
And then I would do my other dataset. Questions.

518
00:56:04,290 --> 00:56:13,680
Fears. Boy, there is going to be this quiet and Fred is going to have to we're going to have to change things around.

519
00:56:15,090 --> 00:56:19,560
All right. So the plan now is to play hardball.

520
00:56:20,340 --> 00:56:28,560
You need a referral score for 1%. So working on your assignments, if you want to leave, you're more than welcome to leave.

521
00:56:28,830 --> 00:56:33,570
This is a great 15 minutes to get input from me on what you've done.

522
00:56:33,900 --> 00:56:39,600
I'm going to hang out and hopefully you talk to each other and you know somebody else who has your data set.

523
00:56:39,600 --> 00:56:43,830
At least one of them. And I look forward to see what everybody comes up with next week.

524
00:56:44,850 --> 00:56:45,179
And again,

525
00:56:45,180 --> 00:56:52,500
the plan is to eventually hopefully find some really good examples of what people found so that we can share with each other what we came up with.

526
00:56:52,830 --> 00:57:08,110
All right. So go forth in R and I'll be here if you want me to.

527
00:57:10,620 --> 00:57:14,350
You to go. So we.

