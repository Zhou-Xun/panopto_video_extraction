1
00:00:07,710 --> 00:00:12,610
No, I don't think so.

2
00:00:14,190 --> 00:00:28,650
Yeah. How do you feel about what's good for their lives?

3
00:00:28,980 --> 00:00:45,390
Right next to you and I would run over cases cause I want to discuss it with me.

4
00:00:46,980 --> 00:01:00,320
I mean, I am in charge, know, which seems really strange.

5
00:01:03,330 --> 00:01:14,700
That's why I was born Boy Scout.

6
00:01:17,130 --> 00:01:41,620
I don't know why I. I know that anyways, so I'm not sure what it's about.

7
00:01:44,490 --> 00:01:56,660
I know I'm doing 36 minutes.

8
00:01:56,660 --> 00:02:00,200
It's pretty bad. I, I.

9
00:02:00,820 --> 00:02:04,050
Hope. Right. That's probably why.

10
00:02:04,920 --> 00:02:24,270
I mean, like I always said, in with settings, absolutely no one is going to turn out.

11
00:02:25,670 --> 00:02:34,570
Why? Oh, I don't.

12
00:02:35,190 --> 00:02:40,800
You. You're about to run out.

13
00:02:43,800 --> 00:03:05,590
I don't know, 20 hours a week to give me, but all that plus additional minutes in every other.

14
00:03:05,660 --> 00:03:13,530
I'm so happy New Year. Morning. And who knows how to bring my scream to be protected here?

15
00:03:14,370 --> 00:03:17,500
I think there's a set up on double screwing or something like that.

16
00:03:17,520 --> 00:03:24,240
I have my PDF file on my screen, but it's not displayed on the screen.

17
00:03:24,270 --> 00:03:30,300
Who knows? That's probably the set up. Okay.

18
00:03:30,550 --> 00:03:36,590
Nobody knows or nobody wants to be a volunteer here.

19
00:03:41,780 --> 00:04:09,170
But when you're in this small state, what can be on this project, I think, is about something that is something else.

20
00:04:10,360 --> 00:04:19,900
Yesterday you told me yesterday to go to 14 hours and 15 minutes.

21
00:04:20,600 --> 00:04:37,420
So I think they need to go back to springs up and say that we're not going to have to do this.

22
00:04:38,110 --> 00:04:43,290
What kind of people works? Yeah, I know.

23
00:04:43,620 --> 00:04:52,070
It's clumsy, like, you know, otherwise that maps out in this book is either, okay,

24
00:04:52,100 --> 00:04:58,960
oh, this is new semester and I'm so happy to see you guys and happy new year,

25
00:05:00,310 --> 00:05:11,560
which you have very good, you know, winter semester and it's so nice to back to the normalcy of our life and have the in-class learning.

26
00:05:12,020 --> 00:05:15,489
Everyone's like so,

27
00:05:15,490 --> 00:05:22,780
so happy that now finally this 620 will be all these this way that it's great

28
00:05:22,780 --> 00:05:30,070
that we have for interactions in classes for all sides of classrooms and so on.

29
00:05:30,580 --> 00:05:37,080
So this is a required course for this health data science sort of track.

30
00:05:37,090 --> 00:05:45,010
And you know that we just had the approval of the master degree in the health state of science in the department.

31
00:05:45,010 --> 00:05:51,440
And so we have been sort of working on this first year class,

32
00:05:51,850 --> 00:05:59,079
which is focusing on data collection and data sort of aspects of statistics about statistics.

33
00:05:59,080 --> 00:06:02,170
Then I've been teaching this course for three years,

34
00:06:02,170 --> 00:06:10,960
so this of course time to teach this course and yeah, so, so but in the previous years, like, you know,

35
00:06:11,170 --> 00:06:14,800
I've been sort of teaching the course in the hybrid mode,

36
00:06:14,950 --> 00:06:23,860
but now we have this full sort of mode of in-person sort of classroom teaching, and that's really something exciting.

37
00:06:25,750 --> 00:06:33,370
So here's what I plan to do today is first I go over the syllabus, it's very typical.

38
00:06:33,640 --> 00:06:39,520
So first class you would expect and I will give a little bit the outline of this course,

39
00:06:40,540 --> 00:06:48,430
then I will talk about informed concern form, because part of this course is that we're going to learn how to design,

40
00:06:48,440 --> 00:06:53,100
study and how to work, how we're going to collect data from ourselves,

41
00:06:53,110 --> 00:07:00,160
and then how we're going to clean up the data we've collected and how we are going to analyze data.

42
00:07:00,310 --> 00:07:08,170
Okay. So basically we're doing, you know, very, very sort of early step of statistics.

43
00:07:08,560 --> 00:07:14,620
You have like taken six 5651 where you probably have been given clean data.

44
00:07:15,040 --> 00:07:17,290
Now we're basically looking at 30 day,

45
00:07:17,300 --> 00:07:24,670
not like the raw data that really that is not from somewhere Internet of work somewhere else that's really coming from your self.

46
00:07:25,180 --> 00:07:34,690
Okay. So that we will learn how to set up a scientific promise and then how we're going to propose hypotheses,

47
00:07:34,700 --> 00:07:40,989
then how we're going to use the data, we collect it and to answer the questions.

48
00:07:40,990 --> 00:07:44,530
Okay. So that's very, very basic training of data science.

49
00:07:44,740 --> 00:07:49,780
And if you think about the future jobs in there, the Google, Amazon,

50
00:07:49,780 --> 00:07:59,400
and we probably have to deal with all that raw data all the time rather than having a, you know, a clean data ready for you to analyze.

51
00:07:59,410 --> 00:08:08,970
And so 80% of the time you would do this, you know, these are cleaning data, normalization data integration, data privacy and all sorts of the,

52
00:08:09,010 --> 00:08:19,629
you know, very basic data science sort of tasks rather than just putting data into gear or error function and run it.

53
00:08:19,630 --> 00:08:26,380
Right. So so you will learn that part of the statistical so the two boxes.

54
00:08:27,370 --> 00:08:34,420
But I own that. We need to learn how to you know, do the on her side of the data.

55
00:08:34,550 --> 00:08:39,820
Right so the data science part so this is the course designed to provide this

56
00:08:40,150 --> 00:08:45,250
sort of introduction to basic skills that are required to do this dirty data,

57
00:08:45,550 --> 00:08:55,810
actually, that you're going to face in practice. So you will learn some principles of basic toolboxes that are very special, you know, for this.

58
00:08:55,810 --> 00:09:03,910
So the early part of data processing and this probably will contribute 80% of your future job.

59
00:09:04,450 --> 00:09:14,380
Okay. So this data analysis overview last part of everything is in the sequence of actions in data science.

60
00:09:14,710 --> 00:09:26,530
But you know, we'll see, you know, is how we're going to still come up with some very basic principles and and data analytics that we

61
00:09:26,530 --> 00:09:33,310
could use to process the data in the early stage of this whole sequence of actions in reality.

62
00:09:33,400 --> 00:09:42,280
Okay. So that's why I want to cover this sort of a systematic introduction to the scope and

63
00:09:42,670 --> 00:09:48,760
context of health data science arising from public health and biomedical sciences.

64
00:09:49,090 --> 00:09:58,390
Okay. So as I said that, so I will focus on the rules and technologies for handling health data.

65
00:09:59,890 --> 00:10:08,200
So we will, you know, use one or two data set as sort of this, you know,

66
00:10:08,290 --> 00:10:18,159
toy data or some data that goes we go through all the steps that I think that are important and, you know, as part of this.

67
00:10:18,160 --> 00:10:31,780
Okay, so the first year of this course, I use a wearable device and so we use this for impractical sympatico wearable devices.

68
00:10:31,780 --> 00:10:39,040
But the devices are so expensive, it's like $1,600 for one device.

69
00:10:39,040 --> 00:10:42,849
I purchased two devices for that course, but unfortunately,

70
00:10:42,850 --> 00:10:48,969
like four years later that those devices are malfunctioning, that no longer working properly,

71
00:10:48,970 --> 00:11:01,750
that I don't have really the budget to buy additional devices that but we're not going to lower down the sort of the scope of the course,

72
00:11:01,750 --> 00:11:12,430
but we will switch to something more manageable and something like we can collect the data we can collect.

73
00:11:13,360 --> 00:11:18,040
And then the whole package of twinning is the same.

74
00:11:18,040 --> 00:11:28,749
It's just now we're looking for different type of data. So last year I used that data like directly collected from our iPhone to or

75
00:11:28,750 --> 00:11:40,150
mobile devices that track our sort of behaviors or were some usage patterns,

76
00:11:40,540 --> 00:11:44,770
how, how much time you spent on the use of your social apps.

77
00:11:45,340 --> 00:11:49,270
So I will teach you where to find this data at your mobile device.

78
00:11:49,930 --> 00:11:55,000
And then last year we only classes are like such a data.

79
00:11:55,540 --> 00:12:03,879
So I want this class which will be the same application so that we can do a may cover part of it.

80
00:12:03,880 --> 00:12:14,020
So so I will teach you like how to integrate data from last year with the data collected from this class using make a learning approach.

81
00:12:14,020 --> 00:12:17,200
Okay. So, so this will help us to, you know,

82
00:12:17,470 --> 00:12:26,460
make a comparison of the behavior of the class last year and the behavior of this year and how do you integrate this data to future.

83
00:12:26,590 --> 00:12:30,729
Okay. So that is part of the sort of training, okay.

84
00:12:30,730 --> 00:12:36,200
Have engaged in data sort of meta analysis later on.

85
00:12:36,220 --> 00:12:44,160
So, so I design a course in the way that we can utilize the data we collected of last year,

86
00:12:44,170 --> 00:12:49,569
but we do the same protocol of data collection this year from you guys.

87
00:12:49,570 --> 00:12:55,360
Then we can see how this data to data can be integrated or use a meta learning approach.

88
00:12:55,360 --> 00:12:58,630
Okay. So that's something that I designed this year.

89
00:12:58,690 --> 00:13:11,799
Okay. So you will see how other sort of, you know, the data preprocessing steps that we can learn from this course.

90
00:13:11,800 --> 00:13:20,440
So how we're going to do data cleaning and export data analysis like looking at outliers, recent data.

91
00:13:21,010 --> 00:13:28,720
This also for us how we will in fact have visualized the data to detect issues that we need to, you know, hand.

92
00:13:29,340 --> 00:13:38,400
And to make to the quality of data. Okay. So I, I strongly believe that the quality of data matters more, much, much more than a sample size.

93
00:13:38,610 --> 00:13:49,770
Now we're talking about big data. Big data all the time. But I think that in biomedical fields, we talk more about rather than data and data quality.

94
00:13:49,840 --> 00:14:01,470
Right. I think that's why in a lot of public health studies or medical study, we start this design study so we can collect relevant data.

95
00:14:01,680 --> 00:14:11,210
We can collect quality data. So data is high quality so that we can really answer questions with, you know,

96
00:14:11,700 --> 00:14:16,649
high precision where we can, you know, if you want, go for costly inference.

97
00:14:16,650 --> 00:14:26,610
You want to go some stronger conclusions rather than simple association sort of completeness of associations.

98
00:14:27,090 --> 00:14:36,450
Then you need to have a better design, your study and you have better quality of your data and so that you can really have a,

99
00:14:36,780 --> 00:14:41,010
you know, scientific conclusion with high trust or confidence.

100
00:14:41,550 --> 00:14:53,140
So that's why we need to do a lot of data quality sort of control and so that we can really enough is enough and,

101
00:14:53,460 --> 00:14:57,030
and then we can start all analysis for that.

102
00:14:57,660 --> 00:15:00,780
So I also talk about data sharing nowadays, right?

103
00:15:00,780 --> 00:15:15,230
So because of availability, a lot of technologies so that data has disappeared becomes a very sort of visible in practice.

104
00:15:15,240 --> 00:15:23,250
And so so data sharing is very sort of common practices sold out of.

105
00:15:23,550 --> 00:15:32,810
So how, what are the rules and technologies that we use to do data sharing and and also reproducibility?

106
00:15:32,850 --> 00:15:43,140
I also want to talk about reproducibility because this is a big issue how how we understand the principles of reproducibility.

107
00:15:43,340 --> 00:15:49,520
Okay. So it's also so there are a lot of other issues probably haven't seen in the order is

108
00:15:49,590 --> 00:15:55,560
the forces and are essential in the House data science so that I'd like to cover.

109
00:15:55,770 --> 00:16:00,419
Of course I cannot cover all the topics, but I will cover some things.

110
00:16:00,420 --> 00:16:06,660
Like I think it's very important for, you know, the training of health data science.

111
00:16:07,160 --> 00:16:16,379
Okay. So Will meets Tuesday and Thursday morning from 830 to I'm very happy to see all of you.

112
00:16:16,380 --> 00:16:22,920
And even this is a very early class. I was a little bit worried about people come to the classroom like this.

113
00:16:23,340 --> 00:16:30,690
It's too early for you guys to come. But and particularly with this matter anyway, I see a big problem with that.

114
00:16:31,920 --> 00:16:39,200
And so. So we currently have 47 students in the class.

115
00:16:41,040 --> 00:16:45,470
It's three sort of students. Sure. Of having a GSI.

116
00:16:46,440 --> 00:16:56,249
I mean, it's school public house that if we have 50 enrollments that we inside will have a GSI, but now we are three students.

117
00:16:56,250 --> 00:16:59,190
Sure. Of that. So we will have a greater.

118
00:16:59,610 --> 00:17:09,209
Oh, let's start you by your friends coming to in early class and so that we can reach the threshold of 50 enrollment.

119
00:17:09,210 --> 00:17:15,090
That's all we can ask for a GSI for this course and but otherwise that we will

120
00:17:15,090 --> 00:17:20,550
have a greater for this course to create your homework and projects and so.

121
00:17:22,630 --> 00:17:34,120
So I do have an office ah, Tuesday afternoon from 330 to 430 if you need additional sort of time to see me outside of the classroom.

122
00:17:34,810 --> 00:17:37,910
Certainly you can email me a full appointment.

123
00:17:38,560 --> 00:17:47,379
And we don't have office hour next week, but of course we have a job candidate coming next week, on Tuesday.

124
00:17:47,380 --> 00:17:55,770
Next Tuesday, so that now there's a similar given by the job kind of Tuesday afternoon, 330, 430.

125
00:17:55,780 --> 00:18:03,730
And so so I don't think there's much to talk about in the next Tuesday.

126
00:18:04,660 --> 00:18:16,510
So that's a start. The office are a week after questions so of all textbooks.

127
00:18:17,830 --> 00:18:22,960
So unfortunately we do not have a very suitable textbooks for this course.

128
00:18:23,110 --> 00:18:39,370
And many places are experimenting or trying to come up with different sort of lectures or notes for this type of course or similar courses.

129
00:18:39,940 --> 00:18:44,709
And I talked to people from all over and John Hopkins, you see,

130
00:18:44,710 --> 00:18:54,000
and the they have similar courses but I asked them which textbooks they use the all you know told me that

131
00:18:54,670 --> 00:19:04,209
they're using materials from multiple books and so so I face the same situation because data science,

132
00:19:04,210 --> 00:19:10,300
health data science is very new things and so that we start as early.

133
00:19:10,300 --> 00:19:17,980
And so I prepared some lecture notes from different sort of floats and so.

134
00:19:21,010 --> 00:19:29,169
I. I have. I will give you the of the lecture unless they're relatively well-written.

135
00:19:29,170 --> 00:19:36,790
But I'm not saying that it's perfect. But you will follow my lectures by reading all the lecture notes.

136
00:19:38,110 --> 00:19:46,780
And also, I have to be selective what topics I believe important to be covered by this course.

137
00:19:46,790 --> 00:19:56,230
So that's that's why I have two elections myself and I will upload lecture notes as the focus of this course.

138
00:19:58,120 --> 00:20:05,799
So we do have guest lectures, we have focus lectures covering different aspects, imaging data,

139
00:20:05,800 --> 00:20:18,520
omics data, you know, this hard data, electronic house record data and, and national registry data.

140
00:20:18,580 --> 00:20:24,700
So, so you will see broader views of this sort of big data work.

141
00:20:25,030 --> 00:20:29,889
So this vast databases that we we handle in practice.

142
00:20:29,890 --> 00:20:34,000
And so I will give you a little bit more detail.

143
00:20:34,000 --> 00:20:40,420
Who will be the guest lecturer coming to give lectures?

144
00:20:40,540 --> 00:20:51,069
Guest lectures. And so I do find that if you if you are good of this hour, then you don't need this book.

145
00:20:51,070 --> 00:21:04,250
But if you want have a book that can help you to sharpen your our skills and expand and grow.

146
00:21:04,270 --> 00:21:08,020
Most of this book is I find this quite useful.

147
00:21:08,020 --> 00:21:16,299
I recommend to people who want to have a little bit more knowledge about our hour will be the software that we are going to use for this course.

148
00:21:16,300 --> 00:21:28,600
So if you want to have a more sort of material to our and to sharpen your skills, that this will be a good book.

149
00:21:28,600 --> 00:21:32,020
I recommend a lot nowadays like Google, right?

150
00:21:32,020 --> 00:21:44,200
So you know, Google probably is the best book that whatever you have that you know, but if you want to have a book aside in your,

151
00:21:44,200 --> 00:21:52,810
you know, full collection that this is the book I use I feel is useful and useful if you want.

152
00:21:55,480 --> 00:22:07,570
So yeah this is the, the first year of course offered you learn like 601 and 650 and I do need your sort of

153
00:22:08,410 --> 00:22:14,649
acquaintances about ah so that's why we ask you to have six or seven particularly in our module.

154
00:22:14,650 --> 00:22:23,390
And so if you did not take a six or seven but have sufficient knowledge of our programing,

155
00:22:23,470 --> 00:22:35,770
that that's fine to stay the course, but we will do a lot of our coding and sort of data processing tools and ah so,

156
00:22:39,280 --> 00:22:45,970
so I will basically cover some of the data cleaning data linkage, data sharing,

157
00:22:46,750 --> 00:22:56,410
data visualization, data privacy and reproducibility and stuff like that along the course.

158
00:22:56,470 --> 00:23:03,820
And those are the basic things that we consider in practice in data science.

159
00:23:06,070 --> 00:23:13,300
So I just probably skip that competence and just talk about, of course, evaluation.

160
00:23:14,290 --> 00:23:18,550
So, well, I plan to give four homework assignments.

161
00:23:19,060 --> 00:23:22,660
Each contains 3 to 5 problems, 5% each.

162
00:23:23,590 --> 00:23:29,320
Then we will have a midterm and then we have two group projects.

163
00:23:29,590 --> 00:23:44,640
So we need to, you know, set up groups and because there's quite a bit to work for for each project, I filled out it for for it.

164
00:23:45,010 --> 00:23:49,300
It's it, you know, is it too much work for a white individual?

165
00:23:49,330 --> 00:24:01,380
So having a group of three members or less of that, you, you, you guys can share some of the, you know,

166
00:24:01,390 --> 00:24:14,650
the workload and that probably is more viable, right, than having a, you know, individual project that's too much work in the in that part.

167
00:24:14,650 --> 00:24:19,420
So and also I want to encourage you to collaborate and because.

168
00:24:19,980 --> 00:24:29,400
This is somewhat mimic the, you know, actual the practical work where you always work with your teammates or your workmates to

169
00:24:29,400 --> 00:24:36,690
come up with something together and so that you can be more productive or you can be,

170
00:24:37,140 --> 00:24:43,140
you know, you can you can develop some sense of collaboration.

171
00:24:43,140 --> 00:24:48,570
So that's certainly I think this is now new and you have probably done in other courses.

172
00:24:48,570 --> 00:24:57,600
But I think particularly for this course, since we are going to process data, we're going to, you know, do a lot of detailed data cleaning.

173
00:24:58,110 --> 00:25:10,740
It's a little bit, you know. You know, the workload is a little bit high so that you probably share the load a little bit too in the closet.

174
00:25:11,340 --> 00:25:18,090
So we have a final exam and also we would do the group presentation.

175
00:25:19,320 --> 00:25:23,940
And so this tour group projects will be somewhat continuous.

176
00:25:24,390 --> 00:25:30,990
It's like. So we have for a school project were handled as it was more data.

177
00:25:31,350 --> 00:25:36,540
But second group project that we're going to handle the data for our entire class.

178
00:25:37,080 --> 00:25:46,140
So so we have this sort of progress of, sort of the complexity of this projects,

179
00:25:46,710 --> 00:25:54,660
starting with it's a relatively small data that we will, you know, do a lot more.

180
00:25:54,660 --> 00:26:01,770
And to to, you know, in the second group project, we will handle bigger things so that you can learn how to,

181
00:26:01,920 --> 00:26:07,780
you know, for westerly build up your project from small one to bigger life.

182
00:26:07,800 --> 00:26:16,020
So that's, you know, two. So that's why I think that same team or project one expect to work on project two.

183
00:26:16,020 --> 00:26:26,910
We don't need to really reform your group because this two projects are connected so that you will stay together and to work through the semester.

184
00:26:27,720 --> 00:26:39,020
So what I plan to do here is that I will set up a Google sheet where I would say you can sign on, and so I will send you a link.

185
00:26:39,030 --> 00:26:46,280
You will sign on. So who you want to talk to for this semester to work on this tool project?

186
00:26:47,530 --> 00:26:51,990
Exactly. Questions. Okay.

187
00:26:52,380 --> 00:26:56,570
Who? So.

188
00:26:57,290 --> 00:27:05,300
So we'll have the midterms scheduled on March seven, right after the of the winter break.

189
00:27:06,320 --> 00:27:13,879
So and I mean, I know that many of you are taking several courses.

190
00:27:13,880 --> 00:27:19,040
I don't know if this this a time are proposed that we can reschedule if you feel necessary.

191
00:27:19,850 --> 00:27:36,799
But this is the time that you tell me whether or not this date is is schedule is enough, is good for considering your other sort of midterms.

192
00:27:36,800 --> 00:27:41,900
I think you will have several other midterms during this period before or after the winter break.

193
00:27:41,910 --> 00:27:45,020
But this is negotiable. Okay.

194
00:27:45,110 --> 00:27:52,850
So this date so that we I not a time if necessary to schedule the midterms.

195
00:27:55,310 --> 00:28:03,170
So the final exam I we don't know schedule if the university will announce the final exam schedule.

196
00:28:05,540 --> 00:28:14,600
So we will update this when we know that schedule of the midterm exam will cover in the two or one week prior to midterm.

197
00:28:16,080 --> 00:28:21,150
And the fine example will come from the Taliban. So that's standard.

198
00:28:23,430 --> 00:28:34,020
So in terms of the classroom expectation, I really want you to come to attend lectures.

199
00:28:35,230 --> 00:28:41,050
You know, certainly this cost is not high. So that I want to attend in person.

200
00:28:42,160 --> 00:28:52,270
So already I have a written lecture notes and many times a little basic presentation on the whiteboard with all the details.

201
00:28:52,300 --> 00:29:04,600
And so that if you missed this impersonal lecture, maybe you will use some of these details I did on the whiteboard.

202
00:29:04,720 --> 00:29:15,780
Okay. So so that I, I think that attending lectures in person would be ideal, I would say.

203
00:29:15,790 --> 00:29:28,750
And of course, that's due to this sort of, you know, possible sort of situations that you could build and other things I record ever lectures.

204
00:29:30,010 --> 00:29:33,640
And so, as you said, I just did it for the lecture today.

205
00:29:34,180 --> 00:29:40,150
And you can always watch my lectures if you do not make the lecture in person.

206
00:29:41,410 --> 00:29:44,770
But I do expect you come to attend lectures.

207
00:29:47,220 --> 00:30:01,200
So also about the homeworks, I, you know, you can discuss this to your peers about your homework sort of questions and solve problems.

208
00:30:02,070 --> 00:30:07,320
They they. But I want you work on your solution independently.

209
00:30:07,500 --> 00:30:11,180
All right. So you give me an independent copy of your homework.

210
00:30:11,220 --> 00:30:21,690
So. So that's something I expect to see your own sort of solutions or your your own or your the solutions coming from yourself.

211
00:30:22,020 --> 00:30:25,790
Okay. Of course, that some discussions are flawed.

212
00:30:26,200 --> 00:30:36,780
So doing homework or still so, so important days.

213
00:30:39,120 --> 00:30:48,929
So. So. We will have the first class today and end of April 18 starts the last day of lecture.

214
00:30:48,930 --> 00:30:53,220
And we do have this lecture on Tuesday, February 18th.

215
00:30:53,910 --> 00:30:58,229
So we have this, you know, nice winter break.

216
00:30:58,230 --> 00:31:03,570
That's the period that you have to have this time off.

217
00:31:03,570 --> 00:31:17,730
And so part our standard on any questions so far, is that clear?

218
00:31:20,840 --> 00:31:24,320
So here is a list of topics that I like to cover.

219
00:31:24,350 --> 00:31:36,310
Maybe I. I don't know if I can cover all of them, but I tried to follow this sort of string of topics for four,

220
00:31:36,350 --> 00:31:43,610
six, 20, so and so I think that I will start with these on this.

221
00:31:45,560 --> 00:31:55,340
So I know that some of you want to take a look at the trials, but clinical trials is very specialized, how to design a study in a clinical setting.

222
00:31:55,910 --> 00:32:03,780
But there is a more general sort of the rules about the design, a study, any study, right.

223
00:32:03,800 --> 00:32:17,360
So so that our Fisher Fisher Ari Fisher started his career in a agriculture research station back in UK,

224
00:32:18,080 --> 00:32:24,470
where he's starting to think about what are the principles of designing a study.

225
00:32:24,900 --> 00:32:32,030
Okay. I mean, why do they want the design designs that because you want to collect data from this thing, right?

226
00:32:32,300 --> 00:32:41,570
So what are the most important sort of principles we should use to design study so that you can collect quality,

227
00:32:41,870 --> 00:32:50,630
high quality data, collect the relevant data, and that the statistic analysis of that is result of would be valid.

228
00:32:50,900 --> 00:32:54,830
Okay. So this is not just a design study design.

229
00:32:54,830 --> 00:33:01,280
A study is trying to, you know, end up with a valid statistic conclusions.

230
00:33:01,280 --> 00:33:10,610
So, so maybe you have learned this in your undergraduate programs or somewhere, but I want to revisit this, the principle of design study.

231
00:33:11,780 --> 00:33:20,810
And this is going to be very important for us to understand what is design study versus observational study?

232
00:33:21,200 --> 00:33:33,500
Because we'll, you know, work on a lot of big data like most of those are big data sort of database are built upon observational studies.

233
00:33:33,530 --> 00:33:37,580
What are issues, observation, study, what are you know,

234
00:33:38,510 --> 00:33:47,720
how is that this method or like linear regression model you learn and other estimation method that you learn will be affected by observational study.

235
00:33:48,710 --> 00:33:57,710
So, so the something I like to cover and to give you some kind of, you know,

236
00:33:59,750 --> 00:34:05,330
a sense of critical thinking, like when you use a study method to analyze the data,

237
00:34:05,960 --> 00:34:12,860
whether or not this analysis is going to generate valid studies results, you can produce whatever you want.

238
00:34:13,250 --> 00:34:18,110
As a statistician, you have so many toolbox available,

239
00:34:18,110 --> 00:34:25,760
you can use one to produce a result that may be favorable to the principal investigator or to your boss.

240
00:34:26,150 --> 00:34:33,110
The result that they like to see. But maybe the analysis is not a very valid analysis.

241
00:34:33,110 --> 00:34:41,900
So so that we as statistician, we're trying to see what are the principal ways that we should use to generate the valid result.

242
00:34:41,930 --> 00:34:47,900
That's something we need to start is designing a study to really see through,

243
00:34:47,930 --> 00:34:53,720
you know, the this does this this sort of data, the principles that we want to use.

244
00:34:54,110 --> 00:35:02,900
I chose this, as I said, that part of the training of the course is that we need to, you know, learn how to do data collection.

245
00:35:03,830 --> 00:35:11,149
And so that's why I like to talk about informed concern for participation.

246
00:35:11,150 --> 00:35:16,820
So I have a forum here, so maybe you can pass around.

247
00:35:18,920 --> 00:35:27,810
It. Yes.

248
00:35:33,960 --> 00:35:47,190
So. So this is the informed consent informed consent form, which is the form that you every study will start.

249
00:35:47,700 --> 00:35:48,110
So.

250
00:35:48,660 --> 00:36:01,860
So if you want to have a subject to participate in the study that you need to really get this kind of information on side and get the permission to,

251
00:36:02,220 --> 00:36:05,610
you know, use the data to collect the data.

252
00:36:05,610 --> 00:36:11,100
And, you know, I will go over this in detail in a few minutes.

253
00:36:11,400 --> 00:36:16,260
So but this is something I'd like to do because this is part of these AI.

254
00:36:16,860 --> 00:36:21,509
These AI is not just running a sort of randomization level.

255
00:36:21,510 --> 00:36:34,470
The subjects were. So this is the one that we start this entity that sort of the collection design study and then you start to recruit participants.

256
00:36:34,950 --> 00:36:49,859
And the first step of doing that is really having this form of sign and that I would cover this what kind of context required to generate form.

257
00:36:49,860 --> 00:36:59,940
And also I need this form signed by you so that we can share with data of this the class and so, so that we can do projects.

258
00:37:02,940 --> 00:37:18,089
So, so as I said that I will work on some of the includes data integration and I work how some propensity score method and subject you know

259
00:37:18,090 --> 00:37:33,730
the practice sampling bias observation study a lot of time we can all control the the sort of the sampling sort of quality and then,

260
00:37:34,000 --> 00:37:36,180
you know, and obvious some biases in the sample,

261
00:37:36,660 --> 00:37:44,490
you can have selection bias or maybe other types of bias in actually the state of the generation processes.

262
00:37:45,090 --> 00:37:52,740
And that we need to find ways to clay to correct to correct that sample sizes.

263
00:37:53,280 --> 00:38:04,829
So one popular method of this is for I will introduce that and this year I will try to introduce instrumental variable that was not done before,

264
00:38:04,830 --> 00:38:16,680
but nowadays instrumental variable becomes very sort of emergent methods in mainly public health studies and medical studies.

265
00:38:18,300 --> 00:38:31,650
And so I will introduce instrument over as well to to as a approach to control simplifies to you look at valid results.

266
00:38:32,760 --> 00:38:39,270
And so I will do a little bit data cleaning the data normalization batch effect.

267
00:38:40,320 --> 00:38:46,170
The batch effect is something like we face quite a bit in practice.

268
00:38:47,190 --> 00:38:52,830
If you have, you know, this data collected from microwave, this works, you know,

269
00:38:53,910 --> 00:39:02,640
this sort of the high throughput technologies you can put all you cannot put all samples like in one array.

270
00:39:02,940 --> 00:39:10,680
You have to somehow put, you know, this your your biological samples and different arrays and process them separately.

271
00:39:11,220 --> 00:39:23,760
Now always the batch. In fact, in process this different arrays may have a different, you know, technical source things that are somehow spatial.

272
00:39:24,780 --> 00:39:28,559
So you expect the some kind of biases,

273
00:39:28,560 --> 00:39:34,050
systematic biases in the data collection from the high throughput data when they are

274
00:39:34,380 --> 00:39:38,550
about like bottle samples from different individuals are put in different arrays,

275
00:39:39,060 --> 00:39:46,709
you know, data processing. So so that's something and batch of fact is related to reproducibility.

276
00:39:46,710 --> 00:39:53,760
So I will probably talk a little bit more on how that's related to reproducibility.

277
00:39:56,340 --> 00:40:03,390
So I will do data visualization and and missing data.

278
00:40:03,780 --> 00:40:12,300
That's another one that we need to, you know, talk a little bit something like a below detection limit on some some,

279
00:40:12,900 --> 00:40:22,290
you know, if you do some of work like environment, how sciences like I involved you, you made sure to let exposure, right?

280
00:40:22,290 --> 00:40:32,830
So sometimes that your technology is not sort of good enough to detect the exposure below a certain level.

281
00:40:33,060 --> 00:40:38,690
So you always have this sort of issue of below detection limit where you mention what you want.

282
00:40:38,700 --> 00:40:49,709
If you have the, you know, blood sample and well, measure some kind of chemical like toxic sort of chemicals like people exposed in the environment,

283
00:40:49,710 --> 00:40:55,920
how scientists whereas you always have this technology limitation that can not detect,

284
00:40:55,920 --> 00:41:01,409
you know, some people below a certain level due to the technology you're facing.

285
00:41:01,410 --> 00:41:08,520
How do you use this kind of recent data words rather than the typical recent data we think about?

286
00:41:08,970 --> 00:41:18,360
So so there are there are some ways that people proposed to do with below detection and.

287
00:41:20,270 --> 00:41:24,300
So I will go to the low to go comparison.

288
00:41:25,160 --> 00:41:38,930
This sort of benching in your hojbjerg procedure and Q value to control the multiple comparison of the

289
00:41:38,930 --> 00:41:52,800
error rate and and finally or in some like robustness and we could do stability and anyway but I,

290
00:41:53,140 --> 00:42:02,080
I want because Cecil, we're going to have the data collected under some protocol that we used for the data collection last year.

291
00:42:02,090 --> 00:42:09,470
So certainly I would cover maintenance as this made me tolerant of this semester.

292
00:42:10,570 --> 00:42:14,650
I don't know if I have time to talk about distributed technologies.

293
00:42:15,370 --> 00:42:25,030
So using Hadoop or the Lambda architecture to do this, distributed computing.

294
00:42:25,040 --> 00:42:29,080
But anyway, so let's see if you have time.

295
00:42:29,260 --> 00:42:36,280
And but that's the sort of the the plan for the lectures.

296
00:42:38,580 --> 00:42:43,700
Semester questions. Hmm.

297
00:42:57,030 --> 00:43:06,729
Plus my clients since. So let me just give you a quick look.

298
00:43:06,730 --> 00:43:12,340
You over this, the outlines, of course, just a little bit more details.

299
00:43:18,330 --> 00:43:29,819
So I already mentioned that there are no proper textbooks available for 620 or similar type of course that I have to really,

300
00:43:29,820 --> 00:43:40,440
you know, pick up the tourist from different books and different sort of places that to to create that lecture room for this course.

301
00:43:40,440 --> 00:43:47,579
So. So basically what I want to do here is to really give you the exposure to the

302
00:43:47,580 --> 00:43:55,500
various basic data analytics are related to health data and so basic concepts,

303
00:43:55,500 --> 00:43:58,590
method and algorithm and deliverables.

304
00:43:59,310 --> 00:44:11,790
So, so I want you learn the sort of the some techniques that we can deal with through data because the raw data we collected from science,

305
00:44:12,390 --> 00:44:17,129
we have to avoid that garbage in, garbage out sort of thing in practice.

306
00:44:17,130 --> 00:44:29,080
So all the time people, I probably this is the fundamental difference between us and the computer science comparisons probably there.

307
00:44:32,440 --> 00:44:38,159
The cannot say they do not emphasize data quality, but data quality.

308
00:44:38,160 --> 00:44:53,670
But I think they believe more about the capacity of their machine or algorithm that somehow can automatically deal with data issues like FIES,

309
00:44:53,670 --> 00:45:01,200
recent data and so on. But here in statistics, we're more healthy.

310
00:45:02,400 --> 00:45:17,130
And if rephrased as we are trained to look through the data more carefully using the principles, and then we emphasize lot more about quality of data.

311
00:45:17,520 --> 00:45:27,690
This we believe that higher quality that will will provide higher likelihood to produce valid results.

312
00:45:27,690 --> 00:45:39,959
So that's why we think a lot about the data quality, confounding and other potential issues that can bias our statistics, policies and conclusions.

313
00:45:39,960 --> 00:45:45,240
We do care about that. We're not thinking about the machine can solve all the problems.

314
00:45:45,600 --> 00:45:49,889
We just think that we need to use our intelligence, our knowledge,

315
00:45:49,890 --> 00:45:55,740
our skills to improve data quality, to make our data more relevant to the questions.

316
00:45:56,790 --> 00:46:05,759
And so that is statistic. We emphasize a lot about quantification, uncertainty, statistics, inference, so on and so forth.

317
00:46:05,760 --> 00:46:12,839
So we're not just saying, okay, here's the result. Produce is this big machine.

318
00:46:12,840 --> 00:46:25,140
And so we put those in a lot of most a lot of the biomedical study, and you don't have sufficient budget or resources to create huge data.

319
00:46:25,290 --> 00:46:25,520
Right?

320
00:46:25,890 --> 00:46:36,080
So what you can do there is really just saying, how can we use limited budget, the resources to, you know, do something that is still valid, right?

321
00:46:36,090 --> 00:46:50,760
So so that's why we like to learn this sort of skills to do this 30 day practice and try to avoid garbage in, garbage out sort of scenario.

322
00:46:50,880 --> 00:46:55,230
So that's something I think it's very good practice.

323
00:46:55,230 --> 00:47:01,770
And so so what we're doing is actually invasive training so students can expect to participate

324
00:47:01,770 --> 00:47:10,799
actively or training activities and possible our classroom design training elements of the course.

325
00:47:10,800 --> 00:47:22,950
So that's why I like to design a course in the way that we are participate in the data collection even from the first sort of action item.

326
00:47:23,520 --> 00:47:32,099
So informed consent for this is first actually recruitment of subjects in data collection in practice.

327
00:47:32,100 --> 00:47:41,670
So, so want to have those sort of basic elements part of the, the training of this house, this is not just algorithm method.

328
00:47:44,400 --> 00:47:47,370
So I do expect you to attend lectures in person.

329
00:47:49,650 --> 00:48:00,000
So as far as data science concerns, a series of training elements should be sort of outlined by this sequence of actions.

330
00:48:00,420 --> 00:48:04,040
Right. So as as you said that, you know,

331
00:48:04,050 --> 00:48:17,459
we start with the study design of neural study design is basically a route in some kind of hypothesis like you like to answer or for example,

332
00:48:17,460 --> 00:48:27,870
like you you want to see whether or not this email or a vaccine is going to be a protective right and will reduce,

333
00:48:27,870 --> 00:48:37,470
you know, our mortality will, you know, enhance human's immune system and so on.

334
00:48:37,500 --> 00:48:50,310
Right? So so this is a hypothesis. You create a company or, you know, some some research lab create, you know, certain vaccine that need to be tested.

335
00:48:50,610 --> 00:49:04,680
Okay. And so that you need to begin this study design, how many how many participants you need to recruit to tested the efficacy of this vaccine.

336
00:49:05,310 --> 00:49:14,170
Right. And in which way you are going to deliver, you know, to to to out the implement vaccination or work.

337
00:49:14,280 --> 00:49:19,409
Right. So some how you're going to divert deliver and you know,

338
00:49:19,410 --> 00:49:25,020
this is if you think of this more carefully and there are a lot of issues and you feel this

339
00:49:25,140 --> 00:49:31,320
you actually the implementation part so you have the rough idea and I like the goal is clear.

340
00:49:31,400 --> 00:49:38,690
Very simple. Like I want to see whether or not this IMR vaccine is going to be useful to

341
00:49:38,690 --> 00:49:44,720
reduce mortality or to reduce severe symptoms of people who are COVID positive.

342
00:49:44,750 --> 00:49:49,650
Right. Or to prevent people from being infected. So this is a good idea.

343
00:49:49,690 --> 00:49:55,820
This is something you want to ask, but we want to implement to finally get through the evidence.

344
00:49:56,150 --> 00:49:59,540
Okay. This becomes a really subtle issue.

345
00:49:59,570 --> 00:50:06,500
There are a lot of things involved actually in the implementation from.

346
00:50:06,620 --> 00:50:09,110
So that's why you need the study design.

347
00:50:09,710 --> 00:50:18,350
This is the part that fast statisticians will be involved in the very early stage, stage of study and typical question.

348
00:50:18,800 --> 00:50:22,600
And this stage of study design is how many?

349
00:50:22,610 --> 00:50:27,590
What's the sample size? Should we have 50,000 people to participate in this trial?

350
00:50:28,280 --> 00:50:34,190
Or we should actually have like 500 subjects to participate in this trial?

351
00:50:34,550 --> 00:50:40,640
So this is the number of subjects, you know, is related to the budget, right?

352
00:50:40,940 --> 00:50:46,460
You can think about a trial with 50 solid subjects with versus 500 subjects.

353
00:50:47,180 --> 00:50:54,200
There's, you know, there's resources and the money involved, you know, financial consideration will be hugely different.

354
00:50:54,680 --> 00:51:05,809
So that the first question that the biostatistician will contribute in practice wouldn't say, we know we will give you a number of this number.

355
00:51:05,810 --> 00:51:13,900
It's not random. Number is a number that we will calculate from certain statistical principles or formulas.

356
00:51:13,910 --> 00:51:25,300
Right. So, so but anyway, here we we we involve in the this study design and also in this study design,

357
00:51:25,430 --> 00:51:30,050
we will all this that is massive that we are going to use for data analysis.

358
00:51:30,590 --> 00:51:37,820
That's a case that after data collection is completed, what kind of standard message that we're going to use to analyze this?

359
00:51:38,180 --> 00:51:41,720
Because this two things are connected to each other, right?

360
00:51:42,050 --> 00:51:45,260
So what's the protocol of the study is fixed.

361
00:51:45,530 --> 00:51:49,220
You can now change the method. You propose the study design.

362
00:51:49,340 --> 00:52:00,340
What do you think the masses will say? Okay, I have 500 subjects, you know, and I'm going to collect it.

363
00:52:00,350 --> 00:52:06,589
Okay. Based on a logistic regression or regression as my sample size.

364
00:52:06,590 --> 00:52:14,090
But after I collect the 500 subjects, now I chance to support that commission, to analyze my data, to draw a conclusion.

365
00:52:14,090 --> 00:52:26,180
This is not a lot. Okay, so in this study design stage, it's not only the sample size and how data will be collected, how to do randomization,

366
00:52:26,980 --> 00:52:34,160
and but also the statistic method that you're going to apply to analyze data after the data collection is complete,

367
00:52:34,220 --> 00:52:37,640
you cannot change the method afterwards.

368
00:52:37,700 --> 00:52:41,120
Okay. So that is why is very, very important.

369
00:52:41,120 --> 00:52:51,199
This is like the protocol that all the investigators, scientists must follow all through this.

370
00:52:51,200 --> 00:52:57,500
So this involved in this third stage and data collection also as part of this action.

371
00:52:58,310 --> 00:53:06,920
So a lot of time that we will be, you know, past that this you will be asked to think about, you know,

372
00:53:07,790 --> 00:53:18,290
sort of grade randomization were some the strategies to amends the data collection procedure for example you some

373
00:53:18,290 --> 00:53:26,779
of study here evolve so we are reaching a plan to collect the 100 senior people according to the original plan.

374
00:53:26,780 --> 00:53:32,090
But we found that it's very difficult to reach out to senior people due to various reasons.

375
00:53:32,150 --> 00:53:39,320
We after one year of data collection, we only we're only able to collect 20 senior people.

376
00:53:39,710 --> 00:53:47,360
Like the question here is like how we're going to deal with this original plan of 100 senior people in the data collection?

377
00:53:48,170 --> 00:53:56,660
I mean, you can calculate if you run this study for another year, you are not able to reach that target sample of 100 senior people.

378
00:53:56,810 --> 00:53:59,480
How you are going to, you know,

379
00:53:59,510 --> 00:54:08,719
adjust the for this data collection procedure and sometimes that in the data collection right and you have recent data you have dropouts.

380
00:54:08,720 --> 00:54:17,780
I mean, you can have a pre estimation of this data attrition rate, for example, you expect there will be 10% of the data attrition.

381
00:54:18,530 --> 00:54:24,920
But after what your study, you found that there are more than 10% of people who drop off the study.

382
00:54:25,860 --> 00:54:31,280
How you deal with that. Right. So, so so we also evolved.

383
00:54:31,420 --> 00:54:36,050
This data collection, actually, the biomedical study, public health studies.

384
00:54:36,140 --> 00:54:42,170
Because the data collection is not, as you know, that easy as we think.

385
00:54:42,180 --> 00:54:51,980
It's just like it's someone asking people to participate or ask them to come to clinical visit every year.

386
00:54:52,010 --> 00:54:58,610
No follow up times that patients may have their own issues or or something, you know,

387
00:54:59,990 --> 00:55:07,370
that leaves off of study and also like data cleaning and data normalization.

388
00:55:07,370 --> 00:55:12,550
This is another step that we evolved very happily because, you know,

389
00:55:12,950 --> 00:55:28,160
of many studies where there are you do now have sort of the capacity to generate like some of the labs.

390
00:55:28,280 --> 00:55:39,080
Okay. Are very specialized, for example, in some environmental health sciences study or even for this sort of this sequencing,

391
00:55:39,290 --> 00:55:43,970
a sort of DNA sequencing labs where, you know,

392
00:55:45,050 --> 00:55:52,700
you just do not have that kind of of sort of tech, technical technology or the technical capacity,

393
00:55:52,850 --> 00:55:59,059
the one place you have to outsource your lab work to somewhere else like us.

394
00:55:59,060 --> 00:56:08,340
We recently send our samples to a H NCI National Cancer Institute to sequencing our DNA.

395
00:56:08,840 --> 00:56:20,240
So so where you have are generated from different labs, from different places, then there are always a normalization issue involved or you're coming,

396
00:56:20,660 --> 00:56:30,380
you know, you have the sample and they're processed in different places and that they're the normalization is it's quite important.

397
00:56:30,620 --> 00:56:33,620
And so there are other issues here.

398
00:56:34,280 --> 00:56:43,790
So then to come to the freedom of data analysis offered that clearly probably when you take the 650 or a linear regression,

399
00:56:43,790 --> 00:56:51,799
you're already had reached to this stage. So this first three stages are not really emphasized much,

400
00:56:51,800 --> 00:56:59,780
but you'll already reach a a stage where clean data has been available and then you are trying to,

401
00:57:00,290 --> 00:57:07,909
you know, run some preliminary data analysis and so on and so forth and then do modeling using the inner model and so on,

402
00:57:07,910 --> 00:57:13,490
and then do validation and delivery. So, so that, you know,

403
00:57:15,440 --> 00:57:29,600
in a lot of courses in statistics and the faster the wait probably for such too much on those two boxes and we do not talk about much of validation.

404
00:57:30,470 --> 00:57:37,280
So probably you heard a word of cross-validation, but that's a baby version of validation, right?

405
00:57:37,640 --> 00:57:44,960
So in some serious scientific of studies, we need to do really validation.

406
00:57:45,020 --> 00:57:53,450
For example, even you pass phase three trial, right? Because clinical trials consist four faces face one face to face three, phase four.

407
00:57:53,450 --> 00:57:57,080
And typically the drug can be proved after phase three trial.

408
00:57:58,070 --> 00:58:06,890
And but after even after phase three trial, he needs to run phase four trial to do some kind of population level analysis to

409
00:58:07,130 --> 00:58:15,950
understand the kind of adverse event and some other issues related to drugs.

410
00:58:15,950 --> 00:58:27,730
So so validation is quite a important part and sort of the sequence of action deliverable as well.

411
00:58:27,740 --> 00:58:41,310
So that's how you deliver this. And for example, you have done all this sequence and then finally you can deliver a product, right?

412
00:58:41,360 --> 00:58:46,790
For example, you know, this credit score, like if you apply your credit card,

413
00:58:46,940 --> 00:58:54,710
you apply your loans somewhere, you will give a credit score for for your credit rating.

414
00:58:54,720 --> 00:59:00,200
So that's the banks where some financing is due, the will give you a loan or something like that.

415
00:59:00,800 --> 00:59:07,440
But that formula, whichever the company you have created,

416
00:59:07,850 --> 00:59:15,860
did that kind of formula or machine to to general request that should be a privacy protected.

417
00:59:16,790 --> 00:59:27,520
So the deliverable is not only about a formula or a mall, it has other issues like how do you protect the privacy.

418
00:59:28,340 --> 00:59:32,180
Right. So, so so you know, there are. Are issues in practice.

419
00:59:32,180 --> 00:59:38,840
People like to, you know, sort of build around to deliver.

420
00:59:39,270 --> 00:59:42,409
Okay. So there's a lot of the practical issues.

421
00:59:42,410 --> 00:59:51,910
We think that more than just a bunch of models or software packages.

422
00:59:54,800 --> 01:00:00,500
So of course, that sits 20th and 22 of the cover of most of opera stream.

423
01:00:00,500 --> 01:00:09,469
The steps of this study design think excitedly about cleaning and do little you know the preliminary analysis.

424
01:00:09,470 --> 01:00:16,550
I believe that there are lot of more courses that you can take to take it second the downstream departments.

425
01:00:19,400 --> 01:00:26,660
So that's something I let you know what the focus of this course.

426
01:00:28,700 --> 01:00:32,080
So let me just oh, this is too small for you to read.

427
01:00:32,240 --> 01:00:35,290
Okay. So but data collection.

428
01:00:37,130 --> 01:00:40,680
So so in this course,

429
01:00:40,680 --> 01:00:47,930
that one tiny element is that you participate the data to collect your own data that that's that's I want sort of

430
01:00:48,740 --> 01:00:57,320
ask you to participate and the process to analyze the data through the homework so project is doing this semester.

431
01:00:57,710 --> 01:01:06,260
So I think this is very important part of training because we is starting to collect data that there are many other issues

432
01:01:06,800 --> 01:01:16,520
sort of naturally arising from this sort of practice or this procedure and you start thinking more about some basic things.

433
01:01:16,520 --> 01:01:25,489
I think that is a part of this data science because you I mean this is baby version,

434
01:01:25,490 --> 01:01:33,680
but I want just you have learned the concrete thing, how the collection will look like and what are the potential issues.

435
01:01:34,370 --> 01:01:45,770
Okay, so maybe you go to job interview and this will be a good experience is usually you know, you know of student you know,

436
01:01:46,160 --> 01:01:55,549
having some more concrete sort of experience and actually data collection is important because there are some under

437
01:01:55,550 --> 01:02:04,590
issue may not be covered by textbooks or you can use more if you think more about what are actually data science.

438
01:02:06,500 --> 01:02:11,540
So we'll see how to share data. So individual will collect your data.

439
01:02:11,540 --> 01:02:15,360
But the question here is how you're going to share data with your peers, right?

440
01:02:15,400 --> 01:02:18,650
So learn the way we share data.

441
01:02:19,280 --> 01:02:23,390
So how do we protect data privacy? Okay.

442
01:02:24,230 --> 01:02:28,670
And how do you how do you collaborate and how do you deliver?

443
01:02:28,790 --> 01:02:40,040
Okay. So this sequence of actions will really build upon the the data collection.

444
01:02:40,310 --> 01:02:48,860
So I saw already mentioned that in 2020s when the semester we use packet E4 which is now going to be used this year.

445
01:02:51,380 --> 01:02:59,990
So, so what we are going to do this semester is that we will use the mobile device screen activity to study

446
01:02:59,990 --> 01:03:08,600
behavior patterns shall be you have more activation my iPhone or iPad or whatever tablet you have.

447
01:03:08,930 --> 01:03:19,580
Okay. So, so I will use the same data collection protocol this year so that we can, you know, do a meta learning,

448
01:03:20,030 --> 01:03:29,870
as I mentioned, of the we use the same sort of data collection protocol for the class last year.

449
01:03:30,110 --> 01:03:38,300
So now we have two data together. Then how do you merge the two data set together with data privacy protection?

450
01:03:38,420 --> 01:03:42,800
Okay. If I'm not going to give you a raw data from previous class,

451
01:03:43,310 --> 01:03:49,550
what kind of information I need to give you so that you can sort of merge the two through that together.

452
01:03:51,680 --> 01:04:06,080
So you have a dataset, so you have a dataset from the class 2020 is already collected.

453
01:04:06,290 --> 01:04:15,530
Okay. So if we have data like from about 50 students, this data is collected now we're going to collect the data from this class, right?

454
01:04:16,460 --> 01:04:24,980
So we have three so we have 47th employment.

455
01:04:25,070 --> 01:04:29,090
Also, we collect the data and I will tell you what they are going to collect.

456
01:04:29,790 --> 01:04:33,169
Now we want to merge the two data. Take care of the simple way to merge.

457
01:04:33,170 --> 01:04:48,799
This is of course your excel, right? So of course I have access to the raw data and I, I have this sort of approval that I can see the data and.

458
01:04:48,800 --> 01:04:52,970
But you do not have our approval. You are not of the.

459
01:04:53,100 --> 01:05:08,670
Previous days. Okay. So so the approve is informed concern foremost not a sort of do does not have a statement this cost with able to use the data.

460
01:05:08,730 --> 01:05:14,700
Okay but the natural way to merge the data of course you append this to data center together

461
01:05:15,000 --> 01:05:21,000
and really into our but now I would tell you that your you have no access to raw data.

462
01:05:22,550 --> 01:05:35,190
Okay. So my question here would be like if I have the privacy or data comparability protection from the data collected from purpose cost,

463
01:05:35,580 --> 01:05:44,460
what kind of information that I should pass to this site so that I would have a valid analysis?

464
01:05:44,790 --> 01:05:49,320
Okay. So you can think about a workable Oracle situation.

465
01:05:49,980 --> 01:05:59,670
So suppose theoretically, suppose the radical if I can append this to theta to I can merge this to the raw data together.

466
01:06:00,000 --> 01:06:06,000
Right. So this is 2022. Theoretically, hypothetically.

467
01:06:06,420 --> 01:06:09,540
Suppose I can put this to theta raw data. Take care.

468
01:06:09,990 --> 01:06:15,899
I can write a meta regression. All right. I use my R function.

469
01:06:15,900 --> 01:06:19,710
I can find a can do a data analysis.

470
01:06:20,400 --> 01:06:28,710
Okay. So that I should have let's see, I have a three to estimate, some kind of a sort of association.

471
01:06:28,740 --> 01:06:34,649
Right. So I have my y, I have my ex, I have this merge the data together.

472
01:06:34,650 --> 01:06:37,980
I can estimate the beta. Okay. No problem.

473
01:06:38,700 --> 01:06:42,810
If you are allowed to access the data from 2020, just append them together.

474
01:06:43,200 --> 01:06:50,520
I can get whatever the result you want. But the question here is okay, this is my oracle one.

475
01:06:57,860 --> 01:07:01,760
This is my oracle, my friend. So this is the one idea I come to you.

476
01:07:02,930 --> 01:07:09,470
Now, the question here, as you do not have the nexus of of the raw data of 2010 is.

477
01:07:13,300 --> 01:07:17,470
You do not have access to Raleigh in 2022 because of data privacy.

478
01:07:17,470 --> 01:07:25,060
Your view that you do not have this sort of the IRP that allow you to access the data.

479
01:07:25,960 --> 01:07:30,490
Now the question here is how I'm going to do Maitland.

480
01:07:31,430 --> 01:07:35,800
Okay, so how I'm going to use summary statistics, right?

481
01:07:39,550 --> 01:07:44,530
What kind of summary statistic I could generate? For example, being very standard deviation.

482
01:07:45,640 --> 01:07:52,570
Because meeting standard deviation or the input data information based on aggregate information,

483
01:07:52,630 --> 01:07:57,940
you cannot tell individual information from being more standard deviation from this data.

484
01:07:58,510 --> 01:08:06,790
So what kind of summer statistic I need to use you more to rate and to to analyze this result

485
01:08:06,790 --> 01:08:15,220
that I could produce estimate that is exactly the same as this one so I can have my main topic.

486
01:08:17,990 --> 01:08:22,640
Okay. So so now I'm under this consideration of data privacy.

487
01:08:23,240 --> 01:08:30,479
I asked the question what kind of summer statistics I need so that when I put

488
01:08:30,480 --> 01:08:35,180
that this summer statistics together with my raw data from 2010 history class,

489
01:08:35,810 --> 01:08:41,150
I can produce a meta analysis that would be exactly same as my oracle.

490
01:08:41,900 --> 01:08:52,790
It sounds like a magic, right? So because this Oracle estimate is coming from the data that Merced this two data set

491
01:08:52,790 --> 01:08:59,719
of raw observations together you produce but now because you have this privacy issue,

492
01:08:59,720 --> 01:09:03,280
you have no way to access the probability of 2022.

493
01:09:03,320 --> 01:09:09,290
But that's what kind of summaries, statistics that you need your work to produce.

494
01:09:09,290 --> 01:09:15,650
A make estimate that you call exactly same as your oracle is not possible.

495
01:09:15,920 --> 01:09:21,050
If so, how? You're going to do that. Okay, that's something we've come to work on.

496
01:09:21,260 --> 01:09:29,930
So that also very important of the the meta learning part of that.

497
01:09:30,620 --> 01:09:40,660
So so you do this sort of data integration analysis, this data privacy consideration.

498
01:09:41,270 --> 01:09:47,340
And of course, this obviously is not a very simple thing to do, right?

499
01:09:47,360 --> 01:09:54,530
It needs a little bit of thing to do and fairer learn a fairer learning.

500
01:09:54,530 --> 01:10:02,420
It's not a one like we have 47 individuals that you decide not going to share your personal data with the class.

501
01:10:03,320 --> 01:10:12,680
Okay. Now what kind of information you like to share with the class and to to achieve the some kind of analysis?

502
01:10:13,190 --> 01:10:20,480
Okay. So, so we will do this fairer learning as well and I would introduce this later.

503
01:10:23,090 --> 01:10:30,020
So now coming to the part of data collection, so, so this is from my iPhone.

504
01:10:31,370 --> 01:10:36,219
So, so this is the name of why I just bought I don't want to,

505
01:10:36,220 --> 01:10:46,610
you know that but but anyway you'll go to your settings general setting and then you can find one place maybe different.

506
01:10:46,610 --> 01:10:55,879
Like if you use Android worth for on the other iPhones, you may have different set up, but this is this Apple iPhone.

507
01:10:55,880 --> 01:11:02,780
Okay, so your goal here and in general setting, you can find screen time and then here,

508
01:11:02,780 --> 01:11:10,520
if you move this side, this is you know, you have this you have the dates at which date.

509
01:11:11,540 --> 01:11:16,670
And there you have essentially say all activities here.

510
01:11:17,060 --> 01:11:22,670
And this world records your usage of the squint, right?

511
01:11:24,770 --> 01:11:33,320
So, for example, this is I think this is 2022 or 2020, I don't remember, but it's January 13.

512
01:11:33,920 --> 01:11:39,350
I use to use my screen time, 3 hours, 36 minutes, that's Wednesday.

513
01:11:40,340 --> 01:11:42,320
And this is the distribution of that.

514
01:11:42,410 --> 01:11:51,350
We don't need to do that until and there are so one place you can create the data like how much time you use your social package.

515
01:11:51,740 --> 01:12:00,740
Okay, so this is total screen time. This is a time social that is the total screen time that you use for your social access here.

516
01:12:02,120 --> 01:12:05,330
So you can say that I use a lot about WeChat. This is Chinese.

517
01:12:06,080 --> 01:12:14,770
So the version of social app that I use a lot of this a lot of time and I will use other like photos and

518
01:12:15,110 --> 01:12:24,409
we don't need to go that detail like this but you can record is social time and also you look at this

519
01:12:24,410 --> 01:12:33,889
size where you can find the the first pickup okay probably number of days everybody will do the first thing

520
01:12:33,890 --> 01:12:43,160
after wake up is get your iPhone either check your your your take to time or check whatever message.

521
01:12:43,190 --> 01:12:52,009
Right? So this first pick up time maybe is the sort of the your wake up time, right?

522
01:12:52,010 --> 01:13:00,800
So for however, you know, if I use the is the standard time this you know, this is over the midnight.

523
01:13:00,890 --> 01:13:11,330
This is not really a sort of the right sort of pick up time corresponding to my wake up time.

524
01:13:11,630 --> 01:13:16,040
Right. This is 1209. I haven't, you know.

525
01:13:17,050 --> 01:13:22,600
Went to bat. Yeah. So this is probably sometime I pick up before I go to sleep.

526
01:13:23,200 --> 01:13:28,239
So what we can do here is that you can change the timezone here.

527
01:13:28,240 --> 01:13:31,990
I use standard timezone, right? You can change the timezone.

528
01:13:32,230 --> 01:13:40,600
And then this time will shift. If I change to classic Pacific Standard Time and my first pick up time will be different.

529
01:13:41,560 --> 01:13:46,990
That's the time that you can use. I can record your actually first pick time.

530
01:13:47,620 --> 01:13:52,030
Of course, it will transfer transformed back by 3 hours or something like that.

531
01:13:52,240 --> 01:13:56,170
But this first pick of time is inaccurate, is not right.

532
01:13:56,560 --> 01:13:58,540
So you say when you do a collection,

533
01:13:58,540 --> 01:14:08,080
you do have this little issue as this 1209 is not actually the first pick up time of that day corresponding to my sort of wake up time.

534
01:14:08,650 --> 01:14:17,330
And then what I could do is in data collection as shift my time zone, 3 hours, six hour, I can use Hawaii time, whatever.

535
01:14:17,590 --> 01:14:23,350
I remember that absolute the parameter I used to shift my time.

536
01:14:23,680 --> 01:14:27,600
Then suppose I use the Pacific time and then.

537
01:14:28,020 --> 01:14:32,230
Then this time will no longer be my first peak time of that day.

538
01:14:32,800 --> 01:14:37,870
So I will have another time and other time will be adjusted back,

539
01:14:37,960 --> 01:14:45,330
right when I actually get my first pick up time of that day so that you can know total number of pick ups.

540
01:14:45,370 --> 01:14:55,750
Right. So for people who have addiction to sort of iPhone or mobile device follow time to pick up a phone with no purpose.

541
01:14:55,870 --> 01:15:00,520
They just want pick up somehow. To have the feeling is the connection to the world.

542
01:15:00,820 --> 01:15:05,010
So they just pick up nothing. They put that and you know, I don't know, like.

543
01:15:05,410 --> 01:15:14,450
So you can see that number of pickups is something of a surrogate marker about the picture to pull more.

544
01:15:14,710 --> 01:15:17,950
Some people like to speak of powerful, visible purpose.

545
01:15:17,950 --> 01:15:28,040
No, there's no random alarm. They just pick up and try to see something with no and reason why they want to pick up a phone to read.

546
01:15:28,150 --> 01:15:32,110
Right. Because they just want feel like be connected in some sense.

547
01:15:32,680 --> 01:15:41,250
So this number pickup will be a useful variable to indicate their addiction to mobile device.

548
01:15:41,260 --> 01:15:48,729
I'm not saying this is the one that, you know, you may have a lot of friends that give you a call like 100 calls, everything.

549
01:15:48,730 --> 01:15:56,020
But but anyway, this is something that we can measure and to see, you know,

550
01:15:57,280 --> 01:16:03,910
and then I give you this sort of the Excel sheet, everyone would have this.

551
01:16:04,450 --> 01:16:13,359
So you have a date, total screen time, you see that and total screen is by this hour.

552
01:16:13,360 --> 01:16:17,500
Then you can I give you our code? You can transform to the minute.

553
01:16:18,190 --> 01:16:22,270
So how many minutes? Okay. And then this is a screen.

554
01:16:22,570 --> 01:16:31,090
3 hours. It's 36 minutes. You can write our code to convert this into minutes.

555
01:16:31,100 --> 01:16:35,230
Then you have total screen time. Okay.

556
01:16:35,530 --> 01:16:39,490
This is that's a number of pickups and the first pickup.

557
01:16:40,220 --> 01:16:45,410
Okay. So those are the things that you can start to record.

558
01:16:45,820 --> 01:16:50,440
You heard this, right, because you have had that influence from me.

559
01:16:51,340 --> 01:16:56,200
Starting today, your data will be somewhat different from the data before,

560
01:16:56,650 --> 01:17:01,840
because before you come to the costs, you did not expect that our to take such data.

561
01:17:01,840 --> 01:17:09,880
Right. So psychologically, you didn't all prepare to collect data before today.

562
01:17:10,240 --> 01:17:13,150
So I want to you generate your baseline data.

563
01:17:13,570 --> 01:17:23,530
So that's your base, the baseline data, your behavior do your your behavior pattern with no inference of what I set up to know.

564
01:17:24,250 --> 01:17:32,920
Okay. So go home and do the whole homework. And I was sure this Excel sort of format then track the data.

565
01:17:32,920 --> 01:17:38,710
That's the absolute baseline you have. So this no inference what I said in the class today.

566
01:17:44,710 --> 01:17:51,880
So finally, I want to tell you that. So we will have all this lectures coming.

567
01:17:51,880 --> 01:18:00,130
We're talking about National Disease Registry data and choose from about electronic health record data.

568
01:18:00,580 --> 01:18:08,530
Just come talk about the imaging data. And Lars is going to talk about principles of personalized medicine since Islamic State.

569
01:18:09,130 --> 01:18:14,770
So we'll help with guest lectures and I don't have time to.

570
01:18:16,600 --> 01:18:20,670
I don't have time to read this for you. You do not have time to read this.

571
01:18:20,680 --> 01:18:25,870
Then we can bring this to the cast Tuesday next week.

572
01:18:26,320 --> 01:18:37,070
We can go over this. And if you agree with this and then we can you can say and can you call me when you much.

