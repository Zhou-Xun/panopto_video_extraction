1
00:00:30,260 --> 00:00:40,970
It's just it's going to our.

2
00:00:46,510 --> 00:00:55,930
So I thought about it to.

3
00:01:07,370 --> 00:01:20,300
Yes. Believes that this is a first step.

4
00:01:21,800 --> 00:01:25,870
You know, I was talking so slowly here.

5
00:01:25,880 --> 00:01:39,470
I'm like so early. We barely got out of the house at, like, 20 years ago.

6
00:01:39,710 --> 00:01:55,050
Yeah. And I was walking, strolling, strolling over, and everybody was like, yeah, I was.

7
00:01:57,490 --> 00:02:08,900
I like your clothes every night, you know?

8
00:02:10,010 --> 00:02:27,860
So I think, honestly, since, you know, I've was looking for something, I.

9
00:02:34,110 --> 00:02:39,730
Yeah. Oh, you're bringing some pictures in here.

10
00:02:40,840 --> 00:02:44,070
What kind of. Well, there's like. No, not really.

11
00:02:45,650 --> 00:02:51,840
That's because as you're saying. Yes, I was thinking about it.

12
00:02:51,840 --> 00:02:58,080
I didn't bring cookies. But anyway, I know how to set up the screen again.

13
00:02:58,100 --> 00:03:03,340
I think there's a double screen again. This time I want to drive to.

14
00:03:04,320 --> 00:03:08,640
The one I saw here is not a match, the one on the screen.

15
00:03:08,920 --> 00:03:14,370
Could you argue with that? How do. I don't know if you see I.

16
00:03:17,430 --> 00:03:21,140
Really? About something about.

17
00:03:22,600 --> 00:03:35,820
So if I get it right and then you catch another batch of them later on, which usually means 30 minutes of a 15 minute run straight.

18
00:03:37,540 --> 00:03:41,740
So my second place here is a 45 minute commute.

19
00:03:42,190 --> 00:03:58,020
Sometimes that's so irritating. I understand, but my wife enjoyed it and I don't really like it.

20
00:04:00,640 --> 00:04:14,540
That's what we did. And I'm like, Yeah, it's a great idea, but if I have to do that.

21
00:04:14,590 --> 00:04:17,900
But it was also great.

22
00:04:17,920 --> 00:04:24,560
And to be mindful, it's not for me to work out anything so much.

23
00:04:25,480 --> 00:04:35,590
I think that's what happens, is that you succeed except that you try to interview again.

24
00:04:35,830 --> 00:04:39,840
I ask the I don't feel that.

25
00:04:44,890 --> 00:04:48,520
Sorry. So is it for this semester?

26
00:04:48,820 --> 00:04:52,510
Yes. Oh, that's weird. I guess we don't have a chance.

27
00:04:54,070 --> 00:04:57,130
Yeah, it's complex systems.

28
00:04:57,610 --> 00:05:04,740
Three 2531. Which means no means so.

29
00:05:04,750 --> 00:05:14,110
Fine. Just means of measles and missing things like how information is spread, how the means spread, and how infectious disease spreads.

30
00:05:14,470 --> 00:05:27,490
Expert showing, I think teachers who pay you for that they probably would like to joke because they something that makes heroin for you.

31
00:05:28,030 --> 00:05:33,740
It's what I was going to say. So I don't want to get.

32
00:05:36,530 --> 00:05:46,860
But. I think they're doing something with the class project.

33
00:05:50,350 --> 00:06:04,940
Is. He never did before.

34
00:06:05,130 --> 00:06:14,200
And I think that rabbit hole I hear this, but I helps the symptoms of this really serious disease.

35
00:06:15,520 --> 00:06:29,310
I fight cancer for like 5 to 6 months for the candidates page of Begley like a relative of.

36
00:06:33,030 --> 00:06:39,270
But you'll see. Sophie, that looks like little, little person up there.

37
00:06:39,360 --> 00:06:45,089
Yeah. Little bit in the air space here.

38
00:06:45,090 --> 00:06:52,260
It looks like it's not really. My people want me to say, you know, I don't.

39
00:06:57,990 --> 00:07:03,590
Supply agreements and interest rates that you worked out in.

40
00:07:15,700 --> 00:07:22,040
The end is there for you so that you can say, I have vertigo.

41
00:07:23,080 --> 00:07:29,890
I have these warning signs. What you can't see.

42
00:07:31,420 --> 00:07:38,260
It really is necessary for of this woman who died from cancer.

43
00:07:42,580 --> 00:07:50,709
There was a fascinating sense of the central podcast that I was listening to,

44
00:07:50,710 --> 00:07:58,870
of course, my wife that was lying about having cancer just like everybody in her life.

45
00:07:59,730 --> 00:08:02,830
So that's why I don't know. I think that.

46
00:08:07,690 --> 00:08:13,430
Winds are six one. Yes, I do procrastinate.

47
00:08:17,500 --> 00:08:27,640
I'll try to ask you.

48
00:08:27,910 --> 00:08:38,470
I'll go with you. I feel good because you heard me talk to you in the face of a load.

49
00:08:41,950 --> 00:08:46,900
So when they finally figure out how to do this stuff.

50
00:08:49,240 --> 00:09:04,680
You can come. Why I don't see this, which is interesting, but not because as a normal person,

51
00:09:05,520 --> 00:09:19,860
there is this side of me that I and literally I take turns like that so much.

52
00:09:28,770 --> 00:09:37,860
It's just that you have a cold or a sinus infection or a for years.

53
00:09:37,860 --> 00:09:45,730
BLOCK Which is a little bit less so.

54
00:09:46,940 --> 00:09:51,120
So sometimes it works.

55
00:09:57,730 --> 00:10:05,500
But you really don't see any work to learn how to do this stuff next.

56
00:10:05,550 --> 00:10:09,690
My brain. I don't know exactly how.

57
00:10:10,230 --> 00:10:13,720
Where to find that parameter to reset it.

58
00:10:15,890 --> 00:10:20,340
Cool. Okay.

59
00:10:20,350 --> 00:10:26,910
So. So good morning.

60
00:10:26,920 --> 00:10:38,550
And we just had a GSI set up for this course and flame name.

61
00:10:38,670 --> 00:10:44,250
It's memorial. Okay, so he will be the GSI for this course.

62
00:10:45,240 --> 00:10:50,250
He will come to see you Tuesday next week. And so he will introduce himself.

63
00:10:52,140 --> 00:11:01,890
So Flynn took a course last year, this 620 last year, and then the GSI for this course of this year and sold out.

64
00:11:03,270 --> 00:11:18,270
[INAUDIBLE] work. He will set up office hours and basically assess, do you do this semester, do all the homework and projects and your data as well.

65
00:11:18,360 --> 00:11:24,569
Okay. So that's all you see.

66
00:11:24,570 --> 00:11:28,170
And oh, by the way, if you haven't signed this,

67
00:11:28,470 --> 00:11:41,129
you form concerned and please do so and also create a folder for the assignment in case that you want something to be more informed,

68
00:11:41,130 --> 00:11:45,450
concerned with online and in the new canvas.

69
00:11:45,450 --> 00:11:53,520
So the way we do need this piece of documentation, you were to move forward for this sort of data collection part.

70
00:11:54,180 --> 00:11:58,230
And so homework one would be based on your data.

71
00:11:58,980 --> 00:12:02,910
I want you to start to play this your data, your baseline data.

72
00:12:03,630 --> 00:12:06,990
So that's why I'm going over my data.

73
00:12:07,170 --> 00:12:20,280
You can see some of my secrets, right? So how I use the my iPhone on the screen time behavior, you know, so that's I want to.

74
00:12:23,180 --> 00:12:35,530
Continue this and. In this shot, the the first pickup that we saw last time and.

75
00:12:41,220 --> 00:12:57,140
So I talk about this. So the the first time and we know that at least this might happen, that when they wake up,

76
00:12:57,140 --> 00:13:04,970
the first thing they do is really just pick up my iPhone to sit in time, work my iPhone alarm.

77
00:13:05,210 --> 00:13:09,070
You know, they sort of ran off and I need to turn it off.

78
00:13:09,080 --> 00:13:17,569
But anyway, I can certainly think in most cases that the the first pickup time of your

79
00:13:17,570 --> 00:13:22,670
iPhone or your mobile device will be approximately the and your break up time.

80
00:13:23,120 --> 00:13:31,730
So this so wake up time could be a variable that we use sometime to study sleep health.

81
00:13:32,000 --> 00:13:41,240
Right. So a sleep house, this maybe is not, you know, a big issue for young people, but it is a, you know,

82
00:13:41,420 --> 00:13:52,670
kind of chronic disease for some senior people who have difficulty of sleep or have sleep disorders and so on.

83
00:13:53,120 --> 00:14:03,919
So people can use the this kind of digital features to study their, you know, number of checkups during the day and so on.

84
00:14:03,920 --> 00:14:15,200
So forced to sleep a study sleep course. But anyway, in this case, we want to see how we're going to display data, visualize the pattern of our,

85
00:14:15,200 --> 00:14:23,030
you know, first pick up of 88, you know, so so this sort of of the wake up time.

86
00:14:23,150 --> 00:14:34,280
So if we, you know, think about this 24 hours as a circle, this kind of manifold, then it's more intuitive for you to,

87
00:14:34,640 --> 00:14:44,930
you know, project your actually first pick up time on the circle so you can say the blue dots indicate one.

88
00:14:44,930 --> 00:14:51,280
First up, you know, so the data I collect from my iPhone, so you can see that clear there.

89
00:14:51,290 --> 00:14:57,170
So you look the largest the data cloud around, you know, this seven, eight.

90
00:14:57,590 --> 00:15:05,300
So basically I'm a birthday person and some people you may have your first pick up somewhere to do and you know,

91
00:15:06,260 --> 00:15:17,839
I don't know like you will see your first pickup time and and see where, you know, the most frequent sort of pick ups you typically do.

92
00:15:17,840 --> 00:15:23,719
But this will display actually your sort of wake up time, approximately.

93
00:15:23,720 --> 00:15:30,200
And so this is the way to, you know, arrange your data on their circle.

94
00:15:30,740 --> 00:15:33,290
Maybe can actually there's a clock, right?

95
00:15:33,290 --> 00:15:49,880
So so so I already mentioned that, you know, this 90 degree will be 6 a.m. and the the 180 degree is, you know, they are a noon and 12 p.m. and then,

96
00:15:50,450 --> 00:15:50,809
you know,

97
00:15:50,810 --> 00:16:02,180
this 6 p.m. will be your sort of three over pie and but anyway so so you can see my are cold how to use our packets circular to display data.

98
00:16:02,300 --> 00:16:09,950
Okay this is just display and then you can go to create histogram right.

99
00:16:09,950 --> 00:16:21,350
So typically in. As typical the statistical sort of analysis.

100
00:16:21,740 --> 00:16:24,920
Your histogram is something like this, right?

101
00:16:28,650 --> 00:16:33,540
So this is the histogram usually you see because you think about your data is collected.

102
00:16:33,540 --> 00:16:43,980
The farmer need your space now because our data is collected from a circle.

103
00:16:46,260 --> 00:16:52,610
His actual angular data is a circle. Meaning that. It was not.

104
00:16:58,560 --> 00:17:05,459
So that you will do the the histogram on the circular.

105
00:17:05,460 --> 00:17:18,210
So you can see basically what time is the, the, the, the time window over which that I have my most frequent pick ups.

106
00:17:18,420 --> 00:17:23,520
So this histogram gives you a summary of the distribution of your data.

107
00:17:23,920 --> 00:17:34,440
Okay. So that can give you kind of a statistic that can see the distribution of your data and look at which time window that I have.

108
00:17:35,100 --> 00:17:48,720
So most of frequent pick ups. Okay. So here I use 30 minutes as the sort of pin size and so so that you can decide, you know, the.

109
00:17:49,050 --> 00:17:49,920
You know, the.

110
00:17:52,320 --> 00:18:01,440
The West of this being site like this is a burger similar to what you do in the histogram because here you will do histogram right plot.

111
00:18:01,800 --> 00:18:06,870
Then you will decide how many beans you like or what's the size of being you like.

112
00:18:06,900 --> 00:18:17,190
Basically, you look at how many data points will fall into this specific little window to look at the different quantity of occurrences of the data.

113
00:18:17,190 --> 00:18:20,550
Okay. So that's basically a distribution of your data.

114
00:18:20,820 --> 00:18:28,620
So very similar idea in this sort of circle or situation, you decide what's the bin size here?

115
00:18:28,620 --> 00:18:38,390
I use 30 minutes, okay. But you have to transform the, you know, actual minutes into the hour cleanse we call the radiant.

116
00:18:38,580 --> 00:18:38,880
Okay.

117
00:18:38,940 --> 00:18:53,670
Because that, you know, as I said, this circle data is based on the, you know, the arc length as this unit to, you know, display this distribution.

118
00:18:53,970 --> 00:19:04,350
You can see it's 30 minutes of because the actually the software does not really read in the way of in terms of minutes.

119
00:19:04,350 --> 00:19:13,940
It just tells you what what's the you know, the pin size is determined by the Oculus or the size of the arc list.

120
00:19:14,250 --> 00:19:16,830
So you transform this automatically.

121
00:19:17,070 --> 00:19:30,120
And then you are you can specify that parameter in the, the histogram plot and you can see this kind of histogram plot in here.

122
00:19:30,570 --> 00:19:36,690
And then of course that you can do the little bit density estimation here.

123
00:19:37,170 --> 00:19:46,770
You can see clearly that this is bimodal distribution most of the time that I have my first pick up around this,

124
00:19:47,400 --> 00:19:51,870
but I have done is at least reading from 7 to 9.

125
00:19:52,230 --> 00:19:55,740
And then I have not heard little of, you know, modes here.

126
00:19:56,220 --> 00:20:04,470
So here I have some sort of activities of the first up around 4 to 6.

127
00:20:04,740 --> 00:20:10,410
Okay. So but anyway, different people may have a different this kind of distribution.

128
00:20:10,410 --> 00:20:14,690
Maybe you have your model rather than the same model.

129
00:20:14,700 --> 00:20:19,829
And, and you know, this can be related to your schedule.

130
00:20:19,830 --> 00:20:23,069
If you have early class, maybe you will wake up early.

131
00:20:23,070 --> 00:20:27,330
But if you do not have early classes, that maybe you'll wake up later.

132
00:20:27,400 --> 00:20:34,590
And it could be I mean, probably you can look at your data and see what that looks like,

133
00:20:35,190 --> 00:20:44,940
that that's the way to, you know, look at the sort of your data first pick up.

134
00:20:46,380 --> 00:21:03,620
And so in terms of distribution, of course, you know, you have so normal distribution is the one that you typically will work out, right?

135
00:21:04,380 --> 00:21:12,060
So, so in a normal distribution, you have to keep parameters to sort of characterize the distribution.

136
00:21:12,060 --> 00:21:17,700
What is the mean? Right. Another one is the standard deviation.

137
00:21:18,420 --> 00:21:21,580
In more general term, people call dispersion, right?

138
00:21:21,660 --> 00:21:30,739
So basically. So basically standard deviation that describes how of what's the level of spread

139
00:21:30,740 --> 00:21:35,330
of the distribution you can have when you have smaller standard deviation,

140
00:21:35,330 --> 00:21:39,620
your distribution will be more concentrated or less spread, right?

141
00:21:39,920 --> 00:21:43,249
So this or more dispersed or less dispersed.

142
00:21:43,250 --> 00:21:48,020
So standard issue in the more general term in the delivery is called dispersion.

143
00:21:48,620 --> 00:21:55,010
So now thinking about this circle of data, you have something very similar.

144
00:21:55,130 --> 00:22:00,240
So this is called. Normal distribution miles markers bad.

145
00:22:03,710 --> 00:22:08,690
Okay. So now you have a circle of data here.

146
00:22:09,020 --> 00:22:12,500
Okay. So you have this.

147
00:22:13,190 --> 00:22:16,820
So basically, you wrap this neuron into a circle.

148
00:22:16,890 --> 00:22:23,630
Right. So that. So now, according to this, I could have a distribution like this.

149
00:22:24,410 --> 00:22:27,980
Okay. So that my meaning is here.

150
00:22:28,400 --> 00:22:32,480
So suppose this is my zero degree and I'm here.

151
00:22:33,080 --> 00:22:37,820
So you can think about this is 90 degrees. Simply is like 6 a.m.

152
00:22:38,450 --> 00:22:43,000
Right. So that. So. So accordingly.

153
00:22:43,010 --> 00:22:49,880
Right. You can't you can imagine that what you the normal distribution that can be mapped on this circle.

154
00:22:50,180 --> 00:22:57,220
So you have this mode here represents sort of the mode of my distribution.

155
00:22:57,230 --> 00:23:01,520
If you have unit mode, then this will be the mean.

156
00:23:01,640 --> 00:23:10,459
And then the shape of the distribution here can define this dispersion of this distribution.

157
00:23:10,460 --> 00:23:15,380
So there are a few such a distribution defined only circle.

158
00:23:15,680 --> 00:23:21,710
Okay. While the most famous one will when the Y is called on this distribution.

159
00:23:24,670 --> 00:23:28,450
So it's very, very similar to the normal distribution,

160
00:23:28,780 --> 00:23:38,499
but it's really just define the unit circle and it's parameter twice the by the mean which is somewhere similar to

161
00:23:38,500 --> 00:23:47,319
normal mode and the dispersion parameter is this sigma information discussion no longer it's very standard deviation,

162
00:23:47,320 --> 00:23:52,719
but is the parameter that describes the spread of the distribution.

163
00:23:52,720 --> 00:23:56,230
So we give the name is the the letter is called dispersion parameter.

164
00:23:56,230 --> 00:24:05,709
But anyway so for misses distribution is implemented in the you know in the the circle who our

165
00:24:05,710 --> 00:24:14,080
package you can estimate the of the mean parameter and the dispersion parameter of the distribution.

166
00:24:14,380 --> 00:24:23,980
Okay. Using your first pick up time, of course that here is your mean of this distribution that you can more or less.

167
00:24:23,980 --> 00:24:39,970
Right. So six 650 well basically 650 is basically, of course, how we model the mean in all of the variability of your mode warming position.

168
00:24:40,150 --> 00:24:44,200
Well, central position of distribution as opposed to covers wide.

169
00:24:44,830 --> 00:24:53,260
I have my sort of mode at 630 was seven over 730.

170
00:24:53,260 --> 00:24:57,040
What some but it has their mode or meaning around nine.

171
00:24:57,040 --> 00:25:00,820
There is a variation across different individuals.

172
00:25:01,060 --> 00:25:09,370
What are the you know, the, you know, predictors that drive this difference across different individuals?

173
00:25:09,790 --> 00:25:14,109
We don't have the same mean right for all of the individuals we have before for me.

174
00:25:14,110 --> 00:25:18,350
So what are the covariance and experiments data?

175
00:25:18,640 --> 00:25:22,420
Make sure the means are different across different individuals.

176
00:25:22,690 --> 00:25:30,910
So this is essentially a model you want model of IMU as a function of a bunch of X, right?

177
00:25:31,270 --> 00:25:41,110
So how many age or gender or, you know, the class could you or you know, your habits or whatever.

178
00:25:41,590 --> 00:25:46,290
And so what do we do here? It's in 650.

179
00:25:46,300 --> 00:25:54,310
It's like we model the meaning of normal distribution as neither function of covers, but the similar model can be,

180
00:25:54,310 --> 00:26:04,310
you know, applied to for Mrs. Distribution, really looking at, you know, what's the mean or medium where the mode of,

181
00:26:05,290 --> 00:26:12,070
you know change according to different covariance like we have 50 some of the individual in class,

182
00:26:12,490 --> 00:26:19,510
we can see how this meaning is associated with some of the X variables that's called a model.

183
00:26:19,720 --> 00:26:28,510
Okay. So that, that's the data that you can use to run a lot circular regression model model the

184
00:26:29,590 --> 00:26:35,920
mean using one Mrs. distribution and you can do likelihood estimation of those

185
00:26:36,060 --> 00:26:41,800
regression coefficients and determine the in the model and you see the model to

186
00:26:42,070 --> 00:26:48,400
understand the change of this distribution according to individual characterizations.

187
00:26:48,520 --> 00:26:51,910
Okay. If that's something we're going to practice.

188
00:26:56,280 --> 00:27:00,730
So here is the circular package that. The.

189
00:27:02,180 --> 00:27:07,170
We're to state which just. To.

190
00:27:10,350 --> 00:27:18,430
Move this up and. Okay. So also we look at the corporations, right?

191
00:27:18,490 --> 00:27:21,750
So there are two types of correlation.

192
00:27:24,810 --> 00:27:25,620
In the data.

193
00:27:26,460 --> 00:27:40,490
So here you can have y t you know, that is sort of the daily proportion screen of sort of social screen time that could be a one variable white.

194
00:27:40,770 --> 00:27:46,060
And then you can have your X two, which is number of pick ups, right?

195
00:27:46,080 --> 00:27:54,959
So you want to look at how of how correlated this two variables are, right?

196
00:27:54,960 --> 00:27:59,430
So, so we can do this sort of cross-sectional correlation.

197
00:27:59,730 --> 00:28:04,560
So why do we need to make this correlation somewhat different in this kind of context?

198
00:28:04,920 --> 00:28:15,480
Because we have time stories, right? So so we have repeat measurements on the databases from individuals.

199
00:28:15,510 --> 00:28:25,450
Right. So. So here for my case, I have, you know, my first measurements starting from January 3rd.

200
00:28:25,720 --> 00:28:30,700
Right. And then up to January 27.

201
00:28:30,910 --> 00:28:36,819
Right. So that's, you know, the time period of, you know, when I collect my data.

202
00:28:36,820 --> 00:28:43,360
So. So here y axis could be, you know, proportion of the total screen time.

203
00:28:44,140 --> 00:28:48,070
That's the new social, social screen time.

204
00:28:50,670 --> 00:28:54,090
So that's the new variable I created by taking the ratio, right.

205
00:28:54,930 --> 00:29:00,180
So that I could. So this is propulsion. So it has to be zero and one.

206
00:29:01,410 --> 00:29:07,050
Okay. So basically percentage of the total time I spend on my social apps.

207
00:29:07,410 --> 00:29:10,870
So this will be on time service. So.

208
00:29:10,870 --> 00:29:17,320
So the data is collected on a daily basis. So I should have 25 or 24 data points.

209
00:29:17,680 --> 00:29:25,820
So basically you have a time to review the measurements so that at the same time you have number of pickups, right?

210
00:29:25,840 --> 00:29:31,720
So, so that you have your what's called swing pickup, this one.

211
00:29:34,090 --> 00:29:43,540
So you have number of pickups from like zero up to 100 or even higher than I don't know, what's the maximum number of pickups doing this.

212
00:29:44,110 --> 00:29:47,060
So that you have another one, right?

213
00:29:47,080 --> 00:29:56,740
So as a source of this this number of pickups, so basically you're asking how strongly this two process will be correlated.

214
00:29:57,280 --> 00:30:03,450
Okay. So one thing you are going to do here is to says cross-sectional correlation.

215
00:30:03,460 --> 00:30:07,020
Basically, you fix the one three. Time.

216
00:30:07,630 --> 00:30:11,710
Okay. So you look at the data point here and data point here,

217
00:30:12,580 --> 00:30:21,160
and you see how similar this sort of observations would be when you look at them on a daily basis.

218
00:30:21,610 --> 00:30:26,350
So that's essentially looking at this formation of X, T and the Y, t,

219
00:30:26,680 --> 00:30:35,290
x t is the number of the peak ups and the T and why t is number the proportion of those social screen time,

220
00:30:35,890 --> 00:30:43,770
you know, and the T and you look at how correlate this to our dataset or to factorial theta.

221
00:30:44,200 --> 00:30:47,409
But you, I mean for me looked at this case.

222
00:30:47,410 --> 00:30:53,590
So this can be easily calculated by using all this R package correlation.

223
00:30:53,590 --> 00:30:57,310
So in my case is pretty low is 1.2 or so.

224
00:30:58,300 --> 00:31:06,790
Okay. That's something like you can calculate whether or not the number of pick ups of this means that

225
00:31:06,820 --> 00:31:17,680
really that the reason I pick up this is not a very strongly correlated with my social screen time.

226
00:31:17,680 --> 00:31:29,620
So the larger number of the ups not really correlated this more time, a higher proportion of time I will spend my social apps, right?

227
00:31:29,620 --> 00:31:31,359
So they are weakly correlate.

228
00:31:31,360 --> 00:31:40,689
But now maybe some of you have a strong correlation because the reason you pick up your phone is mostly for your social apps.

229
00:31:40,690 --> 00:31:53,350
But this in my case, maybe I use this for some for order purposes, maybe I squeeze my photos from here and looking at social sort of things.

230
00:31:53,380 --> 00:32:04,870
And also there is another correlation that we do, which is called also correlation function because we have this time series, right?

231
00:32:05,380 --> 00:32:08,610
So sometimes this could be a markov process, right?

232
00:32:08,650 --> 00:32:15,280
So which means that the behavior today could be related to behavior in yesterday.

233
00:32:15,520 --> 00:32:23,560
Right. So you have let's say you have a time stories, right?

234
00:32:23,620 --> 00:32:26,680
So three times stories. Okay.

235
00:32:26,800 --> 00:32:37,330
So this is the the variable that we repeat, the measured, you know, over time during this sort of study period.

236
00:32:37,840 --> 00:32:46,570
And because this is a sequence observation mission from semi individual, they're supposed to be temporally correlated, right?

237
00:32:46,630 --> 00:32:50,200
So my behavior today, it could be a correlate of this.

238
00:32:50,200 --> 00:32:58,870
My behavior yesterday or today is sort of state my behavior on Thursday could be related to my behavior certainly last week

239
00:32:59,230 --> 00:33:07,750
because I have very similar skewed you observe state I need to do this meeting this meeting to have very regular schedules.

240
00:33:07,750 --> 00:33:16,149
Once things that can do that the schedules will limit it how it goes to use my phone and but anyway so you

241
00:33:16,150 --> 00:33:24,280
have this sort of Markov dependance or temporal dependance or pseudo dependance is that dependance over time.

242
00:33:24,310 --> 00:33:34,780
Right. So, so if you can imagine that if I look at time t here, I wonder how, how much the history.

243
00:33:35,140 --> 00:33:39,970
Okay, the data could you determine my behavior?

244
00:33:40,390 --> 00:33:49,810
So one thing I can do is like one correlation which is released how my behavior yesterday would be a fact.

245
00:33:49,810 --> 00:33:56,469
My today or a B according to my behavior is called Markov Dependance of Order.

246
00:33:56,470 --> 00:33:59,590
What? Right. So you look at t minus one.

247
00:34:01,160 --> 00:34:05,050
So this is the data yesterday. Okay.

248
00:34:05,500 --> 00:34:13,720
So you will see how this time called correlation will look like how my behavior today will related to my behavior.

249
00:34:14,080 --> 00:34:27,200
Of course not. Today or yesterday is, you know, not only the physically to the meaning that, you know, the day, the January,

250
00:34:27,200 --> 00:34:41,229
the 12th or January levels this pair can move it down to, for example, January force could be today in terms of June or third.

251
00:34:41,230 --> 00:34:44,890
Could be the yesterday of January 4th. Right.

252
00:34:45,160 --> 00:34:51,100
And January 4th could be yesterday of January 5th.

253
00:34:51,520 --> 00:34:57,220
So you have this sort of today, yesterday appear together over time.

254
00:34:57,230 --> 00:35:11,140
So then because of this, you can estimate this black one or correlation so that you can, you know, understand that this sort of temple dependance.

255
00:35:11,170 --> 00:35:15,540
So that's exactly what this autocorrelation function possible.

256
00:35:15,850 --> 00:35:26,920
So. So if you take a call as one, then you basically are looking at the stress of correlation of observation today versus yesterday.

257
00:35:27,370 --> 00:35:35,349
Okay. So you look at mark of ten over one dependance, how this time sort of, you know, you look at this temple and of course,

258
00:35:35,350 --> 00:35:43,710
you can use how you could to to you look at the dependance of the behavior today versus the behavior two days before.

259
00:35:43,720 --> 00:35:50,920
Right. So how can vary from one day to, you know, those arbitrary days I since you can compute.

260
00:35:51,250 --> 00:35:59,110
So this becomes out of quotation function becomes a function of the lack rate which like you how is the lag.

261
00:35:59,440 --> 00:36:07,430
So how many? Some of the dates back in the history of looking at you were to construct this dependance so

262
00:36:07,700 --> 00:36:17,510
so this defines this mark of dependance in terms different sort of lacks that you like to use.

263
00:36:17,660 --> 00:36:20,660
So this is a function. So our.

264
00:36:22,770 --> 00:36:32,400
Has. This is a function. So it's a function in our automatic how to calculate the this out operation.

265
00:36:32,820 --> 00:36:38,550
So all operation is the creation of the same process is now the cross time.

266
00:36:39,630 --> 00:36:47,220
The cross time source correlation is the one I just talk about the cross correlation, cross-sectional correlation.

267
00:36:47,220 --> 00:36:56,070
But the older correlation is, you know, the correlation of of the time source is self it looking at the mark of dependance.

268
00:36:56,250 --> 00:37:05,309
Okay. So that's why you can imagine that this the function can be calculated for for processing of your little social screen time.

269
00:37:05,310 --> 00:37:09,780
And this task will be number out of course your function calculate for number of pickups.

270
00:37:10,140 --> 00:37:20,740
So it's basically the outcome is self. So you're looking at the time dependance within the same time stress sequence observation.

271
00:37:21,000 --> 00:37:27,210
Okay. Basically looking at longitudinally, what are the serum dependance.

272
00:37:28,290 --> 00:37:34,349
So of course like y is very interest because you look at how you know the behavior yesterday,

273
00:37:34,350 --> 00:37:39,839
in fact here today, as I say, that seven is another one we call weekly fact.

274
00:37:39,840 --> 00:37:44,249
Right in this case, because given the contacts how data is collected here,

275
00:37:44,250 --> 00:37:50,879
we're looking at our sort of screen time use on a daily basis and we know that we

276
00:37:50,880 --> 00:37:56,280
have the weekly cycle because lot of schedules are very similar on a weekly basis.

277
00:37:57,330 --> 00:38:06,720
So how you could do seven, seven day lag is very interesting whether or not my behavior Thursday last week would be very core.

278
00:38:06,930 --> 00:38:10,180
This might be behavior of sustained today.

279
00:38:10,180 --> 00:38:16,920
Right? So this weekly, weekly dependance is very kind of look at months late, right.

280
00:38:17,040 --> 00:38:17,730
Some so on.

281
00:38:17,910 --> 00:38:29,580
But anyway, you understand that data collection and the context of data so that that you can decide which log is of interest in your interpretation.

282
00:38:29,670 --> 00:38:39,870
Right. So that's anyway can be created easily from you are functioning as F and this function automatic give you a figure.

283
00:38:40,380 --> 00:38:47,640
If you want to get just numbers you can just sit, plop, force so that the hour will only generate the numbers.

284
00:38:48,090 --> 00:38:53,430
So you can pick up the numbers if you do not do this set up.

285
00:38:54,000 --> 00:38:58,720
And then this hour function will automatically give you in future.

286
00:38:58,740 --> 00:39:09,780
Okay, like something like this. So this is the one that I created from our function for my data.

287
00:39:10,170 --> 00:39:16,170
So this autocorrelation function class of social screen time ratio, that's this is the one that I created.

288
00:39:16,860 --> 00:39:24,990
So so here you can how you call to zero this out operation is this one because

289
00:39:25,590 --> 00:39:30,059
the data I collect today is always perfect correctly this up to date right.

290
00:39:30,060 --> 00:39:34,050
So when you call equal to zero, you don't lack.

291
00:39:34,350 --> 00:39:37,580
So you have perfect correlation of the data to data itself.

292
00:39:37,830 --> 00:39:47,910
So so that's always one. And now this, this one is of the, the net negative correlation.

293
00:39:48,120 --> 00:39:57,989
And basically if I use up basically says if I use a lot of social screen time yesterday, I tend to use less today.

294
00:39:57,990 --> 00:40:05,069
So so that's meaning the negative correlation that if I use quite a bit to this, I'll go.

295
00:40:05,070 --> 00:40:12,690
I probably use quite a lot today. Okay. This tool bars are the 95 confidence interval.

296
00:40:13,080 --> 00:40:20,730
So if this bar is over, it is full bar. That means this life, this dependance is statistically significant.

297
00:40:21,840 --> 00:40:34,530
So you can see that for my both cases, none of the sort of the auto correlation are of over the the upper limit or lower limit basically that is my.

298
00:40:35,100 --> 00:40:46,010
So the Markov dependance are statistically insignificant they that my series is now zero and there are a lot of random behaviors.

299
00:40:46,620 --> 00:40:53,060
But your case may be different. You can look at your behavior, see if they are temporally correlates.

300
00:40:53,070 --> 00:41:01,500
But my case is quite a random and I do not see any significant sort of zero dependance.

301
00:41:01,690 --> 00:41:13,590
Okay, so you can see the sort of this direction of association, but this one is not significant unless this excess this lower sort of limits.

302
00:41:13,830 --> 00:41:19,230
So this to me, Toronto blue lines indicate the 9 to 5 confidence interval.

303
00:41:19,770 --> 00:41:28,200
So anything over this? Limits will be regarded as being statistically significant that you can make some.

304
00:41:28,710 --> 00:41:34,740
But in my case you can say now none of this values are over this confidence interval,

305
00:41:34,770 --> 00:41:41,010
meaning that none of the use of this dependance are statistically significant.

306
00:41:41,070 --> 00:41:46,000
So that I have very random behavior. Question.

307
00:41:47,800 --> 00:41:51,040
I have a question in terms of data collection.

308
00:41:51,360 --> 00:41:57,490
Yeah. So you talked about like some people may stay up late after the May night.

309
00:41:57,610 --> 00:42:02,110
That's right. And some people may like just wake up during the midnight.

310
00:42:03,360 --> 00:42:08,319
Can we, like just estimate the wake up time to do that data collection?

311
00:42:08,320 --> 00:42:16,000
Because sometimes I just feel like it's hard to get those data even though we change our daytime zone to kind of fall in that time.

312
00:42:16,450 --> 00:42:28,930
I say, well, you are the person who knows your biological clock the best so that you can decide, you know, what the.

313
00:42:32,760 --> 00:42:37,380
What the the pick up time or wake up time that you like.

314
00:42:38,370 --> 00:42:45,260
So I would suggest you not do that because, you know, sometimes some people have better sleep quality.

315
00:42:45,270 --> 00:42:53,099
They can sleep entirely from the time they start to sleep and wake up in the in the in the morning.

316
00:42:53,100 --> 00:42:58,559
And so they can have like 7 hours, you know, sleep with no interruptions.

317
00:42:58,560 --> 00:43:08,639
Some people have a lot of this little, you know sleep here is that that would indicate that have poor sleep quality.

318
00:43:08,640 --> 00:43:15,990
I, i, I it's I will suggest you not do that to the estimation just recorded say that directly.

319
00:43:16,440 --> 00:43:20,760
And then when you analyze the data and just noticed the,

320
00:43:21,510 --> 00:43:29,879
the differences or the meaning of that first pick up time in the other question is

321
00:43:29,880 --> 00:43:35,520
about the proportion of social screen time is the denominator that total screen time?

322
00:43:35,550 --> 00:43:40,980
That's right. Why are we using the proportion instead of the absolute value here?

323
00:43:41,040 --> 00:43:49,050
Yeah. So you can use both. Right. So total screen time means something like someone's mean spend like 5 hours,

324
00:43:49,350 --> 00:43:54,150
total screen time versus the one person speak like spend the $1 for the screen.

325
00:43:54,450 --> 00:44:02,939
It means something. But if you want to sort of compare the sort of the of course,

326
00:44:02,940 --> 00:44:11,810
differ individuals about the use of the social screen time that the ratio could be a way to make it that comparable right.

327
00:44:12,960 --> 00:44:16,170
Dissolves all time. That's why you want yeah.

328
00:44:21,170 --> 00:44:28,880
So far for the first pickup up. Do you want us to be catching data if we do wake up another night or cause like, for example, Hawaii time.

329
00:44:28,890 --> 00:44:32,120
Yes, you know that. Okay, you do. Because like Hawaii time would make it.

330
00:44:32,120 --> 00:44:38,300
So we would miss like a midnight or a3am wake up, but we would get, you know, whatever time we get up for the day.

331
00:44:39,200 --> 00:44:42,950
Yeah, yeah. Certainly. Yes. Okay. So it does, you know.

332
00:44:47,000 --> 00:44:52,560
But when you do the times on chance, you'll remember that you will add that difference in the direct collection.

333
00:44:52,830 --> 00:45:00,620
So but sometimes that instead of looking at the first like wakeup time, you can look at sort of your sleep quality.

334
00:45:00,940 --> 00:45:09,300
I think the dispersion of the sigma, you know, could be something a, uh,

335
00:45:09,610 --> 00:45:17,269
an indication of your quest that sometimes you like the mode, like I have the bimodal security.

336
00:45:17,270 --> 00:45:25,940
Maybe you have five modes that really you have very unstable sleep during a period of time and can be interpreted in different ways.

337
00:45:25,940 --> 00:45:34,969
I'm now seeing that first speaker time has to be interpreted as the first the wakeup time, but can also look at sleep quality because sleep health,

338
00:45:34,970 --> 00:45:42,650
it's very important because, you know, especially for us in there are people who have very interrupted sleep.

339
00:45:44,210 --> 00:45:51,420
We're not getting old. So so the first homework,

340
00:45:51,420 --> 00:45:57,420
I will basically give you a question to ask you to look at your baseline data and then

341
00:45:58,590 --> 00:46:06,629
follow one that is a step up that I've shown you and you can see how your data look like.

342
00:46:06,630 --> 00:46:10,430
And we will start to point is the data from you all.

343
00:46:10,560 --> 00:46:13,900
And so. Okay.

344
00:46:13,900 --> 00:46:16,690
So let me move on to next topic.

345
00:46:18,460 --> 00:46:29,440
So that's something I like to start early because we will think about this sort of the data integration over the course.

346
00:46:29,980 --> 00:46:36,730
So I want you to talk about this physical learning and distributed computing.

347
00:46:38,140 --> 00:46:45,370
You know that Web 3.0 or blockchain, those decentralized business, you know,

348
00:46:45,370 --> 00:47:00,640
and so it becomes a fashion sort of movement in a business where people start to think about privacy and think of a lot of other issues why,

349
00:47:00,970 --> 00:47:07,720
you know, should be decentralized, where even, you know, the financial piece decentralized.

350
00:47:07,730 --> 00:47:14,150
So in this case that defend is that this learning and computing becomes very essential.

351
00:47:16,000 --> 00:47:21,510
And so let me just talk about this and how we're going to do that.

352
00:47:21,520 --> 00:47:25,330
This is a very baby version. Like, I just give the introduction.

353
00:47:30,010 --> 00:47:39,940
So. So we're maybe in the traditions that this causes that we do not think about data management, but data management is part of data science.

354
00:47:40,120 --> 00:47:48,040
So, I mean, we always assume that data are right in our computer or in one Excel file that we are going to,

355
00:47:48,070 --> 00:47:52,570
you know, load into our work, you'll know Python and then we run analysis.

356
00:47:52,780 --> 00:47:55,230
But in reality, this is not the case, right?

357
00:47:55,240 --> 00:48:06,600
So data management is very, very important in practice and probably more important to, you know, the data analysis itself.

358
00:48:06,610 --> 00:48:15,130
So how do you manage data? How do you take data management into account when you do start a small business?

359
00:48:15,850 --> 00:48:25,720
So nowadays, because of this movement of decentralized sort of activities, distributed data storage, well,

360
00:48:25,730 --> 00:48:32,660
data sharing of data privacy becomes very important nowadays and need to be integrated

361
00:48:32,670 --> 00:48:38,050
into the development statistic model is it's analytics algorithm in the area of data sign.

362
00:48:38,060 --> 00:48:47,170
So this is actually something like even in the statistics itself that we're thinking about,

363
00:48:47,530 --> 00:48:54,720
you know, integrate data management data issues into actual systems analysis.

364
00:48:55,990 --> 00:49:00,910
So as I said before, we think about data is there and we just do it data analysis.

365
00:49:00,910 --> 00:49:08,860
But now we're thinking that, hey, how data is managed to do we really have the data and how this can be incorporated into.

366
00:49:10,420 --> 00:49:15,579
Okay. So particularly your you know, our project, right?

367
00:49:15,580 --> 00:49:21,310
So we have 50 some individuals in the class that we have our individual iPhones.

368
00:49:21,730 --> 00:49:23,830
Well, of course, data are stored in your iPhone.

369
00:49:23,830 --> 00:49:36,459
I mean, you'd export your data from your iPhone, you know, into Excel file and obtain a data sheet or things document.

370
00:49:36,460 --> 00:49:45,840
But in principle, you can directly program your sort of little things on your iPhone because iPhone now is very powerful,

371
00:49:45,850 --> 00:49:53,739
is, you know, a powerful CPA that can do simple data storage and data is no problem.

372
00:49:53,740 --> 00:49:58,780
So now you have these small computers for each individual.

373
00:49:58,960 --> 00:50:01,960
You can imagine that we have 50 and some of the iPhones.

374
00:50:02,170 --> 00:50:02,400
Okay.

375
00:50:02,800 --> 00:50:18,070
That story data and can do the simple calculation, of course that the data store about each iPhone can now be shared this, you know, for the contract.

376
00:50:18,070 --> 00:50:28,389
So we're utilizing supplements, right. So the the capacity of data storage in computing allow us to think about how we're going to,

377
00:50:28,390 --> 00:50:34,660
you know, utilize this kind of set up then to, you know, learn from the data, right?

378
00:50:34,660 --> 00:50:38,620
So, so this is something like we won't think about this.

379
00:50:38,680 --> 00:50:50,350
Okay? So one thing we're trying to do here is to do this federal online where instead of, you know, each iPhone or device,

380
00:50:50,350 --> 00:51:01,239
mobile device gives the data to the is the central Klein or central server owned by Apple

381
00:51:01,240 --> 00:51:08,920
that we we can share some summary statistics or we can refuse to share anything of course,

382
00:51:08,920 --> 00:51:18,219
in that case that this individual will now be part of this sort of network in this data sharing.

383
00:51:18,220 --> 00:51:23,770
If you are waiting to share some summary statistics, then the summary statistic can be,

384
00:51:24,160 --> 00:51:33,430
you know, sort of combined in a central server that can create something useful information.

385
00:51:34,390 --> 00:51:44,620
So this is basically the the set up that we would like to, you know, do statistical analysis under this distributed data storage.

386
00:51:47,380 --> 00:51:50,740
So there are a lot of data sharing varies, right?

387
00:51:50,800 --> 00:52:01,750
So multiple device user naturally from a form, an E call system consisting their device computer that can collect similar data such as data.

388
00:52:02,020 --> 00:52:08,559
So your screen time and so that so typically some form of summary statistic.

389
00:52:08,560 --> 00:52:16,950
Now the raw data will be allowed to communicate with a central server on the end product platforms under certain data sovereignty.

390
00:52:17,930 --> 00:52:21,190
Okay, so raw data are stored in a distributed fashion.

391
00:52:21,190 --> 00:52:27,430
As already mentioned, no data sharing is allowed between devices due to lack of mutual data sharing agreements.

392
00:52:27,940 --> 00:52:36,530
So to ensure data. Of pregnancy. Does this not possible to tweak the model using the combined raw data?

393
00:52:36,680 --> 00:52:44,959
So so because we do not have this sort of the raw data so that you cannot use the traditional method,

394
00:52:44,960 --> 00:52:52,400
but when you do your 650 or 651, you the sixth or one, then you always assume that the individual data are available.

395
00:52:52,400 --> 00:52:54,860
You can tweak them all there.

396
00:52:54,890 --> 00:53:03,380
But now given this distributed data storage, you may not have that privilege to have the combined raw data to train your model.

397
00:53:03,500 --> 00:53:11,870
Right. So the question here is, can we train a model from all users data while address data,

398
00:53:11,870 --> 00:53:17,960
share various and data privacy constraints in a system in a systematic way.

399
00:53:19,040 --> 00:53:28,429
So so if we do not use any of that, we, if we do not have the combined rally, can we do whatever we want to do?

400
00:53:28,430 --> 00:53:34,009
Right. So that's basically the the question people ask this.

401
00:53:34,010 --> 00:53:37,760
So data engineering point of view, the answer is yes.

402
00:53:37,760 --> 00:53:42,650
In some situations, while this solution is provided, a very diverse statistics learning.

403
00:53:43,010 --> 00:53:48,499
Okay, so I already mentioned that in euro well 3.0 blockchain technology,

404
00:53:48,500 --> 00:53:55,130
data storage and management not come in fundamentally different from the traditional centralized operations.

405
00:53:55,310 --> 00:54:00,830
Right. So this is something like moving forward in the business, right?

406
00:54:00,950 --> 00:54:06,919
So data are stored and data security, data privacy are big concerns.

407
00:54:06,920 --> 00:54:17,870
And so not how do you relax that kind of data sharing agreement to sort of only

408
00:54:17,870 --> 00:54:23,900
use summary statistics to to use privacy protected information rather than the

409
00:54:24,420 --> 00:54:30,079
or in the raw data in they are model training where I would do the calculation

410
00:54:30,080 --> 00:54:36,020
or analysis so that that's really something people are thinking about nowadays.

411
00:54:36,020 --> 00:54:39,950
So favorite favorites that is learning is one of the solutions.

412
00:54:41,390 --> 00:54:46,390
So I have a kind of personal story about IPT know that well.

413
00:54:47,600 --> 00:54:58,610
So I was the lead obstetrician designed the insulin therapy for the prevention of new onset diabetes upper kidney transplantation.

414
00:54:59,150 --> 00:55:05,990
This is clinical trial is a cross-border clinical study involving four European hospitals.

415
00:55:07,100 --> 00:55:17,450
So you'll know that patients with end stage renal renal disease, they have to go to dialysis three times a week in order to get the treatment,

416
00:55:17,780 --> 00:55:23,180
basically to wash out all the bad proteins from body in circulation.

417
00:55:23,720 --> 00:55:30,020
And then, you know, the the because their kidneys are no longer functioning very well,

418
00:55:30,020 --> 00:55:37,640
then they need to use a machine to help them dialysis machine to help them to remove those fat proteins.

419
00:55:38,540 --> 00:55:47,840
So of course, that the most effective way to deal with end stage renal disease is the kidney transplant or kidney replacement.

420
00:55:48,860 --> 00:55:57,020
But there's always a, you know, bottleneck of more demand and supply.

421
00:55:57,410 --> 00:56:04,639
You know, you need to actually the organ works into this kind of treatment therapy.

422
00:56:04,640 --> 00:56:18,710
Right. But each year so USA, we have about a 100,000 people who are on the waiting list waiting for, you know, availability of kidney organs.

423
00:56:18,980 --> 00:56:27,260
You work through the transplant and each year we could have about 40,000 transplants completed.

424
00:56:27,590 --> 00:56:35,810
So you can see there is a gap, right? So in terms of number of demands versus number of supplies,

425
00:56:36,410 --> 00:56:44,989
so so for people who do not have opportunity to get kidney transplant, then they always put down the dialysis.

426
00:56:44,990 --> 00:56:54,980
You work to get treatment, but this is low quality life treatment because you have to go to clinic three times a week.

427
00:56:55,550 --> 00:57:00,050
And and for the four patients who receive King transplantation,

428
00:57:00,050 --> 00:57:09,760
they have better quality because they only due to some kind of immune suppression medication, they don't need to really go to clinic.

429
00:57:09,980 --> 00:57:13,490
It's like multiple times a week. So this is a better choice.

430
00:57:14,120 --> 00:57:20,839
Of course, that's one thing that happens in practice is when people risk patients receive new organs,

431
00:57:20,840 --> 00:57:29,150
then they start to take some immune suppression drugs or other, you know, treatments that can.

432
00:57:30,200 --> 00:57:39,200
Trigger some of diabetes because their pancreas does not work properly.

433
00:57:39,270 --> 00:57:43,880
Sometimes when you do kidney transplant, you also do the pancreas transplant at the same time.

434
00:57:44,510 --> 00:57:45,350
So, you know,

435
00:57:45,350 --> 00:57:56,690
there is a issue that involved quite a high prevalence of patients who received the kidney transplant and of develop diabetes after transplant.

436
00:57:58,070 --> 00:58:10,610
So then what we try to do here is to, you know, do a three trial to see whether or not the that you can have some preventive treatment you were to,

437
00:58:10,610 --> 00:58:17,420
you know, reduce the, you know, the prevalence of this diabetes onset.

438
00:58:19,040 --> 00:58:26,119
So what people use you to do here is that, you know, standard protocol is that those patients who grow up with diabetes,

439
00:58:26,120 --> 00:58:29,870
they start to use insulin injections to treat diabetes.

440
00:58:30,530 --> 00:58:38,870
But now in a group of medical doctors in far fewer hospitals,

441
00:58:39,170 --> 00:58:49,220
they start to give this sort of insulin injection two weeks right after kidney transplant before you actually see the symptoms of diabetes.

442
00:58:50,390 --> 00:58:53,209
So that do they want to have this kind of preventive treatment?

443
00:58:53,210 --> 00:59:03,650
And they are looking at the by two years, one year later and two years later, what is the the chance of developing diabetes after a transplant?

444
00:59:03,660 --> 00:59:11,960
So that's sort of the the of the trial of this I designed as a tissue.

445
00:59:12,160 --> 00:59:22,250
Okay. So this IPT Modak study is a multicenter randomized clinical trial conduct and for kidney transplantation centers in Austria.

446
00:59:22,760 --> 00:59:31,339
So and so there are two hospitals why is in Vienna another on grass and also a Fasano

447
00:59:31,340 --> 00:59:36,500
in a hospital and burning hospital where the patients are recruited in the study.

448
00:59:36,830 --> 00:59:40,969
The goal is very clear is really just trying to estimate the average treatment

449
00:59:40,970 --> 00:59:48,600
factor of basal insulin in intervention prevention over post transplantation,

450
00:59:48,620 --> 00:59:51,560
diabetes, yes or not, right.

451
00:59:51,620 --> 00:59:59,150
One year or two years later, using the logistic regression model, it's very, very straightforward analysis of this randomized trial.

452
01:00:00,680 --> 01:00:06,560
There was a mutual agreement of data sharing has to be right in the protocol.

453
01:00:06,950 --> 01:00:12,970
But the process of combined data took about three years to complete it because this is cross-border.

454
01:00:13,400 --> 01:00:19,879
And then there are a lot of the data sharing issues and it took a time to actually get this

455
01:00:19,880 --> 01:00:27,650
data sort of moved to a central server located in the Vienna Hospital General Hospital,

456
01:00:27,740 --> 01:00:31,220
to analyze the data to to, you know, actually.

457
01:00:32,450 --> 01:00:39,080
So here is the data that you have for in-hospital Barcelona Hospital, Vienna Hospital and Grass Hospital.

458
01:00:41,620 --> 01:00:46,040
So. Okay.

459
01:00:47,330 --> 01:00:56,000
So here is actually the paper published in 2021 in the American Journal American Society Nephrology.

460
01:00:56,010 --> 01:01:01,550
This is very, very important clinical journals in nephrology.

461
01:01:03,620 --> 01:01:10,699
So the the the the finding here is that this base,

462
01:01:10,700 --> 01:01:21,620
these the basal insulin was still significant treatment efficacy based on centralized analysis published in this paper in 2021.

463
01:01:23,500 --> 01:01:35,770
So as I said, it took about three years actually to get this data combined and centralize sort of convener us this sort of,

464
01:01:37,480 --> 01:01:42,610
you know, the computer located in in a general hospital at a time.

465
01:01:42,610 --> 01:01:46,809
I was top to the pi of the study. I said, well,

466
01:01:46,810 --> 01:01:52,690
why are we with three years if we can't have better statistical analysis that

467
01:01:52,690 --> 01:02:03,099
do not really need this data sharing agreement to skip this centralizing data,

468
01:02:03,100 --> 01:02:09,850
centralization, sort of a precision step or procedure we can save three years.

469
01:02:10,180 --> 01:02:17,830
The key point here. Can we save this for years that has been wasted on this data management process?

470
01:02:18,100 --> 01:02:29,170
Okay. Can we if we if this treatment becomes significant, is a good treatment could be put for as an option for patients to take.

471
01:02:29,710 --> 01:02:37,840
Why not a we can have this discovered this conclusion delivered three years earlier than, you know,

472
01:02:38,230 --> 01:02:46,150
three years later that we deal with a lot of these bureaucratic paperwork to really work on the data integration together.

473
01:02:46,870 --> 01:02:50,290
So that motivated me a lot of think about fit are.

474
01:02:50,680 --> 01:02:59,830
Okay so can we just do the following this no need of merging raw data from the full hospital can win hub

475
01:03:00,040 --> 01:03:07,350
expert expedite the clinical discovery in this trial and save three years to improve patients quality.

476
01:03:07,660 --> 01:03:14,500
This is type okay. If treatment were found to be effective but few years earlier, how would you do that?

477
01:03:14,830 --> 01:03:19,660
Okay, so why were we three years? Because of this data management issue?

478
01:03:20,950 --> 01:03:24,580
Because of data sharing barriers. Can we do something faster?

479
01:03:24,850 --> 01:03:29,920
Expedite this clinical findings were twins to develop this method.

480
01:03:30,370 --> 01:03:35,050
Can we hope to, you know, save this scheme, this sort of things?

481
01:03:35,500 --> 01:03:41,230
So that's something I, I feel that this is very important to tell you.

482
01:03:41,470 --> 01:03:46,030
And so let's say can we do this?

483
01:03:47,620 --> 01:03:56,070
So. So the central idea here is no sharing of raw data, but only certain summary statistics.

484
01:03:56,250 --> 01:04:00,840
So all only for hospitals. You don't need to tell me the raw data of the patients.

485
01:04:01,200 --> 01:04:07,439
I'll tell you which raw like summers to say, you know, the summer statistic,

486
01:04:07,440 --> 01:04:17,670
the mean standardization that are standard things that are not is a free of, you know, this data security of data privacy issue.

487
01:04:17,980 --> 01:04:26,010
Okay. If we are agree with that, then we can really expedite through this whole clinical discoveries.

488
01:04:26,380 --> 01:04:31,910
Okay. So here is the architecture of the so federate learning.

489
01:04:31,920 --> 01:04:37,620
So you have data, one data to date. Okay. They're all stored in the local server.

490
01:04:37,650 --> 01:04:45,360
I mean, your in our study, we have iPhone. So each iPhone store stores your own data.

491
01:04:45,780 --> 01:04:51,600
So we have a case about 52 or so number of the individuals in the course.

492
01:04:51,870 --> 01:05:02,460
But you might case you may know that study I how can you call two four could could be 205 it doesn't matter is the architecture that you are thinking.

493
01:05:03,240 --> 01:05:12,389
So now you make communication with the central server and the central server does not go that is not going to receive the the raw data,

494
01:05:12,390 --> 01:05:21,390
but just some summaries systems and this is dispute data set and we are going to do the distributed computing to have

495
01:05:21,390 --> 01:05:29,730
maximum protection of your data security and data privacy and without or income or the purpose of statistical analysis.

496
01:05:32,610 --> 01:05:39,090
So the first question we will ask, what type of summer statistics or summer data should be shared?

497
01:05:40,130 --> 01:05:49,030
Okay, so this is how you know, the one that we need to ask that decision?

498
01:05:49,050 --> 01:05:53,010
We need to figure it out. And of course,

499
01:05:53,010 --> 01:05:57,419
this answer to this question depends on what kind of statistical analysis you're doing

500
01:05:57,420 --> 01:06:01,680
or any difference that this analysis may require different types of summer statistics.

501
01:06:02,530 --> 01:06:12,419
And the second question to you is, is the possibility for a fairer learning method being a distributed factor that produce the same or almost

502
01:06:12,420 --> 01:06:19,290
the same result as those obtained by the centralized analysis using combined raw data on the center server?

503
01:06:19,680 --> 01:06:24,209
So of course that you can go forward as sort of federal learning,

504
01:06:24,210 --> 01:06:34,140
but your statistic power where you data analysis quality is completely you know, I'm going to September why you do this.

505
01:06:34,380 --> 01:06:37,770
So the second question is more demanding nicely.

506
01:06:38,220 --> 01:06:41,290
Even I use summer statistics, I'm not going to lose.

507
01:06:41,310 --> 01:06:46,040
My split is the power. I'm now going to lose my statistical precision.

508
01:06:46,590 --> 01:06:52,650
Can we do that right. So that's really a important question.

509
01:06:52,890 --> 01:06:59,280
From a statistical point of view, Thomas and I know that study we do sample size calculation.

510
01:06:59,730 --> 01:07:04,090
The sample size in this trial has been predetermined.

511
01:07:04,130 --> 01:07:12,360
You were to reach a certain statistic power. If you use a different method that gives you lower power of state analysis is not acceptable.

512
01:07:12,450 --> 01:07:18,360
Right? Because you you you decide to recall 200 patients because you want to achieve this power.

513
01:07:18,660 --> 01:07:26,630
But later on, use a new method that's not going to give you the same power why you use this and nobody would agree to do that.

514
01:07:26,670 --> 01:07:34,800
Right. So this question, too, is very important. So you can wait to defer learning that would not boost statistics power.

515
01:07:34,970 --> 01:07:42,450
Okay. Can we do that? So then we'll see how this can be done.

516
01:07:42,840 --> 01:07:48,809
So that so we will. You know, first of all, I want to say the Oracle learning,

517
01:07:48,810 --> 01:07:57,960
which is the one that you get the result of specifically using centralized analysis, supposed data can be combined.

518
01:07:58,640 --> 01:07:58,860
Okay.

519
01:07:59,100 --> 01:08:13,680
You use your own data analysis methods or models, you know, using your the data analysis protocol, that's the gold standard that you're going to use.

520
01:08:13,860 --> 01:08:15,960
But now you change your method to do that.

521
01:08:17,010 --> 01:08:28,460
So so we will refer the results obtained from the centralized and using the combined raw data on a central server as Oracle result.

522
01:08:28,470 --> 01:08:31,740
This is Gholston. That's something you want to achieve. Okay.

523
01:08:31,890 --> 01:08:36,590
Can you do a distributed computing results here?

524
01:08:36,600 --> 01:08:39,810
The raw data to achieve the Oracle result?

525
01:08:39,900 --> 01:08:48,320
That's something they're asking. So let's let's see how we do this in a very simple situation, right?

526
01:08:48,340 --> 01:08:54,690
If this cannot do, then we just give up, right? So we begin something very simple to gain something inside.

527
01:08:54,690 --> 01:08:58,050
Let's say we just have we want to calculate sample mean, right?

528
01:08:58,800 --> 01:09:02,250
So consider a very simple setting of two independent samples.

529
01:09:02,550 --> 01:09:12,120
So you have a bunch of data coming from hospital one and you have a bunch of data collected from Hospital two and D one.

530
01:09:12,330 --> 01:09:19,440
Union D two is the combined data which is hypothetically available on a central server.

531
01:09:20,670 --> 01:09:27,000
So now what you want to do here, of course the calculate the sample mean of this, you know, combining data.

532
01:09:27,540 --> 01:09:36,630
Okay. So, so you going to combine the data, will you're going to copy the sample mean you combine the data from like this is GFR,

533
01:09:36,660 --> 01:09:46,720
you know, this, you know this renal function variable that we typical measured the, you know, the renal function, EGFR or GFR.

534
01:09:46,770 --> 01:09:52,120
So you, you have the measurement from A1 patients from hospital 1 to 2, patient from hospital.

535
01:09:52,140 --> 01:09:56,730
You want to calculate an average EGFR or GFR.

536
01:09:56,820 --> 01:10:03,230
Great. So from from this combine later how you do this in a distributed fashion, these all share this role.

537
01:10:03,540 --> 01:10:10,290
Yes, very simple. Why? Because you can really just write the combined meaning as this we.

538
01:10:12,210 --> 01:10:18,510
Okay. So we'll explain to you why is the sample mean from the first dataset?

539
01:10:18,930 --> 01:10:23,160
How still one provide the sample mean to central server?

540
01:10:23,670 --> 01:10:29,010
So we're not General Hospital and Second Hospital from Birmingham, for example.

541
01:10:29,010 --> 01:10:33,930
I'll give you the mean value of one variable in your interest.

542
01:10:34,470 --> 01:10:41,670
Okay. So you can say that I did the calculation of in that hospital and I did not mean

543
01:10:41,670 --> 01:10:46,940
the thing non hospital I do not share here so I can combine this together.

544
01:10:46,950 --> 01:10:52,410
This will give me exactly same me obtained by the combined data.

545
01:10:53,720 --> 01:11:00,530
Why? Because you have a perfect decomposition of this total sample into distributed sampling.

546
01:11:01,310 --> 01:11:07,190
So you do not lose any precision in estimation, but you do not need to share data.

547
01:11:07,700 --> 01:11:11,810
You don't need to take this third dataset in a calculation.

548
01:11:12,170 --> 01:11:17,900
You can just do this disputed calculation and results your data across hospitals.

549
01:11:18,350 --> 01:11:23,870
Okay. You get similar results. So it's doable, at least in the mean level is very doable.

550
01:11:23,880 --> 01:11:29,450
Right. This is the perfect decomposition that you can do this piece on anything.

551
01:11:31,030 --> 01:11:33,400
So identical result to Oracle.

552
01:11:33,700 --> 01:11:45,370
So this is you are this is your third learning missionary that tells you how this dispute computing can be done without lose anything.

553
01:11:46,510 --> 01:11:53,920
Now, how we do something more complicated, can we tackle the variants so that you can take a standard deviation?

554
01:11:54,160 --> 01:12:03,340
Well, do a little bit more sort of folder sort of calculation mean can be distributable.

555
01:12:03,640 --> 01:12:06,890
Okay, let's see. Can we do this? Okay, so.

556
01:12:07,330 --> 01:12:10,690
So you can this is step that you can do this.

557
01:12:11,340 --> 01:12:23,290
Okay. So the variance is equal to what the the basic components plus the means minus mean square means.

558
01:12:26,950 --> 01:12:36,170
So sample variance can be reading as one over from one to an x squared minus single means square.

559
01:12:37,420 --> 01:12:40,620
Right. So this is famous formula, right?

560
01:12:40,630 --> 01:12:55,270
So, so you can do the calculation and then so, so, so, so so you can being has been proved to be disputable and you can do the federal learning.

561
01:12:55,270 --> 01:13:04,440
And what do you needed to reduce this term sum of x's squared can be just to be able to like computing, distributed fashion.

562
01:13:04,460 --> 01:13:07,570
Yes, same idea. You can distribute it.

563
01:13:07,690 --> 01:13:11,590
So no province is fine. Okay, Sycamore? No, no problem.

564
01:13:12,820 --> 01:13:18,990
How do you do this for correlation? Okay, so so correlation is, you know,

565
01:13:19,030 --> 01:13:30,880
this cross product or minus the cross factor mean and divide by squared with the product very individual variances is doable because this again,

566
01:13:30,910 --> 01:13:35,440
you have this sum which can be distributed.

567
01:13:35,810 --> 01:13:35,930
Okay.

568
01:13:36,250 --> 01:13:44,220
So now, of course, that you need to report different sets of the summary statistic to suit your server because you calculate different statistic.

569
01:13:44,680 --> 01:13:53,229
But anyway, you report the mean we import this cross product term you everything can be compute on this thing.

570
01:13:53,230 --> 01:14:02,410
Each hospital right you don't need to show data. And the Pearson correlation can be calculated in a dispute with the fashion.

571
01:14:02,470 --> 01:14:06,340
Federal learning will give you exactly simple solution. No problem.

572
01:14:07,270 --> 01:14:11,140
Because you can do this decomposition.

573
01:14:12,100 --> 01:14:18,940
Very straightforward. Okay. And the reason you can do this decomposition, because this is a NINA operation.

574
01:14:18,940 --> 01:14:24,700
This is a sample of individual data points and this can be distributed across different hospitals.

575
01:14:25,060 --> 01:14:31,900
So there is no problem. Okay. So next things, can we do linear regression?

576
01:14:32,040 --> 01:14:36,550
Right. So that's that's something you really want to figure out.

577
01:14:38,290 --> 01:14:44,020
So can you do a little bit more sophisticated? And in the near regression, the answer is yes.

578
01:14:44,320 --> 01:14:49,760
And you are going to get exactly similar results to this federal government situation.

579
01:14:49,780 --> 01:14:53,320
Okay. Let's get up set up this sort of notation.

580
01:14:53,830 --> 01:14:55,690
It's a little bit more involved. Okay.

581
01:14:56,770 --> 01:15:10,720
So we also star is a simple case with two users or two groups or two study size two hospitals just to see how this decomposition can be arranged.

582
01:15:11,140 --> 01:15:21,220
Have you realize as long as you can do this decomposition by site, by individual, by user, and you can really just do this free root learning.

583
01:15:21,400 --> 01:15:24,850
All right. So here is my regression model.

584
01:15:24,850 --> 01:15:34,300
Why is my outcome of interest? Right. And for example, here at Y could be the total number of social screen time.

585
01:15:34,750 --> 01:15:38,649
And X is my predictors, could be number of pickups.

586
01:15:38,650 --> 01:15:46,960
Right. So something you like to look at the relationship between one X, so that's our parameter of interest.

587
01:15:49,360 --> 01:15:55,600
Okay. So for simplicity, I show here the error term follows a normal distribution.

588
01:15:56,050 --> 01:16:03,130
These are common variance or squared. Okay, that's the minor regression model you're going to run for the analysis.

589
01:16:03,550 --> 01:16:10,330
But the the issue here is can we do this estimation in a disputed fashion using this single learning?

590
01:16:10,750 --> 01:16:20,680
So this year so I have data collected from my first hospital, so I have a vector y,

591
01:16:21,190 --> 01:16:30,510
so I have one patients from hospital one I collect their like for example, there are renal functions and then I.

592
01:16:30,870 --> 01:16:35,690
My have one that could have a p covered.

593
01:16:35,840 --> 01:16:45,750
Right? So those are like treatment, the age, gender and you know, so, so you just have some personal individual characterizations that's your corpus.

594
01:16:46,850 --> 01:16:53,330
And and this is data from your hospital, one that is is from the hospital, too.

595
01:16:53,930 --> 01:16:57,620
You are in class. Maybe this is data from y user.

596
01:16:57,620 --> 01:17:04,639
This is from not rooms. Okay, so you have sample size and one from hospital.

597
01:17:04,640 --> 01:17:07,720
Well, one and two from group two. Okay.

598
01:17:07,790 --> 01:17:14,629
This is data set up, you know, so now you this is your Oracle data, right?

599
01:17:14,630 --> 01:17:17,870
Suppose owner is a data user. Yeah.

600
01:17:18,170 --> 01:17:19,750
I had a question in the piece. Right.

601
01:17:20,280 --> 01:17:31,100
So your second bullet point isn't it quite two is equal to one extra with oh this is the little why what's the next line or this one.

602
01:17:32,090 --> 01:17:41,630
Yeah, yeah, yeah. Okay. So this copy paste is this is not that they they corrected the to hospital collect the same colors and there's a typo.

603
01:17:42,360 --> 01:17:46,800
Right. Sorry. Yeah. So, so basically you'll understand what I said.

604
01:17:47,120 --> 01:17:50,320
Yeah. So why do is it blue. Why do comma one.

605
01:17:50,420 --> 01:17:53,750
Yeah, this is just this type. Thanks.

606
01:17:54,170 --> 01:17:59,870
Your version. Nice. Good. Yeah. So this is the way to is the X to.

607
01:17:59,870 --> 01:18:03,430
Yeah. So that's table copy paste.

608
01:18:04,700 --> 01:18:14,410
So, so in the central server, suppose you can get the data from this to hospitals and you know, so you can stack them to your right.

609
01:18:14,420 --> 01:18:20,030
So there's this chunk is data, you know, designed matrix from your hospital.

610
01:18:20,030 --> 01:18:27,319
One, this is these are images from the hospital, too. This is I'll come back to her from and my patients from the hospital.

611
01:18:27,320 --> 01:18:36,290
One this is into you know data into element that terms of the outcome from the hospital you just stack them together.

612
01:18:36,290 --> 01:18:46,249
This is something you could do even though you append them to get her data first part from your hospital, one second part from hospital.

613
01:18:46,250 --> 01:18:50,690
I mean, under a centralized operation, you are able to create it.

614
01:18:50,930 --> 01:18:54,140
Of course, then this is ordinary square estimation.

615
01:18:54,650 --> 01:19:00,799
This is the Oracle solution under this flu data, but this is your ghost.

616
01:19:00,800 --> 01:19:07,940
And if you are able to combine the raw individual data from two hospitals, that's the estimate they don't want.

617
01:19:07,940 --> 01:19:11,190
And this is a standard form. You know, you'll learn from 652.

618
01:19:11,230 --> 01:19:15,200
Agree? Do you agree? Yeah. Okay.

619
01:19:16,560 --> 01:19:23,510
Now, the question here is, well, and I'll tell you this, when the individual overall data are not allowed to be conducted,

620
01:19:23,760 --> 01:19:30,629
you cannot do this operation because the you cannot, you know, stack them together.

621
01:19:30,630 --> 01:19:38,370
You don't have as a group to put them in a central server. So this is data is the way you want the team, but you don't have the data.

622
01:19:38,820 --> 01:19:42,800
You'll have the data. But they are not blind. You do not allow to combine.

623
01:19:42,840 --> 01:19:44,610
Do that. Create X and Y.

624
01:19:45,570 --> 01:19:58,470
Now, do we have a similar decomposition that allow you to produce exactly the hat as you would often you would when you have centralized operation?

625
01:19:59,430 --> 01:20:04,630
The answer is yes. You just do points of decomposition.

626
01:20:06,020 --> 01:20:14,990
Complicated. But at the end of the day, you just write a little piece of art code that you can do this.

627
01:20:15,590 --> 01:20:25,410
That's all of your homework. Right to our code. And to implement this finally will ask individuals in the class to contribute some statistics.

628
01:20:25,430 --> 01:20:31,220
Then you will go further in our results year in a row that up and then you can get to the estimate.

629
01:20:31,670 --> 01:20:40,310
You can do IPOs, testing all sorts of things without sharing the raw data.

630
01:20:41,680 --> 01:20:53,690
Okay. I will teach you next week why this is going to give exact this answer as you you have this interaction because this is a table of learning.

631
01:20:55,730 --> 01:20:59,900
Okay. So so I spoke to you and.

632
01:21:10,770 --> 01:21:16,630
That's right.

633
01:21:18,880 --> 01:21:28,410
But they just because it's been confirmed, that was already confirmed. If you see that, there's you.

634
01:21:29,500 --> 01:21:48,150
If you go to the top of that, I will check on the confirmed appointment because that's something you can get through in some cases now.

635
01:21:48,470 --> 01:22:02,630
I really think it should be called the we'll see if it's someone on someone else's hands like me in prison as a sure.

636
01:22:02,650 --> 01:22:06,790
I don't care what the government thinks in advance. So basically I couldn't have said no.

637
01:22:07,840 --> 01:22:11,840
She's like, Do you mind if we train this person in advance?

638
01:22:11,860 --> 01:22:17,319
And I was like, So. And I say, No. I said, Okay, what do you say?

639
01:22:17,320 --> 01:22:24,590
Oh, I'm going in for a ride.

640
01:22:24,830 --> 01:22:31,390
So. So you don't have the visa. That's why this is a missing data problem we're going to hear about all the time before homework.

641
01:22:31,440 --> 01:22:39,669
Why would I use either my data or the. I've been working on this.

642
01:22:39,670 --> 01:22:53,710
Of course, they've been trying to figure it out. They're going to be like, this is how it got there.

643
01:22:54,410 --> 01:22:59,190
That's all I have is basically the results.

644
01:23:01,130 --> 01:23:11,140
The thing I like to look for certain things.

645
01:23:11,240 --> 01:23:28,150
Yeah, I think that's a silver lining here, but you have to take the reference point.

646
01:23:28,480 --> 01:23:32,620
So we all agree that clock the zero.

647
01:23:33,190 --> 01:23:40,310
This is the origin. Yes. Yeah. So that being will be a reference to actually started.

648
01:23:41,610 --> 01:23:45,460
Yeah. So what.

649
01:23:52,190 --> 01:23:52,780
I'm not.

