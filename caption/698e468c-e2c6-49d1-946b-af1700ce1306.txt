1
00:00:07,710 --> 00:00:11,780
Okay. Let's get started.

2
00:00:33,820 --> 00:00:47,860
So today, I think from from today I'm going to go to the first of all today's that you for homework one.

3
00:00:48,080 --> 00:00:55,090
Right. So we're not accepting any late formal submission.

4
00:00:55,390 --> 00:01:02,470
Okay. So I wanted to be clear. So, you know, so there is no exception.

5
00:01:03,010 --> 00:01:09,050
So for increased amount of time. And, uh.

6
00:01:12,570 --> 00:01:21,480
And then regarding homework one. So problem two could be tricky if you wanted to get a full credit.

7
00:01:21,930 --> 00:01:34,799
Yeah. So I believe based on our experience with many of the students, eight examples are relatively easy.

8
00:01:34,800 --> 00:01:47,310
So even so. So the problem is the, the original intention of this problem two and three is to help you well,

9
00:01:47,620 --> 00:01:58,620
try to help you implement the method with as best with the best numerical precision as possible.

10
00:01:58,800 --> 00:02:06,930
And the problem two and problem three is slightly talking of slightly emphasizing different aspects.

11
00:02:06,930 --> 00:02:10,020
And so you will understand what kind of trick you're going to need.

12
00:02:11,460 --> 00:02:19,470
So problem three, I think if you know the trick, I think you'll get it right.

13
00:02:19,950 --> 00:02:31,320
Problem two Uh, so if you, if you know how to solve the problem numerically, usually most of people get eight out of ten, right?

14
00:02:32,160 --> 00:02:35,790
And the less true problem is tricky because the way value is very large.

15
00:02:35,790 --> 00:02:44,790
So, uh, a so even if you get the beta one correct, beta zero could be slightly incorrect because the values lower.

16
00:02:44,920 --> 00:02:51,600
So you have to obtain larger level of precision to, to make the subtraction correct.

17
00:02:52,740 --> 00:02:57,510
So you need to use as least amount of operation so that that could be very tricky.

18
00:02:57,510 --> 00:03:00,930
So I'm going to keep providing health today.

19
00:03:01,230 --> 00:03:06,990
But if it's not, you know, if you don't get a ten out of ten,

20
00:03:07,560 --> 00:03:12,240
please not upset because I am pretty sure that that doesn't really affect your grade at the end.

21
00:03:12,240 --> 00:03:17,850
So I mean, I, I, I understand that it is important, but this is a,

22
00:03:18,410 --> 00:03:27,680
a we have like 150 points or something total in I think 130 or 40, I think two total.

23
00:03:28,170 --> 00:03:31,350
So it's a lot of points just the whole moment that's 40%.

24
00:03:31,350 --> 00:03:37,020
So it's it's really like less than a point you're talking about at the end.

25
00:03:37,020 --> 00:03:49,260
So yeah. So even if, if you don't end up getting it right, I try to give extra a the impulse so, so that you can test as much as possible.

26
00:03:49,860 --> 00:03:53,069
But even if you don't get it right, it's not the end of the world.

27
00:03:53,070 --> 00:03:59,700
So don't get too frustrated. But it it is good to it is good if you get it right.

28
00:03:59,700 --> 00:04:06,179
But there's a lot of like a small, small, trivial thing you need to take care of to get get those two cases.

29
00:04:06,180 --> 00:04:11,700
Right. So it is it is a it is not a perfect problem.

30
00:04:11,700 --> 00:04:19,830
I, I, I think, you know, not, not everyone, you know, when you, when you have to do the solve some something,

31
00:04:20,280 --> 00:04:26,309
you don't have to do that tedious amount of like numerical precision of checking when you do this.

32
00:04:26,310 --> 00:04:34,620
But this is the intention is to checking the numerical precision, as, you know, keeping numerical operation as much as possible.

33
00:04:34,620 --> 00:04:41,790
So I think it's useful exercise about the don't don't care about don't don't be bothered about that outcome too much

34
00:04:41,790 --> 00:04:50,820
so that that that's what I would like to just mention in case you get frustrated not getting exactly ten out of ten.

35
00:04:52,470 --> 00:04:56,820
So yeah. So that, that, that I just wanted to mention.

36
00:04:59,760 --> 00:05:03,569
And yeah, you will see what what the solution is.

37
00:05:03,570 --> 00:05:06,690
But the solution basically, it's not going to be very different.

38
00:05:06,690 --> 00:05:11,309
But, you know, whether whether you use the mean or some or, you know, loop,

39
00:05:11,310 --> 00:05:16,160
sometimes it slightly changes to the numeric numerical question accuracy that

40
00:05:16,170 --> 00:05:20,420
you can test with extra test page to see which one works and which one do that.

41
00:05:22,090 --> 00:05:29,080
Okay. So. And Homer, too, is posted.

42
00:05:29,430 --> 00:05:39,330
So if you are done with the home one, uh, it's, uh, it's a good time to, you know, try to look in and think about it.

43
00:05:39,930 --> 00:05:48,060
One of the problem is bus pass metrics work, which we were going to cover today, but probably not, not, not also.

44
00:05:48,390 --> 00:05:51,450
I think you can immediately start working on three problems.

45
00:05:51,450 --> 00:05:58,350
But there's one problem you need to finish this. The first matrix lecture.

46
00:05:58,920 --> 00:06:03,780
Even after this lecture, I don't think we can finish it today completely, but.

47
00:06:03,990 --> 00:06:09,500
Yeah. So yeah. So.

48
00:06:09,500 --> 00:06:13,760
Yeah, it's a problem. Theresa. Theresa is probably.

49
00:06:13,760 --> 00:06:17,089
You need a next lecture problem. One and two and four, I think.

50
00:06:17,090 --> 00:06:23,120
Okay, sorry. Okay. So let's get to the the the metric decomposition.

51
00:06:24,020 --> 00:06:31,190
So we talked about earlier decomposition and and that that's where we start.

52
00:06:31,730 --> 00:06:35,840
Okay. So. Oh, sorry.

53
00:06:36,140 --> 00:06:40,370
One more thing. So. So I'll try to.

54
00:06:41,090 --> 00:06:50,130
You know, I'm very bad at recognizing name and face together and I'm so in a these class has been very big in the at the start.

55
00:06:50,140 --> 00:07:00,110
So I try to be more you know not not too much of the didn't didn't try to you know,

56
00:07:00,140 --> 00:07:09,650
remember your your your face face in a name pull up and I'm going to try because, you know, we have we have a pretty stable set of students.

57
00:07:09,650 --> 00:07:14,930
And I think it's a good two good size to to relate each other.

58
00:07:14,930 --> 00:07:18,530
So, you know, if you have questions, I'll ask your name.

59
00:07:18,530 --> 00:07:23,510
So, you know, they'll take time. What I'm trying to remembering, but I'll try.

60
00:07:25,160 --> 00:07:31,129
So these days for the roasters that you're that there's a big difference between the what I see in the for the loss the

61
00:07:31,130 --> 00:07:41,209
here it was special with my mascot stuff so I'll it'll take time for for me to get adjusted but I'll try okay okay.

62
00:07:41,210 --> 00:07:46,830
So that's that and before people are starting, okay, so let's, let's try to go to the metrics to accomplish.

63
00:07:46,970 --> 00:07:50,330
Okay, so any question before starting?

64
00:07:52,200 --> 00:07:55,620
Okay. One more thing.

65
00:07:55,650 --> 00:08:08,040
So I know that we have much more student than than these numbers and we're not getting the whole crowd, which is which.

66
00:08:08,040 --> 00:08:13,910
I think it's okay if you don't think we you don't need to come to lecture, you know,

67
00:08:14,130 --> 00:08:22,020
looking at the looking at the video and so on is if you if you if you think that's okay, I'm okay with that.

68
00:08:22,260 --> 00:08:23,850
There's two things I wanted to mention.

69
00:08:24,540 --> 00:08:33,820
One is that I cannot guarantee that, uh, video lectures will be available every time, so there could be technical problems or so.

70
00:08:33,930 --> 00:08:40,110
I'm always trying, but sometimes I've seen that the previously Panopto recording could fail.

71
00:08:40,110 --> 00:08:45,689
And there's. There's no way I can provide the record lecture in that case.

72
00:08:45,690 --> 00:08:54,540
So you got to take that risk if you if you're if you're not coming in class and I sometimes use a whiteboard,

73
00:08:54,630 --> 00:09:03,450
you probably recognize it that that is that could be a that's not been seen in the recorded lectures.

74
00:09:03,450 --> 00:09:06,540
So those are those are the things that you need to remember.

75
00:09:06,540 --> 00:09:11,400
And I do appreciate the, uh, in class interactions.

76
00:09:11,550 --> 00:09:14,970
So later I've seen some students.

77
00:09:14,970 --> 00:09:23,610
Oh, I got a very good grade in your, in your, in your class and a can, could you write me a letter of recommendation when you go to the program.

78
00:09:23,620 --> 00:09:28,060
And I actually don't remember the student interacting with the students at all because they didn't show up.

79
00:09:28,530 --> 00:09:37,649
There are cases like that, so then I can write a letter. But those letter could be terse because those are the factors you might if because I

80
00:09:37,650 --> 00:09:42,030
need to I need to write something I remember about the interaction with the student.

81
00:09:42,030 --> 00:09:45,750
Right. So so those are, those are the things you may want to remember.

82
00:09:46,200 --> 00:09:52,919
Uh, you mean you may want to take into account if you decide not to come to this lecture?

83
00:09:52,920 --> 00:09:56,640
Probably. This is not what the audience I'm talking about.

84
00:09:56,800 --> 00:10:03,180
If you're just looking at this video lecture, those are the factors you might want to consider.

85
00:10:03,570 --> 00:10:12,990
Okay. Okay. So really getting into this, uh, the, the metrics, the competition now.

86
00:10:13,270 --> 00:10:16,580
Okay. So we spent 10 minutes. Okay.

87
00:10:17,870 --> 00:10:20,150
So earlier the competition.

88
00:10:20,270 --> 00:10:27,679
So we're going to talk about earlier the competition trial, the competition, the competition and a singular reality competition.

89
00:10:27,680 --> 00:10:36,629
And again, the competition which is which is kind of similar. So I, I learned how many people knows what kind of competition.

90
00:10:36,630 --> 00:10:39,830
I like the competition pretty much at least a well known thing.

91
00:10:39,830 --> 00:10:43,400
And the which is fine because we're not going to use only the competition much.

92
00:10:43,880 --> 00:10:49,370
But just for complete completeness, I'm going to just explain.

93
00:10:49,910 --> 00:10:53,780
So the competition is not very popular because this is not very well used.

94
00:10:53,900 --> 00:10:57,140
The region is that you need to.

95
00:10:57,410 --> 00:11:05,360
So to be able to find all you the competition, you need to have a matrix law triangular and I triangular matrix that is,

96
00:11:06,140 --> 00:11:16,040
that is uh oh sorry to to make the lower triangular in the upper trade triangle matrix a has to be square and inviolable.

97
00:11:16,100 --> 00:11:23,329
So those are the square is probably okay, okay condition.

98
00:11:23,330 --> 00:11:30,770
But being immutable is not always happening. So that's why this is the less popular.

99
00:11:31,070 --> 00:11:44,900
But if you have upper and lower triangular matrix, uh, solving those linear equation, so basically finding the inverse of a matrix.

100
00:11:46,220 --> 00:11:55,790
When the matrix is the lower triangular or triangular, it should be easier because the finding the inverse is basically you're,

101
00:11:55,800 --> 00:11:59,720
you're trying to multiply a matrix and to make a identity matrix.

102
00:11:59,720 --> 00:12:03,620
So if you have, if you know that there is a limit number of zeros,

103
00:12:04,010 --> 00:12:08,960
you can solve the equation gradually using the cost, elimination or whatever method.

104
00:12:09,530 --> 00:12:17,629
So I'm not going to go into actual details, but basically you, you in the first line, you have only one unknown.

105
00:12:17,630 --> 00:12:22,790
So it's immediately solvable. Second line you have two unknown, but the one one is already known.

106
00:12:22,790 --> 00:12:26,990
So it's a it's a the other is solvable and so on. So you can gradually solve it.

107
00:12:27,920 --> 00:12:32,690
And that's why people made a lot of matrix algebra.

108
00:12:33,050 --> 00:12:38,900
If there is a law triangular or portraying a matrix that that is that is useful.

109
00:12:39,020 --> 00:12:39,320
Okay.

110
00:12:40,010 --> 00:12:47,600
And that's why they're try people are trying to find the low triangular and they'll put triangular matrix in some sort of a matrix to converge bit.

111
00:12:49,580 --> 00:12:54,319
So if you do that, it's a calculation becomes easier.

112
00:12:54,320 --> 00:12:58,430
You need the fewer additions in multiplication. When you do the calculation, these are all good.

113
00:12:58,910 --> 00:13:02,170
Okay. So these are the earlier decomposition. Okay.

114
00:13:02,480 --> 00:13:10,880
And the the thing you basically need to remember is that you can multiply this inverse inverse of matrix twice.

115
00:13:11,150 --> 00:13:16,550
But the calculation calculating the inverse is not of cubic time complexity,

116
00:13:17,090 --> 00:13:22,010
but obviously finding this l of the composition is three cubic time complexity problem.

117
00:13:22,520 --> 00:13:29,060
So overall your the only still is limited by this cubic time complexity overall.

118
00:13:29,750 --> 00:13:38,080
Okay. So, uh, and there is something called ASCII decomposition, which is it?

119
00:13:38,090 --> 00:13:42,020
You probably didn't know either, but this is probably more important.

120
00:13:42,770 --> 00:13:51,500
So interesting the composition, it's a very similar to the earlier decomposition, but you need to have a square matrix.

121
00:13:52,040 --> 00:13:58,070
And in global matrix, well in bottom it put positive in matrix, you also has to be symmetric.

122
00:13:58,190 --> 00:14:07,969
Symmetric. So it's a primitive matrix. So well, I mean, so parameter symmetric is same if the matrix that has a has a real value.

123
00:14:07,970 --> 00:14:13,910
So, so I'll put right the symmetric mean the triangular and or triangular matrix can be folded.

124
00:14:14,120 --> 00:14:18,050
So it's a symmetric. So in that case when you,

125
00:14:18,080 --> 00:14:23,030
when you do the l you the competition in that kind of matrix because it's a symmetric

126
00:14:23,450 --> 00:14:32,089
those low triangular matrix should be the transpose of the upper triangular matrix.

127
00:14:32,090 --> 00:14:39,829
Right. So basically you have a, you have a single matrix that there should be just upper triangular or right angular.

128
00:14:39,830 --> 00:14:47,780
But either way, so you can be represented as a u t u transpose times you or l times l transpose.

129
00:14:47,780 --> 00:14:50,870
Either way is fine, but the first part is low triangular.

130
00:14:50,900 --> 00:14:56,330
The other part is upper triangular. Okay. And this is a special case of l u t competition.

131
00:14:57,380 --> 00:15:04,190
Okay. So then this holistic decomposition is actually used a lot in.

132
00:15:04,570 --> 00:15:08,500
I use in the class multiple times. The reason is that.

133
00:15:08,890 --> 00:15:13,600
So do you. Do you remember, like in the case that you saw?

134
00:15:13,930 --> 00:15:18,009
Well, in which case you deal with this symmetric magic.

135
00:15:18,010 --> 00:15:25,900
So a lot of times. In in your probably description or.

136
00:15:27,700 --> 00:15:38,020
And he made six operations. Sorry. Can you, can you guess that the covariance matrix is a symmetric right?

137
00:15:38,050 --> 00:15:47,740
So when you have a probability matrix and if you need to do some spacial matrix operation with the covariance matrix to make the operation easier,

138
00:15:47,740 --> 00:15:53,800
you often the can utilize this decomposition to make the operation fast.

139
00:15:54,380 --> 00:15:58,150
So this part is more important than obviously.

140
00:15:58,630 --> 00:16:06,640
So let's decomposition is also computation efficient and it's a it is it is often accurate as long as the matrix is square and immutable.

141
00:16:07,090 --> 00:16:11,870
Okay. But but it's a it could it be very stable?

142
00:16:11,920 --> 00:16:19,750
Obviously, sometimes if you if your matrix is has very, very small eigenvalue.

143
00:16:19,780 --> 00:16:28,540
So so if you're if it's not full length, if there's a collegiality in the in your matrix Trotsky, the compression could be unstable to.

144
00:16:31,460 --> 00:16:36,950
Okay. And the next the we wanted to review is a QR decomposition.

145
00:16:37,150 --> 00:16:42,170
Okay. What is it? Decomposition. Alcohol. Decomposition doesn't require it to be symmetric.

146
00:16:42,650 --> 00:16:46,250
Your even if you don't know what occurred. Decomposition.

147
00:16:46,940 --> 00:16:48,919
Uh, now, actually,

148
00:16:48,920 --> 00:16:57,410
you're using the commission a lot when you use a linear model feeding because most of the linear system solver is using the body composition.

149
00:16:57,690 --> 00:17:06,430
Okay, so how does it work? Okay, so you have a two matrix A equals two times.

150
00:17:06,440 --> 00:17:18,330
Ah. Okay. And the Q is a m by M squared matrix and ah is the M by dn upper triangular matrix.

151
00:17:18,350 --> 00:17:21,500
So we are assuming that M is large here. Okay.

152
00:17:22,410 --> 00:17:25,700
So then, uh.

153
00:17:27,770 --> 00:17:35,990
So you so this auto one two matrix meaning that you have a you have a matrix where you transpose

154
00:17:36,590 --> 00:17:44,149
cube transpose cube is always uh is is identity make that that's that's all the matrix.

155
00:17:44,150 --> 00:17:47,809
So each of the column is independent of each other.

156
00:17:47,810 --> 00:17:58,730
So those are orthogonal basis. So some when you, when you, when you think about this MBM matrix as a sort of a n dimensional vector then.

157
00:18:01,850 --> 00:18:11,450
Then each of the dimensions basically can be correlated in, in, in any metrics if there's a linearity.

158
00:18:12,170 --> 00:18:18,780
But if you make auto the these Q metrics that are product between those.

159
00:18:18,800 --> 00:18:26,300
Any of the columns should be zero. So which means that those are uncorrelated in the statistical term.

160
00:18:26,900 --> 00:18:33,440
So that. Yeah. Q Q transpose Q is an identity and artist's output triangular matrix.

161
00:18:34,090 --> 00:18:40,960
So, so basically you can sort so the because of this region.

162
00:18:40,970 --> 00:18:53,150
So why do you why do people like auto want a matrix while it is sort of a matrix of there is a definite your benefit of many cases,

163
00:18:53,150 --> 00:19:01,240
but one of the benefit is that you don't need to invert the matrix if you know that a a matrix of the other one.

164
00:19:02,420 --> 00:19:09,170
Why? Because you transpose times q equals I.

165
00:19:09,350 --> 00:19:15,350
Right. That means. Q transpose is inverse of Q, right.

166
00:19:16,370 --> 00:19:21,799
It makes sense. So. So because of that, you don't need the inverse.

167
00:19:21,800 --> 00:19:25,180
You just need this transporter matrix. And that's the inverse. Okay.

168
00:19:26,750 --> 00:19:44,540
So you have A equals Q or then if you wanted to solve a x equals B patient, then you have a Q are x equals B, right?

169
00:19:44,990 --> 00:19:49,400
Then I know that. Q So here you want it, you want to actually multiply.

170
00:19:49,420 --> 00:20:00,470
Q inverse here. But if you want to multiply instead of multiply universe, you can multiply to transpose, right?

171
00:20:01,430 --> 00:20:05,870
You transpose to transpose. This becomes identity matrix.

172
00:20:06,050 --> 00:20:11,510
So what you have is are times x equals Q, transpose times B.

173
00:20:11,960 --> 00:20:15,440
Okay. So finding sure the composition takes time.

174
00:20:15,710 --> 00:20:21,410
It's A is still an cubic time complex that you can get away with this and cubic time complexity.

175
00:20:21,410 --> 00:20:26,750
But once you have that decomposition, what you need is just solve this equation.

176
00:20:27,200 --> 00:20:33,260
Okay. And solving this equation. So now if you know the Q, this is a vector, right?

177
00:20:33,860 --> 00:20:41,210
So y y this is the problem that this this problem because a was just arbitrary matrix.

178
00:20:41,540 --> 00:20:45,690
R is a triangular matrix, right? Of a triangle matrix.

179
00:20:45,710 --> 00:20:53,060
So you you have to limit it so you can solve it sequentially in this case.

180
00:20:53,060 --> 00:20:55,160
Two, because this is triangle matrix.

181
00:20:55,670 --> 00:21:07,400
So that's why this is a this is a useful way to solve a linear system because you don't need to invert the matrix.

182
00:21:08,150 --> 00:21:17,780
You just need to what after finding the decomposition, just the multiplication and triangular solving algorithm, which is a very straightforward.

183
00:21:19,240 --> 00:21:27,730
Okay. So the last one probably is a singular value.

184
00:21:27,740 --> 00:21:34,390
The competition. Singular value. The competition is the competition.

185
00:21:34,400 --> 00:21:37,700
A matrix in the three different component. Okay.

186
00:21:38,300 --> 00:21:43,030
So, so let's say m m is greater than end.

187
00:21:43,040 --> 00:21:49,520
You have a matrix, M and matrix. Then you can decompose the matrix into three different term.

188
00:21:50,030 --> 00:21:54,440
You is a you is an m by end matrix.

189
00:21:54,530 --> 00:22:02,660
Okay and D is end by end matrix. So whatever the dimension of these always like whatever smaller dimension is actually.

190
00:22:02,690 --> 00:22:09,620
So if it's, if it's a and is greater than M, then this D becomes N by MBM matrix.

191
00:22:09,620 --> 00:22:13,579
But let's because you can make a transpose of the matrix easily.

192
00:22:13,580 --> 00:22:20,060
So let's just assume that a is a m is greater than an equal greater than N.

193
00:22:21,200 --> 00:22:24,350
Then you have these three matrix.

194
00:22:24,530 --> 00:22:36,500
So you D and V in a D and V is a square d matrix and a u is could be not not a security matrix if it if all your matrices not scared.

195
00:22:36,830 --> 00:22:44,150
Okay. But each of them basically have auto and columns.

196
00:22:44,300 --> 00:22:50,810
So this is my in matrix. But still this my end matrix is each of the column is independent.

197
00:22:50,990 --> 00:22:57,110
It's uncorrelated with each other that have a, have a zero dot product between between them.

198
00:22:57,920 --> 00:23:08,479
So you transpose a you is a the identity and we is a security matrix of we we transpose we and we we we we we transpose these.

199
00:23:08,480 --> 00:23:15,350
Both of them are identity matrix. This is a matrix and a D is a diagonal matrix.

200
00:23:15,470 --> 00:23:23,090
So D looks like a matrix, but it's actually effectively a vector because only time will affect taken apart as a nonzero element.

201
00:23:23,360 --> 00:23:26,599
Okay. So singular value.

202
00:23:26,600 --> 00:23:29,130
The component calculation takes a long time. Okay.

203
00:23:29,960 --> 00:23:38,300
But even though the time, complexity and cubic so and you the composition, you are the composition, holistic composition,

204
00:23:38,480 --> 00:23:44,960
singular value recognition, all takes and cubic time complexity by singular value decomposition take the most of the time.

205
00:23:45,200 --> 00:23:50,090
So there is a some additional constant factor, but equally so.

206
00:23:51,840 --> 00:23:54,799
So the computational complexity, you don't need to remember this,

207
00:23:54,800 --> 00:24:04,280
but basically this isn't cubic and this is a if this is not a security matrix, this is M squared times, N plus, m times and square.

208
00:24:04,280 --> 00:24:08,569
So whichever is larger in this case, M is larger.

209
00:24:08,570 --> 00:24:12,110
So M squared times N is a dominant factor here.

210
00:24:12,530 --> 00:24:18,760
Okay. So and if you don't need to calculate the you you've you need to calculate brilliant.

211
00:24:18,890 --> 00:24:22,290
Only that there is a way to make it faster because.

212
00:24:22,370 --> 00:24:31,879
So because if M is very large and if you're not, if you don't need to calculate the U for for some reason sometimes you just just need a the end.

213
00:24:31,880 --> 00:24:36,560
We in the case that becomes a lot faster because I am squared.

214
00:24:36,560 --> 00:24:37,940
Time is gone. Okay.

215
00:24:39,140 --> 00:24:47,540
So, uh, the nice thing about the single singular, really, the composition is this, this, the composition is the numerically the most stable,

216
00:24:48,110 --> 00:24:57,260
which means that even if there is a some perfect, let's say there is a some even perfectly correlated column.

217
00:24:57,470 --> 00:25:06,770
Okay. Then those, even if in that case is single value, the composition still can be as can be calculated in a numerical stable way.

218
00:25:07,070 --> 00:25:14,850
Okay. So some of the some of these journalistic competition or the third, the competition is more robust, but l unrealistic.

219
00:25:15,180 --> 00:25:19,910
You're more more susceptible of that kind of situation. Okay.

220
00:25:21,110 --> 00:25:29,630
And and you can I mean, these are we're not going to go into all the details of these methods, the competition.

221
00:25:29,660 --> 00:25:33,050
So that's that's a for obvious introduction.

222
00:25:33,860 --> 00:25:44,660
But um, yeah the the time complex that just wanted to talk about okay before going to the next

223
00:25:45,230 --> 00:25:49,760
the again the competition is just a special case of a singular value the competition.

224
00:25:49,770 --> 00:25:59,180
So if you say again the competition that usually you assume that a is a symmetric, symmetric matrix or matrix,

225
00:25:59,690 --> 00:26:08,000
and then if it's symmetric, that this this can be this is just a V, we times b d times we transpose that that makes the symmetric.

226
00:26:08,570 --> 00:26:11,950
So that that is a special case of a gigantic complex.

227
00:26:13,160 --> 00:26:19,290
So if you have a combination matrix, you can use a philosophy competition if you build or again, the competition.

228
00:26:19,310 --> 00:26:22,370
So, so let's see, the competition is faster.

229
00:26:23,510 --> 00:26:32,600
The company is the most stable. So if you think that oh this covariance may have like oh, they have perfectly correlated columns,

230
00:26:32,600 --> 00:26:35,780
then maybe you may consider using singularity composition.

231
00:26:36,380 --> 00:26:40,190
Otherwise, if you if you're confident that this is a positive,

232
00:26:40,190 --> 00:26:49,190
definite matrix we're all like in batteries are all eigenvalues are a positive then you

233
00:26:49,190 --> 00:26:55,590
can use the shortlisting as competition and there are some tricks to make it stable.

234
00:26:55,620 --> 00:26:59,390
I'm, I, I can probably talk about during the lecture.

235
00:27:01,280 --> 00:27:04,879
So, uh, yeah, that, that's, that's that.

236
00:27:04,880 --> 00:27:18,610
And again, eigenvalues are, if you just wanted to know a little bit more about singular, the composition eigenvalues are kind of useful because these,

237
00:27:18,620 --> 00:27:33,949
the, the eigenvalue basically tells how much of how they sit, how much of redundancy in each of these column is if there's a in the original matrix.

238
00:27:33,950 --> 00:27:42,050
So let's say you have a piece of matrix and let's say you have, let's say you have a five columns, okay?

239
00:27:42,740 --> 00:27:50,870
And let's say you have a column, one column, two column for from five and let's say column one and column two is perfectly correlated.

240
00:27:51,170 --> 00:27:56,660
Okay? And let's say column three and column for a perfectly correlated.

241
00:27:57,020 --> 00:28:02,570
Okay, then, then let's say these and these are independent.

242
00:28:02,840 --> 00:28:08,480
Okay? So let's say you basically have three columns, but the two columns are exactly duplicated.

243
00:28:09,080 --> 00:28:12,910
In that case, you're going to expect that the eigenvalues to be two.

244
00:28:13,340 --> 00:28:21,260
It was you mean that these are all like summing up to the and assuming that these are these are all if you

245
00:28:21,980 --> 00:28:30,380
if you sum them up if it if it's one then eigenvalues just in the proportionally they should be 222100.

246
00:28:30,470 --> 00:28:33,590
Because you have a two identical columns,

247
00:28:33,740 --> 00:28:40,940
two identical columns and the 111 on opponents and the other the other two eigenvalues

248
00:28:40,940 --> 00:28:44,450
that zero because of you you have some redundancy in your in your matrix.

249
00:28:45,680 --> 00:28:52,940
Obviously this doesn't happen in it. It doesn't happen immediately like, like this because you can then make them perfectly independent.

250
00:28:52,940 --> 00:29:00,230
So then the, the, the amplitude, the largest eigenvalues are basically whatever, you know,

251
00:29:01,040 --> 00:29:07,179
whatever combination of these columns that explains the variation of these value the most.

252
00:29:07,180 --> 00:29:13,670
The that's the that's the first eigenvalues that there is a there's a some more complicated relationship in the real data.

253
00:29:14,000 --> 00:29:21,230
But conceptually, if you expect that there's a lot of redundancy, a lot of correlation between these columns,

254
00:29:21,620 --> 00:29:27,349
I can I am vector shoot through the capture it so I can valuation capture it.

255
00:29:27,350 --> 00:29:33,990
So if you see the eigenvalue that is very close to zero, that means that there are some redundancy in your columns.

256
00:29:34,010 --> 00:29:37,819
So there are. So sometimes sometimes it could be complicated.

257
00:29:37,820 --> 00:29:45,680
So the column five may not be may not be perfectly correlated with any of other columns.

258
00:29:45,680 --> 00:29:50,750
But if you come by column one, two, three, four in specific way, so for each point of view,

259
00:29:51,290 --> 00:29:59,270
column five could be addition of one, two, three, four or five. Then those will make the eigenvalue of a being zero.

260
00:29:59,390 --> 00:30:03,590
And for the for the one of the, the smallest eigenvalue for you.

261
00:30:04,190 --> 00:30:10,250
So there are a lot more. I'm not this is not a linear algebra class of yeah, I'll just stop there.

262
00:30:12,110 --> 00:30:20,089
And with this decomposition, what I'm going to try is to solve the linear regression problem in different ways.

263
00:30:20,090 --> 00:30:26,870
And some of you may have already drawn to some of these techniques,

264
00:30:26,870 --> 00:30:35,360
but we're going to use these things and compare computationally which one works well, which and which one doesn't in the different scenarios.

265
00:30:35,660 --> 00:30:47,630
Okay. So, uh, maybe, maybe not everyone knows the variation yet, but I'm pretty sure you have heard about that, at least in some some case.

266
00:30:47,780 --> 00:30:55,190
Some case. And I'm assuming that you have knowledge of the 650 eventually, so that that's my expectation.

267
00:30:56,660 --> 00:31:12,170
So linear regression is that you have these so you can model Y as X times beta x is a matrix, beta is a vector, so zeta is a vector plus some noise.

268
00:31:12,230 --> 00:31:22,950
Okay. So you assume that the y is a combination of some vectors of noise and under if you assume the normality assumption, that means that the x.

269
00:31:23,100 --> 00:31:28,230
Beta becomes a mean and noise can be just a model as just a Gaussian noise.

270
00:31:30,090 --> 00:31:36,880
So then again, this is not that linear algebra class.

271
00:31:37,050 --> 00:31:41,220
I'm not going to derive this. You can easily find the derivation.

272
00:31:41,820 --> 00:31:48,270
So what people care about in this linear regression is the finding, the effect size beta.

273
00:31:48,490 --> 00:31:56,670
Okay, so usually y you're given a design matrix as X and A, you are given as some responsible value.

274
00:31:56,670 --> 00:32:01,590
So Y you're trying to find the beta that minimize the squared.

275
00:32:02,730 --> 00:32:06,590
So this is the least squared. This is the regression of square.

276
00:32:06,600 --> 00:32:17,990
So you're you're trying to find the beta that makes the smallest squared differences between the predicted value of X beta and the Y.

277
00:32:18,000 --> 00:32:24,340
So that that is one way to interpret the linear outcomes of.

278
00:32:26,370 --> 00:32:31,110
So the just a lot of cases you need to just kind to make this value.

279
00:32:31,470 --> 00:32:39,840
Okay. So usually it axes my end to end nine p matrix messy and is usually large.

280
00:32:39,840 --> 00:32:44,850
P can be sometimes large, but the usually P's are smaller than any typical setting.

281
00:32:45,900 --> 00:32:49,380
So this means that this will make a P by p matrix, right?

282
00:32:49,500 --> 00:32:57,210
And you invert them. And this p x x transpose is a p by and matrix and the y is an by one.

283
00:32:59,450 --> 00:33:04,850
The vector so that that is what you need to calculate the the most.

284
00:33:05,720 --> 00:33:15,590
And sometimes if you wanted to calculate the various standard errors, uh, well the, the variance term that the sigma scare, this is the equation.

285
00:33:16,610 --> 00:33:19,940
So if you, if you get that, get the very Sigma Square.

286
00:33:20,030 --> 00:33:30,920
You can also estimate the what, what is the, uh, the what, what is the variance of these estimate EBITA which,

287
00:33:30,950 --> 00:33:36,200
which is basically tells about the main, the uncertainty about oh, my estimate.

288
00:33:36,560 --> 00:33:39,800
So this is the equation, but we are probably not going to talk about this.

289
00:33:41,270 --> 00:33:45,050
And if you are familiar with the hypothesis testing.

290
00:33:45,770 --> 00:33:55,760
Basically, hypothesis testing is to to do to to perform the hypothesis testing, testing whether beta equals zero or not.

291
00:33:56,060 --> 00:34:02,150
So, you know, equation is not here. But those are the typical setting where when you do the linear regression.

292
00:34:02,150 --> 00:34:11,420
So I must I'm modeling my response variable as a function of these multiple predictors and which predictor has a non-zero coefficient.

293
00:34:12,110 --> 00:34:17,389
You know that, that, that could be interpreted as a, as a, you know what,

294
00:34:17,390 --> 00:34:22,070
which one doesn't have a non-zero coefficient, you know, in the hypothesis testing case.

295
00:34:22,130 --> 00:34:30,650
Those are the, those are the case where that, that, that could reject the null hypothesis and that could become significant.

296
00:34:31,880 --> 00:34:41,390
So yeah, again, you don't need to, you don't need to understand the statistical, statistical interpretation much.

297
00:34:41,390 --> 00:34:46,760
But let's focus on the computation and the less focus on the computation of beta hat in this case.

298
00:34:47,120 --> 00:34:52,580
Okay, so how do we calculate the beta head? Okay. So that's what we're going to talk about.

299
00:34:54,650 --> 00:34:57,920
So for.

300
00:35:01,290 --> 00:35:07,700
Okay. So we work through this and now try to do the ring integration.

301
00:35:07,940 --> 00:35:13,370
Okay. So before doing the implementing our own versions.

302
00:35:13,870 --> 00:35:24,410
Okay, let's try. Just use the use the linear regression method that is available in R.

303
00:35:25,190 --> 00:35:30,420
So then let's say I'm making a hundred random variables.

304
00:35:30,970 --> 00:35:35,330
Okay. As a response variable, my predictor variable is 100 random variable.

305
00:35:35,600 --> 00:35:42,169
Okay. So in this case. And so you have also intercept term.

306
00:35:42,170 --> 00:35:46,550
So you're modeling y as a sum.

307
00:35:46,670 --> 00:35:59,700
So intercept. So in this case, you have a one to the vector one and you have a vector of X on your modeling this and in beta beta.

308
00:36:00,050 --> 00:36:10,250
So you have your beta one and beta two. Well, let's just put the transpose here.

309
00:36:11,090 --> 00:36:17,000
And so. Well, I'm going to say beta zero and beta one.

310
00:36:17,210 --> 00:36:20,720
Okay. So and you're feed fitting this value.

311
00:36:21,020 --> 00:36:26,960
Okay. So it's a very simple linear regression function that basically does that.

312
00:36:27,410 --> 00:36:34,010
And in this case, because I generate a random value between Y and X, there's no reason that they are correlated correlated with each other.

313
00:36:34,700 --> 00:36:38,840
So if you calculate calculate the beta what this is, this is a beta one.

314
00:36:39,440 --> 00:36:44,060
And beta one is some some value, very close to close to zero.

315
00:36:44,360 --> 00:36:49,810
And this is a standard error and that this is a p value, if you know what that means.

316
00:36:49,820 --> 00:36:58,190
But what here what I care about here is this the estimate, so beat estimate.

317
00:36:58,190 --> 00:37:02,510
And how do we get this? Okay. So that's what that's what we are going to try.

318
00:37:04,010 --> 00:37:12,860
And if you have large amount of observations and I'm doing the 5 million observations,

319
00:37:12,860 --> 00:37:17,330
it takes a long time because it it's it's a large number of observations.

320
00:37:17,840 --> 00:37:26,510
And the the beta becomes closer to zero because the three fact should be zero because these are totally independent variables.

321
00:37:28,790 --> 00:37:32,930
So yeah, that that is how you use a alarm function.

322
00:37:33,320 --> 00:37:38,840
Okay. So that that's how, how, how basic linear regression could be performed.

323
00:37:39,350 --> 00:37:49,759
Okay. So as you see, if you if you see this example, this took quite a lot of time, like 5 seconds, 5 seconds.

324
00:37:49,760 --> 00:37:54,490
It's not it's not two to small and to two to slow.

325
00:37:55,250 --> 00:37:59,270
Given that this is 5 million observation and 5 million observations.

326
00:37:59,480 --> 00:38:07,280
Okay. But does it really have to take five? Does it really have to take these 5 seconds?

327
00:38:07,760 --> 00:38:10,760
Can it can we do a lot faster? Okay.

328
00:38:10,790 --> 00:38:13,820
So that that's what I'm trying to figure out first.

329
00:38:14,930 --> 00:38:19,640
So you already probe it because you've done the homework.

330
00:38:19,850 --> 00:38:23,989
You probably know what the simple linear regression is now.

331
00:38:23,990 --> 00:38:32,240
So a simple linear regression is just single predictor and you are just modeling your outcome as does the mean.

332
00:38:32,240 --> 00:38:39,440
And those are so combination of the intercept and that this single predictor and with the caution noise.

333
00:38:40,880 --> 00:38:47,540
And what you need to what you need to solve is, is finding beta zero and beta one.

334
00:38:47,900 --> 00:38:54,530
Okay. So then you can use this equation to calculate that.

335
00:38:54,830 --> 00:38:58,219
Okay. But, you know, already,

336
00:38:58,220 --> 00:39:05,450
I guess calculating this doesn't require this matrix computation if you're talking

337
00:39:05,450 --> 00:39:11,260
about very simple cases because you can calculate each of the elements in this matrix.

338
00:39:11,390 --> 00:39:14,600
If it's just two by two matrix, it's it's not it's not that hard.

339
00:39:15,560 --> 00:39:20,750
Right. So if you sort that out, this is just a fact.

340
00:39:21,080 --> 00:39:31,550
And so if you if you wanted to if you wanted to get the beta, basically, this is what this is what you need.

341
00:39:31,910 --> 00:39:42,110
Okay. So if I if I all of care is a beta one is this is basically you can you can write in very different many different ways.

342
00:39:42,110 --> 00:39:49,250
But here what what I write here is probably different from what you have done in the homework.

343
00:39:49,970 --> 00:39:55,970
But it. So this this is the one way to do it.

344
00:39:56,420 --> 00:40:06,440
So what does that mean? So this means that correlation, the raw explore of X, Y, the correlation between the x and Y.

345
00:40:07,100 --> 00:40:14,510
Okay. And Sigma sigma squared is a variance of x.

346
00:40:14,840 --> 00:40:17,990
And so you give away security of sample variance y.

347
00:40:18,160 --> 00:40:23,300
Okay, so that's what it is. Okay. So if you.

348
00:40:23,630 --> 00:40:39,990
Correlation coefficient itself is correlation coefficient is defined as a the covariance between the X and Y and and and the variance between X and Y.

349
00:40:40,010 --> 00:40:50,150
So if you plug that in, you can also rewrite this as, uh, so sigma x.

350
00:40:51,730 --> 00:41:01,780
Squared and I see my ex-wife right so you can right this way with this is probably easier but it sometimes it's easier to a correlation.

351
00:41:01,780 --> 00:41:07,780
So I'm going to do this. Okay, so let's, let's just stick to this formula that doesn't, doesn't really matter which one you use.

352
00:41:09,670 --> 00:41:14,590
Okay? So basically, if you have a correlation to this interpretation,

353
00:41:14,590 --> 00:41:20,540
if you have a correlation coefficient between those two than correlation coefficient you,

354
00:41:20,760 --> 00:41:30,489
you can understand well because at this scale from 0 to 1 and well -1 to 1 and you just

355
00:41:30,490 --> 00:41:34,810
scale this quadratic equation of coefficient by the variance of each of this term.

356
00:41:34,960 --> 00:41:42,820
So that should be the estimated effect size. So this is the interpreting one like to think about because it's easy to remember.

357
00:41:43,900 --> 00:41:54,250
So then you can calculate all the, all the other values that the linear function and calculate with these simple equations.

358
00:41:54,250 --> 00:41:58,450
And those are very easy to find from the Wikipedia, whatever you have.

359
00:41:58,720 --> 00:42:09,310
Okay. So now, sorry, now let's think about implementing this in, in our function.

360
00:42:09,550 --> 00:42:13,180
Okay. So well you would expect that.

361
00:42:13,180 --> 00:42:17,480
Well, how can I be worried that our ability function should be very fast?

362
00:42:17,860 --> 00:42:24,280
Let's, let's find out. So we're just doing this very simple calculation.

363
00:42:24,280 --> 00:42:27,759
So Y equals y minus mean of y.

364
00:42:27,760 --> 00:42:32,799
So you're basically centering the Y, you're basically centering centering X.

365
00:42:32,800 --> 00:42:36,520
So both the mean of X and Y should be zero.

366
00:42:37,390 --> 00:42:47,410
Okay. And what you what you are doing is calculating the so you can do the correlation function, but let's do the simple arithmetic operations.

367
00:42:47,410 --> 00:42:50,890
So you're calculating the sigma y squared here.

368
00:42:51,400 --> 00:42:56,170
So some sum because y already subtracted the mean.

369
00:42:56,170 --> 00:43:06,400
So you just need to calculate the squared of y and divide by minus one variance of x squared of the x squared summation divide by minus one.

370
00:43:06,940 --> 00:43:13,300
This is a covariance. Okay? And this is a this is a correlation between those two values.

371
00:43:13,720 --> 00:43:16,960
So your your beta, is this right?

372
00:43:17,410 --> 00:43:28,720
So you're using just this equation and other standard error of beta and T statistics, which which is also determined by the alum function.

373
00:43:29,200 --> 00:43:32,320
The equation is given here, so we're not going to derive this.

374
00:43:33,310 --> 00:43:39,460
So you can also calculate the p value. If you have a t statistics and degrees of freedom, you can calculate the p value by.

375
00:43:39,470 --> 00:43:42,550
If you don't know these standard, there are t that p value.

376
00:43:42,630 --> 00:43:45,130
This is not necessary, but this is how you calculate it.

377
00:43:46,120 --> 00:43:52,750
Then what I'm trying to do is just I'm going to return to the same kind of value that this one returns.

378
00:43:53,080 --> 00:43:56,500
I want to return these four values. That's what I wanted to do. Okay.

379
00:43:57,400 --> 00:44:02,440
And so so that's what I did. So let's try to implement your house.

380
00:44:02,440 --> 00:44:08,800
I, I would I don't expect this to necessarily faster, but let's try this, pick it up, let's run it.

381
00:44:10,180 --> 00:44:15,069
And if you compare this value and this value, they are the same.

382
00:44:15,070 --> 00:44:22,809
It's just different precision. But I think the, you know, it's using is showing only the four digits here.

383
00:44:22,810 --> 00:44:26,469
But if you show the full digit, this will be identical.

384
00:44:26,470 --> 00:44:31,780
So it did the correct calculation. How much time did it take?

385
00:44:32,620 --> 00:44:41,050
Oh, this take only .158 second rather than taking 5.4 seconds.

386
00:44:42,310 --> 00:44:48,220
What happened? Why do you think I didn't even use C++ or anything complicated?

387
00:44:48,610 --> 00:44:52,780
So I have an implementation, a simple function.

388
00:44:53,200 --> 00:45:07,360
Why do you think I made it faster? I didn't calculate the intercept.

389
00:45:08,080 --> 00:45:11,230
Good point. Okay, so what is your name?

390
00:45:12,450 --> 00:45:18,560
Um, but I just want you to remember your name, so I'm just asking your name whenever you have a question, if you recall.

391
00:45:20,030 --> 00:45:27,520
Yeah. Okay. Thank you. Okay. So, yeah, so long have said that I didn't calculate the beta zero.

392
00:45:28,000 --> 00:45:29,500
Well, beta, you know, we can add that.

393
00:45:29,500 --> 00:45:38,740
I'm pretty sure that that doesn't like increase the speed for 20 times beta and you just do what you need to do is the beta zero.

394
00:45:39,760 --> 00:45:45,970
Is it just being a series of y minus beta one x?

395
00:45:46,060 --> 00:45:53,650
So this is easy to calculate. So I don't think that that will take a lot of time because the beta one is just a single number here.

396
00:45:55,220 --> 00:46:05,530
So so that that is that there could be part of part origin, but that doesn't explain everything and any other.

397
00:46:06,160 --> 00:46:14,470
But yeah. Is the implementation of our users different because they're also able to solve multiple regression.

398
00:46:15,850 --> 00:46:18,850
So yeah. Right, right, right, right.

399
00:46:18,850 --> 00:46:19,419
So Robert,

400
00:46:19,420 --> 00:46:32,710
Robert said the alarm function is designed to solve a more complicated regression problem and we are trying to solve what's a simpler problem,

401
00:46:33,040 --> 00:46:38,619
and that's why we could do better. Does it make sense? Okay, I think it makes sense.

402
00:46:38,620 --> 00:46:42,730
Right. So sometimes so that that's exactly part I wanted to hit on.

403
00:46:43,540 --> 00:46:51,760
So sometimes, you know, the our our function provides more functionality at the expense of a,

404
00:46:52,060 --> 00:46:56,950
you know, non optimized computational cost in some, some of the settings.

405
00:46:56,950 --> 00:47:01,390
So if you know, you use a computational setting you want to solve.

406
00:47:01,420 --> 00:47:06,549
So in this case I didn't need to solve this multiple regression which when you have a multiple predictor,

407
00:47:06,550 --> 00:47:11,500
you need to check whether each of those was appropriate. You need to do the proper matrix multiplication.

408
00:47:11,510 --> 00:47:18,880
That's much more complicated thing. But if it's just a just a simple linear regression, well, I,

409
00:47:19,270 --> 00:47:28,569
I would argue that RS alarm function should be checking that and probably should do much simpler things just make it faster.

410
00:47:28,570 --> 00:47:30,670
But they don't do that. Okay.

411
00:47:31,300 --> 00:47:46,930
So as a result of all that, our M function can be a lot slower in particular setting, even if it was a very well kept, carefully optimized.

412
00:47:47,170 --> 00:47:49,570
Okay. So kept or carefully implemented.

413
00:47:51,130 --> 00:47:57,820
So that's that's the that's the part I just wanted to say we're obviously not going to solve the simple linear regression,

414
00:47:57,820 --> 00:48:03,220
but this is the one thing you may want to know. I have one I have to run punch of like 11.

415
00:48:03,520 --> 00:48:09,960
Right? I have 5 million that if you are working into the genetics of some, sometimes you need to run a lot of linear models.

416
00:48:09,970 --> 00:48:19,810
Right? Then you don't want to use a limb there. If you care about the care about speed, there's a lot faster way to do it.

417
00:48:19,840 --> 00:48:23,889
Much, much faster way. And there are a lot of competition.

418
00:48:23,890 --> 00:48:33,790
Three already developed for just doing that. Okay. So okay, so let's move on to the multiple regression.

419
00:48:34,360 --> 00:48:37,870
Okay. So now let's try to do so.

420
00:48:37,870 --> 00:48:46,360
Let's focus, let's talk about the problem of solving this problem for multiple regression.

421
00:48:47,110 --> 00:48:54,050
Then there are ways to you can use think so I'm not going to do that earlier the competition but singular

422
00:48:54,070 --> 00:49:00,370
value the competition quality competition and let's get the competition to solve the linear system.

423
00:49:00,640 --> 00:49:06,430
Okay the each of them what you want is to calculate this value.

424
00:49:07,270 --> 00:49:11,980
Okay, this is what you want to do, okay? And you can.

425
00:49:13,180 --> 00:49:22,600
You can. So solving this problem, sometimes you don't need the sometimes you don't need the matrix operations.

426
00:49:23,560 --> 00:49:26,890
So X transpose x high speed ahead.

427
00:49:28,870 --> 00:49:32,439
So this is the sometimes you just need to solve this, right?

428
00:49:32,440 --> 00:49:39,340
So this is equivalent, right? So this is does P by P matrix, key by one matrix.

429
00:49:40,090 --> 00:49:43,840
This will be P by end matrix.

430
00:49:44,080 --> 00:49:47,970
Right. O P by by one matrix.

431
00:49:51,530 --> 00:49:56,640
Okay. Okay.

432
00:49:57,690 --> 00:50:02,730
So, yeah. But this is what you need to solve, right?

433
00:50:04,910 --> 00:50:10,220
So how can we so that there are other settings you can you can make this.

434
00:50:10,530 --> 00:50:15,010
And so we'll get into that. Okay.

435
00:50:18,100 --> 00:50:21,140
So let's talk about this.

436
00:50:21,340 --> 00:50:25,120
Solving this using single reality competition first.

437
00:50:25,960 --> 00:50:30,910
Well, so basically what we're doing, we're doing here.

438
00:50:30,970 --> 00:50:38,710
So this looks complicated. But basically what you what you are doing here is a oh, we're solving this.

439
00:50:39,100 --> 00:50:50,440
Okay. So and if you think about it, the competition you are representing X as you tried, you did you deeply transpose.

440
00:50:51,370 --> 00:50:55,270
And if X transpose is basically you're taking a transpose of it.

441
00:50:55,810 --> 00:51:03,430
So taking a transpose the change in order. So we e d transpose and you transpose, right.

442
00:51:03,430 --> 00:51:07,690
But these are diagonal matrix of d and deep transpose the same.

443
00:51:07,930 --> 00:51:12,100
So you can just write that we d you transpose.

444
00:51:12,550 --> 00:51:15,940
So that's the part that are here. Okay.

445
00:51:16,720 --> 00:51:25,420
So then what you have is a you transpose you is a by definition of the eigenvalue, the again, the compression, compression.

446
00:51:25,420 --> 00:51:28,600
This becomes just the identity matrix.

447
00:51:29,140 --> 00:51:37,180
So this becomes a D squared. So this this is just calculating the we d we transpose.

448
00:51:37,540 --> 00:51:46,560
Okay. And and so okay.

449
00:51:46,570 --> 00:51:50,470
So here I need to I need to explain a little bit.

450
00:51:51,610 --> 00:51:55,390
So and and these parties are is is still complicated.

451
00:51:55,570 --> 00:52:02,890
Right. Then you need to calculate the we the square we transpose and then inverse of it.

452
00:52:03,190 --> 00:52:10,090
Right. So, uh, so what?

453
00:52:10,690 --> 00:52:19,120
What? So what you need to is a, you need to find something that would make an identity matrix.

454
00:52:19,390 --> 00:52:28,110
Right. So, but you know that we transpose we because this is a orthogonal matrix will make identity mixed with this sort of cancel.

455
00:52:28,120 --> 00:52:32,770
Okay. Right. So and this is now you want matrix.

456
00:52:32,830 --> 00:52:34,960
Now you're going to make this this really easy to invert.

457
00:52:35,110 --> 00:52:41,610
You just need to just literally invert the each of that they're going to value and that that's what you need to do.

458
00:52:41,680 --> 00:52:47,950
Now, if you because it is a square just the the the multiply the embodiment is twice.

459
00:52:48,730 --> 00:52:52,270
And if you do, we transpose.

460
00:52:52,360 --> 00:52:56,290
What you will have is that this this will be canceled out.

461
00:52:56,500 --> 00:52:59,520
That this will be canceled out and this will be canceled.

462
00:52:59,720 --> 00:53:04,300
So this is the inverse of this matrix.

463
00:53:04,540 --> 00:53:10,930
Okay. So that's why this part is written in this way.

464
00:53:11,890 --> 00:53:18,640
Okay. So then we transpose we is also identity matrix.

465
00:53:19,420 --> 00:53:25,959
And so this is a. So these are these are just multiplying the diagonals.

466
00:53:25,960 --> 00:53:30,040
So this becomes the just inverse of a diagonal.

467
00:53:30,580 --> 00:53:38,060
So this is what you need to calculate data and at the end get if you have a as the way the.

468
00:53:38,520 --> 00:53:47,770
Okay so so calculating the singular value decomposition takes time obviously.

469
00:53:47,920 --> 00:53:51,670
So this this is a unfair question.

470
00:53:51,670 --> 00:53:58,390
But let's say you already calculate the singular value, the composition of you already have the single decomposition of x.

471
00:53:59,140 --> 00:54:03,160
Then what is the time complex to calculate the data here?

472
00:54:18,750 --> 00:54:23,670
So let me ask the question differently. Would you cut which order?

473
00:54:23,670 --> 00:54:28,440
Did you would you calculate this left to right or right?

474
00:54:28,440 --> 00:54:36,149
Right, right. And if you want to kick it the left to right, raise your hand if you want to calculate this.

475
00:54:36,150 --> 00:54:40,070
Right? Correct. So there are more more right to left.

476
00:54:40,080 --> 00:54:44,430
Why why is it why this is better?

477
00:54:45,780 --> 00:54:54,510
Because this is a matrix vector multiplication. So this is a and and the quadratic algorithm.

478
00:54:54,840 --> 00:54:58,290
Right. And once once you have that, it gives another vector.

479
00:54:58,950 --> 00:55:02,940
So the multiplying. This is also quadratic.

480
00:55:03,360 --> 00:55:10,020
This is also quadratic. So if you do calculate the right to left, it is a quadratic calculation.

481
00:55:11,520 --> 00:55:18,600
But if you do left right, it's not quadratic. It is a cubic argument that one, one part.

482
00:55:19,290 --> 00:55:27,230
And if you want it to be even even better, you can also leverage the fact that these are diagonal matrix.

483
00:55:27,540 --> 00:55:31,590
Okay, so you can make it even faster. So we'll get to that.

484
00:55:31,920 --> 00:55:37,050
But that that doesn't change the time complexity because the other part is still quadratic.

485
00:55:37,980 --> 00:55:42,590
Okay. So let's try that.

486
00:55:42,860 --> 00:55:48,230
Okay. So. And this is what we do.

487
00:55:48,890 --> 00:55:54,980
Okay. So if you wanted to calculate the data here, so this function only calculate the data.

488
00:55:54,980 --> 00:55:58,700
It doesn't calculate the the other values, but it should be trivial.

489
00:55:59,510 --> 00:56:05,510
So what it does is that you have a matrix X and the calculate the single value the competition.

490
00:56:05,750 --> 00:56:13,630
Okay. If you calculate the singular value, the competition, it returns a list that contains a you we the okay.

491
00:56:13,670 --> 00:56:19,249
So you can access the the u matrix using the dollar use sign.

492
00:56:19,250 --> 00:56:25,820
And you can use you can access the de de de eigenvalues or using de and a dollar.

493
00:56:26,180 --> 00:56:29,839
Okay. So this is not exactly the same as this.

494
00:56:29,840 --> 00:56:33,530
So let me let me try to explain how how this came about.

495
00:56:34,190 --> 00:56:38,060
Okay. So what I'm doing first is I'm calculating this.

496
00:56:38,150 --> 00:56:41,540
You transpose you transpose y. Okay.

497
00:56:42,020 --> 00:56:52,070
When you calculate, you transpose y, you can use the you can just use the P of u and you can do you can do y.

498
00:56:52,130 --> 00:56:58,980
This is the one way or the equivalently, you can use this cross product function to calculate that.

499
00:56:59,000 --> 00:57:03,140
So then it doesn't require transposing the matrix. This is slightly fast.

500
00:57:03,710 --> 00:57:11,930
So not not a lot. But if you if you need to transpose and multiply some, some, some other vector matrix,

501
00:57:12,500 --> 00:57:17,840
if you use a just use a cross product product, then you don't need to transpose this.

502
00:57:18,080 --> 00:57:21,360
So I'm calculating the U transpose T first.

503
00:57:21,530 --> 00:57:29,300
Okay. And after that, what I need to do is sorry, I declare lost.

504
00:57:29,840 --> 00:57:37,760
Okay. And after that, you need to calculate the you need to multiply with the de inverse.

505
00:57:38,000 --> 00:57:47,480
Right. So the inverse is basically you have one over the v11 nobody to the all the way to one over the end.

506
00:57:47,960 --> 00:57:51,950
So you have a diagonal matrix and you are, you're multiplying some vector here.

507
00:57:53,150 --> 00:58:01,380
Then what, what's going to happen. You actually dividing first element by de dividing second element by by D2 and so on.

508
00:58:01,400 --> 00:58:08,570
So you're just doing the simple division that this is actually this looks like a matrix operation, but this is actually a vector operation.

509
00:58:09,740 --> 00:58:19,580
So that's what I'm doing here. So this part, I'm just getting the output from the previous step and dividing by the I am value.

510
00:58:19,910 --> 00:58:22,220
And this and value is not a matrix actually.

511
00:58:22,230 --> 00:58:30,980
So when you SBT, when you when it returns, it returns you as a matrix or B is a matrix, but these are diagonal matrix.

512
00:58:30,980 --> 00:58:36,410
So it actually returns as a vector because you know that it doesn't want to waste space.

513
00:58:37,220 --> 00:58:42,500
So these are actually vector, so it's even easier. So you just divide to divide the vector by vector.

514
00:58:43,220 --> 00:58:53,150
So that's why that's what you have. And after that, you can just multiply v where that with this vector, then that that's what you get, right?

515
00:58:53,510 --> 00:58:56,960
So that is, this is what this function does.

516
00:58:59,580 --> 00:59:04,800
Let's try to evaluate whether this works or not later. But that this this is the way to solve it.

517
00:59:05,200 --> 00:59:10,110
Okay. Let's let's talk about the you are the competition.

518
00:59:10,620 --> 00:59:16,290
Okay. Who are the competition? You can make the competition.

519
00:59:16,410 --> 00:59:19,510
So x x x equal to r.

520
00:59:20,490 --> 00:59:25,310
That means that you can represent x transpose as an artist, pose q transpose.

521
00:59:25,320 --> 00:59:28,560
And an x is AQR and you have a beta here. Right.

522
00:59:28,830 --> 00:59:33,330
So you're trying to solve this solve this equation. This is equivalent to solving this equation.

523
00:59:34,110 --> 00:59:37,950
What do you see here? Is there something you can simplify?

524
00:59:39,210 --> 00:59:42,540
I guess you can simplify this part is identity matrix.

525
00:59:42,690 --> 00:59:49,490
Right. And you also probably recognize these are transposed and are transposed can is a same.

526
00:59:49,500 --> 00:59:57,540
So you can cancel them out. So which means that you only have you only need to solve this part.

527
00:59:58,230 --> 01:00:01,730
Okay. So.

528
01:00:02,390 --> 01:00:12,320
So. Well, yeah. So, I mean, you can you can you can try this way or you don't you actually didn't even need to multiple x transpose.

529
01:00:12,320 --> 01:00:19,250
So you can just you know, another way to do it is just to put two out here in a multiply to transpose the other way.

530
01:00:19,250 --> 01:00:23,090
That's that's what I did last time. Right? So both of them are fine.

531
01:00:24,140 --> 01:00:27,560
So are these the upper triangular matrix here?

532
01:00:28,010 --> 01:00:31,250
So an acute transpose y is a vector. Right.

533
01:00:31,700 --> 01:00:38,090
So you just need to solve this thing. Okay. So there are provides a function to solve that.

534
01:00:38,100 --> 01:00:41,330
So let me let me just explain this.

535
01:00:41,660 --> 01:00:49,070
Okay. So what you need to do? Well, actually, this one, uh, this one is not.

536
01:00:49,580 --> 01:00:53,360
Okay. So let me let me let me make the excuse.

537
01:00:53,630 --> 01:01:01,730
Okay. So this this implementation is not very not very satisfactory because this is still doing some cheating.

538
01:01:02,510 --> 01:01:08,030
So what you need to do is you have a vector control in this.

539
01:01:08,480 --> 01:01:13,080
So what what I want to do is solve this.

540
01:01:13,090 --> 01:01:16,170
So this is a this is called the board. Okay.

541
01:01:17,030 --> 01:01:23,270
For the so there's a. So our has something called the for the solve solving backwards solved.

542
01:01:23,420 --> 01:01:27,410
So for the solved means that the you have a triangular matrix the in the beginning

543
01:01:28,100 --> 01:01:34,090
so or because obviously you have a triangular or norteno matrix at the end.

544
01:01:34,100 --> 01:01:38,270
So you just solving them that that is a very straightforward thing that doesn't require matrix.

545
01:01:38,330 --> 01:01:43,490
Very good. Okay. So for the our our data.

546
01:01:46,320 --> 01:01:57,430
For so long I have been outside of art for some love of art and a few principles like this.

547
01:01:57,450 --> 01:01:59,010
This is what I wanted to call.

548
01:01:59,760 --> 01:02:10,650
But surely competition in art is not giving you that the Q and art actually competition is giving us some more complicated outcome.

549
01:02:10,830 --> 01:02:14,070
So this this one is is not very easily.

550
01:02:16,740 --> 01:02:21,450
So it doesn't give a clue and are separated basically. So I could not do this.

551
01:02:22,020 --> 01:02:28,349
So just the artists are so so so ours alone function is using this actually a

552
01:02:28,350 --> 01:02:32,249
commission also there's a some some specific optimization that they had to do.

553
01:02:32,250 --> 01:02:39,750
So you could just read read the read the documentation but I had to use use these a special function.

554
01:02:39,750 --> 01:02:45,630
Q Are coefficient is basically, you know, designed to get beat up based on the purity component.

555
01:02:45,720 --> 01:02:48,300
So but it basically does the same thing. Okay.

556
01:02:48,900 --> 01:02:56,130
And the reason why I'm not doing the for the solve is just because this data structure is a little weird.

557
01:02:56,400 --> 01:03:01,500
Okay. So so that that's how you solve with the decomposition.

558
01:03:01,530 --> 01:03:08,280
Okay. So next, so this is a bit more interesting.

559
01:03:08,280 --> 01:03:14,010
So you can also solve the linear list is scared. You can actually see decomposition but you need some trick here.

560
01:03:15,210 --> 01:03:23,550
So in this case, you are working with this X transpose as X to matrix because these are symmetric matrix.

561
01:03:23,850 --> 01:03:31,230
So you can represent you can have a realistic decomposition of this matrix as you transpose it t you transport you,

562
01:03:32,940 --> 01:03:44,580
then what you have is that we are solving we are solving this, right?

563
01:03:45,420 --> 01:03:54,629
So let's say this as a just a the vector B, then instead of saying this is it as an extensive text,

564
01:03:54,630 --> 01:04:00,420
you can say you transpose you and beta hat and this is, this is how you solve it.

565
01:04:02,880 --> 01:04:08,340
And okay, so then how do you how do you stop it?

566
01:04:08,340 --> 01:04:12,239
Now, this is the law triangular matrix. This is a portraying a matrix.

567
01:04:12,240 --> 01:04:16,860
This is a vector, right? So you can actually solve that.

568
01:04:17,760 --> 01:04:29,250
So solve this first. So let's say this is a Z, then you resolve that, you transpose Z, you can be you can you solve this problem once, okay?

569
01:04:29,730 --> 01:04:36,959
After that, if you find a Z, then again you you find that you beat up equal T.

570
01:04:36,960 --> 01:04:51,420
So you solve this simple or the simpler nor more triangular, the or quadrangular regular matrix solving problem in the equation.

571
01:04:51,690 --> 01:04:55,340
So this is how you implement this.

572
01:04:56,760 --> 01:04:59,850
So uh. Okay.

573
01:05:01,500 --> 01:05:13,230
So uh. So in this case I calculate x transpose x first and look at the x transpose y which is

574
01:05:13,500 --> 01:05:20,579
which is effectively and to this function choke calculate ritualistic decomposition of x,

575
01:05:20,580 --> 01:05:23,790
transpose x and return this u matrix.

576
01:05:24,320 --> 01:05:28,860
Okay, then you are actually doing this.

577
01:05:28,990 --> 01:05:36,000
This looks like a for this. So. Well, this is of this is.

578
01:05:36,120 --> 01:05:42,500
Let me see. So.

579
01:05:42,520 --> 01:05:45,640
Well. So. So.

580
01:05:45,820 --> 01:05:49,930
The function for the solve the piece is for the source.

581
01:05:50,020 --> 01:05:59,049
The for the solve is intended to the case where this is the upper triangular matrix

582
01:05:59,050 --> 01:06:03,520
as a lower triangular matrix for the forward service used when this matrix,

583
01:06:03,520 --> 01:06:13,239
the lower triangular matrix and Bec solve. Becs always used the design to be a case where you actually have some data.

584
01:06:13,240 --> 01:06:26,290
Well, let's not say beta let, let's say you have some lower triangular matrix at the end and let's say you have a gamma or something and a equals,

585
01:06:26,500 --> 01:06:31,780
you know, delta or something. So this is, this is the intended use of a, of a back.

586
01:06:31,780 --> 01:06:42,340
So but obviously, if you transpose this, this is also a triangular matrix in some some beta is this this is a case you can solve.

587
01:06:43,000 --> 01:06:50,830
So so basically the first case I'm using the for the soul because this is a lower triangular matrix.

588
01:06:50,980 --> 01:06:54,280
Okay. But, but actually is instead.

589
01:06:54,490 --> 01:06:58,660
So you can provide a transpose a transpose of this matrix.

590
01:06:58,660 --> 01:07:04,270
But you can also see that I'm, I'm writing for the solve and I'm keeping a matrix,

591
01:07:04,270 --> 01:07:11,469
but the matrix that I'm giving is actually a triangular matrix or you need to do the proper trans transpose you need.

592
01:07:11,470 --> 01:07:20,260
So I'm giving my my matrix is upper triangular and the do the transpose for me so that that's what that's what it does and after that

593
01:07:20,890 --> 01:07:28,510
you're adding the pixels the best solve is basically if you give a so back so this basically giving a upper triangular matrix basically.

594
01:07:28,510 --> 01:07:32,560
So because of this so you are just supposed to give it actually a triangular matrix.

595
01:07:33,100 --> 01:07:40,799
And this is this is called back. So I know that it's a little tricky naming scheme, but that's all.

596
01:07:40,800 --> 01:07:44,230
Well, basically means that you're just giving off a triangular matrix to solve this.

597
01:07:44,440 --> 01:07:49,120
Okay, so bec solve is actually well actually voxel.

598
01:07:49,120 --> 01:07:58,059
So this is probably easier to remember. So for the solve is basically solving starting with the low triangular matrix.

599
01:07:58,060 --> 01:08:01,390
So what so this is,

600
01:08:01,480 --> 01:08:10,740
this is actually what is designed to don't remember because so for this always basically you are solving from the from the beginning to the bottom.

601
01:08:10,750 --> 01:08:14,079
So this this means for the solar back story.

602
01:08:14,080 --> 01:08:17,860
So you need to solve with the bottom to the opposite. This is why it's called bec solve.

603
01:08:18,070 --> 01:08:27,250
Okay, so bec. So the meaning means that you are giving up an open triangular matrix and a for so we are giving the lower triangular matrix.

604
01:08:28,180 --> 01:08:34,419
But both of you can say that, oh, I'm actually giving up a triangle matrix up to the transports for me.

605
01:08:34,420 --> 01:08:38,360
So that's what that's what these parties get.

606
01:08:38,620 --> 01:08:42,900
So for the confusion. Okay.

607
01:08:46,530 --> 01:08:49,950
So now we're done? I think so.

608
01:08:49,950 --> 01:08:53,910
Let's try to run these cells, okay.

609
01:08:54,720 --> 01:09:12,730
And see how they work. Okay. So now what I'm going to try is a setting where you have 10,000 observations and 800,000 as our 800 3800 variables.

610
01:09:15,250 --> 01:09:20,680
But out of 800 variables, five of variables actually have non-zero effect size.

611
01:09:21,910 --> 01:09:30,340
And but the the tricky part is that all my columns have a very, very high correlation.

612
01:09:30,610 --> 01:09:37,210
There's a high multicore collegiality in the in my in my input file just to make the case a little bit more interesting.

613
01:09:37,900 --> 01:09:43,600
So I'm going to just simulate these matrix that are very highly correlated.

614
01:09:43,610 --> 01:09:46,870
So they have a very, very high quality covariance structure.

615
01:09:46,870 --> 01:09:51,490
So every, every columns are correlate by point 99%.

616
01:09:52,090 --> 01:09:55,780
And I'm going to I'm going to solve this a linear regression problem.

617
01:09:56,140 --> 01:10:04,630
Okay. So in this case, I'm just generating the random samples, X and Y using these assumptions.

618
01:10:05,530 --> 01:10:11,720
And this one is basically calculating this a fitting this model.

619
01:10:11,770 --> 01:10:19,530
So so, you know, 10,000 is a lot of observations, but also there's 800 columns, each of them.

620
01:10:19,540 --> 01:10:30,250
So this takes quite a long time. So each of them I counted at a value at a time and just storing the TM variables here.

621
01:10:31,360 --> 01:10:35,140
So what I'm going to do is. So what?

622
01:10:35,350 --> 01:10:43,659
What am I doing here? So I'm using l am. And then the print is basically running l m the default l m and sb the minimum.

623
01:10:43,660 --> 01:10:51,220
The theory is what I what I just implemented with s we'd call the limit of theories of implementation of actual decomposition.

624
01:10:51,300 --> 01:10:56,950
So this is implementation ritualistic. So these are the implementation we did in the class.

625
01:10:57,640 --> 01:11:01,810
This is the implementation that is actually in the R by default.

626
01:11:03,010 --> 01:11:06,549
And let's see, what are the actual outcomes?

627
01:11:06,550 --> 01:11:09,090
So there are like 800 variables, right?

628
01:11:09,100 --> 01:11:18,020
So I'm not I'm assuming on the first ten the vectors to show that these are keeping identical output no matter what.

629
01:11:18,040 --> 01:11:25,420
So even for the explicit listing decomposition, this is not there is not perfect correlation between the column.

630
01:11:25,430 --> 01:11:28,780
There is only 99% correlation. So they still were stable.

631
01:11:28,810 --> 01:11:34,180
Very well. So this looks very impressive here. So this works very well.

632
01:11:35,320 --> 01:11:40,750
So the correct in these y's, these two looks correct. And how much time did it take?

633
01:11:42,550 --> 01:11:53,020
So this is the actual time it took. Okay. So so I'm just tabulating the time output for each of each of each of the method.

634
01:11:53,260 --> 01:12:00,969
And MZ is just the mean square that are between the actual predictive value y and Zeta.

635
01:12:00,970 --> 01:12:05,410
So to to just make sure that they give exactly same answer so that that's the intention.

636
01:12:06,220 --> 01:12:19,840
So let's focus on the elapsed time. So linear model is what actually is built in and R is actually slowest, which is surprising, I think.

637
01:12:20,170 --> 01:12:29,050
Now this is still multiple regression, but if you use the as we did, this is a relatively quick if you do decomposition actually,

638
01:12:29,320 --> 01:12:39,310
if you have 800 columns that that could be actually quite quite to try to comprehend take quite a long time.

639
01:12:39,370 --> 01:12:43,120
So it's not only sometimes the speed is slower, I think.

640
01:12:43,120 --> 01:12:46,419
So if you rerun it, you might find a speedy slow.

641
01:12:46,420 --> 01:12:55,480
But those are two are roughly equivalent in my experience, whether using touristy decomposition tree is actually faster here.

642
01:12:55,750 --> 01:13:05,880
Okay. So if you if you think that your design methods are relatively stable, so let's take the linear serving medium model.

643
01:13:06,060 --> 01:13:10,450
A linear regression with ritualistic decomposition is almost always faster.

644
01:13:10,660 --> 01:13:15,610
The reason why people are doesn't implement that way is that if there is a height,

645
01:13:16,240 --> 01:13:21,370
so if the some columns are actually done, done for example, then so this decomposition could fail.

646
01:13:21,790 --> 01:13:31,450
So that's why you know it. So l m is using more reliable method, which is a the competition to avoid that problem.

647
01:13:32,650 --> 01:13:43,930
But computationally ritualistic composition could be a better way to solve a faster way to solve the multiple regression problem in general.

648
01:13:46,010 --> 01:13:50,660
Okay. So that's the end of the to this lecture.

649
01:13:50,920 --> 01:14:00,049
Okay. So I don't know if I have time to go through the, uh, the sparse matrix.

650
01:14:00,050 --> 01:14:05,330
I at least wanted to cover the very basics of sparse matrix, but probably we should.

651
01:14:05,330 --> 01:14:08,780
We should do it next week. So you have 5 minutes.

652
01:14:09,060 --> 01:14:15,020
Okay, so, uh, let's talk about this part because I went through a lot of complicated parts.

653
01:14:15,020 --> 01:14:18,469
I'm pretty sure that there are some part that it's not clear.

654
01:14:18,470 --> 01:14:24,170
So I wanted to have time to respond to your questions to maybe, maybe three, 3 minutes, 4 minutes.

655
01:14:24,770 --> 01:14:29,490
And let's get back to the to answering this question.

656
01:14:29,510 --> 01:14:39,440
So I strongly encourage you to talk to others and try to improve your understanding, because this is definitely a lot of material we discussed today.

657
01:14:40,900 --> 01:14:48,270
Okay. So.

658
01:14:48,510 --> 01:14:51,720
Yeah. Which method was the fastest?

659
01:14:51,990 --> 01:14:55,970
What are the advantages and disadvantages? Okay, so let's talk about this.

660
01:15:38,120 --> 01:15:45,820
But. Just.

661
01:15:51,670 --> 01:16:05,360
What? Reporter So what?

662
01:16:06,950 --> 01:16:55,210
She's. Lost the business here.

663
01:16:56,560 --> 01:17:07,460
This is. This one.

664
01:17:15,810 --> 01:17:24,590
Okay. So, yeah, let's let's talk about this question.

665
01:17:24,600 --> 01:17:28,319
So. So which which question is correct?

666
01:17:28,320 --> 01:17:32,520
Answer. So maybe, maybe the second one, which we should start.

667
01:17:34,830 --> 01:17:41,290
So if you if you use your own way, which one which method you will use.

668
01:17:45,300 --> 01:17:49,950
What are the what are the advantages and disadvantages of. How would you how would you summarize?

669
01:17:53,940 --> 01:18:03,129
What? Any questions or. So is it worth checking whether a lot of you fall asleep after lunch?

670
01:18:03,130 --> 01:18:10,330
And so. Yeah. Good question. So there are faster ways to check whether you have a full rank matrix or not.

671
01:18:10,330 --> 01:18:17,860
So. But the easiest way to do that, again, the compulsion to to check whether what, what's the smallest value in your eigenvalues.

672
01:18:17,860 --> 01:18:22,100
And that obviously takes more time than actual decomposition.

673
01:18:22,120 --> 01:18:26,500
But that there are. So there are there are ways to do it.

674
01:18:26,920 --> 01:18:30,700
I don't I don't think that's worse than doing the first decomposition.

675
01:18:30,880 --> 01:18:33,040
So ritualistic decomposition is just you.

676
01:18:33,730 --> 01:18:43,000
In this case, the reason whitelist decomposition fast is that you in this case, you need to deal with a 10,000 by 800 matrix.

677
01:18:43,540 --> 01:18:47,079
But in this decomposition sitting, this is a PDP matrix.

678
01:18:47,080 --> 01:18:51,820
So you just deal with the 800 by 800 matrix so that that's why it's faster.

679
01:18:52,600 --> 01:19:04,090
So you can make 800 by 800 matrix first and try to check the code to check that in value so that if you do the eigen decomposition here,

680
01:19:04,870 --> 01:19:08,980
that still is okay, that that can be faster.

681
01:19:09,310 --> 01:19:16,210
So if that makes sense. So if you wanted to check and if you're okay with that, oh,

682
01:19:16,240 --> 01:19:20,190
I don't have a full increment because I'm going to use other so I'm going to fall back to

683
01:19:20,190 --> 01:19:28,790
other methods so you can make this matrix and do that as we the checking and you can.

684
01:19:28,810 --> 01:19:38,950
What you can do is, oh, I'm going to say that this matrix is not stable if my smallest eigenvalues like less than a certain threshold.

685
01:19:38,980 --> 01:19:44,750
So you can you can still do that. So, yes. So the short answer is that, yes, you know,

686
01:19:44,830 --> 01:19:50,200
you can do use a sort of see the compulsions default and to fall back to the quality compression

687
01:19:50,200 --> 01:19:57,060
if this is a this is dangerous to do is that increased runtime isn't going to fall.

688
01:19:57,070 --> 01:20:02,440
Usually it should be faster because in most of the case you shouldn't have these, you know, redundant column.

689
01:20:02,440 --> 01:20:07,510
But if you do have written the column, you do you do have to pay for like this much of time, right?

690
01:20:07,530 --> 01:20:11,740
So but still compared to the overall time, it's a negligible amount of time.

691
01:20:12,590 --> 01:20:17,639
One thing you want to remember is that holistic. So. So, yeah, maybe.

692
01:20:17,640 --> 01:20:27,070
Maybe that's the question I should ask directly. But holistic approach is not necessarily faster if the design matrix is almost square.

693
01:20:27,550 --> 01:20:34,120
Right? Well then if you have a 10,000 variables, then you probably you have you probably should worry about the identity.

694
01:20:34,130 --> 01:20:42,880
I identify with the problem first, but in that case you still need to do the group, which is a very large matrix, so it's not very fair.

695
01:20:42,900 --> 01:20:48,490
So sometimes you thought that holistic compression should be faster, but sometimes not especially.

696
01:20:49,180 --> 01:20:54,190
So you may you may encounter these problems, but especially sometimes P is larger than.

697
01:20:55,060 --> 01:21:03,220
So you will encounter that kind of problem in the sparse matrix situation or the third bagel on education in that case, definitely.

698
01:21:03,240 --> 01:21:06,970
Touristic obviously is not the way to do it if you have a very large p. Okay.

699
01:21:09,800 --> 01:21:15,840
Okay. Other questions. We will let it out. So feel free to ask what questions come up and ask question more.

700
01:21:16,250 --> 01:21:18,410
But that's that's it for today.

