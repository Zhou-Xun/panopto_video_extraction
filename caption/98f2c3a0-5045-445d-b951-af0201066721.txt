1
00:00:07,830 --> 00:00:15,180
Okay. Yes. I have a question for when you understand of the assignment.

2
00:00:17,760 --> 00:00:22,470
So I'm the one. So. So and one is the sum total.

3
00:00:24,570 --> 00:00:27,710
Well, it's a sum of the eyes of the air. Zero one.

4
00:00:27,720 --> 00:00:32,100
So some of it is the number of individuals and treatments of some of the air is equal to one.

5
00:00:33,090 --> 00:00:42,660
So it is an and so I guess implicit in this and I assume it is more explicit.

6
00:00:43,650 --> 00:00:47,190
It could be. But I'm assuming you've got a fixed you fixed in one and in zero.

7
00:00:47,790 --> 00:00:51,630
So it's just you're going to it's not quite a it's not like a coin flip.

8
00:00:51,780 --> 00:00:58,950
You could do that. But it's a situation where you have it essentially and you're assigning these you have

9
00:00:58,950 --> 00:01:03,780
fixed in one an and zero and you can assign them randomly subject to that constraint.

10
00:01:04,140 --> 00:01:22,020
Thank you, sir. It's a good point. With more work.

11
00:01:22,020 --> 00:01:24,060
You could you could, I think, get the same result.

12
00:01:24,780 --> 00:01:32,640
So, I mean, it's just we have a one around one chance of being sort of a little bit of uncertainty then because then they become,

13
00:01:33,870 --> 00:01:39,690
you're right in the recovery of variables and samples will be very, very close to whatever those probabilities were.

14
00:01:40,410 --> 00:01:58,710
Can you explain a little bit more like going from the averages of when I get into the patterns, but basically it is part of the trigger.

15
00:02:00,060 --> 00:02:06,629
Okay. So. Okay.

16
00:02:06,630 --> 00:02:13,020
So basically, who is going to be equal to one in this population?

17
00:02:15,090 --> 00:02:22,700
Individuals, right. If there are some, there is going to be one person to treat these individuals.

18
00:02:23,930 --> 00:02:37,730
Not these individuals or those so the total number but some of this overall in I'm going to get in one one and then one zero of those.

19
00:02:38,390 --> 00:02:44,730
So that's the number here. Okay. And then over here, the same game.

20
00:02:45,930 --> 00:02:51,510
Yes. Yes. And what does it mean to.

21
00:02:55,460 --> 00:03:01,290
Yeah. It's a little thing. It gets a little tricky sometimes with binary outcomes because you have to find your treatment by your outcomes.

22
00:03:04,250 --> 00:03:08,690
But probably this reticence is also helping drive home the concept of potential outcome.

23
00:03:09,770 --> 00:03:22,190
You don't have to be actual. People think it's a sign. Is it wrong to think of it as like the film Y?

24
00:03:22,320 --> 00:03:26,870
Why? It's like one one times plus.

25
00:03:28,860 --> 00:03:32,460
Sure. We figured that one zero. Yeah. You it like.

26
00:03:33,170 --> 00:03:36,330
Yeah. Yeah. Basically the sum of all the one. Yeah.

27
00:03:36,640 --> 00:03:45,200
Okay. Yeah. Clarifying location is the number in the parentheses like shape or control.

28
00:03:46,140 --> 00:03:49,950
It's the yes. It's the outcome under treatment.

29
00:03:50,390 --> 00:03:54,630
Your parentheses equal to one and the outcome or control in the parentheses is equal to zero.

30
00:03:54,930 --> 00:04:00,780
So it's like a hypothetical world where each subject to the same about right in control.

31
00:04:01,110 --> 00:04:06,540
Right. That was that was class one. Right. Where we talked about this concept of potential outcome.

32
00:04:07,870 --> 00:04:17,050
So you said you were the standard two, but you should look at the first notes for the notation on.

33
00:04:23,100 --> 00:04:26,799
Right. So all right.

34
00:04:26,800 --> 00:04:30,370
So that's the expectation. Variance is another matter together.

35
00:04:31,630 --> 00:04:38,020
So that's a little more work. I'm going to go through a lot of it, but not all of it.

36
00:04:42,280 --> 00:04:46,629
Some of it I'm defaulting that your text kind of goes through some of the goriest details.

37
00:04:46,630 --> 00:04:49,630
But but I'll try to hit the most of the really high points.

38
00:04:50,440 --> 00:04:54,610
So the variance of this, how happy. Right.

39
00:04:54,640 --> 00:04:56,830
So even though we have the full population available to us,

40
00:04:56,830 --> 00:05:03,340
we still have the sampling variability because we have and choose N1 ways to assign vector of a ones.

41
00:05:05,560 --> 00:05:15,070
So this is the the question about them. So assuming we have this, this in one vixen, this one in 16906, then it's fixed.

42
00:05:15,910 --> 00:05:23,860
So you can show that this variance of this estimate or the variance of the mean observed means and the

43
00:05:23,860 --> 00:05:31,720
population is actually going to be given by the variability of potential outcomes under under treatment,

44
00:05:32,110 --> 00:05:43,750
divided by population, under treatment, variability of the population or control divided by personal population, side control.

45
00:05:44,140 --> 00:05:53,650
And then this covariance term which is really kind of the or they can think of it as the the variance of the differences,

46
00:05:53,860 --> 00:05:57,560
the variance of the difference in the values themselves. Right.

47
00:05:57,580 --> 00:06:05,410
This sort of subject level difference. So there's no really exact straightforward way of the results.

48
00:06:06,250 --> 00:06:09,550
Appendix A In the chapter that I gave you gives all the details.

49
00:06:11,440 --> 00:06:14,440
I'll sort of skip skim over the the main points here.

50
00:06:15,610 --> 00:06:22,270
To minimize computation, they suggest replacing this II with a centered value, which basically is AA minus mean.

51
00:06:23,710 --> 00:06:30,910
So basically this DI is equal to and not over in variables one.

52
00:06:30,910 --> 00:06:35,080
So if you're in treatment, then the guy is equal to the portion of control.

53
00:06:35,710 --> 00:06:42,760
And if you're control, it's the negative of the portion of the treatment. So this has some handy points.

54
00:06:43,030 --> 00:06:51,700
Most notably, the expectation of this DX is going to be right takes the expected value when

55
00:06:51,700 --> 00:06:55,989
it is equal to one times the proportion of subjects with equal one right.

56
00:06:55,990 --> 00:06:58,930
So that's just not that and not a random set.

57
00:07:01,030 --> 00:07:10,630
And similarly, the expected value when a zero which is this minus in I over in terms of probability is equals zero.

58
00:07:11,650 --> 00:07:14,800
So right so the probability that's equal to one is n one over n.

59
00:07:15,700 --> 00:07:22,510
So the product that becomes and not times n one over and squared and then same thing over here.

60
00:07:23,260 --> 00:07:26,919
So those cancel. So the expectation of the zero which is makes sense, right,

61
00:07:26,920 --> 00:07:35,380
because this is the expected value of a so if you take expectation of this, you get expected value B minus A0 variance.

62
00:07:36,580 --> 00:07:42,370
Now that's a little more work, although we have the fact that since the expected value D is zero,

63
00:07:42,820 --> 00:07:46,600
the variance of D is going to be the expected value of the squared number.

64
00:07:47,170 --> 00:07:57,250
Variance is the expected value of the quantity squared minus the expected value of the quantity itself squared.

65
00:07:58,120 --> 00:08:04,330
So we can, we can get that out as right.

66
00:08:04,810 --> 00:08:12,940
We just kind of square these components here. So that gives us a not over in quantity squared.

67
00:08:13,750 --> 00:08:17,650
And then the same thing and one over n quantity squared and these probabilities.

68
00:08:18,520 --> 00:08:24,460
So multiply that by in one over n and and not over n, you get this piece here.

69
00:08:25,480 --> 00:08:35,590
So you get a little bit of a in fact root and then not an end one from and not plus n one,

70
00:08:35,590 --> 00:08:40,209
which is just n so you end up with an order and one and not times.

71
00:08:40,210 --> 00:08:45,840
Then one over in squared. Just some algebra.

72
00:08:49,990 --> 00:09:01,570
Okay. Now, when we have, we think about the covariance between these ideas and JS, that's not going to be zero.

73
00:09:01,990 --> 00:09:05,020
As it turns out, these find population things. It gets a little trickier.

74
00:09:06,310 --> 00:09:12,640
So the fact that they're expected value zero means that the covariance is just going to be the expected value of the product.

75
00:09:13,990 --> 00:09:33,220
So we can write that in as the probability that an age of four equal to 1.6 to the product of die and J, which is this piece here expected value.

76
00:09:33,970 --> 00:09:38,260
And then same thing for this part here. Right.

77
00:09:39,550 --> 00:09:48,010
So going back to this here and then finally and one times not over and squared minus two.

78
00:09:48,090 --> 00:09:56,870
Right. So if we take the product of these, you get, you know, at times only when we're in squared minus two piece.

79
00:09:56,890 --> 00:10:03,880
Right. So and then we have these, these, these probabilities.

80
00:10:04,810 --> 00:10:08,469
So I'm just going to take it to the next line.

81
00:10:08,470 --> 00:10:38,610
I want to write a little bit out here. By the way,

82
00:10:38,630 --> 00:10:47,970
I have to mention the reason I'm always using this board is that I tried it on the other board and the little helping hand quite picked it up.

83
00:10:48,390 --> 00:10:56,080
The reporter just makes it look as so. Can you hear the recording?

84
00:10:57,670 --> 00:11:00,730
So. So he was all here for this.

85
00:11:02,510 --> 00:11:09,830
Hmm. Since the population fixed.

86
00:11:24,310 --> 00:11:34,760
And the number of. And one aside, the treatment is based.

87
00:11:50,970 --> 00:11:55,470
Right, this joint distribution of these two variants.

88
00:11:56,980 --> 00:12:13,650
And so the probability that they're equal to one can rewrite that as a conditional probability kind of margin.

89
00:12:18,480 --> 00:12:23,150
So. All right, so what's that probably going to be here?

90
00:12:25,150 --> 00:12:34,870
Probably. And then one over in.

91
00:12:38,700 --> 00:12:47,860
But what about this problem? And condition.

92
00:12:47,980 --> 00:12:55,840
In fact, I've already got one observation system. Yeah.

93
00:12:56,290 --> 00:13:00,750
Right, right. So.

94
00:13:02,470 --> 00:13:07,000
Right. So I've already taken one out so you can see it or not. We have this kind of conditioning.

95
00:13:08,080 --> 00:13:12,650
So in my class. So I played the whole thing.

96
00:13:27,480 --> 00:13:37,880
Right. So the fact that both zero. And not last one over ten nights when the novel ran and.

97
00:13:53,650 --> 00:13:58,000
It doesn't matter which one is zero, which was one very different values.

98
00:14:12,300 --> 00:14:15,690
I just don't have the energy minus one denominator.

99
00:14:15,690 --> 00:14:20,850
But if I don't take away one, I have a full satellite system.

100
00:14:23,350 --> 00:14:33,800
So. That gives us our second value here.

101
00:14:37,690 --> 00:14:43,350
So what's going to replace these probabilities with the things?

102
00:14:43,360 --> 00:14:48,730
I just arrived there. And there's a messy bit of algebra.

103
00:14:50,180 --> 00:14:53,230
You kind of look at that in your own time.

104
00:14:53,680 --> 00:15:02,830
Well, down to this piece here. So. So we've got essentially, we've got all the pieces we need to compute the variance.

105
00:15:10,670 --> 00:15:20,090
So if we go back to our original expression, maybe we just rewrite one bar and maybe zero bar no involving this these terms in our fixed monitors.

106
00:15:21,380 --> 00:15:28,700
So I'm going to replace remember, I have this transformation.

107
00:15:28,960 --> 00:15:34,970
Indeed. So it is going to be equal to die plus and one over n.

108
00:15:37,010 --> 00:15:43,399
So that gives me that piece there. And similarly, one -80 is going to be in not over.

109
00:15:43,400 --> 00:15:52,390
And so minus VII is one minus and one over in is a not a minus the eyepiece.

110
00:15:54,020 --> 00:16:10,040
So, okay, so I did a bit of bit of factorization here my N1 over and cancel and around one piece here.

111
00:16:11,330 --> 00:16:14,710
And so that assumes y one. Same thing here.

112
00:16:14,760 --> 00:16:19,610
There's ay0. And then I bring together the dice.

113
00:16:20,930 --> 00:16:31,940
So I have a D times in around 1y1 minus die or actually plus minus a minus.

114
00:16:31,940 --> 00:16:35,450
So plus the i and over in 0.0.

115
00:16:38,490 --> 00:17:09,540
So. So then to get this last piece is a little more work.

116
00:17:09,540 --> 00:17:15,270
You can see right away this is a constant, right? So the variance of a concern is just going to be zero.

117
00:17:16,110 --> 00:17:19,920
So the only piece I have left with, if we have this over here, is taking the variance of this.

118
00:17:21,110 --> 00:17:24,129
So whenever it ends, a constant.

119
00:17:24,130 --> 00:17:31,770
So the variance, the constant times, a random variable is just going to be the constant squared times the variance of this piece here.

120
00:17:32,850 --> 00:18:03,630
So. Shelf.

121
00:18:22,730 --> 00:18:30,490
So. But I think that just the expected value of his that he's left in there.

122
00:18:47,770 --> 00:19:00,120
Let me just say this, but say. And this whole thing here.

123
00:19:03,640 --> 00:19:06,930
There's a constant. Right.

124
00:19:07,000 --> 00:19:14,120
So I can write out this whole thing.

125
00:19:21,360 --> 00:19:35,060
Subscribe. That's right.

126
00:19:35,150 --> 00:19:41,180
So that's just expected that this is just zero. So this whole thing is zero.

127
00:19:42,440 --> 00:19:50,310
So then. I'm just left with.

128
00:19:52,660 --> 00:19:57,760
The variance being the expected value of this quantity squared.

129
00:19:57,760 --> 00:20:01,930
And I think there's a little error here. I think this squared part should be on the outside piece.

130
00:20:31,850 --> 00:20:37,760
Okay. Questions? No, we're a little in the weeds here, but we're going to we're going to pop back up in a second.

131
00:20:45,040 --> 00:20:50,620
Okay. So I'm not going to go through this if we spend the rest of the day doing this.

132
00:20:50,630 --> 00:20:59,050
But so once we have this part here, we can again, you can go see we'll give you the reference.

133
00:20:59,050 --> 00:21:00,730
You can go to there if you really want to get into it.

134
00:21:02,770 --> 00:21:12,850
We basically able to kind of get this back into these quantities, but now it's starting to look like the S1 and it's not pieces that we wanted.

135
00:21:15,230 --> 00:21:29,490
So. So remember one over in times when we're in times that.

136
00:21:34,550 --> 00:21:38,660
Gives. It's one square and it's not square. So I'm left with.

137
00:21:40,460 --> 00:21:44,240
I'm sorry. So I want to write a minus one. Minus one can say.

138
00:21:44,720 --> 00:21:51,740
So this is the variance definition of variance of this one squared and the best finish the variance of it's not squared.

139
00:21:52,700 --> 00:21:56,090
So I'm left with n not over n time zone one.

140
00:21:56,220 --> 00:22:03,170
I'm just one squared and one over in the and not it's not squared.

141
00:22:03,770 --> 00:22:07,860
And in this piece here. It's okay.

142
00:22:07,870 --> 00:22:18,350
I'm not. So we go back and look at our definition of this, this one zero squared piece here.

143
00:22:19,970 --> 00:22:24,110
Right. We can expand the quadratic term.

144
00:22:25,190 --> 00:22:36,470
Right. So we have a piece that now becomes as one square piece becomes it's not squared in this little kind of coverage piece as well.

145
00:22:37,460 --> 00:22:47,240
So we can rewrite this last part here as this one plus s, not minus this one squared.

146
00:22:47,480 --> 00:23:03,970
Right. So bringing everything together.

147
00:23:09,410 --> 00:23:18,620
Right. Replacing this part with one over end times s one squared plus s squared minus s ones.

148
00:23:18,980 --> 00:23:22,910
That's one zero squared and finish up the older we have this part.

149
00:23:22,910 --> 00:23:38,270
Here is what we want to show. So furthermore.

150
00:23:39,800 --> 00:23:41,960
Right. I mean, this is also within in the population.

151
00:23:42,770 --> 00:23:48,830
But we don't this is this unblinded, if you will, for for the observed potential outcome population.

152
00:23:50,150 --> 00:23:53,330
So we have to estimate it from what we see based on treatment assignment.

153
00:23:54,290 --> 00:24:07,420
But assuming we have random treatment assignment, then an unbiased estimate of this variance of the of the potential outcomes of treatment,

154
00:24:07,820 --> 00:24:17,270
they're just going to be the variance one hundreds of the subjects assigned to the treatment.

155
00:24:17,900 --> 00:24:22,220
And similarly, on the bias estimate of the variance in the potential outcomes of control,

156
00:24:22,850 --> 00:24:27,290
it's going to be the sample variance of the subjects assigned to control.

157
00:24:29,720 --> 00:24:39,020
But we cannot estimate S01 squared because that would violate this fundamental problem of causal inference.

158
00:24:39,250 --> 00:24:44,680
Right. Definition.

159
00:24:49,230 --> 00:24:52,780
Right. We need to get this value.

160
00:24:54,550 --> 00:24:57,550
And unless I'm giving you a toy, a human problem, you don't have that.

161
00:25:04,680 --> 00:25:11,890
I. The only thing we really say for certain is it's going to be positive.

162
00:25:13,510 --> 00:25:16,390
So you're subtracting off something that's positive.

163
00:25:17,500 --> 00:25:38,260
So that means if you just take and add together the variances of the subjects under control and subjects under treatment,

164
00:25:38,830 --> 00:25:42,920
the mean of those quantities, then you're going to get a conservative estimate, right?

165
00:25:43,060 --> 00:25:47,200
This is going to be bigger than the true variance of how happy.

166
00:25:49,180 --> 00:25:53,500
Now there is a situation in which it's unbiased.

167
00:25:54,790 --> 00:26:01,000
Right. So if essentially, if there's no variability in the treatment effect across individuals.

168
00:26:01,840 --> 00:26:14,560
Right, this is the same for all. Then the variance would be picked up by that is one zero quantity, there'd be zero right there, no variance.

169
00:26:15,850 --> 00:26:20,230
So it's sometimes called a sharp null hypothesis and it's just like a shift.

170
00:26:23,160 --> 00:26:31,050
So not realistic. Probably most settings actually in settings where Y is dichotomous.

171
00:26:32,380 --> 00:26:41,500
But the only way it could be true is if there's one. I remember earlier a discussion of the dichotomous variable.

172
00:26:43,060 --> 00:26:46,660
What would the sharp null hypothesis be true for a dichotomous outcome?

173
00:26:53,040 --> 00:26:57,240
What would it look like for him? What can we say about the treatment effect?

174
00:26:57,660 --> 00:27:17,770
If the sharp but this is true for a dichotomous outcome. Everyone has true experiences and it's okay.

175
00:27:19,900 --> 00:27:28,780
Yeah. If you're on the right track. See, I didn't draw either and have to redraw the picture.

176
00:27:30,550 --> 00:27:35,710
So. So if this is going to be who?

177
00:27:35,980 --> 00:27:51,690
What are the subjects for which. Well, if if we have a dichotomous outcome and this is always going to be true, how can that occur?

178
00:27:51,870 --> 00:27:59,100
How could the for every observation that the treatment effect is the same because it's either zero or one?

179
00:28:00,600 --> 00:28:03,720
And I think it's a good question, actually, some people think.

180
00:28:11,190 --> 00:28:17,170
I'm not sure what kind of following but maybe like the for everyone assigned one

181
00:28:17,190 --> 00:28:23,550
they would all have this exact same right and everyone is nine zero all V0.

182
00:28:24,180 --> 00:28:30,120
Well you're looking at the difference though, right? So if some if there's a.

183
00:28:37,190 --> 00:28:41,450
So remember this test, this this towelhead pee has to be the same for all observations.

184
00:28:45,470 --> 00:28:52,160
There's only one situation that can occur. There's no difference between the treatment.

185
00:28:52,850 --> 00:28:56,270
Precisely. There's no treatment effect. Right.

186
00:28:56,750 --> 00:28:59,840
It's either has to be equal to one or equal to zero.

187
00:29:00,170 --> 00:29:03,659
We had the we said they only to jointly with dichotomous outcomes only matters.

188
00:29:03,660 --> 00:29:06,890
This is when there's a flip from 0 to 1 2120.

189
00:29:07,550 --> 00:29:16,700
So if some subjects if this is equal to one for some subjects and minus one for others or one in zero or one.

190
00:29:17,720 --> 00:29:24,320
So, so you see they're going to be one -0, zero minus one, one minus one or zero, -0.

191
00:29:24,980 --> 00:29:28,250
So the only way it can be constant for all of them if is it's always zero.

192
00:29:29,180 --> 00:29:32,180
So. What generally then?

193
00:29:32,630 --> 00:29:40,400
The only way to sharpen or help us can occur in a non like degenerate setting is really when y is continuous,

194
00:29:40,970 --> 00:29:46,580
approximately continuous because then you can imagine you have some sort of distribution and again it gets shifted.

195
00:29:47,600 --> 00:29:54,050
Everybody kind of just bounces up or bounces down, but only when you have a kind of weight of sort of a continuous support for that.

196
00:29:55,550 --> 00:29:59,240
So. Right. Slightly to her.

197
00:30:01,220 --> 00:30:06,470
Hope one. Okay.

198
00:30:08,600 --> 00:30:15,130
Questions for me. Okay.

199
00:30:15,220 --> 00:30:22,900
So, right, so this first part of this class, we've kind of just pretending that our sample size enters our population of interest,

200
00:30:23,770 --> 00:30:27,250
but often ours are object of inferences as a super population.

201
00:30:27,550 --> 00:30:33,940
So you might think it was a typically a large if and large population from which we assume our population is drawn.

202
00:30:35,170 --> 00:30:39,639
I think it was a Rubin kind of implement this idea, an arbitrarily larger finite population.

203
00:30:39,640 --> 00:30:49,299
They really like this finite population concept. So you could imagine we have a randomized trial of people that are now represent, you know,

204
00:30:49,300 --> 00:30:52,720
individuals in the United States or whatever country the trial been running or whatever.

205
00:30:52,900 --> 00:30:59,650
So still finite but sort of arbitrarily large. But I'm just going to sort of focus on the infinity part here.

206
00:31:00,700 --> 00:31:11,170
So we now we now have to think about means and variances of the wise as well as the A's.

207
00:31:12,250 --> 00:31:24,190
Right? So we can think about the expected value of potential outcomes under treatment as one potential mean of

208
00:31:24,190 --> 00:31:31,240
potential outcomes under control as MI zero variance under treatment to one squared one sigma squared zero.

209
00:31:32,380 --> 00:31:43,720
So the mean then of the differences or our super population case right is going to be me one minus me zero or tell.

210
00:31:44,830 --> 00:31:50,740
Right. We dropped the subscript p here because it's not population, this sort of super population idea.

211
00:31:53,080 --> 00:32:01,570
And then the variance of these differences, particular outcomes, again not estimable is sigma zero one.

212
00:32:06,570 --> 00:32:10,209
So if we consider expectation and variance with respect to both the sampling distribution

213
00:32:10,210 --> 00:32:15,390
to be in respect of the sampling variance and distribution of potential outcomes of Y,

214
00:32:16,710 --> 00:32:19,650
right, then we have our usual rules.

215
00:32:21,000 --> 00:32:30,180
We now want to think about conditioning on why we can still think about this assignment of a to this fixed sample of observations.

216
00:32:30,180 --> 00:32:40,680
Why? And then we then if we have that expectation, we then take expectation with respect to Y across the population,

217
00:32:42,390 --> 00:32:47,910
across the super population and for variance we were expectation the variances marginal variance

218
00:32:47,910 --> 00:32:52,020
is the expectation of the conditional variance plus the variance of the conditional expectation.

219
00:32:58,280 --> 00:32:58,760
So.

220
00:33:09,580 --> 00:33:18,640
So now if I have this, I'm still going to call this tower happy because I have this little my sample and pop in my sample now to be the population.

221
00:33:18,730 --> 00:33:21,940
Or you could think of it as the population that's now going to the super population.

222
00:33:22,660 --> 00:33:27,459
So I still have my tail happy, which is just the preferences around what that is.

223
00:33:27,460 --> 00:33:30,820
It's going to be state that again.

224
00:33:35,350 --> 00:33:43,450
You kind of get rid of down here. But it's the difference in means from the what.

225
00:33:45,650 --> 00:33:51,470
Treatment minus the mean under control.

226
00:33:51,530 --> 00:33:57,460
Thank you. So, y one bar minus y zero more.

227
00:33:58,370 --> 00:34:03,230
So work that out. So my.

228
00:34:04,040 --> 00:34:07,699
Because I've already worked out this part. Right.

229
00:34:07,700 --> 00:34:14,510
I know that's going to be this. So I just take expectation away.

230
00:34:16,130 --> 00:34:20,270
So again, linear thing constant.

231
00:34:20,720 --> 00:34:24,620
So we one new zero. These are just constants.

232
00:34:24,620 --> 00:34:27,820
So the mean of the constant is a constant. So one minus me zero.

233
00:34:27,830 --> 00:34:30,990
But how do I know?

234
00:34:31,010 --> 00:34:35,540
Implicit in this is that our sample is a simple random sample from the super population.

235
00:34:36,230 --> 00:34:41,840
So these things are kind of i-D. We're going to assume this throughout the course implicitly, explicitly note.

236
00:34:41,840 --> 00:34:46,549
Otherwise, you may occasionally have it explicitly noted.

237
00:34:46,550 --> 00:34:52,610
Otherwise it's equal. But but to keep things simple, that's what we're going to say.

238
00:34:54,470 --> 00:35:05,150
So basically same thing are usual estimate of our causal effect from the observed data when biased for the population.

239
00:35:05,150 --> 00:35:11,000
B And as I have the sample of sort of the sort of population means some of the random

240
00:35:11,000 --> 00:35:18,440
sample variance for population since I've already worked out that it's this quantity here.

241
00:35:18,980 --> 00:35:24,860
Right. So again, the take expectation with respect to y of that and then of course I need the

242
00:35:24,860 --> 00:35:34,400
variance with respect to variance with respect to y of the the mean of how happy.

243
00:35:35,740 --> 00:35:43,720
Good morning. All right. So I already know the expectation at this point, and I can just.

244
00:35:46,250 --> 00:35:57,110
Use the usual thing of just estimating these with an unbiased estimate is I mean, these are unbiased estimates of these super population quantities.

245
00:35:59,240 --> 00:36:02,930
All right. So for this, we're going to do a bit more work, but not too much.

246
00:36:04,010 --> 00:36:15,470
So now I'm in a happy situation where I have independent observations and assuming that separate sample from our super population.

247
00:36:16,160 --> 00:36:24,960
So the covariance of these terms here is going to be zero. So we really have to deal with is the variance of this part here.

248
00:36:26,700 --> 00:36:33,940
So. I did hear.

249
00:36:37,860 --> 00:36:45,950
All right. So basically. We know this single one zero squared.

250
00:36:47,600 --> 00:36:52,159
So I have entered them. So I have ten divided by ten squared.

251
00:36:52,160 --> 00:36:58,670
That's one over ten. But I also have one over in consequence squared.

252
00:36:59,030 --> 00:37:03,349
I'm sorry. Simply one zero squared here but with a negative term.

253
00:37:03,350 --> 00:37:10,190
Right. So around this cancel. So the standard variance estimate holds for super population difference.

254
00:37:11,390 --> 00:37:16,100
Right. It'll hold approximately in find population difference at the sample fraction small.

255
00:37:19,290 --> 00:37:27,660
So. So it was good luck for a jump on.

256
00:37:31,580 --> 00:37:39,350
So basically the next stuff's going to be pretty straightforward. We can construct confidence intervals the usual way.

257
00:37:39,380 --> 00:37:45,530
Now we've got, you know, our means and variance estimate. And we usually just use a normal approximation.

258
00:37:47,360 --> 00:37:51,470
So a two sided test can be just given by the Z statistic.

259
00:37:52,430 --> 00:38:00,200
Right. This estimator of two divided by its estimated variance.

260
00:38:00,700 --> 00:38:06,290
Oh, yeah. And so I get a confidence interval.

261
00:38:06,920 --> 00:38:17,989
Oh, right. You see, in the usual variance estimate, a screw to the variance estimate or standard error of happy plus or minus whatever is east.

262
00:38:17,990 --> 00:38:27,500
The district is here too. So. So you might say 1.96 for a 95% confidence interval.

263
00:38:29,260 --> 00:38:33,700
So. Questions there.

264
00:38:33,970 --> 00:38:37,540
It's pretty straightforward. Okay.

265
00:38:39,220 --> 00:38:46,060
So all right. So we'll finish up today who's getting into this discussion, you know, finishing.

266
00:38:46,090 --> 00:38:49,780
But I'm talking about model based inference from randomized experiment.

267
00:38:49,780 --> 00:38:55,900
So we're a new model here, right? We just we assume an expectation of variance for why it's true.

268
00:38:56,500 --> 00:39:03,040
So if you have a really obtuse Y that doesn't have a variance, you can't construct confidence intervals,

269
00:39:03,040 --> 00:39:09,750
but that's just the nature of the game so if you're outside Koshy world that so.

270
00:39:10,950 --> 00:39:22,530
That's probably not going to be a problem. So but there are times we might want to consider model based inference.

271
00:39:23,400 --> 00:39:31,550
So one is if we want to sort of think about functions of potential outcomes beyond the sort of difference in the means, like the quantile.

272
00:39:34,230 --> 00:39:40,460
We also might want to do things like condition on treatment assignments. So effective treatment on the treated, right?

273
00:39:40,470 --> 00:39:47,640
So if we just restrict ourselves to the treated up, we have a problem. We don't have any controls so we can't compute y0 in the treated.

274
00:39:49,170 --> 00:39:54,240
We might want a conditional covariance so there could be concerns about effect modification.

275
00:39:54,760 --> 00:39:59,190
Right. So we might think that treatment works differently. Different subsets of the population.

276
00:40:02,580 --> 00:40:12,110
So this may also be, I think, importantly sort of moving to observational studies in sessions settings or so.

277
00:40:15,590 --> 00:40:19,290
So the estimates of interest here are going to be the particular outcomes or functions thereof,

278
00:40:19,440 --> 00:40:23,670
let's say in a finite population where functions of model parameters,

279
00:40:24,300 --> 00:40:29,220
the difference in needs, not necessarily the sort of components of the model parameters themselves.

280
00:40:33,780 --> 00:40:39,300
And so I'm going to discuss this in the missing data paradigm.

281
00:40:40,350 --> 00:40:45,550
So this is where. You know, I don't want to put people on the spot.

282
00:40:46,990 --> 00:40:54,069
I'll just say, if you haven't seen any if you haven't taken 682 or something, you know, sort of broadly,

283
00:40:54,070 --> 00:40:57,580
roughly equivalent class as either undergrad or grad student,

284
00:40:58,480 --> 00:41:04,450
you're probably going to want to dove in and take a look at the notes that I have right here.

285
00:41:10,160 --> 00:41:14,900
So this module, this base review.

286
00:41:15,890 --> 00:41:22,980
So taking this from my own thought classes and based before, so it's this kind of a summary.

287
00:41:23,000 --> 00:41:30,080
I mean, I know it's 82 pages, but it will be it will be helpful to kind of at least at least skim this,

288
00:41:34,220 --> 00:41:38,120
because in this missing data paradigm, we're going to build that in a Bayesian framework.

289
00:41:39,110 --> 00:41:48,049
So the the idea here is that okay, so I guess just to point out, right, if this this potential outcome, right.

290
00:41:48,050 --> 00:41:53,720
It's observed if you're assigned treatment, but it's missing if you're assigned to control and then vice versa.

291
00:41:53,730 --> 00:41:58,640
Right. To observe the potential outcome under control as observed before assigning control or missing in treatment.

292
00:41:59,300 --> 00:42:03,500
So the idea here in this basic framework is to get posterior predictive draws of

293
00:42:03,500 --> 00:42:09,140
the missing observations and the potential outcomes to combine with the observed.

294
00:42:09,920 --> 00:42:16,820
Then we have we're back at least within this single invitation or draw.

295
00:42:17,180 --> 00:42:19,700
We're actually in the situation. We are in the homework, right?

296
00:42:19,700 --> 00:42:28,370
We actually can have our full potential outcomes and then we can just compute one from that.

297
00:42:29,660 --> 00:42:32,930
So now you do that over and over.

298
00:42:33,980 --> 00:42:39,350
You can then take the resulting sort of means and variances of those quantities you

299
00:42:39,350 --> 00:42:46,040
generate to get the posterior inference of your results ultimately generally delivered.

300
00:42:46,040 --> 00:42:50,230
You're interested. Okay.

301
00:42:50,300 --> 00:42:59,780
So this is a little messy. You know, if you haven't sort of hung out in bars world too long, maybe a little dense.

302
00:43:00,500 --> 00:43:09,950
But basically the idea here is that we want to think about generating the distributions of the missing values conditional on the observed values,

303
00:43:10,430 --> 00:43:13,760
whatever treatments things have been assigned to in possible covariance.

304
00:43:15,200 --> 00:43:19,610
So basically then.

305
00:43:21,510 --> 00:43:38,159
With this with this derivation, we can think about obtaining draws from this piece up here and plugging it in to our, our general distribution.

306
00:43:38,160 --> 00:43:45,420
Right? So and then using this to get the inputs about this, this complete data conditional we've observed.

307
00:43:47,130 --> 00:43:56,070
So it's, it's kind of ugly, but it's kind of easy to implement if we use Monte Carlo, Monte Carlo methods here and empty methods.

308
00:43:57,000 --> 00:44:07,170
So basically what happens is we have some kind of simple initial simple model to kick off computations.

309
00:44:07,320 --> 00:44:17,060
So you're sort of zero with imputation. It might be just the mean values and the unobserved form is just you give them by the mean of of the.

310
00:44:18,030 --> 00:44:21,149
So for the subjects that are assigned to treatment,

311
00:44:21,150 --> 00:44:28,860
we might assume that their unobserved controls are just equal to the mean of the control values that we do observe and vice versa.

312
00:44:30,420 --> 00:44:36,060
But there are treatments among controls, so we that gives us some sort of initial kick off for this.

313
00:44:37,230 --> 00:44:41,430
We then take our model for a joint distribution.

314
00:44:45,190 --> 00:44:50,830
So we have some sort of underlying likelihood from this complete data that includes all the natural outcomes.

315
00:44:51,580 --> 00:45:00,520
We get a draw of the parameters for this model. And then given that broad parameters, we impute the missing data.

316
00:45:01,780 --> 00:45:10,180
Now this time, using our model piece here, we get again the draw of this for joint distribution.

317
00:45:11,020 --> 00:45:22,780
We then jump back to one. We draw the model parameters, we impute back and forth until you have a sufficient number of draws demarcated by B,

318
00:45:23,500 --> 00:45:27,100
there's obviously correlation that's going to be induced here because, you know,

319
00:45:27,100 --> 00:45:30,970
whatever draw you make here, it's going to affect your next estimate theta.

320
00:45:31,600 --> 00:45:37,630
So sometimes you might want to space these out, but from that you could then make inference.

321
00:45:38,950 --> 00:45:43,779
So let me let me try to get this boiled down to a right.

322
00:45:43,780 --> 00:45:51,820
And so the information right, we would think about either estimate or in a finite population or a super population.

323
00:45:52,570 --> 00:45:56,379
So we could take the difference again, generating these imputations.

324
00:45:56,380 --> 00:46:06,490
So you could actually compute the the mean of the potential outcomes under treatment, the mean of potential outcomes that are control.

325
00:46:06,850 --> 00:46:14,890
We're giving one of these draws, take that difference and then take the average of those over the B draws from the variance.

326
00:46:18,730 --> 00:46:24,120
Can they be missing? No. That's why I would just take my time.

327
00:46:26,610 --> 00:46:36,420
My overall misdemeanor I got from that and plug that in to compute the variance of those differences across the simulations.

328
00:46:36,430 --> 00:46:43,499
You know, it'd be give me a variance estimate and then if I was working the super population, I wouldn't bother with the draws.

329
00:46:43,500 --> 00:46:53,790
I'd focus on the rolling parameters. So essentially the difference in leans is the variance of this.

330
00:46:56,680 --> 00:47:02,050
So all this is this CMC world. You haven't seen it all before.

331
00:47:02,980 --> 00:47:09,250
Take a little read. The basic idea is, is not to intuition data with computation.

332
00:47:09,790 --> 00:47:17,350
You're doing these these draws of the imputed values of the population with the data,

333
00:47:17,620 --> 00:47:23,680
the data you're doing, draws of the unobserved elements as well as the parameters.

334
00:47:24,070 --> 00:47:28,930
And then you can sort of pick and choose among those how we compute your target of inference.

335
00:47:35,200 --> 00:47:49,399
So. The students. So I guess the last point here is that you think construct the confidence intervals just using the empirical distribution

336
00:47:49,400 --> 00:48:01,280
of these draws where you could just use a normal approximation plus or -1.6 times the square root of variance literals.

337
00:48:04,610 --> 00:48:11,450
Okay. So I think we have time to really start here kind of work through a simple example.

338
00:48:15,350 --> 00:48:24,950
So, so there are issues that arise involving the benefit correlation between my one of my zero great fits mission therapy.

339
00:48:27,130 --> 00:48:35,240
We see that. I mean, I guess I'm why would I expect this to be the case? We just talked about 15 minutes ago.

340
00:48:44,470 --> 00:48:50,310
I see one person who wants to answer emails. She says. All right.

341
00:48:50,530 --> 00:48:53,730
Oh, this. That's right.

342
00:48:53,760 --> 00:48:59,530
I mean, it's this. This issue here.

343
00:48:59,620 --> 00:49:03,310
Right. You can't do this to me.

344
00:49:06,010 --> 00:49:09,950
Observe data. So.

345
00:49:17,160 --> 00:49:23,160
So um, so have them taken I think is actually real data.

346
00:49:24,270 --> 00:49:26,069
Just six observations. I always use it.

347
00:49:26,070 --> 00:49:37,320
For example, it's a randomized trial that enrolled low income workers and job training control got sort of the usual care,

348
00:49:39,390 --> 00:49:49,660
and then there was sort of an extra specific job training for the folks in treatment, and the outcome was income and posturepedic.

349
00:49:51,240 --> 00:49:55,260
So it's an older study you can see by these incomes, which I think are $10,000.

350
00:49:58,440 --> 00:50:10,410
So so we have three subjects assigned to treatment that had incomes anywhere from 3.6 to $24.9 thousand,

351
00:50:11,250 --> 00:50:16,770
and then subjects assigned to control, two of which had essentially zero income and another one had about $25,000.

352
00:50:17,910 --> 00:50:21,600
So. Right.

353
00:50:21,610 --> 00:50:32,340
So we could immediately compute sort of an estimate of the effect between Y one minus Y zero and then the computer variance.

354
00:50:34,230 --> 00:50:39,090
But we don't know, of course, is the sort of off sort of S0 one piece.

355
00:50:40,160 --> 00:50:48,030
So but we can do like a sensitivity type analysis where we actually fixed row.

356
00:50:49,110 --> 00:50:51,540
We assume there's some sort of underlying correlation there,

357
00:50:51,540 --> 00:50:57,870
but we're going to say it's equal to this and you could have several values and it becomes sort of a sensitivity analysis, the impact of that.

358
00:50:59,670 --> 00:51:04,740
So I'm going to treat just to simplify is here and we treat the variances known.

359
00:51:06,300 --> 00:51:11,640
And I'm going to use a simple non informative prior review,

360
00:51:11,670 --> 00:51:18,840
which is basically just going to be of course one so often since means a well estimated from the data.

361
00:51:18,900 --> 00:51:23,260
We typically don't need to use a lot of prior information to estimate these

362
00:51:23,260 --> 00:51:27,440
so we can actually completely ignore information just sort of be spread out.

363
00:51:27,480 --> 00:51:31,890
So it's called a the nonprofit Priors. It's not really a distribution.

364
00:51:32,340 --> 00:51:39,840
It's Infinity Times one. The area where you could use a normal prior, but typically one that might be very widely distributed.

365
00:51:39,840 --> 00:51:43,170
And it's going to keep that simple because the computations are simple.

366
00:51:45,120 --> 00:51:50,190
So so for some some basic kind of Bayesian results.

367
00:51:53,220 --> 00:52:01,230
So it turns out the posterior of the means is basically normally distributed with means equal to the observed means,

368
00:52:02,790 --> 00:52:13,290
and then variance is given by this one squared over n for.

369
00:52:20,900 --> 00:52:24,230
Treating this as no. So.

370
00:52:26,060 --> 00:52:36,830
And then if we assume that road zero under this model, then we get draws of the missing data.

371
00:52:37,820 --> 00:52:48,799
When we distributed a new zero, that's one. And then you zero as zero, you plug those in to get our full values.

372
00:52:48,800 --> 00:52:56,000
And I remember again, this y bar parentheses ones is not the mean in the observed potential outcomes.

373
00:52:56,000 --> 00:53:03,230
I mean, there's not even observed treatments. This is the mean across all of the potential outcomes under treatment,

374
00:53:03,710 --> 00:53:10,280
so including the controls, but then plugging in their imputed values from this process.

375
00:53:12,710 --> 00:53:15,440
So here's a little bit of our code.

376
00:53:15,440 --> 00:53:26,990
I think there's also on your it's on the A text file, not a fancy R program or as you can see from this very simple, but it gets the job done.

377
00:53:28,550 --> 00:53:35,690
So we have a total of six observations, three assigned to control, three assigned to treatment.

378
00:53:35,690 --> 00:53:42,020
These are the observed under treatment observer control. So these are the observed means.

379
00:53:45,390 --> 00:53:48,960
So I'm going to do capital B equal to 10,000 here.

380
00:53:49,920 --> 00:54:02,880
So just setting aside a place to hold both the population mean and a super population mean differences.

381
00:54:04,080 --> 00:54:08,010
So I'm going to kick things off. As I said, you can do something simple.

382
00:54:08,020 --> 00:54:17,730
So I just took the mean of from the observed values and plugged those into the to the missing ones of the civilian you to control.

383
00:54:18,540 --> 00:54:23,880
And I'm assuming the first three subjects in six here are assigned to control and then assign treatment.

384
00:54:24,450 --> 00:54:30,870
And then similarly for the subjects assigned to to the treatment I plugged in the mean of their control values.

385
00:54:32,460 --> 00:54:39,450
So all right. Now so this is the full form, the whole thing here, right?

386
00:54:39,450 --> 00:54:41,310
This is the whole vector of all of the.

387
00:54:45,150 --> 00:54:51,660
All of the potential outcomes of treatment, potential outcomes under control, treated elsewhere and it's not as has fixed known.

388
00:54:52,320 --> 00:54:58,170
So I just do these draws here to get me one in zero.

389
00:55:00,100 --> 00:55:05,080
Right. And then now I can have some uncertainty here.

390
00:55:05,110 --> 00:55:14,800
Right. I do. Three draws center, two view one and then three doors, and it can be zero with the appropriate variances.

391
00:55:15,790 --> 00:55:22,150
So from that, I can get the difference of means, either for the parameters or for the population.

392
00:55:23,380 --> 00:55:26,380
And you can kind of see.

393
00:55:29,420 --> 00:55:39,440
The difference is so. So the beans have pretty much the same posterior means that pretty much the same values.

394
00:55:40,160 --> 00:55:43,520
The confidence intervals are a little different.

395
00:55:44,300 --> 00:55:52,070
So we'll we'll we'll stop here. But just maybe one last question can even sort of say, well,

396
00:55:52,310 --> 00:56:00,500
clearly these are confidence intervals for these are wider than for these and this is for the super population versus the population.

397
00:56:00,590 --> 00:56:06,110
Anyone want to comment on why that difference is there?

398
00:56:09,240 --> 00:56:13,879
Yes. In super population.

399
00:56:13,880 --> 00:56:24,050
Population. Think about what we're making a difference to where the uncertainty is.

400
00:56:28,320 --> 00:56:44,850
When you escape. I.

401
00:56:49,740 --> 00:56:54,540
All right. Has to do differently. So this interval clearly is wider in this interval.

402
00:56:55,830 --> 00:57:00,060
So there's more uncertainty in a super population. Comment on why that is.

403
00:57:02,720 --> 00:57:06,340
Probably try to generalize to a lot more. Right. Right.

404
00:57:06,350 --> 00:57:10,190
We only have essentially we have our fully observed.

405
00:57:10,430 --> 00:57:15,860
There's no there's no uncertainty about the values.

406
00:57:17,000 --> 00:57:21,260
The only uncertainty in the in the population is for the unobserved elements.

407
00:57:22,070 --> 00:57:26,590
There's no rest of the population to make inference about, you know, the rest of the population.

408
00:57:26,600 --> 00:57:30,700
There's uncertainty essentially in the distribution of the whys that you need to account for.

409
00:57:31,500 --> 00:57:35,600
And so that's really what makes it makes the difference here, sort of the little hidden,

410
00:57:37,190 --> 00:57:40,310
I guess, you know, a little hidden things about populations versus super populations.

411
00:57:40,700 --> 00:57:51,860
Bonus part of the base. Okay, so I know we're running past time, so we will we'll wrap it up and finish up and get on to propensity scores.

412
00:57:52,190 --> 00:58:05,080
Your class Monday Labor Day and I'll be up for office hours tomorrow at 230 in the morning.

413
00:58:05,100 --> 00:58:10,350
This is maybe just a handful to do.

414
00:58:13,100 --> 00:58:14,460
To continue throughout the year.

