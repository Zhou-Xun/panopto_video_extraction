1
00:00:09,120 --> 00:00:15,590
And this. So we'll have.

2
00:00:24,050 --> 00:01:09,520
10 p.m. Let's get started.

3
00:01:10,210 --> 00:01:17,920
I have your homework. One here. Originally, I wanted to prepare a solution, but you did almost perfectly.

4
00:01:17,930 --> 00:01:21,790
So why? I needed my solution. All so.

5
00:01:22,190 --> 00:01:31,720
So in a minor point, I just wrote directly on your homework, so I don't think there's a need for actually my solution.

6
00:01:32,140 --> 00:01:36,400
It's. It's it's very, very well done.

7
00:01:36,470 --> 00:01:43,300
Okay, so I will return that to you at the end of the lecture today.

8
00:01:45,040 --> 00:01:53,559
Okay. So there are some question about a data set of the second homework.

9
00:01:53,560 --> 00:01:57,240
And so there are two data files.

10
00:01:57,250 --> 00:02:02,170
Why is the mortality data and not or what is the vaccination data?

11
00:02:02,560 --> 00:02:07,410
The problem here is that there may be a little bit misalignment of the age groups.

12
00:02:07,420 --> 00:02:18,940
Is that right? So. So I don't know how we're going to come up with some way that we can sort of find a right sort of cut up of the age groups.

13
00:02:19,510 --> 00:02:31,030
So an idea I have for that is if we assume that a group of 15 to 24 that nets are uniformly distributed per age groups that we could just

14
00:02:31,030 --> 00:02:39,790
like divide it by two and put one into the zero half of those that's in the zero 18 group and half of them in the 18 to 65 possibly that.

15
00:02:42,410 --> 00:02:48,240
Right. Because usually we have the. But.

16
00:02:50,280 --> 00:02:54,240
Just looking at the older older age groups if.

17
00:02:58,570 --> 00:03:05,970
So the way you did is just do it. For me, uniform distribution and cutting have.

18
00:03:08,460 --> 00:03:15,280
The. Or even look at weekly.

19
00:03:25,610 --> 00:03:31,250
Oh. Well, data capture, of course, we have six t.

20
00:03:32,720 --> 00:03:36,790
The. Reading your essay.

21
00:03:37,240 --> 00:03:44,860
First of all, I first question that I come up with some of this idea is that how we can collectively collect interesting data?

22
00:03:45,160 --> 00:03:50,330
Each team responsible for one data sort of capture one type of data captured.

23
00:03:50,330 --> 00:03:55,710
Then we can put it. I'm in place that we can shear and and so on, so forth.

24
00:03:56,610 --> 00:04:00,179
Oh. Sort of. See, I.

25
00:04:00,180 --> 00:04:07,480
Some of my consideration is. So one team, one kind.

26
00:04:09,140 --> 00:04:16,549
But they're saying we need to nationalize the sensor sensors.

27
00:04:16,550 --> 00:04:24,410
Also see that there is a 20, 20, very recent nationwide, a sense of start of collecting all of the data.

28
00:04:24,500 --> 00:04:29,990
But certainly we can get to this issue of distribution across different countries.

29
00:04:30,230 --> 00:04:35,120
That's the variable that I like to set our meeting to discuss with you.

30
00:04:35,120 --> 00:04:40,490
Some of these variables like the average income and.

31
00:04:42,660 --> 00:04:51,450
I mean, there are a lot of variables that we will see. A lot of variables will be related to the of the Kobe situation.

32
00:04:51,450 --> 00:04:55,889
We can capture a lot of data, maybe 5 to 10 variables every count.

33
00:04:55,890 --> 00:05:03,750
But anyway, the first group will look at that data if I personally or fine agree with that kind of work.

34
00:05:03,840 --> 00:05:08,780
So I hope that we create an interest data now for this course, maybe for automation researchers.

35
00:05:08,790 --> 00:05:11,880
Right. So we can share the database with other people.

36
00:05:11,970 --> 00:05:12,180
Okay.

37
00:05:12,600 --> 00:05:23,940
So so we're doing this one dataset looking at the nationwide census data and see what kind of variable at a county level, a variable that we can.

38
00:05:26,100 --> 00:05:31,260
Mortality or vaccination. Acceptance or vaccination.

39
00:05:34,820 --> 00:05:43,969
Happens or something like that that we can marvel about at some some of the sense related to comic book that the second group can

40
00:05:43,970 --> 00:05:58,940
go to this up to the culture where basically it is quite the urbanization because you know that the Kobe has much heavier heat.

41
00:05:59,420 --> 00:06:09,139
The people live in cities like metropolitan metropolitan areas where you have less open space, less open field.

42
00:06:09,140 --> 00:06:21,410
And so like they are in this type of this indicator function of greenness, how much green land in a given county like you feel.

43
00:06:21,410 --> 00:06:31,670
You can look at some of the variables that are related to openness and crowdedness and city population and see how that would be really a variable

44
00:06:31,670 --> 00:06:46,010
that we can we can use to define the possible sort of power of network or contraction because trans people are more likely to meet each other.

45
00:06:46,960 --> 00:06:50,420
And that that's certainly a fact that the.

46
00:06:54,830 --> 00:07:00,389
So. So we will set up a meeting and state you first to do some exploration.

47
00:07:00,390 --> 00:07:03,390
Now we can set out what variable we can pick up to form that.

48
00:07:03,840 --> 00:07:09,450
So our level thinking, of course, is try to get at least at the county level.

49
00:07:09,770 --> 00:07:14,099
Okay. Because there are we would do a lot of county level spatial through that analysis.

50
00:07:14,100 --> 00:07:18,360
And that resolution at the county level is ideal.

51
00:07:18,720 --> 00:07:30,370
Well, we'll see how how we can do that. And then someone mentioned about the possible study how the pollution will related to COVID.

52
00:07:30,960 --> 00:07:39,150
So the EPA database provide all the, you know, air pollution variables that you could go to, you know.

53
00:07:41,920 --> 00:07:47,230
Humidity and temperature and also the PM 2.5 PM ten.

54
00:07:47,560 --> 00:07:53,290
That can be really a download not downloadable from EPA database.

55
00:07:53,480 --> 00:07:58,140
Okay. That's something you. For the 13.

56
00:07:59,110 --> 00:08:06,820
And then we have the the one France's. Uh.

57
00:08:10,630 --> 00:08:19,460
A lot of investors of a comfort level that we can get a lot of those economy up.

58
00:08:33,690 --> 00:08:42,320
This is not a database that we can use to look at some variables that characterize the mobility of people,

59
00:08:42,330 --> 00:08:47,460
even where the airports are located, how many highways are connected.

60
00:08:48,010 --> 00:08:53,910
If people are living in a place with no highways, no airport, people tend to less likely to move.

61
00:08:54,480 --> 00:09:00,030
People have a lot of highways. That means people are more likely to travel from one place to another and so on.

62
00:09:00,060 --> 00:09:16,360
So the Bureau of. Oh.

63
00:09:18,030 --> 00:09:21,370
Okay. I see. Oh, okay. Sorry about the table.

64
00:09:21,470 --> 00:09:40,410
Oh, yeah. Okay. I just. Of this political point of views would a factor the vaccination acceptance or, you know, the hesitation, all of that.

65
00:09:42,500 --> 00:09:54,410
So I have this data like from this 2020 election data I already emailed and link to and then I work on that data for this.

66
00:09:54,420 --> 00:10:02,610
So I said that the election how to use the the pollsters right to to predict the date you know.

67
00:10:05,550 --> 00:10:14,250
So so we can look out the door the the split in Craddick and Repub and uncomfortable.

68
00:10:14,340 --> 00:10:17,790
So that's something that we can.

69
00:10:20,140 --> 00:10:23,980
All the as a surrogate of this sort of.

70
00:10:30,170 --> 00:10:35,200
We have this kind of data that you use.

71
00:10:35,280 --> 00:10:41,130
We talk to each team and put them together in one database.

72
00:10:41,140 --> 00:10:47,400
Then you can say, you know, maybe each team can go, give us your presentation,

73
00:10:47,440 --> 00:10:54,010
what the variables are available and they all are going to do the project.

74
00:10:54,480 --> 00:11:04,230
So after we get all the data ready in the next couple of weeks, then we will start a group project to, you know, do something.

75
00:11:04,770 --> 00:11:12,599
And then this data will be, you know, use also different to define a project of semester.

76
00:11:12,600 --> 00:11:20,360
And you can see that how you can continue to improve your project when you receive the feedback from you, from your project.

77
00:11:26,600 --> 00:11:33,160
I. Both the same projector.

78
00:11:33,540 --> 00:11:36,920
Yeah. So. I think that I mean, good projects.

79
00:11:37,520 --> 00:11:40,579
I think they do probably do a lot of consumer data analysis.

80
00:11:40,580 --> 00:11:42,860
Right. So so, you know,

81
00:11:43,040 --> 00:11:51,739
you now you get a data and you just have a rough idea what you want to do and probably do a sort of the first half of your project where you say,

82
00:11:51,740 --> 00:11:56,299
I want to explore the data and see how did they look like?

83
00:11:56,300 --> 00:12:00,230
I can answer some very, very basic questions there.

84
00:12:00,230 --> 00:12:07,820
You use it now to form or you know, that's part of basic of doing research.

85
00:12:07,820 --> 00:12:12,620
You can now really have a big finding just, you know, one wrong, right.

86
00:12:12,650 --> 00:12:24,440
You have to do more to run an investigation and try this finding where to find some very sensible way to deliver a solid project or some interest.

87
00:12:24,440 --> 00:12:30,500
And finding a good project will be mostly focused on consumer data analysis using a different,

88
00:12:31,010 --> 00:12:34,309
you know, the visualization tools are different, you know,

89
00:12:34,310 --> 00:12:39,890
simple analysis tools, you know, methods and try to understand the data,

90
00:12:40,400 --> 00:12:47,540
then come up with a conclusion that what are the big things I want to try it on.

91
00:12:47,720 --> 00:12:56,510
Tom I said that I hope that you you do want to do something good that you can submit your paper to for publication.

92
00:12:56,510 --> 00:13:02,030
And of course, you know if you want. Some findings are very interesting.

93
00:13:02,450 --> 00:13:10,190
I think when we have all the data together, all the some interesting things that you can this basic, basic discovery.

94
00:13:10,340 --> 00:13:13,370
Right. Discovered, I should say, far from the notebook.

95
00:13:14,650 --> 00:13:17,810
Isn't that so? The data that we're going to capture. The.

96
00:13:21,720 --> 00:14:07,320
One. So today I was turned out to start the topic of the sort of the statistics the.

97
00:14:09,450 --> 00:14:14,990
It's. So this topic will be divided into two parts.

98
00:14:15,380 --> 00:14:20,360
The first part is called the frequencies or approaches.

99
00:14:21,380 --> 00:14:29,090
Then the second part will be basic methods or basic approaches, where I will say how in the same can be done.

100
00:14:29,330 --> 00:14:35,810
Okay. So today I want to talk about this list, the square and method movement as.

101
00:14:40,110 --> 00:14:44,220
So. So the goals are very clear.

102
00:14:44,520 --> 00:14:47,790
We like to focus on some basic necessities.

103
00:14:50,030 --> 00:14:53,010
The basic reproduction number are zero,

104
00:14:53,450 --> 00:15:02,780
and that's certainly a very important quantity that we need to estimate and in the early stage of disease outbreak.

105
00:15:03,860 --> 00:15:11,370
So when we have very sparse data and the very early stage of the outbreak, right, like the monkey pox.

106
00:15:11,690 --> 00:15:19,790
So you have very few cases. Now, what you want to do here is to understand what is the basic reproduction number and what's the,

107
00:15:19,820 --> 00:15:25,940
you know, the severity of contagions of the disease.

108
00:15:25,940 --> 00:15:32,580
So so you have you in that case, you basically of the simple modicum of model,

109
00:15:33,550 --> 00:15:41,060
our model and the earliest phase of disease outbreak our with limited surveillance data available.

110
00:15:41,780 --> 00:15:45,320
You were to figure out the infectious disease dynamics.

111
00:15:46,730 --> 00:15:52,250
But in this case, we don't use computers in the mission.

112
00:15:52,370 --> 00:16:04,710
Okay. But now that, you know, for the pandemic, where we're now in the current stage, we have a lot of sort of complicity in this sort of dynamics.

113
00:16:04,730 --> 00:16:11,809
We're trying to understand more about more deeply about what's going on, but, you know, the dynamics of universities.

114
00:16:11,810 --> 00:16:15,820
And so that's why we're bringing a lot of careers into the investigation.

115
00:16:15,830 --> 00:16:32,330
But for this, the methods I'm going to introduce are coming from the consideration of doing the sort of investigation of the infectious disease,

116
00:16:33,840 --> 00:16:41,600
like, for example, estimating R0. So, so because you have very limited data,

117
00:16:41,840 --> 00:16:48,020
people tend to be hesitant to assume very specific distributions so that like

118
00:16:48,210 --> 00:16:53,420
square meter of mass of moments become our become appealing in that situation.

119
00:16:53,540 --> 00:16:57,650
Okay. So I will cover at least a square estimation.

120
00:16:57,890 --> 00:17:09,740
And, you know, because this is the method that has the highest sort of stability numerically that people like to use when you have very sparse data.

121
00:17:10,130 --> 00:17:16,220
You want something numerically stable, you work to get reliable estimation of your result.

122
00:17:17,150 --> 00:17:27,260
And people also want to avoid, you know, the use of specific parametric distributions in the estimating because you don't have

123
00:17:27,260 --> 00:17:35,149
enough data to make a sensible diagnosis or model diagnosis to to check whether or not

124
00:17:35,150 --> 00:17:41,420
the distributions for the data analysis would be the right distribution so that people

125
00:17:41,420 --> 00:17:47,300
tend to use sort of distribution free method like method moment to do their best.

126
00:17:50,810 --> 00:18:00,410
It's sort of the the method that I could estimation because this is regarded this sort of standard method.

127
00:18:00,410 --> 00:18:02,000
It's statistical theory.

128
00:18:02,000 --> 00:18:12,650
And part of the reason that people want to do inference to do some kind of reasonable statistical inference, if that's the purpose,

129
00:18:12,770 --> 00:18:21,049
instead of giving the point estimate of all of our zero, you want to give a confident confidence interval estimation for R zero.

130
00:18:21,050 --> 00:18:26,000
Then MLC would be the method of choice in which to do that kind of inference.

131
00:18:27,650 --> 00:18:32,660
So that's basically the things that I like to cover in this part.

132
00:18:34,280 --> 00:18:38,660
So let's begin with this very simple model. We can extend this model later.

133
00:18:38,690 --> 00:18:40,669
Let's start something the simplest.

134
00:18:40,670 --> 00:18:49,819
Okay, so you have this CIA model, you have done homework, number one, problem three, you have everybody got that problem?

135
00:18:49,820 --> 00:18:57,150
Perfectly correct. I, I know you do understand this dynamics, even how to balance it.

136
00:18:57,290 --> 00:19:09,920
Okay. So that's that's cool. So this is the was, you know, the most comparable model that we work on and under bunch of assumptions, of course.

137
00:19:10,670 --> 00:19:22,400
Okay. So then you have the. Successful individuals, compartment crashes, individuals and compartment removed features.

138
00:19:27,420 --> 00:19:31,290
Sort of subpopulations, I.

139
00:19:35,150 --> 00:19:39,950
Put in a differential equation, you start describing the dynamics here.

140
00:19:39,950 --> 00:19:50,250
You have all of their derivatives or rate of change. Themselves.

141
00:19:50,260 --> 00:20:03,000
So this is a different model and sort of a strategy of what were formulation, as we typically see in your 650 or 651.

142
00:20:03,020 --> 00:20:07,120
Right. Where are you basically looking at neither model, generalized, newer model.

143
00:20:07,600 --> 00:20:12,370
You never look at the modeling through some differential equations where you

144
00:20:12,370 --> 00:20:18,610
focused on certain models for there are derivatives right there or dynamics.

145
00:20:19,750 --> 00:20:22,060
But anyway, we know that why would do this also?

146
00:20:22,780 --> 00:20:32,920
Well, the second problem of homework, one, to see how this system can be numerically solved if our leader in a comma are given parameters.

147
00:20:33,970 --> 00:20:44,130
So now the quite. Well, you know, I collected from the CDC seven assistant.

148
00:20:44,510 --> 00:20:50,470
I'm going to ask them about. So those are two parameters that are certainly not a given.

149
00:20:50,480 --> 00:20:55,070
Right. So so both parameters are non negative parameters.

150
00:20:55,550 --> 00:21:07,580
And then. 30 follow up this so the disease for two weeks for example now you really want to understand what is the transmission rate,

151
00:21:07,580 --> 00:21:21,170
what is the recovery rate or this removed and the ratio will give you the, you know, the parameter of interest that is the basic reproduction number.

152
00:21:21,470 --> 00:21:30,560
Okay. And this is t over and would give you the most effective protection.

153
00:21:30,860 --> 00:21:35,070
Also, R zero is really the essential parameter like estimate.

154
00:21:35,530 --> 00:21:40,160
We have very well defined objective for statistical estimation.

155
00:21:40,820 --> 00:21:44,130
That is, we'd like to estimate three parameters of data.

156
00:21:44,740 --> 00:21:48,440
So of course if you estimate two of them, it would look. No, the third one.

157
00:21:48,560 --> 00:21:52,910
Right. So that's something very clear.

158
00:21:53,090 --> 00:22:01,070
I'm. So people noticed that is it, you know.

159
00:22:02,420 --> 00:22:15,470
So if I know if I know a pair of the parameters are beta and gamma, as you did in the second question of Maguire, you can easily.

160
00:22:16,750 --> 00:22:24,710
Yeah. This curves right? Sold this s t i t and r t right.

161
00:22:25,160 --> 00:22:34,700
So, so if if you're an operator that you can use this s flower model to compute this idea.

162
00:22:36,670 --> 00:22:46,790
So using this arcade method, the fourth order approximation for this ordinary differential equation, you can make this a little bit more complicated.

163
00:22:46,790 --> 00:22:52,160
But the principle applies here. If you give them all the parameters, I can generate, the.

164
00:22:55,250 --> 00:22:58,550
The same problem. This mode compartment. Okay.

165
00:22:58,940 --> 00:23:05,920
What? Okay. It's.

166
00:23:08,440 --> 00:23:12,870
It. Well, this is whatever you like.

167
00:23:12,880 --> 00:23:15,940
Let's say 18 a.

168
00:23:20,780 --> 00:23:26,800
CBC is interested in hearing how many people are infected to this day.

169
00:23:26,810 --> 00:23:34,720
But it's a post coming from. From CBC prices.

170
00:23:38,530 --> 00:23:43,940
So it could be nationalized. Total calm world could be, you know, the.

171
00:23:45,950 --> 00:23:52,499
Within the state of Michigan. So that. Whatever this is.

172
00:23:52,500 --> 00:23:56,190
Observe. I'm sorry. Okay. Let's see. You have two weeks.

173
00:24:00,200 --> 00:24:04,160
And when looking at it. Right. Well, this update, this estimate can be up.

174
00:24:17,430 --> 00:24:23,910
So because they are sequentially arrive arriving over time.

175
00:24:24,180 --> 00:24:27,180
Okay. So so certainly better now.

176
00:24:28,900 --> 00:24:33,790
Our zero will be updated when you have new data. So as the.

177
00:24:37,870 --> 00:24:42,580
All the beta gamma are there. We can have data collected to.

178
00:24:47,570 --> 00:24:52,650
The. You know, quickly afterwards, my estimate, right.

179
00:24:52,770 --> 00:25:02,430
Oh, well, that's something there's also. Arriving sequentially in a perpetual sort of sort of fashion.

180
00:25:04,350 --> 00:25:09,720
Oh, well, I have a duty to my. You.

181
00:25:10,080 --> 00:25:14,640
I would update my estimate anyway. So this is your observed.

182
00:25:17,860 --> 00:25:30,900
Okay. Number comes. I feel like I'm a given that I can generate a curve, right?

183
00:25:41,940 --> 00:25:45,550
This is the one given by a fixed rate.

184
00:25:46,020 --> 00:25:49,650
If I give you a pair of out of bed item you can you.

185
00:25:51,100 --> 00:25:59,370
Oh, our. You your second problem from a point.

186
00:26:01,610 --> 00:26:06,450
There. Oh, so there's a gap.

187
00:26:08,430 --> 00:26:13,950
Betrayal. What is generated from your sire Marlowe and what is observed though?

188
00:26:14,000 --> 00:26:21,630
Of course, Donald, what you want here is you. So.

189
00:26:21,700 --> 00:26:27,450
So you want your estimate added which.

190
00:26:29,320 --> 00:26:35,970
What gave you the smallest gap between. Clear.

191
00:26:36,000 --> 00:26:41,010
Right. So you want to find those apparel that have been uncommon and which.

192
00:26:43,950 --> 00:26:49,880
Or as the smallest guy who's. This is the square principle.

193
00:26:49,950 --> 00:26:58,250
Okay. You want some better research on the optimal ISR model at beta had an outcome that.

194
00:27:02,220 --> 00:27:05,580
The best of the best. The best is defined.

195
00:27:06,450 --> 00:27:14,860
That's the. Oh.

196
00:27:17,580 --> 00:27:34,070
The guy who flies. Was defined up to value at which you have chewed the best goodness feet.

197
00:27:36,270 --> 00:27:41,920
So that the optimization is carried out by minimizing the certain loss function.

198
00:27:42,600 --> 00:27:49,890
So you so how you define the gap? Well, the simple way to define the gap is really just the sum of squared errors.

199
00:27:50,440 --> 00:27:55,050
Right, given the date. So property today is.

200
00:28:04,870 --> 00:28:08,230
Amal, so I won't add them all together.

201
00:28:13,300 --> 00:28:19,480
A lost function defined as a squared error right between this.

202
00:28:20,580 --> 00:28:36,240
That I look and I will define that as my distance or loss function and want minimize that write it my best estimate of my friend.

203
00:28:36,540 --> 00:28:41,860
Right. So that's the way people think.

204
00:28:41,880 --> 00:28:43,470
Very, very intuitive. Okay.

205
00:28:44,040 --> 00:28:54,390
So you want clarify minimizing the lost function between our generated trajectories or the model based estimate and the observer trajectories?

206
00:28:54,750 --> 00:28:58,280
Okay. That's that's certainly the way that you do.

207
00:28:59,280 --> 00:29:04,919
You know, when they lost function, some of the squared arrows or R2 norm,

208
00:29:04,920 --> 00:29:11,729
the resulting estimation is oddly square estimation and the same model is two parameter.

209
00:29:11,730 --> 00:29:15,030
Even a gritty search is computationally cheap. Okay.

210
00:29:15,480 --> 00:29:19,680
So it's really you just generate, you know, a sequence.

211
00:29:19,680 --> 00:29:27,910
Great point. Now for our computer, you can run meetings of scenarios and pick up the one that gives you the smallest.

212
00:29:28,290 --> 00:29:33,910
So that's very good.

213
00:29:34,980 --> 00:29:39,810
You are making search, but you can have a smart way to do this.

214
00:29:41,830 --> 00:29:46,660
The curve, but you can use new rafts if you want because you know the derivative.

215
00:29:47,110 --> 00:29:52,480
But for this simple model, you have to traverse people just run this, you know?

216
00:29:55,890 --> 00:30:01,210
Well, you'll know that our square, right? So you can forget about it.

217
00:30:01,360 --> 00:30:09,450
What do you. What do you. A break and beginning artist greets an obvious moment.

218
00:30:09,450 --> 00:30:17,310
Why otherwise is. Great offering to be bigger than what they look set up like an act.

219
00:30:19,630 --> 00:30:25,600
From 1 to 2 and 2 to 3 and so on, so forth.

220
00:30:25,720 --> 00:30:33,830
They improvised a certain. So in this case, if you put the range from 1 to 10, you only need to.

221
00:30:37,720 --> 00:30:41,530
Finnerman employs between 1 to 10 that are too many.

222
00:30:45,730 --> 00:30:52,810
I didn't mean to step into the. But.

223
00:30:57,050 --> 00:31:01,130
Say that something like that very.

224
00:31:01,410 --> 00:31:04,900
You use using the modern computing facility, right.

225
00:31:08,160 --> 00:31:14,520
This can be done. Okay, so what I am doing here as I just simulate the observe time series.

226
00:31:21,050 --> 00:31:27,140
82.5 in common to this is my true value just to simulate the.

227
00:31:29,540 --> 00:31:37,720
But for from. Not all this procedure can be done so.

228
00:31:39,400 --> 00:31:53,250
Or did they know the true value? Let us just fixed comma two point to ..2 and I don't want estimate your cost.

229
00:31:55,320 --> 00:31:58,980
Right. You know, here instead three one dimension start.

230
00:32:01,770 --> 00:32:04,720
Anyway. So here you fix.

231
00:32:08,930 --> 00:32:19,310
Well, basically under some kind of prior knowledge or some assumption that on average five, five days of sort of infection in the of your girl,

232
00:32:19,370 --> 00:32:26,719
in fact, that you should spend five days being infectious before you becomes an infectious.

233
00:32:26,720 --> 00:32:35,330
Right. So you can guess this roughly of one value, but we're fix that to about it in the simulation study.

234
00:32:35,330 --> 00:32:38,510
But you can estimate it anyway. It's no problem.

235
00:32:38,810 --> 00:32:43,580
Okay. So what we're trying to do here is really find valuable data.

236
00:32:43,610 --> 00:32:48,400
What's the transmission rate? Okay. Under this is our model.

237
00:32:49,580 --> 00:32:55,910
So we've added some squared square error loss between to expect the cumulative

238
00:32:57,200 --> 00:33:03,860
infections comes 80 which is generated by this model and the model can be wrong.

239
00:33:03,920 --> 00:33:09,499
The model can now may not be perfect, but the simulation, of course, the data is similar from that model.

240
00:33:09,500 --> 00:33:20,750
That new model specification is correct. But you practice where you observe process may not come from our model A model is model A box and omos round,

241
00:33:20,750 --> 00:33:25,940
but you just use a model as approximation to understand that the beta generation mechanism.

242
00:33:26,450 --> 00:33:34,370
Okay. In this case, of course, it's our model is the two mile because you observe forces time source is generated from that model.

243
00:33:36,020 --> 00:33:41,600
So so what you're trying to do here is, of course, now what you don't know is the.

244
00:33:43,740 --> 00:33:47,100
But would you like to use the data to figure it out?

245
00:33:48,290 --> 00:33:59,450
Oh. Oh. You have the I.T. generated from the SRM and you have this the observed one which is simulated in the first place from.

246
00:34:02,110 --> 00:34:05,229
Though, so only the date of IPR.

247
00:34:05,230 --> 00:34:12,620
I use the calculation of some of the scoring errors due to the added comma is

248
00:34:12,620 --> 00:34:18,700
the fixed and added data is typically stamped supplied in the opinion system.

249
00:34:19,480 --> 00:34:26,320
Okay, you can use more data to do it, but you're only estimating one parameter out which is transmission rate.

250
00:34:26,320 --> 00:34:38,470
Yet you just use the data available which is the I.T, which is typically data captured by CBC as part of the for the primary estimation.

251
00:34:39,700 --> 00:34:45,130
So never in this generation if you from s or model involves all three compartments,

252
00:34:45,490 --> 00:34:53,170
although I only use the simulated or generated i.t in the defined loss function.

253
00:34:53,920 --> 00:35:03,820
Okay. You were to compare vs observed i.t. But the system itself what general research curves s t i.t and r t.

254
00:35:04,360 --> 00:35:07,360
Okay, but only pick up one trajectory.

255
00:35:07,810 --> 00:35:13,620
That's the i.t trajectory gender assigned to define that loss.

256
00:35:13,960 --> 00:35:22,200
What you can define more physical loss. But here I just define very simple loss function which is lost function i.t.

257
00:35:23,050 --> 00:35:31,680
Generated from all and observe i.t. Well, here is a result.

258
00:35:31,940 --> 00:35:40,650
Okay. So I consider three situations.

259
00:35:41,250 --> 00:35:54,850
Why is that you based on the data from ten days after outbreak it based on data like one one half week and almost three weeks and.

260
00:35:55,440 --> 00:35:58,800
And you know, almost two months.

261
00:35:58,810 --> 00:36:06,150
50 days. Okay. So you have ten day times, which is sort of 20 days and time.

262
00:36:06,150 --> 00:36:09,450
Space of 50 days. Okay. Depends on.

263
00:36:17,330 --> 00:36:32,930
We're only for ten days. But what? Well, here is the.

264
00:36:38,000 --> 00:36:42,530
We can see that there's this this very flat one.

265
00:36:43,220 --> 00:36:50,480
It's the one coming from because this is too about 45 actuaries to fact that of Peter this is the VAT is that you're

266
00:36:50,480 --> 00:36:58,220
trying to figure out of course you won't find the beta at which the assets e the lost function is minimized.

267
00:36:58,310 --> 00:37:05,180
Right. So this is the point that you'd like to pick up as your estimate, because that's how the B Square is trying to find.

268
00:37:05,300 --> 00:37:11,870
Now, I'm not doing an analytic a solution x transpose x inverse x transpose Y.

269
00:37:11,870 --> 00:37:15,140
I'm trying to use a numerical solution to do that.

270
00:37:15,680 --> 00:37:23,600
Okay. So I just. 2.0.

271
00:37:23,990 --> 00:37:33,680
I have a comma as well. Right. So if I pick up 1.0 as my beta a kind of about it, I don't know.

272
00:37:33,710 --> 00:37:40,110
Okay. And I have my comma point point to this, this pair of that I can simulate.

273
00:37:40,550 --> 00:37:44,650
I can now see me. I can generate a trajectory from my arm.

274
00:37:44,670 --> 00:37:48,410
All right. Then after I put this one.

275
00:37:51,590 --> 00:37:59,170
The. The value will be one value on the current year.

276
00:37:59,710 --> 00:38:05,230
So you have a sequence of three point. What?

277
00:38:06,800 --> 00:38:16,910
When you generate this using the d solve to generate it and then you calculate C you know, and then you can create.

278
00:38:20,840 --> 00:38:24,500
Of Bayda and of which this city is minimized, is minimal.

279
00:38:25,460 --> 00:38:32,810
That's your solution, because that's the value you want to have that gives you the the best goodness of it.

280
00:38:33,450 --> 00:38:41,970
Okay, so. I.

281
00:38:43,150 --> 00:38:46,210
But. But.

282
00:38:47,750 --> 00:38:53,280
There is some data the curves tend to be. It was.

283
00:38:55,850 --> 00:39:03,850
But. It's the valley valid.

284
00:39:03,920 --> 00:39:15,559
Right. The minimal that if you increase. This increase at the curve is even more bent, this shopper point.

285
00:39:15,560 --> 00:39:19,280
Right. But clearly you can see that.

286
00:39:20,120 --> 00:39:28,980
What? Good enough for you to actually comfortably identify this sort of minimal point.

287
00:39:29,430 --> 00:39:35,720
The. So in this area slip a flat here.

288
00:39:36,890 --> 00:39:41,120
But. We'll be better estimate.

289
00:39:41,900 --> 00:39:46,760
So the question here is, in practice, of course, that the data will have equal to ten.

290
00:39:48,240 --> 00:39:54,460
All of a sudden, so forth. So every day when you have more data, you would update your estimate.

291
00:39:54,640 --> 00:40:05,709
Okay. So the the observation here is clear that longer observed sequence, the more curved around point five appears.

292
00:40:05,710 --> 00:40:14,800
So the better we can identify the minimum distance e curve, which is basically the bad if you want to capture best estimate of your best.

293
00:40:15,370 --> 00:40:19,810
So clear the procedure is very, very straightforward, very, very computational.

294
00:40:20,500 --> 00:40:26,780
Here's the code. Right. So here is your data.

295
00:40:27,500 --> 00:40:33,560
You know you put into your a car for this is the function you use right.

296
00:40:34,070 --> 00:40:37,640
So R K for is the function used to generate it.

297
00:40:37,910 --> 00:40:46,639
And then you have observed data, you calculate residual well then you sum the risk you the square residual and now

298
00:40:46,640 --> 00:40:52,340
you're trying to find the minimum value of that that you basically get the estimate.

299
00:40:52,600 --> 00:40:59,210
Yeah. So usually we have data for like didn't like people didn't like it.

300
00:41:01,220 --> 00:41:07,780
Overt and important. It's. So why do they're not using the C4 on?

301
00:41:08,150 --> 00:41:17,570
I wasn't going to go for the did. It's easy on infections and I.

302
00:41:19,400 --> 00:41:27,080
It's pretty reasonable. I will say that you want to use data, whichever data that you feel most life.

303
00:41:34,610 --> 00:41:39,230
If we stay, the sauce is more like which type is most reliable.

304
00:41:39,240 --> 00:41:44,390
That's the one you should use, right? By convenience, I shouldn't do it.

305
00:41:44,570 --> 00:41:48,420
Observe that one. Don't typically have to obey the.

306
00:41:50,880 --> 00:41:57,320
The. I.

307
00:41:59,200 --> 00:42:07,770
This is not the only choice. Basically telling you the procedure, how this estimate came to you.

308
00:42:07,940 --> 00:42:11,680
So we got to see that for which we you get estimates of some.

309
00:42:12,990 --> 00:42:20,690
Right. Phelps also that this is one thing people do in practice, of course, to find investment.

310
00:42:22,660 --> 00:42:29,590
So of course, that everything will be really about this arc for solver.

311
00:42:29,800 --> 00:42:40,010
So this is really d solve is the critical component that we need to use in the estimation resolve this one,

312
00:42:40,040 --> 00:42:49,629
we will not be able to do this destination. Now, to point out that the two dimensional research can be done with little difficulty, I mean,

313
00:42:49,630 --> 00:42:55,030
given the computing power now we have and also we have the capacity of paralyzed,

314
00:42:55,150 --> 00:42:59,340
paralyzing our computing, I don't think that's a big deal for this simple model.

315
00:43:02,990 --> 00:43:09,500
Couple of remarks. So this is the squamous that does not require any some treatment.

316
00:43:09,500 --> 00:43:13,520
Distribution thus works for both counts and proportions.

317
00:43:14,270 --> 00:43:25,129
Right. Sometimes people like to, you know, work out the proportions because the ink capital n changes all the time.

318
00:43:25,130 --> 00:43:35,270
Over time, people write it and using the it they want to scale with the population size like like a university campus.

319
00:43:35,510 --> 00:43:44,690
The population size changes during the summer time. People are students of you know, when home that population size drops and so on and so forth.

320
00:43:44,930 --> 00:43:47,910
In some regions or some occasions, you know,

321
00:43:47,930 --> 00:43:58,490
the population size changes in that situation that instead of doing this sort of counts as your data, you can use proportions.

322
00:44:00,620 --> 00:44:08,240
So I had some mild the regular air conditioner gives consistent estimation of model parameters and we people have proved that,

323
00:44:08,420 --> 00:44:16,010
you know, if that's the case. So you get the consistent estimate of the the specifications, right?

324
00:44:16,790 --> 00:44:24,410
So obviously this method ignores their dependance when they do the they are necessary.

325
00:44:25,520 --> 00:44:29,540
All the data points are all equally right.

326
00:44:29,570 --> 00:44:37,549
So there is assuming that data points are somehow independent, identical, distributed.

327
00:44:37,550 --> 00:44:45,530
So that may not be the case. So, so usually people don't use this method for inference,

328
00:44:46,160 --> 00:44:56,059
but you can you look at the point estimate from this ignoring the silver dependance of your data because this is really a time source.

329
00:44:56,060 --> 00:45:04,340
You have this autocorrelation inside the data that you need to really take care of it if you want to really get a good power from your

330
00:45:04,340 --> 00:45:15,440
statistical analysis currently that there are no weights in the SS you so that typically all the data points are equally sort of sampled.

331
00:45:18,460 --> 00:45:27,800
So estimation freezing may be improved by incorporating autocorrelation estimation by so-called with this score estimation method that weaves.

332
00:45:28,390 --> 00:45:35,440
If you wanted to win on this score, you really need to work out the old correlation function that can be done easily.

333
00:45:38,630 --> 00:45:47,960
So and you can see that in this exercise there was no consideration on the random components.

334
00:45:48,530 --> 00:45:56,839
So they use this loss function to estimate the the parameters of the system.

335
00:45:56,840 --> 00:46:04,190
But they don't really know whether or not the arrows are running the arrows where there are just systematic arrows.

336
00:46:04,190 --> 00:46:14,180
So. So that that's something like people trying to figure out if you don't have random components,

337
00:46:14,750 --> 00:46:17,989
you don't need to really talk about the inference statistic.

338
00:46:17,990 --> 00:46:23,600
The inference is the principle we use to address sampling uncertainty.

339
00:46:23,810 --> 00:46:26,930
If you don't have sampling uncertainty, you don't need inference.

340
00:46:27,200 --> 00:46:31,610
So there is systematic error is not random here.

341
00:46:32,090 --> 00:46:41,570
So that so people are trying to see why you need to do inference if you believe that there is no random sampling uncertainty.

342
00:46:42,890 --> 00:46:47,050
But that's not true, right? We do have sampling uncertainty.

343
00:46:47,060 --> 00:46:52,600
If you have observed process, there is always a sufficiently certain sampling uncertainty.

344
00:46:52,660 --> 00:46:57,890
It is the system. So our model that's done now to take that into consideration.

345
00:46:58,460 --> 00:47:05,240
So later on I will talk about this so stochastic RSI model where people try to bring in some

346
00:47:05,240 --> 00:47:12,500
uncertainty into it to model sampling uncertainty so that we can do inference on that.

347
00:47:14,740 --> 00:47:24,229
But the. So this method has been explicitly assumed in the implementation of the square estimation

348
00:47:24,230 --> 00:47:32,300
and disquisition about this the range to the force order approximation method.

349
00:47:33,340 --> 00:47:38,870
Okay. So for example, if the discrete associations corralled in that unit one day,

350
00:47:38,870 --> 00:47:44,420
then sampling for instance, is implicitly assumed to be set at the daily level.

351
00:47:45,110 --> 00:47:53,930
In other words, this overall contributes only the mean model and errors are not seriously handled, which may affect the quality of citizen numerous.

352
00:47:54,030 --> 00:48:02,150
Okay, so that's really a very simple model as some kind of sort of shortcomings that we.

353
00:48:05,100 --> 00:48:10,709
And not use. So that will cause some issues on our insurance,

354
00:48:10,710 --> 00:48:19,710
but we would do some adjustment later on how to turn the model to stochastic model so that we can incorporate the sampling uncertainties,

355
00:48:19,710 --> 00:48:24,570
so that we can propose a legitimate framework for strategy inference.

356
00:48:27,440 --> 00:48:34,100
So this is very computational compute, computation driven method.

357
00:48:34,230 --> 00:48:39,770
Right. So you give a pair of value. I do calibration and find the best of calibration matters.

358
00:48:39,950 --> 00:48:44,840
That's it. So now people are trying to find something a little smarter on that.

359
00:48:45,230 --> 00:48:56,480
Like, for example, what? What what do you bet on the game hour time during you believe that the data transmission rate is not the same over time.

360
00:48:57,380 --> 00:49:11,270
Okay. If that's the case, if you build a better game of. And this sort of simulation meant that we're no longer hearing or a viable to be implemented.

361
00:49:11,480 --> 00:49:20,740
You have to allow something, you know, in a lot of malls on the bigger garment into consideration.

362
00:49:20,750 --> 00:49:24,050
You need a more gentle method basically than this simulation method.

363
00:49:24,590 --> 00:49:30,500
So that comes the method of moments. Let's just see how we're going to develop the, you know,

364
00:49:31,010 --> 00:49:42,379
the master of moments in Somalia when Peter and Goma are constant, a simple, just two simple, simple parameters.

365
00:49:42,380 --> 00:49:48,170
Then we will see how this method can be extended to do this more complex case about better.

366
00:49:48,560 --> 00:49:53,740
Okay. So what they're trying to do here is back to the mall here.

367
00:49:54,160 --> 00:50:01,720
Okay. So in the early outbreak, right in the large population s t and in ah, almost this century,

368
00:50:02,620 --> 00:50:08,720
so number of people are susceptible individuals and the population and the population size itself.

369
00:50:09,550 --> 00:50:12,879
Now the exactly same. You have 1 million people. You live in the city.

370
00:50:12,880 --> 00:50:16,480
You have 50 people who are infected.

371
00:50:17,560 --> 00:50:22,390
And then you know that then this city and are very, very close.

372
00:50:22,540 --> 00:50:25,690
Okay. So so let's put that way.

373
00:50:25,720 --> 00:50:30,700
Okay. So that's the beginning of everything because you really want the estimates parameters.

374
00:50:32,140 --> 00:50:36,760
Of their if not now, now, like a lot of people are being affected.

375
00:50:36,760 --> 00:50:43,000
Right. So so for that, Kobe. But in the beginning, the first two weeks were first two months.

376
00:50:43,900 --> 00:50:48,370
So in this case, S-T and e-mail approximate it said same.

377
00:50:48,370 --> 00:50:57,610
So this approximately equal to all of these is more than one, whichever way you could put put nine, nine, nine if you want represent that one.

378
00:50:58,120 --> 00:51:04,629
Just whichever the value you can put a value typically you put at one.

379
00:51:04,630 --> 00:51:12,160
If you own 2.999, that's fine. But anyway, people don't deal with this factor replaced by a constant.

380
00:51:13,390 --> 00:51:21,790
Okay. If you put this as a one approximately one day, you said to the the time difference, it's a one day.

381
00:51:22,390 --> 00:51:30,400
What a time. You need it one day. Then you can have this approximation for this second ordinary differential equation.

382
00:51:31,390 --> 00:51:45,850
Okay. Here you can see that if asked and and you replaced by Y, you have b that ty and minus gamma ty so that you can write this up as a as this.

383
00:51:46,140 --> 00:51:54,490
Okay. So basically in the first order derivative of the D, it will roughly equal to Peter.

384
00:51:55,650 --> 00:52:01,670
Scammer 80. Right.

385
00:52:02,490 --> 00:52:05,730
Well, i t t t.

386
00:52:06,810 --> 00:52:09,930
Roughly theta, minus gamma i.

387
00:52:17,350 --> 00:52:21,580
Right. So you can say that that this will you call to a block?

388
00:52:29,060 --> 00:52:32,390
Peter. So this is constant.

389
00:52:33,200 --> 00:52:40,640
So then what I will be is this. You call the first order of the of this, then this log.

390
00:52:40,670 --> 00:52:46,550
I will be in the property of this.

391
00:52:48,050 --> 00:53:12,490
What's a minor function, right? Yeah. Well, now you have the Mormon condition.

392
00:53:19,310 --> 00:53:26,900
He. But this this term is zero, as you'll see.

393
00:53:27,650 --> 00:53:35,090
So the boundary condition helps you to figure out the C. So C is the constant, the O.

394
00:53:37,780 --> 00:53:49,290
Well, your work to make it easier to be illegible. If this is zero then log I.

395
00:53:49,890 --> 00:53:54,280
I will be minus infinity with all that region the value.

396
00:53:54,670 --> 00:53:58,870
So you say what if I have this one? I have no infections.

397
00:53:59,260 --> 00:54:03,350
If you all have you. Infectious disease.

398
00:54:03,380 --> 00:54:06,890
Right. Right. So you can move your time.

399
00:54:08,690 --> 00:54:23,450
Reset. Otherwise.

400
00:54:23,810 --> 00:54:29,660
Why are you talking about outbreak? There's no outbreak, right? So? So this one below zero.

401
00:54:29,990 --> 00:54:33,400
That's certainly something that you.

402
00:54:35,970 --> 00:54:40,860
So. So when you do this, then you can.

403
00:54:42,360 --> 00:54:48,360
You can loosen the ice 80 equal to zero.

404
00:54:49,080 --> 00:54:58,540
It's okay. Oh, so.

405
00:54:58,550 --> 00:55:03,400
So you have the base like. I.

406
00:55:04,980 --> 00:55:09,330
Of the change when time goes to the future time.

407
00:55:09,660 --> 00:55:12,860
This is actually function you'll get right.

408
00:55:13,890 --> 00:55:18,570
If you use this, you'll get a solution. That's exactly the solution you talk about.

409
00:55:18,570 --> 00:55:26,730
So you have it0e of the minus domino t.

410
00:55:35,880 --> 00:55:41,620
Okay. Oh.

411
00:55:42,520 --> 00:55:45,770
So. So how do you estimate based on the karma. Right.

412
00:55:45,790 --> 00:55:55,060
If you look at log i.t, you have the intercept term log zero plus be minus garmin t.

413
00:55:57,910 --> 00:56:03,430
Now you have this data from TIME series and you have time.

414
00:56:04,870 --> 00:56:10,750
So what we are trying to do here is really you you you take a log of it as your y.

415
00:56:18,020 --> 00:56:25,210
But. In a regression of Y on X.

416
00:56:27,230 --> 00:56:30,860
This is your intercept. This is your slope.

417
00:56:32,030 --> 00:56:36,980
This is your zero. This is beta one. Okay.

418
00:56:37,050 --> 00:56:49,430
You okay? How do you estimate parameters? You estimate the parameters by running the linear regression of log number of, in fact, cases on time.

419
00:56:50,850 --> 00:56:55,700
Okay. That gives you estimate the weight of minus our forces the difference.

420
00:56:56,660 --> 00:57:01,810
Okay. You haven't you haven't got the you haven't called the actual values.

421
00:57:01,910 --> 00:57:05,540
You need the second equation. But first, one irony.

422
00:57:05,540 --> 00:57:09,949
This Nino regression, you're already called estimate beta minus gamma.

423
00:57:09,950 --> 00:57:17,870
Have the difference of the back. But it's karma that you need this day.

424
00:57:18,160 --> 00:57:25,550
The second one. Look at the second one here.

425
00:57:27,420 --> 00:57:36,630
You know, you look at the 33rd ordinary French equation, this one, this ties this.

426
00:57:37,130 --> 00:57:40,920
Okay. So this quintiles it d d equal to one d.

427
00:57:42,090 --> 00:57:59,530
Okay. So you take a one day. One day.

428
00:57:59,540 --> 00:58:06,090
So. So that you have this. So you have the art.

429
00:58:09,200 --> 00:58:13,200
It's karma. This is your third differential equation.

430
00:58:13,200 --> 00:58:19,329
This a defi on the continuous dynamics. Acquisition.

431
00:58:19,330 --> 00:58:23,260
So these are people from.

432
00:58:26,510 --> 00:58:30,080
And you know, I said to you one day.

433
00:58:38,570 --> 00:58:44,440
Our team lost one. Our team, right?

434
00:58:53,700 --> 00:59:01,120
Now. I have got to this. Okay.

435
00:59:04,780 --> 00:59:08,390
Oh. This is what? The increment, right?

436
00:59:12,450 --> 00:59:21,750
Okay. Authorities were removed. But look at what did the increments of that.

437
00:59:23,240 --> 00:59:28,310
So what are you trying to do here? As this is why this.

438
00:59:28,460 --> 00:59:36,210
This is your ex. So the X is actually number of in case two.

439
00:59:41,200 --> 00:59:49,100
Why? Of the interceptor.

440
00:59:49,520 --> 01:00:01,700
So this is your plane. No one with a zero is zero for you round the the increment of the removed cases on the number of infected fact case.

441
01:00:06,110 --> 01:00:09,770
This one guy is running a regression. This no intercept, right?

442
01:00:11,250 --> 01:00:17,200
But. From this hour.

443
01:00:20,260 --> 01:00:23,470
Then from here, you'll get estimates from all then.

444
01:00:26,260 --> 01:00:31,390
Peter Myers. Oops, this one plus the gala estimate.

445
01:00:32,500 --> 01:00:42,110
Right. That will give you estimate of beta. Oh.

446
01:00:45,350 --> 01:00:48,409
Okay. So. So you say all this is so complicated.

447
01:00:48,410 --> 01:00:53,570
Why would we want border to do this? Well, this is because.

448
01:00:57,600 --> 01:01:02,070
Well right. The business to.

449
01:01:07,920 --> 01:01:14,940
You say, Oh, I don't believe that, Satan. Uh, constant over time.

450
01:01:15,390 --> 01:01:19,550
So now you want introduce time. Oh.

451
01:01:19,570 --> 01:01:27,430
You believe my bailout is speed up time? You can put this much one because we talk about that's the transmission.

452
01:01:27,520 --> 01:01:31,480
It can be related to seasonal like spring or winter time.

453
01:01:31,490 --> 01:01:38,170
Have more transmission rate, higher transmitting, been summer or you just do number of metrically.

454
01:01:38,410 --> 01:01:51,850
Right. So so what you can do here is you can do a non parameter regression so you can use again our function pre generalized additive model.

455
01:01:53,110 --> 01:02:01,870
You can gam will basically estimate there's theta T minus gamma t as a function.

456
01:02:03,300 --> 01:02:13,970
Functional. This scam function is is a supply based supply based nonprofit estimate of this.

457
01:02:14,450 --> 01:02:18,050
So you're basically two gam oops. Our function.

458
01:02:22,190 --> 01:02:34,669
Yeah. Oh, sorry. Whichever you want to do this time.

459
01:02:34,670 --> 01:02:38,270
Very efficient. This is this is time.

460
01:02:41,160 --> 01:02:50,280
And. I'm very. Oh, one thing you could do here is really just make it time.

461
01:02:53,040 --> 01:02:57,840
Like that or you two time very I very quickly.

462
01:03:05,450 --> 01:03:08,630
Package time should.

463
01:03:11,500 --> 01:03:17,290
You can do that. Here.

464
01:03:18,400 --> 01:03:23,650
You can also relax this to be time during content time for now.

465
01:03:23,710 --> 01:03:28,240
What you should do here is you can do r t minus one plus one.

466
01:03:30,130 --> 01:03:34,740
But I asked your why. So here you have gamma t.

467
01:03:37,270 --> 01:03:44,140
Okay. So that you can run. Get rid of this lie and hide.

468
01:03:55,620 --> 01:03:58,680
If you this number metrically, of course as I said.

469
01:03:59,810 --> 01:04:08,080
Is that like drug treatment or something more better hospitalization if.

470
01:04:12,580 --> 01:04:16,560
Or we can view mall as a functioning walkover.

471
01:04:17,010 --> 01:04:26,480
Right. But what I'm inclined to see here is no matter how you are going to incorporate additional features you.

472
01:04:28,270 --> 01:04:41,770
Well. Or a better facility in a hospital ward than emergency room beds or you know, just that it is this to sort of function like.

473
01:04:44,510 --> 01:04:50,410
Then you can look at the interaction with this. Jonestown we're talking about.

474
01:04:50,900 --> 01:05:06,130
Good. Let's do this. It's.

475
01:05:10,180 --> 01:05:13,240
Some converts you'd like to like drive to the.

476
01:05:14,710 --> 01:05:20,030
Oh. Paxil. Right. Or some kind of number of beds can.

477
01:05:23,090 --> 01:05:28,280
But they're going to come to you, for example, and write this one.

478
01:05:31,910 --> 01:05:40,070
All of this. As you are focused, your thoughts are on whether or not a drug is available and.

479
01:05:41,000 --> 01:05:48,010
And. What?

480
01:05:58,220 --> 01:06:02,900
So that you can have this sort of this.

481
01:06:03,410 --> 01:06:08,710
Well, here this is also covered that we'll have the interaction with this one detective.

482
01:06:09,670 --> 01:06:16,370
Right. So your computer your your model of certain functioning of cougars,

483
01:06:16,790 --> 01:06:24,980
you believe that are potentially related to the recover rate like we have so many data.

484
01:06:26,030 --> 01:06:32,690
So it captures things like I can see that I can build up 2.2.5.

485
01:06:39,730 --> 01:06:44,710
Her pool. She will be a factor. Hence the rate of.

486
01:06:48,090 --> 01:06:54,990
Air pollution tend to slow down. It's true.

487
01:06:55,200 --> 01:07:02,330
But you can invest it if you want. And also I'm.

488
01:07:05,910 --> 01:07:09,540
Right. So you look at the current global housing price.

489
01:07:09,810 --> 01:07:12,960
Right now, the people are in the.

490
01:07:14,310 --> 01:07:17,680
Uh. Right.

491
01:07:17,770 --> 01:07:22,660
You want model debater here? What are all the people living in the place?

492
01:07:22,680 --> 01:07:25,920
The counties with higher priced housing price,

493
01:07:26,460 --> 01:07:39,720
median price tend to be less likely to be factors you'll look for based on some hypotheses into your model so that you could really invest something.

494
01:07:39,990 --> 01:07:43,160
What I'm seeing here is the power of this method movements.

495
01:07:43,620 --> 01:07:46,860
You basically run minor regression or nonlinear regression.

496
01:07:49,790 --> 01:08:00,940
Right. Into makeovers into the mall, which cannot be done by the first month.

497
01:08:01,270 --> 01:08:05,889
The first message is basic simulation, calibration method, straightforward,

498
01:08:05,890 --> 01:08:13,120
and these need to do but this no lack of flexibility of understanding some.

499
01:08:15,280 --> 01:08:24,419
Right. So, so this is great method that can I just give you the first sort of presentation of this that

500
01:08:24,420 --> 01:08:32,340
you understand that debate on the comment below can be formulated in some way that you can,

501
01:08:32,610 --> 01:08:40,590
you know, make the interaction of your coverage and time and then invest it that.

502
01:08:41,580 --> 01:08:46,739
That's a movement. Okay. Well, I think this is very flat.

503
01:08:46,740 --> 01:08:51,870
Some method that could be investigated. You can see your group party, for example.

504
01:08:52,740 --> 01:08:57,960
All that can be done. And you have some ideas what you like to invest.

505
01:08:58,260 --> 01:09:03,780
Some people say, I want to see whether or not the you know, the political point of view, the fact.

506
01:09:07,150 --> 01:09:14,200
Seeing a sentence or some people say, I want to see whether or not the word or some people will see some economic, social,

507
01:09:14,200 --> 01:09:20,160
economic or investment determinants or determine what we are a a fact that, you know,

508
01:09:20,170 --> 01:09:26,380
the debate and garment and everything sort of something that you can build into the this kind of mob.

509
01:09:27,370 --> 01:09:28,329
But overall,

510
01:09:28,330 --> 01:09:40,720
the big structure is still ISI are but you turn is this are mall to a more flexible situation to allow some careers to played a role in the system.

511
01:09:41,920 --> 01:09:47,770
So that's something I really think is very interesting to invest it and showcase.

512
01:09:50,870 --> 01:09:56,540
So that's something I like to talk about this part about the.

513
01:09:58,200 --> 01:10:03,370
The Mall. Very flexible.

514
01:10:03,370 --> 01:10:08,230
You can use existing software package to run neither model or no neither model.

515
01:10:08,770 --> 01:10:13,300
None. None permit remote. Okay. Knockabout.

516
01:10:16,430 --> 01:10:26,930
He. So here I'd like to talk about two method.

517
01:10:27,320 --> 01:10:32,360
One is the maxim like the estimation, not is the compulsive estimation.

518
01:10:36,290 --> 01:10:42,620
Well here, the goal here is clear that we like to make inference.

519
01:10:42,930 --> 01:10:49,700
Okay. So we are not only interested as to getting the point estimate, but also we want to see how we're going to,

520
01:10:49,730 --> 01:10:57,860
you know, account for this among simply uncertainty or some uncertainty in the data, perhaps in process.

521
01:11:00,580 --> 01:11:05,680
Well, this is the mall that we talk about. This is a center model.

522
01:11:06,130 --> 01:11:12,340
I mean, the whole process can be extended to order type model.

523
01:11:12,340 --> 01:11:25,799
But here we start with this desirable. So I have been repeatedly criticizing this first part of the model where we do not have to

524
01:11:25,800 --> 01:11:33,000
really incorporate sampling uncertainty into the data caption of data collection procedure.

525
01:11:33,630 --> 01:11:42,510
So there is a little problem that how do you deal with the uncertainty of the cost the city.

526
01:11:42,840 --> 01:11:48,070
Okay, now let's see how we're going to extend that to stochastic etc. model.

527
01:11:48,090 --> 01:11:54,870
Okay. So they in the literature, the first thing people do is the pricing process.

528
01:11:56,410 --> 01:12:00,370
So no problem process.

529
01:12:00,580 --> 01:12:07,840
Okay. So now you have this three components that are dynamic specified previously.

530
01:12:08,500 --> 01:12:15,310
So now what we want to impose here is some kind of randomness in the generation of increments.

531
01:12:15,680 --> 01:12:23,680
Okay. So this is the way to bring some stochastic city into the S.R. model while the approximate process.

532
01:12:23,970 --> 01:12:28,210
Yeah. So. So what are trying to do here. Is that okay.

533
01:12:29,290 --> 01:12:41,230
Here you have a is ten on this time interval from T to t plus d t very, very tiny time sort of moment.

534
01:12:42,580 --> 01:12:49,690
So you look at this very tiny time interval instead of this time interval.

535
01:12:50,410 --> 01:12:52,630
So you look at that, this change,

536
01:12:52,870 --> 01:13:06,790
sort of the instantaneous change in this compartment of our susceptible individuals and recover a removed sort of compartment.

537
01:13:07,480 --> 01:13:11,380
So now you impose the partial distribution.

538
01:13:11,770 --> 01:13:15,020
Possible distribution as a distribution for random variable.

539
01:13:15,040 --> 01:13:20,740
Right. You look at this increments. Right. So distribution.

540
01:13:21,100 --> 01:13:25,790
Okay. So first of all, you look at this, the change.

541
01:13:27,700 --> 01:13:33,040
So as I said, that the this s compartment is always decreasing.

542
01:13:33,700 --> 01:13:41,440
So the the the number of cars today is always I mean, now, yeah.

543
01:13:41,950 --> 01:13:52,840
At this moment, it's always bigger than the count in the next moment because this is a you always have this sort of individuals moving out from this.

544
01:13:54,000 --> 01:13:57,690
Well as compartment is always decreasing or not increasing.

545
01:13:58,470 --> 01:14:07,470
So this if this difference, this increment is always non-negative because partial distribution can now generated negative integer values.

546
01:14:07,740 --> 01:14:16,649
So so this difference is the, the cumulative count now minus the cumulative count.

547
01:14:16,650 --> 01:14:26,640
Next moment. Okay. So that's always a non-negative integer value that seemed to be to follow a parcel distribution this this rate.

548
01:14:27,250 --> 01:14:32,300
This is really coming from the insider model.

549
01:14:32,370 --> 01:14:36,270
This is the rate. Okay. Or expand about this.

550
01:14:37,770 --> 01:14:41,159
And this is the the rate times.

551
01:14:41,160 --> 01:14:45,240
The deep is the time. Okay.

552
01:14:46,380 --> 01:14:56,010
Now, how the this right. How this event would occur in a very tiny time time interval.

553
01:14:56,670 --> 01:15:07,440
This is the rate now looking at the recovery than you have, you know, this ten increment of removed cases from the our compartment.

554
01:15:08,100 --> 01:15:15,179
So our compartment is always increasing because this absorbing compartment always have everybody coming to that.

555
01:15:15,180 --> 01:15:18,300
And it's sort of destination or destiny.

556
01:15:18,810 --> 01:15:28,720
So that you use the R at the next moment like minus the R T and this movement also falls apart on distribution with this rate.

557
01:15:28,740 --> 01:15:33,570
Our times i t times this very, very tiny interval.

558
01:15:34,260 --> 01:15:37,170
So that's the rate to generate increment.

559
01:15:38,010 --> 01:15:47,370
So you say why not the you put to the the I the I t in the partial distribution you can because it the either

560
01:15:47,370 --> 01:15:54,240
way the it could be increasing or decreasing so that the increment could be positive or could be negative.

561
01:15:54,900 --> 01:15:59,040
Right. You'll remember the curve of I. Right.

562
01:16:02,930 --> 01:16:10,570
We also did this and. This is the future moment minus.

563
01:16:18,680 --> 01:16:22,370
Although we should pass on distribution for I. But it is sufficient.

564
01:16:22,670 --> 01:16:25,670
Plus, you have this identity, right? It is.

565
01:16:25,670 --> 01:16:29,900
You could of randomness.

566
01:16:30,200 --> 01:16:34,190
As to randomness are to then the randomness.

567
01:16:34,210 --> 01:16:37,430
It is automatic according to this relationship.

568
01:16:38,030 --> 01:16:49,610
Okay. So in this way, because the amount of times that you can do Poisson distribution assumption and the increments and the increments of art,

569
01:16:50,360 --> 01:17:04,130
so this whole thing define a stochastic se essentially tell people how those increments are generated according to proximal distribution.

570
01:17:04,670 --> 01:17:14,690
Okay. Now how do you forming likelihood based on the, you know, the pricing process to really estimate the parameter, a beta and the gamma?

571
01:17:15,050 --> 01:17:18,709
That's something that we need to discuss on Thursday. But anyway, now, you know,

572
01:17:18,710 --> 01:17:25,220
the distribution next thing is that how do you use quantum passes to come up with a

573
01:17:25,220 --> 01:17:29,600
like it function and maximum like a function to get your parameters than the beta.

574
01:17:34,480 --> 01:17:38,559
You go back to work a little bit on your homework too,

575
01:17:38,560 --> 01:17:45,639
and also have your group meeting to discuss how to capture data because we need those values to put into the model so you

576
01:17:45,640 --> 01:17:55,090
can see all the speed that would change according to some of your hypotheses that you are basing your homework back.

577
01:18:31,250 --> 01:19:04,500
Oh, you know what? So.

578
01:19:10,950 --> 01:19:23,880
I say, okay. The.

579
01:19:34,030 --> 01:19:44,640
Hanging. Okay. Were not.

580
01:19:51,060 --> 01:19:57,250
But. A ULA is not here.

581
01:19:58,810 --> 01:20:03,070
So if you have anything you want to discuss me, come to see me at 330.

582
01:20:03,070 --> 01:20:07,330
I have an officer in my office, which is his office.

583
01:20:07,840 --> 01:20:11,410
Maybe I have a table there. So you can. We can have that.

584
01:20:12,760 --> 01:20:13,230
You start.

