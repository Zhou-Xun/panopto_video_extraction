1
00:00:00,360 --> 00:00:27,059
My first taste of that is going by the fact that like actually you could bring me over by management from the government,

2
00:00:27,060 --> 00:00:31,380
which I cannot talk to you about.

3
00:00:32,820 --> 00:00:38,010
Yeah, I want to start for other reasons.

4
00:00:38,780 --> 00:00:51,010
You've got to sell something and you want to start making jams.

5
00:00:53,200 --> 00:01:14,339
Yo, yo, if you want to do something like this, you can do all three.

6
00:01:14,340 --> 00:01:19,270
It is also not now, but it's more fun. Oh, I'm not doing anything.

7
00:01:20,370 --> 00:01:31,920
Juicing chicken. I'm pretty sure to just start out with.

8
00:01:32,700 --> 00:01:42,720
And you'd be the second that one of you would be all you need to be main course, which is even worse.

9
00:01:43,440 --> 00:01:53,280
Yeah, that's me. No, no, no, you won't.

10
00:01:58,680 --> 00:02:04,510
That's what I saw in first glance.

11
00:02:05,160 --> 00:02:24,420
That this is really hard, but this is something you always had come up with was eggs.

12
00:02:24,430 --> 00:02:30,470
You can use one piece.

13
00:02:32,100 --> 00:02:42,000
What is this? Oh, let's start with our bonds.

14
00:02:42,090 --> 00:02:57,780
The other side of 60% or bonds and their moms of grape casseroles.

15
00:03:00,750 --> 00:03:04,860
Yeah, yeah.

16
00:03:04,980 --> 00:03:08,190
They come up with a food and he's got.

17
00:03:09,410 --> 00:03:18,250
Look here. Yeah. And it used to be something like green screen.

18
00:03:18,600 --> 00:03:21,810
But then it's useless like to draw from before.

19
00:03:23,580 --> 00:03:41,489
It sounds really cool. Way to put into question what's going to happen to us let's start out board so so that first half not

20
00:03:41,490 --> 00:03:51,920
everybody throws or second hand that's because I mean we all are you know why I like this is all going on?

21
00:03:55,500 --> 00:03:59,720
Both You want it to work, they need to bring kids.

22
00:03:59,850 --> 00:04:03,690
300, that was very great.

23
00:04:05,160 --> 00:04:36,300
Yeah, I don't know about that, but you can tell me that you could put it in do the Oh, I love making these like and I don't even like,

24
00:04:36,570 --> 00:04:53,390
I don't even like maybe like he's like, well, I'm sorry you have such a humble initials you take them right away with.

25
00:04:53,400 --> 00:04:56,970
You can see you've never had one because that's a right.

26
00:04:57,690 --> 00:05:01,200
Then you keep. Yes.

27
00:05:01,970 --> 00:05:10,090
Mr. Taylor, you want to do.

28
00:05:17,040 --> 00:05:25,460
You say it was just useful in the garden?

29
00:05:25,500 --> 00:05:29,260
Sure. How did it go after garlic?

30
00:05:30,190 --> 00:05:36,230
Garlic, ginger, garlic, cloves, garlic, minced garlic.

31
00:05:38,810 --> 00:05:44,640
And you've doctor tender stuff frozen in stock at the moment.

32
00:05:46,260 --> 00:05:52,030
And the student had graduated about 15 months ago. But at the same time, she's left.

33
00:05:52,030 --> 00:05:55,329
She couldn't actually find a room and she couldn't figure out her password.

34
00:05:55,330 --> 00:06:01,510
So after you leave, your memory goes. But anyway, so welcome she's getting to since she's working.

35
00:06:03,880 --> 00:06:09,130
Hello everyone. Sorry for the delay. I updated my slides yesterday till the last minute.

36
00:06:10,330 --> 00:06:14,000
I graduated last year. October.

37
00:06:15,370 --> 00:06:20,109
I went. I went to Harvard about that for a postdoc.

38
00:06:20,110 --> 00:06:25,929
I just started my second year of postdoc and this is my reason to work with my stock advisor.

39
00:06:25,930 --> 00:06:31,570
Reply at Harvard about that. This was the work over this past summer,

40
00:06:31,780 --> 00:06:39,460
a transfer learning approach based on random forest with application to breast cancer prediction, a underrepresented population.

41
00:06:40,240 --> 00:06:46,390
And as you can see here, I highlighted for you four different colors, for different key elements.

42
00:06:46,690 --> 00:06:49,990
And I will introduce each of them one by one.

43
00:06:52,320 --> 00:06:55,380
A very important acknowledgment.

44
00:06:55,770 --> 00:06:59,220
This is the joint work with my postdoc advisory board.

45
00:06:59,490 --> 00:07:08,020
She's amazing. She's an assistant professor and also from Shanghai University.

46
00:07:08,040 --> 00:07:12,240
She's an undergraduate research assistant. She's also being very helpful.

47
00:07:15,530 --> 00:07:25,280
Okay. This is the title. And our goal is to field our risk prediction prediction model using our target data,

48
00:07:25,340 --> 00:07:31,220
Y and X, and we are targeting for those underrepresented the population.

49
00:07:31,580 --> 00:07:39,170
So the first question is who are those in the population and why is it important to target for them?

50
00:07:42,960 --> 00:07:47,310
This is a famous figure in the genomics research,

51
00:07:47,550 --> 00:07:55,140
as you can see in red are the European populations and there are overrepresented in the genomic research.

52
00:07:55,470 --> 00:08:01,740
Those are the number of the study, the participants in the July study, Genome Wide Association study,

53
00:08:02,130 --> 00:08:09,900
and a disproportionate majority, over 78% of them are of European descent.

54
00:08:10,710 --> 00:08:15,750
And all the other colors represented for those underrepresented populations.

55
00:08:16,050 --> 00:08:22,530
We mostly refer to those of non-European populations, and in particular to minorities.

56
00:08:22,530 --> 00:08:26,820
And racial ethnicity is of group.

57
00:08:28,580 --> 00:08:37,700
And they also have a lack of representation in the large biobanks and those biobanks clerk patients genomic genotype

58
00:08:37,850 --> 00:08:45,739
samples and there are survey data and can be linked with electronic health records and you can see highlighted in yellow,

59
00:08:45,740 --> 00:08:53,000
those are the percentage of the European population there, or over 70%, 80%.

60
00:08:53,000 --> 00:08:58,969
Sometimes in a UK Biobank, it's over 90% on the bottom of the table.

61
00:08:58,970 --> 00:09:09,920
There is more recently Bute Biobanks such as Total Mad and all those those contains like in total Galway contains more other populations,

62
00:09:10,190 --> 00:09:18,260
but even though there are still over, over half percent of them are of over 50% of them are European population.

63
00:09:18,770 --> 00:09:26,420
And highlighted in red are those underrepresented population African-America, Hispanics, Asians.

64
00:09:26,690 --> 00:09:40,919
Those are like little numbers. This tab is also as also exist in other body omics research where the reservoir represents

65
00:09:40,920 --> 00:09:47,700
the working population and other bars are the little represent the popular population.

66
00:09:48,460 --> 00:10:03,810
Now what is more challenging is the model we built based on European population has very little transferability and the generalizability,

67
00:10:04,140 --> 00:10:10,320
which means they cannot be directly applied to those underrepresented populations.

68
00:10:10,740 --> 00:10:18,420
This is a distribution of the risk score, which is which measures additive genetic risk.

69
00:10:18,810 --> 00:10:26,780
And you can see the clear mean shift across different populations and the different shape and height of this distribution,

70
00:10:28,050 --> 00:10:38,490
which means we it is critical to build models and view the statistical methods that can increase the

71
00:10:38,790 --> 00:10:46,080
risk prediction of those populations as all as well as addressing those differences across populations.

72
00:10:48,710 --> 00:10:54,170
So that's the first sequester. We. The second one is our solution.

73
00:10:54,590 --> 00:11:00,440
So we aim to address such lack of representation through data integration methods.

74
00:11:01,100 --> 00:11:10,309
And we in this work, we want to transfer the shared knowledge from other ancestry like European population who are overrepresented,

75
00:11:10,310 --> 00:11:19,639
has larger sample size. The information from them from both data as the source data to improve the risk risk model

76
00:11:19,640 --> 00:11:28,160
view risk prediction model we our target population because of those representation of facts.

77
00:11:28,430 --> 00:11:31,730
Our target data is a small, relatively small sample size.

78
00:11:32,120 --> 00:11:36,320
If we use this target data alone, the performance will be limited.

79
00:11:36,650 --> 00:11:41,660
And that's the reason we want to borrow information from all outside.

80
00:11:43,660 --> 00:11:49,900
In this work, we will leverage the shared knowledge from adverse populations and the you our other works.

81
00:11:49,930 --> 00:11:58,360
We also have other methods to integrate larger bodies of data from multiple health care centers,

82
00:11:58,840 --> 00:12:08,390
which is feasible because there's a lot of Biobank data has been built throughout the world in this.

83
00:12:09,160 --> 00:12:14,050
In this figure, you can see there's 24 biobanks globally.

84
00:12:14,440 --> 00:12:24,610
There's a global biobank, institutional biobank with different origins and ancestries that have joined this global Biobank initiative.

85
00:12:25,750 --> 00:12:30,400
There's data all over the world represents different populations.

86
00:12:30,700 --> 00:12:38,140
Even though before you the table we saw each of them, the percentage of underrepresented population is low.

87
00:12:38,470 --> 00:12:45,100
But if we have ways to combine them together while addressing challenges such as cross

88
00:12:45,100 --> 00:12:52,270
institutional data heterogeneity and how to address the data protection privacy issue,

89
00:12:52,660 --> 00:13:07,300
then these data are of great value. There's this Massachusetts general, Brigham Biobank is one of the data that I've be using quite often.

90
00:13:07,740 --> 00:13:12,900
It is part of the Emerge Network, which is institutional based biobank.

91
00:13:14,100 --> 00:13:24,960
It has participants from 11 different sites across the United States, including including this plus this MGB Biobank from Harvard.

92
00:13:25,290 --> 00:13:33,300
There's also different hospitals from University of Washington all over the place, all in this particular project.

93
00:13:33,780 --> 00:13:42,329
I used we used a UK Biobank, which has a mostly European population,

94
00:13:42,330 --> 00:13:51,000
and we view the model for the South Asia and the U.S. as well as the affirmed African ancestry.

95
00:13:51,450 --> 00:13:56,460
So those are two of the under underrepresented populations in this data.

96
00:13:56,700 --> 00:14:05,070
And we borrow the information from the source European ancestry, which has so much larger sample size.

97
00:14:07,880 --> 00:14:10,940
And we said we are going to use trustful word approach.

98
00:14:11,360 --> 00:14:15,740
And what is transforming? It is a data integration technique.

99
00:14:16,130 --> 00:14:19,490
It is a term invented in a machine learning field.

100
00:14:20,240 --> 00:14:32,690
Generally speaking, it's a process where a model trained on one problem is used to it's used in some way or a second related problem, for example.

101
00:14:32,960 --> 00:14:37,370
It has been widely used in using image recognition.

102
00:14:37,670 --> 00:14:51,979
If you build a model, a classifier on some common animals such as cats, and you can use that as a base to view them more fancy or really more fancy.

103
00:14:51,980 --> 00:14:55,730
Classifier on rare wild animals relating to cats.

104
00:14:56,180 --> 00:15:02,479
So for another example is in the deep learning, in a deep transfer learning in my Thursday,

105
00:15:02,480 --> 00:15:13,310
also of the autumn for entering our newer network, a large dataset and they used that partially or fully applied to all related problem.

106
00:15:13,550 --> 00:15:21,740
They just directly used up retrieved the first a couple of layers and they used their target data to refine the last couple of layers.

107
00:15:22,430 --> 00:15:28,550
So as the newly trained model is more aligned with the target new problem.

108
00:15:29,820 --> 00:15:38,040
And this transfer learning technique is very suitable in our setting because we have a small sample size in our target

109
00:15:38,040 --> 00:15:47,339
population and it can transfer to share the knowledge from from pre-trained models from the European population,

110
00:15:47,340 --> 00:15:57,900
the larger source population. And what we are transporting here in our study is the correlated information from another ancestry,

111
00:15:58,860 --> 00:16:05,460
which is the European population of European ancestry through the underlying share the genetic architecture.

112
00:16:08,290 --> 00:16:16,870
For example, in the first the role we have, we want to predict the risk of breast cancer in African ancestry.

113
00:16:16,900 --> 00:16:26,230
This is our outcome, wide target outcome, and our predictors are both habitable factors and not heritable factors.

114
00:16:26,770 --> 00:16:32,919
However, what factors are those genetic variants? The danger is really high.

115
00:16:32,920 --> 00:16:36,370
Could be about 100,000 or even more.

116
00:16:37,360 --> 00:16:44,140
The non heritable factors are clinical factors such as age, gender, race,

117
00:16:45,010 --> 00:16:53,900
family histories and the person rho is our target population in green and the second row is a source population.

118
00:16:54,310 --> 00:17:04,840
All in all, and the data represents there the effect size of this genetic variants on the outcome and they are our own.

119
00:17:05,020 --> 00:17:14,020
There are some that are somehow correlated and there's a lot there's a lot of existing research showing the correlated structure.

120
00:17:14,630 --> 00:17:24,550
But because of the high devotion and this complicated curly correlated structure, the relationship between them might not be unique here.

121
00:17:25,060 --> 00:17:31,180
That's why we want to use that's why we want to try to make we don't want to try

122
00:17:31,180 --> 00:17:36,820
to make any distributional assumptions on the mean structure of why give an X.

123
00:17:37,660 --> 00:17:44,710
That's also one of the reasons we want to use random forest, which makes no distributional assumption.

124
00:17:47,690 --> 00:17:51,230
Yes. 35 bags of people without breasts. All right.

125
00:17:51,530 --> 00:17:58,700
So breast cancer patients are just people. So how do you we we can use their electronic health record,

126
00:17:58,700 --> 00:18:07,340
their ICD codes to their of through their visits to the hospital to decide whether they are have breast cancer diagnosis.

127
00:18:07,820 --> 00:18:11,690
So it contains a lot of different disease information.

128
00:18:11,720 --> 00:18:15,950
You can define the outcomes based on their case in control.

129
00:18:16,640 --> 00:18:17,720
I will talk about later.

130
00:18:18,150 --> 00:18:29,470
So this those biobanks are great resource can do analysis on a lot of different types diseases treats whatever you are interested in.

131
00:18:32,100 --> 00:18:40,620
In case you are not familiar with Random Forest, our test for our first full learning approach is based on random forest,

132
00:18:41,070 --> 00:18:48,200
and this random forest is an ensemble of multiple trees.

133
00:18:48,990 --> 00:18:57,420
So each tree, random list, woods, subset, all features from all of our acts.

134
00:18:58,350 --> 00:19:03,180
So for each tree, it starts with a subset, a random subset to all features.

135
00:19:04,080 --> 00:19:12,960
This means many of these trees might not be that useful because features has different importance.

136
00:19:13,140 --> 00:19:20,270
Some feature are important. More important have larger larger data, larger, in fact, size on all counts.

137
00:19:20,490 --> 00:19:33,660
So if your tree sucks at a lot of low or smaller importance of x, then this tree will be somehow useless.

138
00:19:34,320 --> 00:19:37,860
All that's there for each tribute.

139
00:19:40,200 --> 00:19:43,560
We own the random forest combined them together.

140
00:19:43,830 --> 00:19:47,760
Every time you have a new a new patients you want to predict,

141
00:19:48,150 --> 00:19:56,100
it will go through each of the tree, the body, for the final results based on the frequency.

142
00:19:57,350 --> 00:20:02,150
And it is it has many advantages, this tree based method.

143
00:20:02,450 --> 00:20:06,650
It is model free. It can have no nonlinear relationships.

144
00:20:07,460 --> 00:20:13,220
Also, as I said, it can learn the future importance after a fishing boat or tree.

145
00:20:13,370 --> 00:20:20,270
It will tell you which feature which is more important in the whole model beauty.

146
00:20:20,870 --> 00:20:27,980
And it also has fairly good interpretability compared to other machine learning models.

147
00:20:28,730 --> 00:20:39,700
All random forest has been widely used in biomedical research for risk prediction, disease diagnosis, digital phenotype and other areas.

148
00:20:42,340 --> 00:20:47,920
The last AP element in the title is was application to Breast Cancer Prediction.

149
00:20:48,730 --> 00:20:57,400
Oh, it is. Breast cancer is the second leading cause of cancer death among women all over overall in the United States.

150
00:20:57,880 --> 00:21:02,290
And it is the leading cause of cancer death among Hispanic women.

151
00:21:02,710 --> 00:21:08,230
And the research also shows there are some severe subtype of breast cancer,

152
00:21:08,620 --> 00:21:14,140
has higher problems in underrepresented populations such as African-American women.

153
00:21:14,500 --> 00:21:18,850
And they have higher problems, more severe disease.

154
00:21:18,850 --> 00:21:31,240
But they have like a poor risk prediction towards was was less predictive and the prevention ability that's also one of the importance in this work.

155
00:21:34,470 --> 00:21:44,790
So that's all the background. I will now introduced a specific problem setting in our our proposed framework.

156
00:21:46,730 --> 00:21:56,420
Our target model is a small limited sample size size the data for underrepresented populations such as African ancestry.

157
00:21:56,990 --> 00:22:03,600
And our goal is to build a random forest model to estimate this condition domain structure.

158
00:22:03,620 --> 00:22:10,460
Why give an X? You cannot give us X because it's because we use a random forest.

159
00:22:10,910 --> 00:22:22,130
It has no linear relationship. This function has knowing the relationship and the t the subscript T represents for target.

160
00:22:22,760 --> 00:22:25,760
We will use s later represents for our source.

161
00:22:27,860 --> 00:22:36,290
We can start before we can use this data alone. We can also achieve our goal of building this model by using this data alone.

162
00:22:37,200 --> 00:22:48,300
But due to a sample size, small sample size or the performers might not be that great and the model is not reliable or has limited power.

163
00:22:48,540 --> 00:22:56,190
That's why we want to borrow information from larger sample size, larger sample size data, other European population.

164
00:22:56,880 --> 00:23:03,490
And this is why and what I annex here. Why is our target outcome?

165
00:23:03,540 --> 00:23:16,650
For example, the breast cancer diagnosis and acts on a sub before it's a combination of oh genetic variants and non heritable factors.

166
00:23:17,430 --> 00:23:20,670
This is a case control study outcome.

167
00:23:20,880 --> 00:23:30,730
Yes, it can also be continuous measures. For example, some traits in lipids to measure all of the what those of the.

168
00:23:30,780 --> 00:23:36,189
It also can be continuous. Access to predictor and we want to find a model.

169
00:23:36,190 --> 00:23:42,679
Use X to predict Y. US same in those source data we have the Y.

170
00:23:42,680 --> 00:23:47,960
S and the access there to the same outcome in the same covariates.

171
00:23:48,440 --> 00:23:52,340
This is very feasible, even genetic risk prediction.

172
00:23:52,550 --> 00:23:56,840
Because all human beings we have the same set of genetic variants.

173
00:23:57,770 --> 00:24:04,670
So usually we assumed that the acts, the predictors, the genetic variants are the same.

174
00:24:05,900 --> 00:24:15,920
And they also offered to increase the model of the proposed two methods their applicability and usability.

175
00:24:16,730 --> 00:24:21,050
They're not going to use their individual level data.

176
00:24:21,290 --> 00:24:26,600
So the source data, we don't require their all patient level information.

177
00:24:26,900 --> 00:24:31,170
We only need their fitted model. So they fit a breadth.

178
00:24:31,370 --> 00:24:41,660
They fit our random forest model y. Ask about access and the information we need is this fitted random force model.

179
00:24:41,990 --> 00:24:55,370
And they're of variable importance for us so that we can protect the data privacy because we don't have we don't need their access, our patient data.

180
00:24:55,760 --> 00:25:09,709
And this is more more is easier to be applied to cross center data integration, because data sharing has a lot of restrictions to protect data,

181
00:25:09,710 --> 00:25:15,380
to reduce the communication across centers, though, there's a lot of constraints.

182
00:25:19,270 --> 00:25:29,160
So then the goal is to use both the target data with which we have the patient level data as well as the fitted to source model,

183
00:25:29,170 --> 00:25:37,630
this mass x as well as as well as the variable importance for close to both our summary level information.

184
00:25:41,900 --> 00:25:46,970
This is the skeleton, the structure of our proposed a method.

185
00:25:47,510 --> 00:25:50,930
We propose three different transfer learning models.

186
00:25:51,320 --> 00:25:54,740
Each correspond to three different scenarios.

187
00:25:56,060 --> 00:26:09,980
Those scenarios are carefully considered to provide practical in practice and also scenario zero, which is our target only model.

188
00:26:10,850 --> 00:26:15,410
Why is it important to also consider the target only model?

189
00:26:15,740 --> 00:26:24,590
Target only model is the model that is out. We only use the target data without incorporating any external information.

190
00:26:25,130 --> 00:26:34,430
It is important because there is this final phenomenon called negative transfer, which is after you transfer the information.

191
00:26:34,700 --> 00:26:39,470
The model performance is even worse than you not combining other information.

192
00:26:40,010 --> 00:26:47,180
This this is this can happen when the source data has very different distribution.

193
00:26:47,510 --> 00:26:53,510
There are different level of data heterogeneity and so different from your own data.

194
00:26:54,050 --> 00:27:01,130
Moreover, information is no, no, no help compared to not performing it.

195
00:27:02,700 --> 00:27:10,380
So we have all four of these models in total to improve the robustness to prevent negative transfer.

196
00:27:10,680 --> 00:27:23,430
We combine all these four together by calculating data driven weights of the ensemble of all these four models of this,

197
00:27:23,430 --> 00:27:31,620
because in reality we do not know which scenario is the true scenario or if there is no way to test, to test.

198
00:27:31,980 --> 00:27:35,550
And we will show later a simulation and the real data.

199
00:27:35,790 --> 00:27:39,870
Each methods works the best in different scenarios.

200
00:27:40,170 --> 00:27:47,940
So by Ensemble de Vie by the ensemble of all these, we can further improve the prediction performance.

201
00:27:49,170 --> 00:27:54,630
And the robustness because we also hold us in our role zero zero, which is the target only.

202
00:27:54,930 --> 00:27:58,830
So the final model trends random forest.

203
00:27:59,040 --> 00:28:08,069
This is our name. Trans RF will be no worse than the target only models performance and will have

204
00:28:08,070 --> 00:28:14,310
improvement if there there's any shared information across target and source information.

205
00:28:16,890 --> 00:28:20,490
Below. I'll go detail into each of the scenario.

206
00:28:20,850 --> 00:28:27,270
So the scenario zero I already introduced, this is when the target and the source model are highly different.

207
00:28:27,660 --> 00:28:33,450
That's why we need this to protect the protect from the negative transfer.

208
00:28:35,440 --> 00:28:41,519
And, you know, first, the scenario we considered is a situation where the source of the target model,

209
00:28:41,520 --> 00:28:51,090
they have similar features importance of in this scenario we will only use the importance score s from the source model.

210
00:28:52,660 --> 00:29:02,710
What this means is back to the previous example, the important variable means to have large absolute value off impact size.

211
00:29:03,040 --> 00:29:12,970
So this paid off or no matter what sign they have, either either positive or negative, as long as they have large effect size,

212
00:29:13,180 --> 00:29:19,720
absolute value of effect size, they will they're considered as important because they contribute more to the outcome.

213
00:29:20,750 --> 00:29:25,250
Or the Ori the random forest. They can be the early split in trees.

214
00:29:25,460 --> 00:29:37,820
Those variable are important variables. And what we are borrowing here is we are we are assuming we are assuming the ranking

215
00:29:37,820 --> 00:29:44,200
of these betas are similar across two populations in the statistical landbridge.

216
00:29:44,450 --> 00:29:49,010
This is the Kendall correlation of their absolute effect size.

217
00:29:49,700 --> 00:29:58,520
They have similar ranking of the effect size, which which is practical in genetic research.

218
00:29:59,270 --> 00:30:07,160
They can have totally different magnitude of their effect size, different sign, positive and active.

219
00:30:07,640 --> 00:30:14,450
But as long as they have similar ranking, they their candle correlation is high.

220
00:30:14,780 --> 00:30:21,379
Then we can directly use trained importance or from the source model to guide

221
00:30:21,380 --> 00:30:32,330
our order to guide our target trust a random forest by forming a random clause.

222
00:30:32,580 --> 00:30:39,110
I guess what I don't know what betas are. Yes, this is because they don't have beta.

223
00:30:39,560 --> 00:30:51,140
Then it's harder to have the intuition. I'm using the beta from the parametric model as the intuition to help you understand what are we doing here?

224
00:30:51,920 --> 00:30:57,050
Otherwise, it's random forest. They're just doing everything underlying a black box.

225
00:30:57,950 --> 00:31:04,390
This is the rock to a connect connection between the of consider.

226
00:31:04,430 --> 00:31:13,280
You have a linear regression. The beta will be the the regression linear regression speed up or we have x.

227
00:31:13,280 --> 00:31:20,060
We can have different nonlinear transformation on X than the model.

228
00:31:21,570 --> 00:31:25,650
The linear transformation can be linear inc with stalled.

229
00:31:27,230 --> 00:31:32,120
We That's true. We we don't really have this beta in the random forest.

230
00:31:39,700 --> 00:31:47,440
How do we know? The next question is how do we tell the forest that we already have this variable important score?

231
00:31:47,680 --> 00:31:56,120
How do we incorporate this information? We use that as the weights are fitting the random forest.

232
00:31:57,250 --> 00:32:00,250
So back to this Queens.

233
00:32:00,250 --> 00:32:07,569
I'm telling each tree when random forest is fitting each tree at random with a subset of feature with each quote,

234
00:32:07,570 --> 00:32:16,420
you pull weights by giving this verbal importance score as we will give each feature different ways.

235
00:32:16,990 --> 00:32:30,580
Then when it wrote, well, it's society wasn't selecting the subsets, those important features, all that was showing no source of source model.

236
00:32:30,730 --> 00:32:35,020
Well have higher chose being selected to fit each tree.

237
00:32:36,190 --> 00:32:39,700
Then we will reduce our chance of those useless trees.

238
00:32:39,940 --> 00:32:41,500
Well, for those.

239
00:32:42,400 --> 00:32:52,660
For those features, with low probability for those trees with low importance score, it will have lower probability of being selected each time.

240
00:32:53,550 --> 00:32:59,840
Oh, that is tell. That is how we tell you that the poorest, those futures are more important.

241
00:33:02,370 --> 00:33:11,820
So this is the first scenario. We only use the important score and we also have this fitted random forest model.

242
00:33:12,690 --> 00:33:18,780
How can we best utilize this fitted random forest model arms hat?

243
00:33:19,560 --> 00:33:25,620
So for each observation, our target data, we can obtain their predicted outcome.

244
00:33:26,190 --> 00:33:34,320
I plug the X into the fitted source model and we will have there estimated to have as.

245
00:33:35,790 --> 00:33:42,540
And one approaches, we can directly use this function and function,

246
00:33:42,960 --> 00:33:53,550
which means we can directly use the model fitting European population to our Africa ancestry population.

247
00:33:54,120 --> 00:34:02,010
But we saw earlier they have they have little generalizability, they have large difference.

248
00:34:02,670 --> 00:34:07,920
They're actually using them using this model well lead to biased results.

249
00:34:08,280 --> 00:34:12,030
So we want to correct this bias. How do we do that?

250
00:34:12,030 --> 00:34:20,370
We calibrate this source model by learning this discrepancy, a discrepancy term required delta.

251
00:34:22,770 --> 00:34:27,750
So here we have the target model and we also have the fitted a source model.

252
00:34:28,230 --> 00:34:36,420
I'm also well with both the genetic variants, X as well as other clinical factors.

253
00:34:37,470 --> 00:34:45,870
And we define this delta term. Oh, did you set out the difference of the outcome of the two models?

254
00:34:47,940 --> 00:34:55,080
So in step one, we use our target data, Y and X to learn this discrepancy term.

255
00:34:56,280 --> 00:35:04,650
What X means in continuous outcome. When we have continuous measure, it is modeling the mean difference in all column.

256
00:35:05,370 --> 00:35:10,710
While if we have binary outcome, for example, the breast cancer diagnosis,

257
00:35:10,950 --> 00:35:16,889
it's a binary outcome either case control zero or what this discrepancy term is

258
00:35:16,890 --> 00:35:23,550
to model whether we would need to flip between zero and when we apply this,

259
00:35:24,450 --> 00:35:30,950
when we apply the source model to our target data. Given this delta that we learned.

260
00:35:31,610 --> 00:35:42,200
And in the second step, we can multiply it with the external source model EMS hat in order to achieve our target model.

261
00:35:43,150 --> 00:35:50,980
Because based on a definition of the target model, empty equals two mass plus don't.

262
00:35:55,380 --> 00:35:59,970
So back to this diagram. The step one is to learn the delta.

263
00:36:00,940 --> 00:36:06,190
Because we have this hat estimate and source model estimation.

264
00:36:06,670 --> 00:36:17,709
So we used this residual term, our target Y, the Y, our target data Y minus the estimates.

265
00:36:17,710 --> 00:36:23,740
The source estimates have us and the start of modeling on the original outcome.

266
00:36:23,740 --> 00:36:36,700
Y We will use this residual term us as our outcome, which is the Delta hat, and we use X to predict this Delta function.

267
00:36:38,050 --> 00:36:48,820
And after we receive that, we kill plus plus add together with the source model to have our final target the model.

268
00:36:49,180 --> 00:36:57,660
And we call this model model to. There is another interpretation of this approach.

269
00:37:00,360 --> 00:37:08,520
This so does her term as independent or weakly correlated with the source model,

270
00:37:08,850 --> 00:37:17,070
meaning that the delta function captures the complementary information of the source model.

271
00:37:17,550 --> 00:37:20,820
This is the most efficient way when we feed this model.

272
00:37:22,800 --> 00:37:33,830
The. What's that mean? So, you know, in this example, when we're following this discrepancy term,

273
00:37:35,300 --> 00:37:43,850
this just discrepancy term might be related only to a subset of our covariates,

274
00:37:44,540 --> 00:37:58,160
their color, their effect size, maybe sparse, maybe sparse, which means a lot of them are not contributing to this delta.

275
00:37:58,670 --> 00:38:06,320
So a lot of the covariates are not contributing to the difference between Target and the source model.

276
00:38:06,710 --> 00:38:10,040
Only a handful of them are contributing.

277
00:38:10,790 --> 00:38:23,740
And if this is the case, then there are clean model modeling on this difference will be the most efficient way to to achieve our final, final goal.

278
00:38:26,720 --> 00:38:32,810
While correspondingly there's another case when they're correlated,

279
00:38:33,140 --> 00:38:40,400
which means the delta term does not only explain the complementary information, it could.

280
00:38:40,640 --> 00:38:46,250
It also is correlated with the source function.

281
00:38:47,420 --> 00:38:51,560
Just considering when you are calculating this difference,

282
00:38:51,740 --> 00:39:02,090
you minus the information from source that can be explained by the source model for this delta term store correlated with the source function,

283
00:39:02,930 --> 00:39:08,090
which means the source function could be an important feature for predicting this

284
00:39:08,390 --> 00:39:15,200
delta outcome so as to predict to the final outcome why we are in this case.

285
00:39:15,350 --> 00:39:25,670
We propose to directly using the source model prediction as a feature or as well as it is an important feature.

286
00:39:26,090 --> 00:39:32,660
So we assign larger weights to this to this source model prediction.

287
00:39:33,350 --> 00:39:37,159
So you can see the model become the original outcome.

288
00:39:37,160 --> 00:39:50,360
Why, with our predictors X and the source estimates hat to us and we will assign larger weights to this source prediction.

289
00:39:51,170 --> 00:39:59,060
And for all the access that we have, it can help you call equal probability being selected in random forest.

290
00:39:59,690 --> 00:40:06,260
Or we if we have other prior information, we can also assign different weights to that.

291
00:40:07,790 --> 00:40:14,510
And those are both are those three practical scenarios being considered.

292
00:40:17,030 --> 00:40:20,750
The last stop has to combine all of them together.

293
00:40:21,380 --> 00:40:24,560
What we do is there's a theoretical a fear,

294
00:40:24,570 --> 00:40:32,690
a theoretical guarantee that we can train away a set of weights to linearly combine

295
00:40:32,690 --> 00:40:39,350
them and that the performance will be no worse than the the best a single model.

296
00:40:39,920 --> 00:40:44,630
So what we do is we split a small sample from our target data.

297
00:40:45,380 --> 00:40:49,050
We know our target data already have a limited sample bus,

298
00:40:49,070 --> 00:40:58,550
but we still need a very small number of them to train the weights so that we can combine all these four models together.

299
00:40:59,180 --> 00:41:09,830
So given this small validation data, we can predict to the models each model's outcome and we put it into a linear model.

300
00:41:10,460 --> 00:41:20,030
So the outcome would be the true observable outcome, and the predictor will be each model's prediction.

301
00:41:20,630 --> 00:41:28,430
Then just in a linear regression, we will we can estimate their base, their estimates,

302
00:41:28,430 --> 00:41:35,210
their effect size, and we use that as the weights to combine these four different models.

303
00:41:38,530 --> 00:41:44,019
The ends you get for me. You got a numbers between zero and 100 prediction probabilities.

304
00:41:44,020 --> 00:41:47,830
Is that what you get from each of the scenario models?

305
00:41:48,610 --> 00:41:52,640
Yes. Also, we can have the estimate of probability, I think.

306
00:41:55,040 --> 00:42:02,840
Yes, the estimated probability. So you just take a weighted average of various estimates of the risks for each model?

307
00:42:02,860 --> 00:42:06,140
Yes. As the final result.

308
00:42:07,920 --> 00:42:10,980
Any questions so far? Before I go to the simulation.

309
00:42:14,340 --> 00:42:18,720
This. Oh. Okay. Oh.

310
00:42:18,730 --> 00:42:20,740
Simulation results is trivial.

311
00:42:20,760 --> 00:42:34,450
So we use a simulation to demonstrate to the validate to validate the ability of our method, how our method outperforms others.

312
00:42:35,440 --> 00:42:40,720
Here, in each scenario, we propose to three different scenarios, three different scenarios,

313
00:42:41,020 --> 00:42:49,930
and we conducted three sets of all simulations to show that in each scenario, each model performs the best.

314
00:42:50,500 --> 00:42:55,570
So you study why model one performs best, and after all the ensemble,

315
00:42:55,660 --> 00:43:00,580
the trends are the proposed are trends are off well further improve the performance

316
00:43:00,580 --> 00:43:06,430
a little bit on the y axis as to MSE compared to the target only model,

317
00:43:06,700 --> 00:43:16,980
we use that as a baseline reference on the out and on the x axis we just a changed like how used for as

318
00:43:16,990 --> 00:43:27,280
the source model which you see in a map notation as the variance explained by the source model of all.

319
00:43:27,760 --> 00:43:36,969
But we set it to a low, medium and high or high means the source model are more helpful to the target setting

320
00:43:36,970 --> 00:43:44,590
to model to is the state most useful and in model three you can set history.

321
00:43:44,590 --> 00:43:59,870
Model three is the most useful. When we apply this to predicted risk of South Asian breast cancer risk as well as African ancestry risk,

322
00:43:59,870 --> 00:44:06,080
we define our case under control using the UK Biobank data target.

323
00:44:07,490 --> 00:44:11,300
We use those to population target by task.

324
00:44:12,590 --> 00:44:17,840
So in South Asia we have full South Asian and African-American.

325
00:44:17,840 --> 00:44:23,330
We have around 450 samples, which are a very small sample size.

326
00:44:24,260 --> 00:44:28,510
We define the case using the ICD code from the electronic health record,

327
00:44:28,970 --> 00:44:38,390
and the controls are those who do not have breast cancer or other related female cancers cause they are sure to be highly correlated.

328
00:44:39,590 --> 00:44:43,190
And the source model is the large European population.

329
00:44:43,370 --> 00:44:49,700
We have over 40,000 samples with over 50 over 14,000 cases.

330
00:44:51,100 --> 00:45:00,340
The predictors we used was around 300 novel snips that have been previously previously identified in the

331
00:45:00,700 --> 00:45:08,950
Genome Wide Association study and other heritable predictors was shown to be associated with the outcome.

332
00:45:11,840 --> 00:45:23,330
This is the South Asia model result. The middle dashed vertical vertical line represents the model zero, the target only model.

333
00:45:23,660 --> 00:45:30,410
And if the line is on the left direction, which means a worse performance,

334
00:45:30,710 --> 00:45:38,810
you'll see performance compared to the target only model if it's on the right hand side, which means it has improved the performance.

335
00:45:39,470 --> 00:45:43,280
In this case, we also consider another SDR model,

336
00:45:43,460 --> 00:45:52,250
which is a random forest model that directly borrowed the tree structure from the Pre-Trained random forest model.

337
00:45:52,940 --> 00:46:03,740
This is an existing method and we will see the performance of this comparison is not as good as others,

338
00:46:04,720 --> 00:46:12,140
or maybe due to their different tree structure of a cross population between Europe and South Asia.

339
00:46:12,500 --> 00:46:19,520
And in this case, model three, by directly using the source source prediction as one of the predictor,

340
00:46:19,520 --> 00:46:23,600
has the best performance as well as star trends are of.

341
00:46:25,700 --> 00:46:33,420
You know, when we use African-Americans to target a population, the usefulness of the model changed.

342
00:46:33,790 --> 00:46:37,689
We have model one and two has better performance in Africa.

343
00:46:37,690 --> 00:46:46,149
America. Why? Our Model three has similar performances to target only one of the we did a model.

344
00:46:46,150 --> 00:46:51,220
The way that model here is just naive weighted average target in the source only model.

345
00:46:51,820 --> 00:47:01,720
And when you compare these two figures, you will see across across different ancestry population, different models.

346
00:47:01,870 --> 00:47:08,380
Oh works differently. Each might have their best performance.

347
00:47:10,590 --> 00:47:19,920
Here. We also plot the top 20 important variables of the casino source model, the European model,

348
00:47:20,130 --> 00:47:27,480
as well as to the target sales after sales, Asian and African-American African ancestry.

349
00:47:28,050 --> 00:47:39,480
So I highlighted in red the overlapping snips and oh and below was dig overlapping between the source and the South Asia.

350
00:47:39,900 --> 00:47:43,860
The red list overlap between Europe and Africa.

351
00:47:43,860 --> 00:47:56,490
America, all while the vote block, which is the age of period, was identified as important predictor in all three models.

352
00:47:56,970 --> 00:48:07,440
And we have found a corresponding literature to say to to prove that these snips are associated with

353
00:48:07,440 --> 00:48:17,590
breast cancer in a diverse different ancestry in both European East Asian mostly European East Asian on.

354
00:48:18,000 --> 00:48:25,140
This is the evidence of the correlated genetic obvious arches texture across populations.

355
00:48:27,590 --> 00:48:33,680
All this paper was submitted to a conference, which is the Pacific Symposium on bio computing.

356
00:48:34,070 --> 00:48:40,620
And we got accepted to this Overcoming Health Disparities in Precision Medicine chapter.

357
00:48:41,540 --> 00:48:46,430
And the conference is going to be happens next January in Hawaii.

358
00:48:48,490 --> 00:48:54,190
That's. That's it. If you have questions, you can check out our paper.

359
00:48:54,190 --> 00:48:58,300
It will be on biochar sometime soon. Or you can email me.

360
00:49:04,130 --> 00:49:08,660
Hey, listen, buddy. If.

361
00:49:08,660 --> 00:49:12,590
No question, I have two supplementary slides.

362
00:49:13,850 --> 00:49:21,410
I have to introduce myself. I graduated last year on July 8th.

363
00:49:22,640 --> 00:49:26,180
Supervised, supervised Bibles from our interview.

364
00:49:26,690 --> 00:49:33,260
I think Stormy never saw this picture before. She used this.

365
00:49:33,560 --> 00:49:38,930
Yeah. So my dissertation topic was very similar to what I'm doing right now.

366
00:49:39,230 --> 00:49:47,000
It's a it falls under the same big data data integration umbrella.

367
00:49:47,360 --> 00:49:57,830
And my dissertation was about using external summary level information to improve the internal small data model prediction.

