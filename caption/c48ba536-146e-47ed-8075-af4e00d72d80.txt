1
00:00:01,140 --> 00:00:13,170
To work for linear regression. We have the so-called line assumption and in the energy independence, normality and the equal variance assumption.

2
00:00:13,440 --> 00:00:21,650
So we are here. We are looking at them one by one. There are consequences when these assumptions are violated.

3
00:00:22,020 --> 00:00:25,290
So it's very important to check in practice.

4
00:00:25,290 --> 00:00:30,650
And so that you've heard that. Now, some of you actually asked, Oh, what did I do in What shall we do in the third act?

5
00:00:30,660 --> 00:00:36,390
So we just stuck in a linear regression model. And then we part of the result and interpret the results.

6
00:00:36,630 --> 00:00:45,930
That's it. So no answer there. So the code act, well, we think it's better to think of the product as, you know, a real word.

7
00:00:46,260 --> 00:00:53,610
I know that. Right. So when we build the model, we have to do all the motor diagnostics or I mean, it's arbitrary.

8
00:00:53,610 --> 00:01:01,229
You and I remember. But but if people want to do, you know, different types of model diagnostics and to see the models,

9
00:01:01,230 --> 00:01:05,520
they build it to see if the assumptions are any those objects are validated.

10
00:01:06,090 --> 00:01:13,829
And if the assignment does not hold in how to how to make a some transformation on the data, for example,

11
00:01:13,830 --> 00:01:20,810
or how to make some other revenues to to actually to commission to make sure that the assumption of

12
00:01:20,820 --> 00:01:28,230
housing the and also to do for any of our models like forward backwards action and things like that.

13
00:01:28,240 --> 00:01:36,059
So it's it's not as simple as, you know, just, oh, I have this ten miracles.

14
00:01:36,060 --> 00:01:40,920
I just throw them into the model, I run the model, then I interpret the results.

15
00:01:41,550 --> 00:01:50,610
It's not as simple as that because because you know that nobody knows what the tomorrow is.

16
00:01:50,970 --> 00:01:56,700
So, so we have to go on for four things to come up with a model that we've seen.

17
00:01:58,980 --> 00:02:04,740
So here on this model model, okay, we focus on checking different assumptions.

18
00:02:04,980 --> 00:02:11,190
So we talk about so in our last lecture, we talk about the residual different copies of residuals.

19
00:02:11,190 --> 00:02:20,549
Women talk about four types of residuals in this that we talk about from the bottom up.

20
00:02:20,550 --> 00:02:23,520
That's the raw or conventional residual.

21
00:02:23,820 --> 00:02:31,470
And then we talk about the standardized residual and then internal is still the nice and from the student residuals.

22
00:02:32,940 --> 00:02:39,510
And this one here the problem this is the most sensitive in terms of detecting outliers.

23
00:02:39,750 --> 00:02:50,970
And so and the last one here and also the bottom one, this one, it depends on the unit of so so that's that's one drawback of this rock.

24
00:02:52,440 --> 00:02:59,280
So we talk about this for residuals, oh, by the way, and the midterm covers of into this slide that slide 15.

25
00:03:00,900 --> 00:03:05,430
And then we go without checking linearity and fraternal marriage.

26
00:03:07,050 --> 00:03:18,630
Now let's get a refresh memory. So protecting the RV making plot is one very effective way of checking to error.

27
00:03:19,950 --> 00:03:23,340
Well, we have to be careful exactly what plots we make.

28
00:03:24,960 --> 00:03:38,190
So for a single linear regression, the plot we make is sort of the plot you kind of like what rather residual impact versus either X,

29
00:03:38,880 --> 00:03:44,010
the single covariate or versus, you know, versus y hat here.

30
00:03:45,060 --> 00:03:53,760
So and in terms of checking the narrative, you know, these are equal active.

31
00:03:54,450 --> 00:04:04,590
So the way to check the narrative by looking at this plot is that now if linearity holds or if there is nothing wrong,

32
00:04:04,640 --> 00:04:07,890
your assumption what work is in the narrative assumption.

33
00:04:08,400 --> 00:04:13,540
When you make a plot of errors or have lumberjacks, you're going to see this group.

34
00:04:13,590 --> 00:04:18,149
The pattern that the residuals there are randomly disputed.

35
00:04:18,150 --> 00:04:24,810
There is no clear pattern. There shouldn't be any clear pattern and they should be run in radius around to zero,

36
00:04:25,860 --> 00:04:29,160
absolute people to zero, because that's residuals that is equal to zero.

37
00:04:30,810 --> 00:04:37,020
If your plot you see a clear pattern like this, you're are making out of this example.

38
00:04:37,020 --> 00:04:41,220
But if you see a clear pattern, that means Woody Harrelson is violated.

39
00:04:41,790 --> 00:04:47,189
And we want to check what a function of war of X you want to include is.

40
00:04:47,190 --> 00:04:54,900
For example, if the plot look like this, then you might consider the square a square term of X in the model.

41
00:04:55,710 --> 00:04:59,930
So that is linear assembling your not for multiple linear regression.

42
00:05:00,030 --> 00:05:06,809
Things become a little bit more complex. We cannot invent a simple plot versus a single.

43
00:05:06,810 --> 00:05:10,380
X is not, in fact, in many works.

44
00:05:10,410 --> 00:05:14,010
The reason is that for Moneyball being rational, how different x?

45
00:05:14,010 --> 00:05:17,579
Well, Model X and his model X are, usually they have a correlation.

46
00:05:17,580 --> 00:05:23,630
They're correlated. Sometimes high correlations are not local, but as they are, you know, affordable.

47
00:05:24,610 --> 00:05:37,440
Now, if one covariate, let's say X acting as K created with x ray, then if x j has no linear association with Y, that's not linear association.

48
00:05:37,830 --> 00:05:50,730
But if you made a plaque absorbing match versus X, k, and because of the correlation between S and X and because MJ has a nonlinear association,

49
00:05:50,910 --> 00:06:00,510
then when you make this plot, you will see that the plot is not given me all the strength of would line in different narrative.

50
00:06:00,750 --> 00:06:08,160
So the plot may be not linear, but not that linear is caused by high correlation with the traffic at stake.

51
00:06:08,430 --> 00:06:11,780
And this actually has no linear association with that watch.

52
00:06:12,060 --> 00:06:15,300
So in this case, the point is that if for multiple linear regression,

53
00:06:15,840 --> 00:06:24,840
a simple plot of absolute patch versus one single x may not reveal the true association between X,

54
00:06:25,800 --> 00:06:32,850
between Y and X, whether that's association is linked by whether that relationship is lenient.

55
00:06:33,420 --> 00:06:38,580
So then what do we do in this case is we make the so-called partial regression plots.

56
00:06:39,930 --> 00:06:47,550
Okay part of our plus this is by intuitive it it removes the effects of all the optical barriers.

57
00:06:47,730 --> 00:06:52,350
So what we do here is not suppose we have the design matrix, we have all the X.

58
00:06:53,550 --> 00:06:57,960
So we have, we have x1xp, so p x.

59
00:07:00,060 --> 00:07:10,090
And then we use this notation as some squared minus to denote all the x except x.

60
00:07:10,120 --> 00:07:13,409
Okay, so we will x k from, from this front.

61
00:07:13,410 --> 00:07:19,180
This is our matrix or from this vector as an x minus k compared to all the other text.

62
00:07:20,280 --> 00:07:24,599
So for pleasure result we first regress y on x minus k.

63
00:07:24,600 --> 00:07:35,550
That means for all the x we have to remove x k and then you will run our opinion aggression of Y on all the rest x or the rest x.

64
00:07:36,360 --> 00:07:44,190
And that's the linear regression model present regression of Y on all the other x without asking.

65
00:07:44,970 --> 00:07:52,590
And this notation here, I mean this simply explicitly note this is a partial reason why.

66
00:07:53,010 --> 00:08:03,719
Well, this is a residual for the right of y of x minus K by a operational wise if you want to really carry this out.

67
00:08:03,720 --> 00:08:10,260
It was simply a regression of Y on all the other x x and then get in the residual.

68
00:08:10,260 --> 00:08:13,440
That's that's this residual. That's this residual right here.

69
00:08:13,950 --> 00:08:24,570
And then you run the regression of X, okay, this single covariate, this x Ukrainian, you're treated as, as if it were response, right?

70
00:08:25,410 --> 00:08:34,030
You treat it as a result. So you run a regression, the x came on all the other x and then you get a residual, right?

71
00:08:34,050 --> 00:08:47,130
So this is x k on all the other x and then we get to the residual and then we make a plot of about this residual versus this residual or I mean,

72
00:08:47,400 --> 00:08:58,500
because switch for the second one versus the first one. So and this plot, this is the so-called partial residual partial regarding plot.

73
00:08:58,830 --> 00:09:03,389
This one should reveal the association.

74
00:09:03,390 --> 00:09:11,600
The true relationship between Y and the reason is that both sets of residuals are so

75
00:09:11,610 --> 00:09:17,280
called a covariate a drastic so they remove the effect of all the other covariance.

76
00:09:17,850 --> 00:09:21,030
So we call that all the y you can ask.

77
00:09:21,270 --> 00:09:27,780
They may be correlated with all the other x so that a correlation actually could complicate

78
00:09:28,650 --> 00:09:33,870
things when we try to figure out what the relationship is between life and death.

79
00:09:34,320 --> 00:09:39,600
So what we do is we remove all the impact coming from all the other acts by running

80
00:09:40,170 --> 00:09:44,790
this regression and this regression and fight and finding through residuals.

81
00:09:45,450 --> 00:09:49,020
And then the residuals, our coverage advantage.

82
00:09:50,490 --> 00:09:53,970
So then we make a block sort of work here.

83
00:09:54,330 --> 00:10:06,479
You heard and people made a plot of, you know, the residual of of Y versus part of the partial residual for Y versus the first x y axis.

84
00:10:06,480 --> 00:10:18,120
So this is Y versus X one after removing the effect of x two and this is the Y versus X who after removing the effect of x one.

85
00:10:19,140 --> 00:10:31,230
So and then if there is a linear association like for example, this is a one, this is a example.

86
00:10:31,230 --> 00:10:37,080
So if we look at the the partial regression box, we see that clearly there's a increasing pattern.

87
00:10:37,530 --> 00:10:48,420
This seems to be linear. So in this case, it means that indeed when we this by model, like in this case, this model that is for x one,

88
00:10:51,090 --> 00:10:58,590
for x one, it seems reasonable to divide the suppose to the religion to be linear.

89
00:10:58,860 --> 00:11:04,470
Adjusting for x two, adjusting for as to why for x two is not so clear yet.

90
00:11:04,950 --> 00:11:10,319
But then from this plot here we see that the movement is on x two.

91
00:11:10,320 --> 00:11:16,980
Also, there is a clear decreasing trend. A linear assumption seems to be a good assumption.

92
00:11:17,010 --> 00:11:30,090
So in this case, it's it's it's safe to say that about subheadings all adds to is also linear then then by tacking this to plots,

93
00:11:34,200 --> 00:11:37,890
it seems that this model seems to be a very good model.

94
00:11:38,250 --> 00:11:47,290
And then, of course. You could run this model. And again, I don't have for this absolutely not for this drawing to model and event to make.

95
00:11:58,900 --> 00:12:05,390
So saying that so that you could run this one tomorrow and then get another abstract.

96
00:12:05,590 --> 00:12:16,330
That is from the drawing board. And then you could make a plot of ideas on hand versus either X or versus y hat or versus my hat.

97
00:12:16,750 --> 00:12:23,770
And again, so if this is an unreasonable, more reasonable model than your absolute hat.

98
00:12:23,980 --> 00:12:29,460
If you make a plot. So.

99
00:12:30,220 --> 00:12:39,590
So here, if you've got an abstract and then if you make absolute hand versus either x one or x two or y hat,

100
00:12:40,250 --> 00:12:48,500
if you make such a plot and if this is a really good model, then absolute hat should be randomly distributed around zero.

101
00:12:48,770 --> 00:12:55,620
So you was then able to see that the number is distributed around zero.

102
00:12:55,650 --> 00:12:58,430
There's there shouldn't be any clear pattern. Yeah.

103
00:12:58,570 --> 00:13:09,020
If you see some clear pattern then if to go back to the model and then try to include all the terms I'm about to see the pattern from.

104
00:13:09,620 --> 00:13:16,220
So we go back and forth, back and forth into we think that this whole plot looks random.

105
00:13:18,380 --> 00:13:26,120
So that's the so-called partial regression plots through targeting the R2 assumption for particle.

106
00:13:32,560 --> 00:13:53,430
Okay. And questions. So this is just one example, I mean, a concrete example, but it's just exactly the same procedure as we just described.

107
00:13:53,850 --> 00:14:06,740
So suppose the response is SBP systolic blood pressure and we have two covariates, one is age and one is this index.

108
00:14:07,560 --> 00:14:12,840
There's no way to know about exactly where that is. And also, we have a smoking status.

109
00:14:12,900 --> 00:14:18,860
Yes. So we are interested in feeding this model of.

110
00:14:21,270 --> 00:14:23,340
So three cameras and a wire response.

111
00:14:23,700 --> 00:14:30,600
So this is the model we are interested in feeding, but we are not we are not sure whether indeed this is a good enough model,

112
00:14:31,110 --> 00:14:38,130
because this model just employs all three covariates or linear terms of three covariates.

113
00:14:38,400 --> 00:14:48,150
And so we are not sure whether linear terms are good enough, but the linear terms captures all dependance of one on corresponding x.

114
00:14:48,750 --> 00:14:56,460
So in this case, now we are able to check, for example, let's say you don't know whether as we develop our age in a linear way.

115
00:14:57,570 --> 00:15:05,010
And one way to check this is, well, first we run the model of SBP on the other two covariates,

116
00:15:05,880 --> 00:15:18,330
on the other two covariates and without age and you get the residual and then of you run a second model that where you use age as the response,

117
00:15:19,710 --> 00:15:27,070
the state has the response and then you run a regression of age on the other two covariates and again you get out of the terms of the disease.

118
00:15:28,740 --> 00:15:35,550
After we get this to residuals, then we will make a block of the first residual versus the second residual.

119
00:15:37,870 --> 00:15:43,900
And then we see how much of what the plot looks like and if there's any.

120
00:15:44,550 --> 00:15:54,940
If not, if you see a clear, very clear increasing or decreasing trend and it seems to be a linear trend that it's safe to say that.

121
00:15:55,180 --> 00:15:58,569
Okay, so we should include linear association.

122
00:15:58,570 --> 00:16:01,660
What does the linear term of H into the model matter?

123
00:16:01,660 --> 00:16:06,910
If you see some other dependance on other coverage already for others,

124
00:16:07,240 --> 00:16:17,980
then you might consider adding some other functions of age in the model system to help to further approximate the dependance as we age.

125
00:16:18,910 --> 00:16:25,960
So this is how this partial plots particles should be created.

126
00:16:31,150 --> 00:16:36,610
Okay. So that's one approach of checking the narrative assumption based on plots.

127
00:16:37,510 --> 00:16:44,860
Another one is another approach. We sort of sort of talked about this.

128
00:16:45,550 --> 00:16:49,600
That is to categorize a continuous predictor.

129
00:16:50,530 --> 00:16:56,320
So if we use the same example, if we using example,

130
00:16:57,760 --> 00:17:08,110
so we are interested in seeing whether the dependance of SBP on edge is leaning, adjusting for the other two variables.

131
00:17:08,920 --> 00:17:19,810
So if you directly include edge in the model as continuous, now of course we are forcing this association to be near this relation, to be linear.

132
00:17:20,380 --> 00:17:25,480
And because this is how you specify model, this is how you assume the model that you're forcing that dependance to be.

133
00:17:26,980 --> 00:17:35,020
Now, when we do check whether this this whether indeed the true really should be sitting here is to replace this continuous

134
00:17:35,260 --> 00:17:43,480
variable clarity by categorical covariates and then examine the pattern in coefficient of indicator interval.

135
00:17:45,880 --> 00:17:48,910
So the idea is actually based on this.

136
00:17:50,440 --> 00:17:59,430
This is. So suppose that. Suppose that dependance looks like this, right?

137
00:18:02,540 --> 00:18:05,750
I'm making this up, but I suppose with the balance looks like this.

138
00:18:06,350 --> 00:18:13,310
So this is not linear. I mean, there is an increase in trend, but overall, this is not not a pure anemia.

139
00:18:13,970 --> 00:18:19,720
So but if you if you're adding old age in the model, then you're forcing the dependance of it in here.

140
00:18:20,210 --> 00:18:25,940
However, if we bring age into a different category, let's say we look at, we break age into,

141
00:18:28,040 --> 00:18:34,880
let's say this one, two, three, four, four categories for categories and then.

142
00:18:39,840 --> 00:18:50,970
Yeah. For example, here. So we break Adra into four categories and then clearly we can see that the age, in fact, within the first window.

143
00:18:52,200 --> 00:18:59,010
I mean, of course was we break it into different categories, then every factor within each window is the same.

144
00:18:59,040 --> 00:19:04,409
So it's not within each window. We've not considered, you know, linear, non-linear anymore.

145
00:19:04,410 --> 00:19:12,360
So but anyway, so if we look at a baby, in fact within the first window of course, is going to be differ from in fact, in the second window.

146
00:19:12,870 --> 00:19:13,890
Overall, I mean,

147
00:19:14,520 --> 00:19:26,040
the very fact in the second window is about the same as the fact that the third with the third third category and then the the third category,

148
00:19:26,040 --> 00:19:35,840
third window is different from the it out from the last window and in the last window is higher than the third then maybe,

149
00:19:35,850 --> 00:19:40,680
but similar to the second, which may be higher than the first one.

150
00:19:40,980 --> 00:19:50,040
So by breaking age into different categories and then compare that in fact across this different, different windows, different categories,

151
00:19:50,600 --> 00:20:01,200
it might reveal the association of the relation between between as V and age in my review was not linear relationship if indeed there is

152
00:20:01,200 --> 00:20:11,160
a 1992 because in this case it will tell us that we found here is smallest now the same a window and third window there about the same,

153
00:20:11,280 --> 00:20:15,160
and the both are higher and then the last window is the highest, right?

154
00:20:15,240 --> 00:20:21,420
So if you simply include a linear effect of age, then we wouldn't be able to have this.

155
00:20:21,660 --> 00:20:30,180
We would simply assume we would simply ask, well, here we are, simply assume, assuming that for every one unit increase in age,

156
00:20:30,180 --> 00:20:35,850
we see the same effect while saying increase in SBP doesn't matter where it is.

157
00:20:37,020 --> 00:20:41,250
Okay. So this is the the another approach.

158
00:20:41,310 --> 00:20:55,380
So we we eliminate the linear linearity assumption by treating age as a categorical break page into a few different categories now.

159
00:20:56,250 --> 00:21:03,690
And usually we break it up into windows that are content roughly equal number of subjects.

160
00:21:03,930 --> 00:21:09,690
So in other words, I mean, when we look at how we break age here into this four categories,

161
00:21:09,960 --> 00:21:15,450
we choose this cut up, cut off points so that, you know, number of subjects.

162
00:21:16,170 --> 00:21:21,270
So I'm going to use the total number of sample sizes in result.

163
00:21:21,540 --> 00:21:25,520
So let's say that this is in one and two and three and four.

164
00:21:25,640 --> 00:21:30,060
So so that the number of subjects fall into this D for windows.

165
00:21:30,060 --> 00:21:33,360
There are roughly there are roughly equal size.

166
00:21:33,360 --> 00:21:38,560
And the reason is that not if you have a dramatic difference in sample size, like,

167
00:21:38,610 --> 00:21:44,430
let's say the first window has only five subjects, but the second window has 500 subjects.

168
00:21:44,760 --> 00:21:46,469
Then there's that.

169
00:21:46,470 --> 00:21:56,170
The first window has much a lot more variation and randomness compared to the second window, so that the comparison may not be bigger.

170
00:21:56,640 --> 00:22:04,530
There may be too much noise in the comparison. So that's the reason why we want to divide this into roughly equal size windows so

171
00:22:04,530 --> 00:22:09,870
that half the noise within each window or noise across windows are comparable.

172
00:22:12,120 --> 00:22:20,489
So that's that's one thing we need to pay attention to. So in other words, while we're channeling the RB, the cutoffs here you are.

173
00:22:20,490 --> 00:22:28,620
If you are not driven by, sometimes you would also take this scientific knowledge into into account.

174
00:22:29,250 --> 00:22:34,510
But they are not a purity. They may not be pure two based on, you know, something, the knowledge.

175
00:22:34,770 --> 00:22:46,920
So for example, let's say from purely a scientific perspective, it might be of more of interest to bring age into younger than 60 and older than 60.

176
00:22:47,430 --> 00:22:50,100
Right. So that might be of of scientific interest.

177
00:22:50,370 --> 00:22:59,640
However, depending on, you know, based on the consideration of equal number of subjects where we should have roughly equal number of subjects.

178
00:23:00,770 --> 00:23:05,010
We check with the RGV. We might want to bring in different ways.

179
00:23:06,630 --> 00:23:17,590
Okay. So. How do we break it into different categories then?

180
00:23:18,070 --> 00:23:24,850
Here we look at we.

181
00:23:25,060 --> 00:23:31,030
We include a three age category. We divide it on the variables, but we only include a three because we have an intercept here.

182
00:23:31,900 --> 00:23:42,820
And then after that, we will make a plot of the estimating value for the four betas from this table, from this model.

183
00:23:44,270 --> 00:23:50,740
And so here, if we say to this model, we are saying, you know, these betas,

184
00:23:50,770 --> 00:23:56,829
Li represents the difference of each category compared to the reference category.

185
00:23:56,830 --> 00:24:01,390
We will make a plot of the difference of each category compared to the reference category.

186
00:24:01,630 --> 00:24:07,780
But now we are using the the first age group as the reference category.

187
00:24:07,780 --> 00:24:10,780
So the difference of course, we forced the difference to be equal to zero.

188
00:24:11,410 --> 00:24:23,469
And then this one is the difference between the four of the secondary category and then this is between the secondary category in the first category.

189
00:24:23,470 --> 00:24:30,430
And the last one is the first category coming up with all of you first.

190
00:24:30,700 --> 00:24:40,930
So then we will make a plot of all these differences. So here that if you look at what we we make a plot on beta versus we versus

191
00:24:41,210 --> 00:24:46,960
the group plot while these vetos are addressing the group mean differences.

192
00:24:47,140 --> 00:24:51,870
Okay. So comparing each group for the reference category, what are the differences are?

193
00:24:52,210 --> 00:24:57,970
And then we make a plot of this Thetas versus the median age within each group.

194
00:24:58,300 --> 00:25:04,660
So in other words, here, for example, we have four age groups and the women in this age group,

195
00:25:05,030 --> 00:25:12,850
we will find the median age and then we make a plot of the status versus the corresponding median age.

196
00:25:13,690 --> 00:25:18,180
And so that's why here you see that in this plot that star from from zero, right?

197
00:25:18,220 --> 00:25:21,700
So that the beta is equal to zero.

198
00:25:21,700 --> 00:25:31,050
This is after a beta beta, a one that I'm very happy to have been a three had before that.

199
00:25:32,110 --> 00:25:36,429
So the reason that a beta a one has a star from zero is equal to zero.

200
00:25:36,430 --> 00:25:41,649
The reason is that while we are forcing you to to be to be zero because beta,

201
00:25:41,650 --> 00:25:50,200
what happens is the difference between a one category and a reference, but a one is treated as equals equals zero.

202
00:25:52,140 --> 00:26:00,040
And then in this case, if we have such a plot, then clearly we see that the difference there is a linear event.

203
00:26:00,340 --> 00:26:17,840
Very clearly you are trapped in a difference. So in other words, now that there is a linear association between age and SBP, however,

204
00:26:17,890 --> 00:26:24,290
in this case, in the second case, we see that well, I mean, there is there's definitely curvature.

205
00:26:24,820 --> 00:26:30,550
And so that means day and age is not linear.

206
00:26:32,530 --> 00:26:38,530
So this is another way of checking in the RVO assumption.

207
00:26:39,610 --> 00:26:48,939
So for either worse, I mean very even without making a plot, we could sort of have a rough idea.

208
00:26:48,940 --> 00:26:59,380
So for example, after calculating, after calculating the betas for different categories, the betas for different categories,

209
00:27:02,500 --> 00:27:09,520
we look at of the three, the three values are the three value represents how each category differ from the reference category.

210
00:27:10,990 --> 00:27:21,010
And if we see that the difference come from the category change so dramatically, it's not linear.

211
00:27:21,010 --> 00:27:24,250
So for example, let's say this is on a five assignment in the flow.

212
00:27:25,000 --> 00:27:32,680
This is this is one by something in this become ten. So then definitely vaccines will be different because the difference here is five.

213
00:27:33,100 --> 00:27:36,639
Now this is one that's on a long jump to ten.

214
00:27:36,640 --> 00:27:43,510
So so then the effect of age in the very last category, there is much,

215
00:27:43,510 --> 00:27:54,489
much higher association or association between the very last age category with SBP compared with one it became.

216
00:27:54,490 --> 00:28:01,090
So even by looking at this numbers, we have a rough idea whether in the already host or not but these plots and they

217
00:28:01,090 --> 00:28:11,890
are a better tool to inform us whether the association is okay so this is the.

218
00:28:12,520 --> 00:28:22,709
Another. Approach to attacking the narrative so easily.

219
00:28:22,710 --> 00:28:28,440
Narrative fails. Let's see what happens now if linearity host fails.

220
00:28:28,970 --> 00:28:36,660
Therefore, Baidu had a recall that Baidu had is defined as this.

221
00:28:36,990 --> 00:28:47,400
That's the reason why I asked major LP that had not for Baidu had we still calculate foundation of beta that's equal to.

222
00:28:53,100 --> 00:29:07,410
That's equal to this. And now what?

223
00:29:07,890 --> 00:29:13,010
Okay, we'll talk about. Okay.

224
00:29:13,440 --> 00:29:20,219
Sorry. Let's forget about this. I mean, what? I was trying to make it more detailed, but I think it's not necessary.

225
00:29:20,220 --> 00:29:24,240
So let's let's. I think it's a lot easier to think of this just by intuition.

226
00:29:26,520 --> 00:29:30,200
So if linearity fails. So let's say the dependance.

227
00:29:30,210 --> 00:29:32,760
Just imagine, let's say that you had an is a quadratic.

228
00:29:33,180 --> 00:29:39,900
But somehow but but but as we all we felt we did have that we we we dragged through the linear regression model.

229
00:29:40,230 --> 00:29:48,270
Of course the model itself is incorrect. And so the leading theorists, because it was in, does not approximate any association.

230
00:29:48,840 --> 00:29:58,410
So then the data we are asked where our estimate is that they're going to have bias compared to the truth.

231
00:29:58,950 --> 00:30:04,050
It's not going to be the crack, the estimate of the truth, because the truth is the contrary is a contrary function.

232
00:30:04,290 --> 00:30:09,149
So how can we use an estimate from linear function to to approximate cognitive function?

233
00:30:09,150 --> 00:30:15,810
So. So in other words, this beta will be the beta had a will be biased.

234
00:30:17,100 --> 00:30:26,160
And then we may well we use this model to make a prediction of what the critical value is also going to be by the model.

235
00:30:26,190 --> 00:30:32,720
So it's not true. And then the corresponding a c must wear a hat and a variance of bear happened.

236
00:30:33,360 --> 00:30:40,379
We are all biased and the cognitive the zero becomes invalid because because the center errors are.

237
00:30:40,380 --> 00:30:45,930
In fact the the point has been is incorrect. And I have always asked this is well, we'll get the wrong answer.

238
00:30:46,140 --> 00:30:50,970
So the the point here is that if linearity or assumption is valid, it.

239
00:30:52,440 --> 00:30:56,670
Now, basically, everything we do, anything we do is becomes meaningless.

240
00:30:57,390 --> 00:31:06,060
So so there is no point of going any further. And I think we do not make make sense anymore or I mean,

241
00:31:06,060 --> 00:31:11,790
if you still want to go ahead and then follow the conclusion we make, then may be far away from the truth.

242
00:31:12,690 --> 00:31:21,740
So that's that's the reason why we have to make sure linearity assumption holds or possibly helps.

243
00:31:22,260 --> 00:31:26,190
Of course, we never believe it's just about the underlying truth.

244
00:31:26,550 --> 00:31:29,400
But at least it should be a good approximation.

245
00:31:29,700 --> 00:31:38,550
Because was linearity assumptions violated anything we do so subsequently, then they become meaningless.

246
00:31:40,080 --> 00:31:48,900
So there are certain ways that while if linearity assumption is violated, there are certain ways that we can certain things we can do to address that.

247
00:31:49,350 --> 00:32:00,000
So first, we might consider, while we could consider adding more covariates, adjusted for more covariates might help depending on how,

248
00:32:00,420 --> 00:32:08,370
you know, association between different words for the dependance or relation between among different covariates.

249
00:32:08,880 --> 00:32:14,400
Now another commonly common practice is to transfer sort of x.

250
00:32:14,610 --> 00:32:21,980
So if you think there's a linear association outside of linear regression is not a is not the truth, then you could add a formatting,

251
00:32:21,990 --> 00:32:32,100
a term or parliamentary tool to see whether that actually leads to a better approximation to the truth.

252
00:32:32,430 --> 00:32:35,730
Or we could also consider trends of transformation of the Y.

253
00:32:35,760 --> 00:32:38,790
We could take like a wide and square with a wide.

254
00:32:39,030 --> 00:32:46,590
So, for example, the lot of Y is a very commonly used transformation where you have highly skilled distribution.

255
00:32:46,890 --> 00:32:53,700
So for example, this is if you have a Y that the distribution looks like.

256
00:33:01,120 --> 00:33:07,490
And so if you have a wire that disputed like like this, it has a very heavy tail.

257
00:33:07,970 --> 00:33:09,230
Right. Every tail.

258
00:33:10,120 --> 00:33:20,660
And then the last transformation could be and could be used if you take like of why that would make the distribution slightly more symmetric.

259
00:33:23,120 --> 00:33:31,460
So that might help to shape the narrative a better approximation.

260
00:33:32,420 --> 00:33:40,490
So these are different ways that there are things that we can do to differently you some response to the linearity assumption.

261
00:33:40,510 --> 00:33:44,340
So the things that if linearity fails, we listed all these things that it violates.

262
00:33:44,390 --> 00:33:55,590
Does that change depending on how you misclassified your model, including additional predictors that didn't actually matter or.

263
00:33:56,810 --> 00:34:04,760
Yeah. So this is good. Good point. So, um, when we specify a model, there are two things.

264
00:34:05,420 --> 00:34:13,190
Of course there are many considerations. Among them, there are two major factors that drive this particular model.

265
00:34:13,190 --> 00:34:16,100
One is prior scientific knowledge.

266
00:34:17,210 --> 00:34:26,660
So from the literature, people have already found that, you know, as we do, depends on age by the dimensions is not linear.

267
00:34:27,050 --> 00:34:34,950
And then when I when I analyzed this particular data that I might rather consider sat about, like in terms of age to include.

268
00:34:35,960 --> 00:34:39,350
So that's one was actually based on prior knowledge.

269
00:34:39,890 --> 00:34:43,010
Another, of course, is by exploring the data.

270
00:34:43,700 --> 00:34:54,320
So because not every product we do as an internal there is there is highly relevant existing literature.

271
00:34:54,500 --> 00:35:05,170
So with a new data that sometimes we do, a lot of times we do our work, maybe other times we do exploratory do not asses.

272
00:35:05,450 --> 00:35:15,800
But that's part of the purpose is to actually try to reveal what are the truly true dimensions is how the response depends on different covariates.

273
00:35:16,040 --> 00:35:19,400
And that's another important factor.

274
00:35:19,700 --> 00:35:26,630
So you are it's not it's very challenging or it's not.

275
00:35:26,870 --> 00:35:30,650
Well, it's maybe it may be impossible due to the rapid spaceflight,

276
00:35:30,920 --> 00:35:35,930
a model without prior knowledge, without looking at the data to derive this business model.

277
00:35:35,940 --> 00:35:42,709
And, and and it's it's is a good approximation to the truth or do we need to to do different

278
00:35:42,710 --> 00:35:48,500
types of technology diagnostics and model tracking to to come out with a reasonable model.

279
00:35:51,200 --> 00:35:56,299
So that's actually something that you guys probably want to do.

280
00:35:56,300 --> 00:36:02,930
E Or your final product. So we've two different model tracking models like Gnostics to come up with.

281
00:36:07,470 --> 00:36:13,110
Okay. So that's linearity assumption. Now let's look at about the independence assumption.

282
00:36:14,370 --> 00:36:24,089
So the independence assumption means that. The four individuals in the dataset fever responses from different individuals.

283
00:36:24,090 --> 00:36:32,190
They are independent. Now, because first residuals we actually assume the normality of the residual.

284
00:36:32,190 --> 00:36:38,820
Rather, we assume that all this aerosolized they follow, they follow some normal distribution.

285
00:36:39,240 --> 00:36:46,590
The other value assumption that in abundance is equivalent to the citation or coherence equal to zero.

286
00:36:47,430 --> 00:36:55,200
But the absolute I would assume it has mean zero. So then that further reduces to these manageable product X absolutely zero.

287
00:36:55,680 --> 00:36:59,249
So that's that's the independent assumption for a linear regression model.

288
00:36:59,250 --> 00:37:03,710
Under the assumption that doesn't follow, this is future.

289
00:37:04,200 --> 00:37:09,510
But generally speaking, in uveitis, it means that for individuals in the datasets, they are independent.

290
00:37:10,920 --> 00:37:15,210
So agenda for independence, of course, there are different techniques of checking independence,

291
00:37:15,750 --> 00:37:22,530
but the most important and most intuitive one is to look at of a study of design, to look at a holiday,

292
00:37:22,530 --> 00:37:30,300
to wipe it all either by looking at a study design, by looking at a data collection, the chasm,

293
00:37:30,600 --> 00:37:38,250
we're able to tell whether it's a good approximation, whether anyone is is a good assumption to make.

294
00:37:39,870 --> 00:37:44,970
So violation of anybody is often clear from the official sampling scheme.

295
00:37:45,450 --> 00:37:56,010
So for example, if my eyes are time honored, as are you, look at a, you know, the, the financial market, they look at the stock price.

296
00:37:56,280 --> 00:38:02,160
So it goes up and down every day and we especially during the past few weeks.

297
00:38:02,340 --> 00:38:11,540
So I think there are some. Anyway.

298
00:38:11,540 --> 00:38:15,259
Let's not talk too much about that. So. So you have you have this going up and down.

299
00:38:15,260 --> 00:38:25,460
So if the the of course, the the stock price, if you look at at different points, these guys are ahead on decent points.

300
00:38:25,500 --> 00:38:32,629
They are they're highly correlated. So in that case, of course, we do not believe that, you know,

301
00:38:32,630 --> 00:38:41,180
y I and y y y y at a time point and y at another address and time point, they are independent.

302
00:38:41,660 --> 00:38:45,920
So in this case, there may be a serial correlation.

303
00:38:47,270 --> 00:38:52,489
Um, and so that's, that's one example.

304
00:38:52,490 --> 00:38:56,270
Another example is that study subjects, they are from clusters.

305
00:38:56,450 --> 00:38:59,360
So for example, I, we study the income, people's income.

306
00:39:00,440 --> 00:39:07,610
Now if you sample people from different households, then people live living within the same household.

307
00:39:07,820 --> 00:39:12,710
Well, it's reasonable to think that their incomes are highly correlated.

308
00:39:13,160 --> 00:39:17,000
Right. Or people live within the same neighborhood.

309
00:39:17,270 --> 00:39:24,530
I mean, it's sometimes it's reasonable to think that their incomes are highly correlated in that case, you know, the people from the same household.

310
00:39:26,030 --> 00:39:30,340
It may not be a good assumption to assume that their income is is independent of each other.

311
00:39:32,990 --> 00:39:35,450
Yeah. This reasonable assumption.

312
00:39:35,780 --> 00:39:43,760
I mean, if you look at a paper, if you read the papers, if you read many paper, you will see that some papers, they may address this problem.

313
00:39:43,880 --> 00:39:48,170
They may they may explain it to say all of the other. Can you find that something is valid?

314
00:39:48,220 --> 00:39:52,400
Some of the papers, you know, if they have the client data from the same household,

315
00:39:52,730 --> 00:40:01,090
they will still go ahead and make independent assumptions, even though from a mathematical point of view, we know that a clear validate.

316
00:40:01,790 --> 00:40:09,950
So again, this becomes the point of, you know, how well we think the model approximate the truth because of course,

317
00:40:10,190 --> 00:40:15,440
even if you I mean, even if everything is independent, it's truly independent.

318
00:40:16,010 --> 00:40:22,310
Linearity linear experiment is still an approximation as we imagine any parametric models approximation.

319
00:40:22,730 --> 00:40:27,980
So we need to decide where to stop.

320
00:40:28,250 --> 00:40:33,410
I mean, under what assumption we think, what assumptions are good enough to approximate the truth.

321
00:40:33,710 --> 00:40:40,100
So your where really the paper is, you will see that sometimes even clearly mathematically.

322
00:40:40,100 --> 00:40:48,290
And then there are hundreds of sorry there there is a seems to be the individuals those idea

323
00:40:48,290 --> 00:40:53,330
seems to be violated because for example that the data are clearly from the same household,

324
00:40:53,840 --> 00:40:59,930
but people might still go ahead and run the interaction model, assuming even that.

325
00:41:00,830 --> 00:41:07,700
So that's that's another example. So which we can clearly see that from the sampling scheme,

326
00:41:07,850 --> 00:41:15,800
whether even this assumption holds and another one is response measured repeatedly almost on the same subject.

327
00:41:16,280 --> 00:41:18,880
So this is especially true in clinical trial.

328
00:41:18,890 --> 00:41:26,390
If you have a longitudinal study, you follow a group of subjects for, say, for one year or two or three years,

329
00:41:26,840 --> 00:41:35,820
and then you repeat it to collect data from the same individuals, then that the matter from the same individual, they are highlighted.

330
00:41:36,830 --> 00:41:46,730
So for example, if you matter to my blood pressure and even a matter of my brow pleasure today, tomorrow, and about the day after I say relapse.

331
00:41:47,000 --> 00:41:51,020
Now, of course, these three values are highly correlated. And those three values.

332
00:41:51,980 --> 00:41:57,860
But but if you look at but if you measure, like your absorption and your your blood pressure and my blood pressure,

333
00:41:57,860 --> 00:42:01,050
we can consider these values to be independent. Right.

334
00:42:01,070 --> 00:42:08,990
But but my blood pressure at different days is it's a there is there's no reason to think that we are completely independent.

335
00:42:09,740 --> 00:42:16,100
So this is another example that if by looking at how the data are collected, what the sampling scheme is,

336
00:42:17,220 --> 00:42:22,340
usually we have a pretty good idea of whether, you know, independence of social pulse or not.

337
00:42:25,520 --> 00:42:31,969
So for. In public health, in biostatistics,

338
00:42:31,970 --> 00:42:42,590
one important type of violation in evaders is when the errors are sort of a photo credit or the so-called autocorrelation.

339
00:42:42,980 --> 00:42:46,190
This occurs when the responses matter over time.

340
00:42:46,400 --> 00:42:50,150
This is very common to see past as well.

341
00:42:50,600 --> 00:42:57,050
We're in medical research because we have to follow the subjects over a certain period.

342
00:42:57,530 --> 00:43:03,920
We do matter. The variable of interest, the quality of interest repeatedly over time.

343
00:43:04,340 --> 00:43:12,260
So in this case, these residuals at a different time points, they may not be independent.

344
00:43:13,910 --> 00:43:16,950
And this error here, sometimes we call it a lack.

345
00:43:16,970 --> 00:43:28,670
So, again, depends. Well, here to deal with this other variation over time, people make the assumption that if the medical errors,

346
00:43:28,670 --> 00:43:36,050
if they are very far apart, let's say, like my blood pressure mattered today and my blood pressure mattered.

347
00:43:36,480 --> 00:43:40,340
You know, one year after, like, one year apart.

348
00:43:40,610 --> 00:43:46,550
Then while people might assume that at least to balance, they are independent.

349
00:43:46,910 --> 00:43:50,540
But my blood pressure better today and a better tomorrow.

350
00:43:51,360 --> 00:43:55,600
And people like me, there is a much higher formation.

351
00:43:56,240 --> 00:44:00,140
So. So this l here is the social network.

352
00:44:01,040 --> 00:44:10,340
So it it it is a quantity that within, you know, for for land smaller than this l.

353
00:44:10,610 --> 00:44:24,020
The correlation part of the coherence is not equal to zero, but for the time points of that are more than, you know, l time units apart.

354
00:44:24,830 --> 00:44:28,590
People are saying that, okay, so maybe we can treat them as right.

355
00:44:29,120 --> 00:44:34,250
So again, these are approximations to the truth.

356
00:44:34,640 --> 00:44:42,560
Of course there is sometimes sometimes it's the operations are never exactly able to see Peter how far away they are apart.

357
00:44:43,090 --> 00:44:49,969
Must you people. I think that okay to you know for easy of model feeding for is for ease of motor feeding for ease

358
00:44:49,970 --> 00:44:58,510
of interpretation for ease of matter development that we simply are still not about true matters.

359
00:44:58,520 --> 00:45:01,790
They are very far away. We assume they're in the moment.

360
00:45:02,150 --> 00:45:09,950
So, for example, little autocorrelation means that in adjacent matters they are thought in development.

361
00:45:10,400 --> 00:45:17,360
And then for matters that are, you know, two units, two time units apart, they are not independent.

362
00:45:17,900 --> 00:45:26,250
However, for any matters that are more than two time units apart, we think that they are they become independent.

363
00:45:26,300 --> 00:45:32,180
There is not much formation anymore. So this is the sort of like to organization.

364
00:45:33,860 --> 00:45:40,060
Okay. So this is a commonly used terminology when we deal with very response matter over time.

365
00:45:41,600 --> 00:45:48,220
Like, is it something that we calculate or is it just like something that we just that we choose of?

366
00:45:50,950 --> 00:45:59,990
There is a very good question. So there might be I think there is in public health.

367
00:46:01,000 --> 00:46:04,400
Ambassador, is you are this is determined by assumption.

368
00:46:04,850 --> 00:46:10,639
So you're always saying, oh, maybe like five times apart, then five communities apart away.

369
00:46:10,640 --> 00:46:14,570
And then we find that the condition is negligible.

370
00:46:14,570 --> 00:46:21,590
So we've not heard much about that. But I think in economics, the U.S. has been able to consider this data.

371
00:46:21,740 --> 00:46:34,040
And there are there are many techniques developed to try to calculate or to figure like to determine what a this l is based on the data.

372
00:46:34,730 --> 00:46:39,620
But I think in public health, Europe is based on the assumption.

373
00:46:40,310 --> 00:46:45,680
And also I think that there are certain ways certain techniques can apply to to determine this.

374
00:46:45,680 --> 00:46:52,340
L But I think not too many people do that in their permanent.

375
00:47:01,170 --> 00:47:11,580
Okay. And another thing is to what there how do you determine how do we determine autocorrelation and how do we define auto correlation?

376
00:47:12,270 --> 00:47:20,560
So the way to do that on operation is to plot the absolute, the residual versus time.

377
00:47:20,580 --> 00:47:25,360
So in this case is time. When we talk about auto provision already, we look at a time.

378
00:47:25,380 --> 00:47:39,330
So we plotted this versus times. And then if we look if we have such a plot that indicates positive auto correlation.

379
00:47:41,980 --> 00:47:52,290
The reason is very intuitive. So positive on operation means that no absolute I am absolute I plus one they are party of the correlated.

380
00:47:52,980 --> 00:47:57,410
For example, let's say to make things concrete, let's say carnation is equal to politics.

381
00:47:57,410 --> 00:48:03,809
Six of encouraged not a means what you see a large absolute high than you would

382
00:48:03,810 --> 00:48:08,920
expect to see a large absolute I plus one will see a small arms from high.

383
00:48:08,940 --> 00:48:14,200
You will see a small advance because that's what we mean by one group.

384
00:48:15,360 --> 00:48:23,700
And indeed, then we see that, you know, if you look at adjacent arms from us, there are values do not vary too much.

385
00:48:24,180 --> 00:48:28,500
They are they're relatively close to each other, although overall they may vary.

386
00:48:28,830 --> 00:48:37,800
But if you look at adjacent regions, their values are close versus if you have a negative correlation,

387
00:48:38,340 --> 00:48:45,660
that means, well, if you have a large absolute, then you will have to expand to see a small plus one.

388
00:48:46,350 --> 00:48:50,940
But if you see a large, absolute large well, a small arms, right.

389
00:48:50,970 --> 00:48:58,440
They're able to see a large epsilon. So in other words, you will see that adjacent values, they would vary quite a bit.

390
00:48:59,280 --> 00:49:06,210
If you see a small value here, then you will see you would expect to see a large value for the next time points.

391
00:49:06,510 --> 00:49:22,530
So this would enable narrative autocorrelation. So such a plot would actually is a very effective tool to review autocorrelation

392
00:49:24,300 --> 00:49:29,130
and then you could also make a plot of absolute versus absolute by minus one.

393
00:49:32,290 --> 00:49:37,620
If we see a positive while, if we see a positive trend, then there is a party recommendation.

394
00:49:37,640 --> 00:49:44,240
Again, this is based on the intuition that if they have a positive correlation that if everyone is large,

395
00:49:44,360 --> 00:49:50,900
then the next episode plus one should also be fairly large because they have lots of information,

396
00:49:51,440 --> 00:49:55,760
but overall they would have this this positive trend.

397
00:49:57,800 --> 00:50:03,230
So but if but if you see a negative trend, then you at a that indicates negative correlation.

398
00:50:04,790 --> 00:50:14,030
So these are some very effective tool plots, graphical tools that can help review the organization, whether they are positive or negative.

399
00:50:17,250 --> 00:50:20,880
Okay. And then it's.

400
00:50:23,560 --> 00:50:28,590
Let's finish this and.

401
00:50:32,010 --> 00:50:38,850
Yes. Right. Okay. So there is a there is a past.

402
00:50:38,910 --> 00:50:42,120
The so-called Durbin wasn't passed. That is from past.

403
00:50:42,990 --> 00:50:47,190
If the correlation low is equal to zero, we're not.

404
00:50:47,400 --> 00:50:57,950
But this test is for lack of revision. So if you assume or lack one on information and that the Kurdish Kurdish coalition is ruled that there is okay,

405
00:50:58,410 --> 00:51:04,040
this is equal to zero, we're not going to be honest in [INAUDIBLE].

406
00:51:04,200 --> 00:51:14,400
I've never seen people actually apply this test. And so this this this is I guess this is I mean, probably most of you in the other areas.

407
00:51:15,600 --> 00:51:22,169
So I myself for sure, I've never used this test. And, and also this test is quite restrictive.

408
00:51:22,170 --> 00:51:31,970
So it's it's actually for like one autocorrelation and it has to whether this Kurdish is equal to zero versus is a positive or negative situation.

409
00:51:33,000 --> 00:51:37,650
And the way to test this is well, this is the test statistic.

410
00:51:38,070 --> 00:51:41,639
So you calculate the amount after recovering the residual,

411
00:51:41,640 --> 00:51:49,260
you are able to counter this test the statistic that on arrival this is it has a value approximately equal to two.

412
00:51:49,410 --> 00:51:56,070
So this is quite a strange test. It does not have a distribution, but it has a value of a present equal to two.

413
00:51:56,430 --> 00:52:02,580
And then people have led table for about an hour for these pass the statistic.

414
00:52:02,910 --> 00:52:07,110
So now you would compare this to your lower back and to the out.

415
00:52:07,500 --> 00:52:12,330
And then based on that, you will reject or fails to reject analysis.

416
00:52:13,320 --> 00:52:16,950
Okay, so let's not worry too much about this test.

417
00:52:17,700 --> 00:52:22,860
So I think for our purposes, we just need to know that if there exist such a test,

418
00:52:23,520 --> 00:52:33,959
this test and later in your per bag or your other program in this course, we definitely I don't think we need this test or for other course vertex.

419
00:52:33,960 --> 00:52:39,660
I mean, if you need this test or if you want to pass on correlation like one operation,

420
00:52:40,830 --> 00:52:48,390
then you could always come back for this and then look more and get more details.

421
00:52:49,500 --> 00:52:53,130
But for now, I don't think we need to worry too much about this test.

422
00:52:54,210 --> 00:52:59,970
Okay. I don't think it's commonly used in in our statistics.

423
00:53:04,550 --> 00:53:10,330
Oh. Okay, let's. Oh.

424
00:53:14,180 --> 00:53:21,200
Thank you. Yeah, this slide is quite important. I'm thinking that maybe we should actually skip those.

425
00:53:21,860 --> 00:53:26,460
Of course, but that's okay.

426
00:53:28,550 --> 00:53:33,190
You know what? Let's let's let's start here, because I really do want to run this next one.

427
00:53:33,190 --> 00:53:41,349
This is quite important. Okay. So let's take kind of a five minute break that we will come back and then we

428
00:53:41,350 --> 00:53:50,061
will look at some problems from organizations that you might want to listen to,

