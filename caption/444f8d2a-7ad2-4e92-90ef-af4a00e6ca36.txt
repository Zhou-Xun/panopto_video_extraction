1
00:00:06,050 --> 00:00:15,040
It's a lot of different flags. Good morning, everybody.

2
00:00:27,850 --> 00:00:32,330
So. We got that pesky midterm out of the way.

3
00:00:34,400 --> 00:00:40,010
And we get to start on some new stuff today in regards to the midterm.

4
00:00:41,630 --> 00:00:45,980
We've got those all graded. I posted the grades this morning for those.

5
00:00:47,400 --> 00:00:50,820
You'll have a chance to look at those during lab today.

6
00:00:51,970 --> 00:00:58,120
And asked questions about that. Overall, they could have been could have been better.

7
00:00:58,270 --> 00:01:03,520
But what the you know, that's the way things go is a difficult exam.

8
00:01:04,240 --> 00:01:10,630
You should feel pretty good. I'd say if you get in the sixties of 80 points.

9
00:01:11,440 --> 00:01:18,620
So if you're if you're in that range, it's you should consider yourself having done pretty well less than that.

10
00:01:18,620 --> 00:01:28,330
And you've got a lot more work to do. Okay. Now, just because the average is low and things could have been better isn't devastating.

11
00:01:28,720 --> 00:01:33,730
There's lots more points. The final is worth 30% of your grade.

12
00:01:33,730 --> 00:01:34,600
It's a take home.

13
00:01:35,650 --> 00:01:47,709
We still have homework to to go as well and if you labs and certainly I'm not going to give everybody a bad grade if that's the end of it.

14
00:01:47,710 --> 00:01:50,980
So if I have to adjust the grade scale at the end, that's what I'll do.

15
00:01:52,000 --> 00:01:56,060
Okay. I recognize it was a difficult exam, so.

16
00:01:59,170 --> 00:02:02,170
Is that understood by everyone? Yeah.

17
00:02:04,820 --> 00:02:11,840
All right. Like I said, you'll have a chance to look at those at the beginning of lab.

18
00:02:13,130 --> 00:02:19,250
We will have to collect those tests back, but we'll have them for you to look at and ask any questions that you want.

19
00:02:21,230 --> 00:02:27,510
Some good. All right.

20
00:02:29,460 --> 00:02:39,200
Any questions before we start today? No.

21
00:02:43,610 --> 00:02:48,100
All right. Well.

22
00:02:49,540 --> 00:02:56,760
Today. We actually get to do some statistics with SAS.

23
00:02:56,820 --> 00:03:06,510
So that's cool. You know, so far we've done done a lot of data manipulation and cleaning and stuff like that.

24
00:03:07,410 --> 00:03:14,200
Today starts when we actually going to. Begin some actual real statistical analysis.

25
00:03:16,130 --> 00:03:21,500
So we're going to talk about differences in proportions like Chi Square tests, T tests,

26
00:03:21,500 --> 00:03:27,440
ANOVA correlations, little regression, but we'll save that for next week for the most part.

27
00:03:30,490 --> 00:03:40,420
So learning objectives for this for this section, we want to run basic statistical procedures for categorical and continuous data,

28
00:03:41,500 --> 00:03:51,910
and that includes Chi Square tests, some other versions of that like Fisher's exact McNamara's as well as T tests and ANOVA correlation.

29
00:03:54,540 --> 00:04:04,800
So let's start with proportions. So what I mean by proportions is when you have discrete data.

30
00:04:05,760 --> 00:04:08,940
So, for example, does vitamin C prevent respiratory infections?

31
00:04:09,900 --> 00:04:14,010
You're either taking vitamin C or you're not. So it's a kind of a dichotomous variable.

32
00:04:14,940 --> 00:04:19,890
And respiratory infections as well as dichotomous. Either you have some infection or you don't.

33
00:04:21,150 --> 00:04:24,450
So these are proportions we're talking about.

34
00:04:26,310 --> 00:04:29,340
Here's some other examples. Are smokers more likely to develop dementia?

35
00:04:30,000 --> 00:04:37,850
Again, these are categorical variables. And we're looking at. We've seen Pratt Freak already.

36
00:04:40,180 --> 00:04:46,100
So here's a. Frog freak of physical activity by hypertension.

37
00:04:48,340 --> 00:04:55,910
And you get this lovely two by two table. Hopefully you remember how to code something like this for your exam.

38
00:04:56,750 --> 00:05:03,230
Like the second second questionnaire, right? So you had to do this, and you're going to have to do more of it for sure.

39
00:05:03,680 --> 00:05:09,930
So it's important. Freak will allow you to get a lot of stats as well.

40
00:05:11,030 --> 00:05:16,100
I think we may have talked about how to get an odds ratio previously.

41
00:05:17,920 --> 00:05:23,080
But we can talk about that some more. Um.

42
00:05:24,790 --> 00:05:27,970
Now, one thing you'll notice here in this two by two table.

43
00:05:28,870 --> 00:05:40,120
If you just run the hypertension by the physical activity, it orders it by the numerical order of those values.

44
00:05:40,390 --> 00:05:47,770
So zero and one. Now, if you were to actually try to kind of cross multiply these like you would to get an odds ratio.

45
00:05:50,450 --> 00:05:58,850
The interpretation is a little funky because of the fact that the zeros are in the upper left corner of that table.

46
00:06:00,700 --> 00:06:10,930
So you'd really be getting like a in odds of not having hypertension compared to the odds of not having physical activity kind of thing,

47
00:06:11,560 --> 00:06:19,450
which is not exactly what you want. So you'd like the ones, both ones to be in the upper left corner to interpret that.

48
00:06:20,510 --> 00:06:27,230
In a normal way. One way you can do that in SAS is to.

49
00:06:28,230 --> 00:06:34,030
Sort them. So in this case, using practice sort, we can sort both variables.

50
00:06:35,620 --> 00:06:40,690
By descending order using that descending keyword in the proc sort.

51
00:06:42,970 --> 00:06:48,080
And then when you run the proc freak. You can add this option.

52
00:06:48,080 --> 00:06:51,140
This order. Sorry, this order equals data.

53
00:06:51,890 --> 00:06:56,600
So that will order it in the order in which the data is actually ordered.

54
00:06:57,470 --> 00:07:03,290
So since you've sorted it by descending hypertension and physical activity, all the ones will show up at the top.

55
00:07:03,290 --> 00:07:07,370
And so that's what I'll show up first in the ordered table.

56
00:07:09,950 --> 00:07:17,710
So that makes sense. Another way you could possibly do this is to add a format to the variables.

57
00:07:18,910 --> 00:07:22,840
Where you make sure whatever format you use has.

58
00:07:24,110 --> 00:07:28,499
The. The one you want in the upper left.

59
00:07:28,500 --> 00:07:34,620
The ones in this case make that happen alphabetically first.

60
00:07:36,810 --> 00:07:45,090
Right. So you might do a. For example, you might this might be a yes or no type of scenario, which it is here.

61
00:07:45,090 --> 00:07:48,840
Right. So you could actually format the ones as space.

62
00:07:49,140 --> 00:07:52,480
Yes. And the zeros as no.

63
00:07:53,770 --> 00:08:01,060
And because that space happens alphabetically, first it would show up at the top and be another way to go about that.

64
00:08:02,930 --> 00:08:07,710
There's various ways to do it. Okay.

65
00:08:09,220 --> 00:08:17,030
So let's talk about the Chi Square test. This is the general test you'll use for testing proportions.

66
00:08:19,980 --> 00:08:30,630
And you know, obviously you'll learn this formula in your bio stats class and talk about a lot more in-depth, but it's the sum of the observed mind.

67
00:08:30,640 --> 00:08:38,080
Expected, over, expected. And Proc freak can give you this test with the chi square option.

68
00:08:40,030 --> 00:08:49,690
This expected option will give you the expected counts as well as the regular frequencies that you see in Normal Freak.

69
00:08:52,370 --> 00:08:59,090
So when you run this, you'll get the same table we normally get, plus the expected counts and you'll get this.

70
00:08:59,670 --> 00:09:03,440
This table of statistics is down below as well.

71
00:09:03,800 --> 00:09:13,540
And the first one there is the Chi Square test. So it gives you the chi square value plus the p value that goes along with that.

72
00:09:16,990 --> 00:09:20,470
So in this case, the probability the P-value is 0.001.

73
00:09:20,740 --> 00:09:22,990
Looks like it's highly significant, right?

74
00:09:26,380 --> 00:09:33,100
So there does appear to be some kind of relationship between physical activity and hypertension, which you would expect.

75
00:09:33,850 --> 00:09:39,150
Right. So it makes sense to everyone.

76
00:09:42,520 --> 00:09:49,150
You can also get odds ratios and relative risk by adding the measures option in the table statement.

77
00:09:50,530 --> 00:09:56,920
So if you had that option, you'll get a table similar to this one here, which will give you the odds ratio.

78
00:09:58,240 --> 00:10:01,660
And that column, one risk is the relative risk.

79
00:10:05,720 --> 00:10:13,720
Okay. Yes, sir. Yeah.

80
00:10:13,730 --> 00:10:17,470
Yeah. So you'll want. So that's why I was talking about this order.

81
00:10:17,480 --> 00:10:26,390
You want the. The category you're interested in to be at that upper left there to get the odds ratio that you're interested in.

82
00:10:28,010 --> 00:10:35,990
So yeah, the order is important there. Worst case scenario, if you don't order them correctly, you could you could flip this odds ratio.

83
00:10:37,490 --> 00:10:44,700
Right. You could take its reciprocal. All right.

84
00:10:47,320 --> 00:10:51,340
It's either the odds of something happening or it's the odds of not happening, depending on the order.

85
00:10:51,790 --> 00:10:57,400
So you can always flip that or you can take the reciprocal. Makes sense.

86
00:11:01,580 --> 00:11:04,910
All right. You can do this for multi-category groups as well.

87
00:11:07,060 --> 00:11:12,940
So in this case, it's a. Frequency table of region by hypertension.

88
00:11:14,870 --> 00:11:24,049
You can see the expected options, giving the expected frequency as that second value up in the upper left hand of the table.

89
00:11:24,050 --> 00:11:29,750
There's a key, right. So you've got frequency, expected frequency, the percent row and column percents.

90
00:11:31,900 --> 00:11:38,510
And. You can also run a chi square on this and get a similar type of thing.

91
00:11:39,380 --> 00:11:42,770
That first row will give you the chi square value and the p value to go along with it.

92
00:11:45,510 --> 00:11:49,780
So in this case. Based on that p value.

93
00:11:51,290 --> 00:11:55,160
It does not look like there is any relationship between hypertension.

94
00:11:56,030 --> 00:11:59,540
And the region. Right.

95
00:12:03,330 --> 00:12:07,140
We usually have use a cut up p value around .05.

96
00:12:08,140 --> 00:12:12,550
But obviously that can be said ahead of time for whatever test you're doing.

97
00:12:17,900 --> 00:12:26,050
So that makes sense, everyone. Now you might notice this warning at the bottom.

98
00:12:26,770 --> 00:12:32,560
This will happen sometimes. You'll say the warning. 63% of the cells have expected accounts, less than five.

99
00:12:33,220 --> 00:12:37,440
Chi Square may not be a valid test. And you can see the frequencies here.

100
00:12:37,450 --> 00:12:41,710
There's several cells that have counts, less than five.

101
00:12:44,210 --> 00:12:47,420
Chi Square test doesn't work very well with sparse data.

102
00:12:49,110 --> 00:12:54,370
It's not super. Reliable. So what do we do?

103
00:12:55,060 --> 00:13:00,200
Well. We can use a different test. The Fisher exact test.

104
00:13:04,610 --> 00:13:12,500
You can get that Fisher exact test by adding the Fisher option. And I'll give you this additional table for the Fishers exact test.

105
00:13:15,750 --> 00:13:22,840
Okay. If you actually I believe if you have a two by two table,

106
00:13:22,840 --> 00:13:28,660
the chi squared option will also aside from the chi square tests also give you a fisher exact test.

107
00:13:29,200 --> 00:13:33,040
But if you don't, if it doesn't show up and you want it, you can just add this Fisher option.

108
00:13:34,590 --> 00:13:42,960
Okay. And you can see from this Fischer's exact test, the P-value .69 is Nonsignificant.

109
00:13:43,200 --> 00:13:48,760
So there's still doesn't look like there's any. Relationship between the two variables.

110
00:13:50,300 --> 00:13:57,040
In a question. Just show probability.

111
00:13:58,490 --> 00:14:01,580
Yeah. So that's a p value. Yeah, that's the P value.

112
00:14:01,790 --> 00:14:09,080
So the, the test statistic is the, is the point or two and the P value is the point 69.

113
00:14:10,700 --> 00:14:13,990
Okay. Yes.

114
00:14:15,990 --> 00:14:27,270
What is it? Well, it's it's like the Chi square test, but it's not reliant on having large numbers.

115
00:14:28,570 --> 00:14:36,969
So if you have sparse numbers in your table like this and you notice that you should be using a fishers exact test instead,

116
00:14:36,970 --> 00:14:41,770
it's really just an alternate when you have sparse data. Okay.

117
00:14:46,160 --> 00:14:51,020
Makes sense. It's certainly something you'll get to in your bio sites if you haven't.

118
00:14:54,520 --> 00:14:57,640
All right. What about paired observations?

119
00:15:00,680 --> 00:15:03,800
Well when you have paired categorical data.

120
00:15:05,570 --> 00:15:13,510
Meaning your. Your variables of interest are not independent of each other.

121
00:15:16,110 --> 00:15:19,350
Well, now you should use another type of test.

122
00:15:20,650 --> 00:15:30,700
The McNamara's test. So here's an example where we have two measurements of the pulse rate.

123
00:15:33,240 --> 00:15:38,440
Okay for for the same people. We've got time.

124
00:15:38,440 --> 00:15:42,820
Point one is high pulse one and 9.2, high pulse two.

125
00:15:43,420 --> 00:15:48,850
So it's just measuring whether they had a high rate or not. Okay.

126
00:15:50,050 --> 00:15:56,790
Because these variables are paired. Meaning? The the observations should go together, right?

127
00:15:57,910 --> 00:16:03,490
The chi square test is invalid for that either. So we should use the McNamara's test in that scenario.

128
00:16:04,540 --> 00:16:08,469
And we can get it using the agreed option in the table.

129
00:16:08,470 --> 00:16:14,110
Statement of free. Okay.

130
00:16:15,970 --> 00:16:21,880
So I'll give you the test statistic here. The degrees of freedom and the p value to go along with the test.

131
00:16:24,680 --> 00:16:33,920
So it does look like there is some some association between the two polls race which you would would expect here with a P-value 0.007.

132
00:16:37,390 --> 00:16:37,750
Okay.

133
00:16:38,260 --> 00:16:46,060
I'll also give you a Kappa statistic to go along with that, which is kind of just a measurement of how much the two variables agree with each other.

134
00:16:46,870 --> 00:16:54,020
Where it ranges from 0 to 1. Okay.

135
00:16:54,200 --> 00:16:59,480
Actually, mine is 1 to 1 if they're in agree in exact opposite directions.

136
00:17:01,470 --> 00:17:08,770
Okay. Kind of like a kind of like a correlation coefficient except for paired data.

137
00:17:09,700 --> 00:17:18,140
Okay. So key takeaways in that section, we're going to use proxy to get basically all of those categorical statistics.

138
00:17:19,400 --> 00:17:22,820
Use case squared when comparing independent groups with sufficient counts.

139
00:17:23,690 --> 00:17:26,960
Fisher's exact test when comparing independent groups with low counts.

140
00:17:28,420 --> 00:17:32,020
And use McNamara's test when comparing dependent groups.

141
00:17:34,950 --> 00:17:37,950
Okay. So you're going to need to use the proper options to get each of those.

142
00:17:39,930 --> 00:17:47,850
And you may need to reorder your data in order to get the proper odds ratios that you want.

143
00:17:49,980 --> 00:17:55,090
Okay. Questions there.

144
00:18:01,020 --> 00:18:05,240
All right. Let's compare means of two groups.

145
00:18:06,920 --> 00:18:12,710
So these are some examples of the type of questions that might be related to this.

146
00:18:13,790 --> 00:18:20,480
Do men have higher blood pressure than women? Well, in this case, you've got one categorical variable, which is gender.

147
00:18:21,640 --> 00:18:28,960
And you have a continuous variable blood pressure. So you're trying to compare the blood pressures between men and women.

148
00:18:31,660 --> 00:18:35,940
Likewise with these other examples. Okay.

149
00:18:37,430 --> 00:18:42,560
So what test do we run here? Well, the T test is the general one we use for this.

150
00:18:45,230 --> 00:18:49,340
We've learned about this symbiosis. Yes. Extensively, I'm sure.

151
00:18:50,600 --> 00:18:58,880
Well, yeah. So you've got a null hypothesis where Mena equals mean B and the alternative hypothesis being where they're not equal to each other.

152
00:19:01,010 --> 00:19:04,070
So you're really just kind of comparing those distributions to each other.

153
00:19:07,830 --> 00:19:15,460
So to get these. Uh, well, I guess we're not going to show that just yet.

154
00:19:15,990 --> 00:19:20,130
So you can also compare continuous outcomes between many groups.

155
00:19:21,960 --> 00:19:26,370
So that was two groups we were just talking about. You can also do between many groups.

156
00:19:28,540 --> 00:19:33,690
Like these examples here. And this calls for a nova.

157
00:19:36,630 --> 00:19:47,990
Right. Now, one thing I'll emphasize here is there is a practice nova in SAS.

158
00:19:49,310 --> 00:19:57,080
But generally we will not be using that. It really is only valid when you have balanced data, meaning the.

159
00:19:58,220 --> 00:20:03,200
The groups are are similar sized. Which isn't usually the case.

160
00:20:04,280 --> 00:20:08,930
So if it's not the case, you shouldn't really be using tracking of it,

161
00:20:09,680 --> 00:20:16,910
even though it seems to be the intuitive one when you want to run an ANOVA for for comparing means of several groups.

162
00:20:18,600 --> 00:20:24,560
We'll talk about how to do it with GLM here instead. So fragile.

163
00:20:24,610 --> 00:20:30,460
M is for general general linear models. It's what we'll use for regression as well.

164
00:20:31,150 --> 00:20:34,320
But when you. Use it for.

165
00:20:35,800 --> 00:20:40,750
Groups. Variables like this. It will give you an ANOVA test.

166
00:20:42,870 --> 00:20:50,630
As well. The class statement here is going to tell SAS that that variable group two is categorical.

167
00:20:52,600 --> 00:20:56,100
And then your model statement will set up the actual model.

168
00:20:56,110 --> 00:20:59,259
So age is what we're trying to compare between the groups.

169
00:20:59,260 --> 00:21:06,490
So that's the outcome variable. And Group two is the predictor, if you will, of this model.

170
00:21:15,220 --> 00:21:22,940
So when we run this. [INAUDIBLE] give us lots of nice information, but it'll include this nice box plot.

171
00:21:24,670 --> 00:21:29,380
Here to compare between the groups. In this case, there's five groups.

172
00:21:31,180 --> 00:21:36,460
And it gives you an F test up there that's very nova and a p value to go along with it.

173
00:21:42,300 --> 00:21:46,650
Okay. Actually gives it to you in this form here.

174
00:21:48,950 --> 00:21:54,820
So I'll give you. The F test here and the p value to go along with.

175
00:21:57,570 --> 00:22:04,110
Doesn't a couple varieties, type one and type three. You don't really need to concern yourself too much with the difference there.

176
00:22:04,110 --> 00:22:07,370
You'll probably learn that in your bio stats. Okay.

177
00:22:07,490 --> 00:22:11,670
In this case, you can see they're exactly the same anyway. Okay.

178
00:22:16,560 --> 00:22:19,620
So that's great. It gives you lots of nice information.

179
00:22:19,770 --> 00:22:23,100
Some of Square's mean square and so on as well.

180
00:22:24,450 --> 00:22:31,500
If you need that information. So this is how we'll do our Innova tests.

181
00:22:37,030 --> 00:22:41,730
Now if you want to test specific groups. You can do that as well.

182
00:22:44,040 --> 00:22:47,340
So first of all, there's some there's an option. The solution option.

183
00:22:48,370 --> 00:22:56,230
But you can add to the model's statement, which will actually give you the parameter estimates for the model.

184
00:22:57,130 --> 00:23:01,390
This is what will generally be doing when we do a regression model.

185
00:23:02,750 --> 00:23:06,410
In this. At this point, we're really only interested in the ANOVA test,

186
00:23:06,740 --> 00:23:12,530
but eventually we're going to also want to actually run a regression model and to get the parameter estimates of that model,

187
00:23:12,530 --> 00:23:17,190
you'll need the solution option. Okay.

188
00:23:18,760 --> 00:23:22,270
You can also add a means statement to this program.

189
00:23:24,030 --> 00:23:28,470
Which compute will compute the means and the standard deviations for each group.

190
00:23:31,000 --> 00:23:36,610
And if you add the cookie option along with that, you get the pairwise tests.

191
00:23:39,790 --> 00:23:43,860
Between groups. Okay.

192
00:23:47,460 --> 00:23:52,310
So. When you run this model.

193
00:23:54,000 --> 00:23:59,770
And the solution option is going to give you a parameter estimates, but they're all going to be compared to a reference group.

194
00:24:01,230 --> 00:24:04,590
Right. It's the way it does the model.

195
00:24:05,870 --> 00:24:09,260
But if you want to compare the means between all the groups.

196
00:24:10,630 --> 00:24:11,500
This is how to do it.

197
00:24:11,920 --> 00:24:19,750
Use this means statement with the Tukey option will give you pairwise between each group, whether they're significantly different from each other.

198
00:24:21,440 --> 00:24:32,160
So that makes sense. Because as you probably know, a Nova and a Nova test will only tell you if one of the groups is different from any of the others.

199
00:24:33,200 --> 00:24:36,890
Right. So if you want to pairwise, compare them.

200
00:24:37,760 --> 00:24:42,710
Is the nuclear option here? Got it.

201
00:24:46,400 --> 00:24:52,990
This is a lot of stuff, but it's good stuff. So you'll see this.

202
00:24:54,220 --> 00:24:57,520
When you use a solution option in the program.

203
00:24:59,180 --> 00:25:02,720
So I'm just trying to point out here not to be alarmed by those little bees in there.

204
00:25:03,020 --> 00:25:08,749
It will give you a note that says the next prime matrix has been found to be

205
00:25:08,750 --> 00:25:12,470
singular and a generalized inverse was used to solve the normal equations.

206
00:25:13,440 --> 00:25:16,590
So this is not generally a concern.

207
00:25:17,700 --> 00:25:22,410
All it really means is that you haven't explicitly assigned a reference group.

208
00:25:26,070 --> 00:25:30,120
Okay. It's not no need for concern. It's not a fatal problem.

209
00:25:34,010 --> 00:25:41,360
So essentially you can just ignore that. You can see here it shows the one value is the reference is zeroed out there.

210
00:25:41,360 --> 00:25:46,810
Right. So when we interpret this parameter estimate.

211
00:25:48,600 --> 00:25:58,400
It will be compared to the wonder. So compared to ones the twos, I believe this is a.

212
00:25:59,550 --> 00:26:03,060
Physical activity maybe this variable. So.

213
00:26:04,670 --> 00:26:10,310
If you don't exercise, whatever the outcome is here, you do a on average.

214
00:26:10,520 --> 00:26:17,560
Say it. Let's say it's weight. The outcome is weight. They need you to be £121 more.

215
00:26:17,590 --> 00:26:21,880
Obviously, that's not the outcome, but that's how you would interpret that, right?

216
00:26:23,260 --> 00:26:30,210
So everybody remember that. Maybe. We've done some of this regression stuff in bio stats here now yet.

217
00:26:31,370 --> 00:26:34,510
You certainly will. We'll do some next week. Lot next week.

218
00:26:37,660 --> 00:26:42,510
That's the general idea. We won't concern ourselves too much with those until next week.

219
00:26:45,460 --> 00:26:51,210
So key takeaways in that section. For Inova is the GLM, not Inova procedure.

220
00:26:53,010 --> 00:26:58,980
You're going to need the solution option to get the statistics that you'd like. And don't worry about those bees in your output.

221
00:27:00,120 --> 00:27:11,210
It's just telling you that you haven't picked a reference group. So some key questions regarding which test to run.

222
00:27:13,720 --> 00:27:18,730
You need to. Be mindful of whether your data is independent or dependent.

223
00:27:20,250 --> 00:27:23,310
And if they're independent, do you have equal variances?

224
00:27:26,870 --> 00:27:30,440
And you also need to be aware of whether your data is normally distributed.

225
00:27:31,610 --> 00:27:37,040
He test in a nova are what are known as parametric tests, which means they.

226
00:27:38,130 --> 00:27:42,850
They? They?

227
00:27:45,940 --> 00:27:52,840
Requires a strong word, but they want the distributions to be normally distributed.

228
00:27:54,300 --> 00:28:02,530
They assume that they are. Okay. So let's talk about that t test.

229
00:28:04,410 --> 00:28:10,520
There is a t test and says. And here's the basic format of how that would work.

230
00:28:11,330 --> 00:28:14,540
Invoke the procedure with frac t test. Tell the data you're looking at.

231
00:28:15,560 --> 00:28:19,760
You'd use a class statement to indicate or categorical variable.

232
00:28:21,830 --> 00:28:28,250
In a statement to tell the. The continuous variable that you're interested in.

233
00:28:34,350 --> 00:28:37,590
And running this. You'll get a couple nice tables here.

234
00:28:39,800 --> 00:28:45,200
It'll give you the means. Standard deviations mean max and so on between the two groups.

235
00:28:48,780 --> 00:28:54,000
That's nice. You can also get the confidence levels and so on from this.

236
00:28:56,230 --> 00:29:03,460
Now, as we just mentioned, one of the things we need to keep in mind is do the groups have equal variance?

237
00:29:05,470 --> 00:29:08,860
So this is going to be important when you look at do proxy test.

238
00:29:10,790 --> 00:29:17,240
It gives you the standard deviations. Do you have any instinct whether these we statistically different or not.

239
00:29:19,490 --> 00:29:25,640
We've got a standard deviation in Group one of 1.6 and Group two about 2.2.

240
00:29:29,430 --> 00:29:33,660
Well, they're certainly different. Whether they're statistically different or not, it's another story.

241
00:29:35,320 --> 00:29:38,890
Well, luckily, when you run practice test, it will give you.

242
00:29:40,290 --> 00:29:46,220
A test for equality of variants. Okay.

243
00:29:46,740 --> 00:29:54,880
In this highlighted table here. It's a folded F statistic.

244
00:29:56,320 --> 00:30:01,170
And it gives you a p value there of 0.03. So this test.

245
00:30:02,240 --> 00:30:07,520
Uses a null hypothesis, which is that the variances are equal.

246
00:30:09,790 --> 00:30:14,780
So what would we. Conclude here about the variances.

247
00:30:21,270 --> 00:30:28,400
We think. Is there evidence that they're equal?

248
00:30:28,670 --> 00:30:40,170
Is there evidence that they're not equal? Sure you'd reject the null based on 2.05 at least.

249
00:30:40,170 --> 00:30:45,570
Right. And the knowledge, as I mentioned, is that the variances are equal.

250
00:30:46,640 --> 00:30:53,340
So it does look like there's some evidence here. That's the variances are not equal to each other.

251
00:30:57,350 --> 00:31:08,840
So this is going to inform. Yes, sir. Yes.

252
00:31:08,850 --> 00:31:13,940
I'm basing that on the p value here. So this is the p value of the test.

253
00:31:15,420 --> 00:31:20,940
So it looks like we should reject the null hypothesis, which is that the variances are equal.

254
00:31:22,970 --> 00:31:29,720
Does that make sense? So this is going to inform you which.

255
00:31:30,860 --> 00:31:35,000
T test p value you should. Report.

256
00:31:38,260 --> 00:31:43,990
If the variances are equal, the t test should use a pooled variance.

257
00:31:45,230 --> 00:31:53,740
P-value. So that's this first one. If the variances are unequal, you should use the Satterwhite.

258
00:31:56,830 --> 00:32:02,310
Alternative. For the variance calculation. All right.

259
00:32:03,430 --> 00:32:08,200
Obviously here the values aren't a lot different and you'd come up with the same conclusion.

260
00:32:10,340 --> 00:32:15,620
But this second one is the the better one to report for this particular test.

261
00:32:16,650 --> 00:32:24,300
Yes. Well.

262
00:32:27,200 --> 00:32:30,470
That's that's a very good question.

263
00:32:30,920 --> 00:32:37,930
And. Your P value should always be determined before you run any tests.

264
00:32:38,020 --> 00:32:42,230
You need to decide. Ahead of time.

265
00:32:42,600 --> 00:32:46,390
What you're cut off is going to be. This is always true.

266
00:32:46,660 --> 00:32:49,900
Usually we use a point of five. But if you want to.

267
00:32:49,930 --> 00:32:56,630
If you know that. You need to be a little bit more strict and.

268
00:32:59,890 --> 00:33:04,840
And whether you're going to risk a type two error, right.

269
00:33:06,040 --> 00:33:09,270
You can you can set Alpha at .01.

270
00:33:11,080 --> 00:33:18,400
Right or .001 if you really want to be stringent.

271
00:33:18,430 --> 00:33:25,360
Right. But that's something that should be determined ahead of time before you run any tests.

272
00:33:28,400 --> 00:33:34,110
Okay. For general purposes, we'll use point five in this class.

273
00:33:35,120 --> 00:33:42,080
Unless. Unless it says otherwise. That's a good question.

274
00:33:45,790 --> 00:33:56,960
Anything else? So based on this the of variance test that's significant, we should report the center weight.

275
00:33:59,030 --> 00:34:07,070
P-value there. If this had not been significant, we should just report the pooled version.

276
00:34:08,690 --> 00:34:13,540
It's clear to everyone. Right.

277
00:34:14,290 --> 00:34:21,490
Proxy test also gives you these lovely day diagnostics. So we'll give you some histograms for each group.

278
00:34:23,010 --> 00:34:28,080
And Kuku plots, both of which are helpful to determine normality.

279
00:34:29,880 --> 00:34:34,100
The T test does assume normality. Okay.

280
00:34:35,330 --> 00:34:43,700
So if these are way skewed or not at all normal, it may not be a good test.

281
00:34:43,700 --> 00:34:50,280
And you'll have to think of another test to use. And we'll talk about those alternatives shortly.

282
00:34:51,810 --> 00:34:56,590
But these look fairly okay. You know, we have a huge end here.

283
00:34:57,810 --> 00:35:01,260
It might be a little left skewed, but it's not terrible.

284
00:35:02,170 --> 00:35:06,640
And the T test is robust enough to handle small deviations from normality.

285
00:35:07,240 --> 00:35:12,510
You just don't want them to be way off from normal. Okay.

286
00:35:17,690 --> 00:35:23,870
Well, it's a good segue into this. What happens when you don't have normally distributed data?

287
00:35:27,310 --> 00:35:30,580
Well, as long as you're still dealing with independent groups.

288
00:35:32,360 --> 00:35:37,400
The Mann-whitney test, also known as the Wilcoxon rank sum test, will work for that.

289
00:35:39,470 --> 00:35:43,160
We're going to need a new procedure. It's called proc in PA one way.

290
00:35:45,890 --> 00:35:49,490
And you'll need the Wilcoxon option to get this test.

291
00:35:50,420 --> 00:35:56,750
If you leave it off, it'll give you a lot of tests and give you apparently a dozen tests, and it'll be difficult to sort through them.

292
00:35:57,590 --> 00:36:02,690
So make sure you add this option so that it's nice and easy and clear for you.

293
00:36:04,700 --> 00:36:06,770
But it works just like a proxy test.

294
00:36:07,040 --> 00:36:15,230
You'll need a class statement to indicate your grouping variable in a statement to indicate the continuous variable that you're comparing.

295
00:36:18,430 --> 00:36:25,710
It's okay. This test is different from a t test that it actually uses.

296
00:36:26,640 --> 00:36:33,980
Ranks. Instead of the actual values of the DVR, the age variable in this case.

297
00:36:34,760 --> 00:36:39,200
So it it'll actually rank all the ages and I'll do a test based on the rings.

298
00:36:42,390 --> 00:36:45,570
And you'll get results similar to this here.

299
00:36:46,290 --> 00:36:54,330
So you've got the Wilcoxon two sample test over here and it gives you a two sided P value.

300
00:36:55,310 --> 00:37:01,310
Either based on a normal approximation, which is a Z at the top or the T approximation.

301
00:37:04,250 --> 00:37:09,230
The T approximation is probably the safer one to use. But if you do have large ends.

302
00:37:10,250 --> 00:37:14,850
The Z would work just fine. You'll find that they're usually not very different.

303
00:37:16,460 --> 00:37:20,350
Usually identical if you have large ends. Okay.

304
00:37:25,160 --> 00:37:30,230
Now you see in the upper right, we've got a WILCOXON scores for the variable age.

305
00:37:31,070 --> 00:37:35,750
So these are the scores are the actual ranks that I was talking about before.

306
00:37:36,020 --> 00:37:39,440
So there's a sum of the ranks for each group.

307
00:37:40,100 --> 00:37:45,810
So that's really what this test is comparing. Okay.

308
00:37:48,900 --> 00:37:56,610
You can see the mean score. The mean rank for group two is 59 and for Group one is 54.

309
00:38:00,260 --> 00:38:05,750
But it does not look like that's a significant difference based on the test p value here.

310
00:38:09,790 --> 00:38:15,480
Okay. All right.

311
00:38:17,260 --> 00:38:24,250
Well, now, what if you have dependent paired data and you want to run a T test?

312
00:38:26,090 --> 00:38:29,540
Well, you can do this and practice tests, but you'll need the paired statement.

313
00:38:33,260 --> 00:38:40,580
So this presumably is comparing the age of a respondent and the age of their sibling.

314
00:38:44,080 --> 00:38:47,320
These are not independent of each other. Right.

315
00:38:48,920 --> 00:38:54,740
Obviously siblings will generally be in a similar age bracket.

316
00:38:54,920 --> 00:38:58,390
Right. Within, say, ten years of each other.

317
00:38:59,350 --> 00:39:03,310
So these are not. Independent of each other.

318
00:39:03,610 --> 00:39:10,280
And so you'd want to use this paired t test. Essentially all this does is it subtracts.

319
00:39:11,300 --> 00:39:15,230
The second variable from the first. Looks at the difference.

320
00:39:17,370 --> 00:39:22,590
Does a a test on that difference. So I'll give you the main difference here.

321
00:39:23,220 --> 00:39:28,260
Standard deviation and so on as well as a p value for the t test.

322
00:39:30,250 --> 00:39:33,890
And in this case. It definitely looks like.

323
00:39:35,620 --> 00:39:43,300
The two ages are related to each other. Okay.

324
00:39:46,630 --> 00:39:51,070
So again, there's lots of things we need to consider when deciding what tests to run.

325
00:39:51,850 --> 00:39:58,800
Are things independent or dependent? Are things normally distributed or not?

326
00:40:00,390 --> 00:40:05,490
Are the variances the same or not? Those are the main three things we need to consider.

327
00:40:06,890 --> 00:40:12,620
How many groups do we have? That will determine whether you do t test or i know the right.

328
00:40:14,110 --> 00:40:22,600
That's another one. And in this test, we'll give you these diagnostics again for the paired test.

329
00:40:27,020 --> 00:40:30,260
Again, they're fairly normally distributed. Little less cute, but not bad.

330
00:40:37,430 --> 00:40:44,960
When you this is the lesson there, I think that we haven't talked about when we have not normally distributed data and dependent cases.

331
00:40:48,420 --> 00:40:52,380
So this is the Wilcoxon signed rank test that you should use in this case.

332
00:40:53,790 --> 00:40:57,690
And this one's fairly easy. You can use proc univariate to get this.

333
00:41:00,080 --> 00:41:03,080
However, you will have to first actually subtract.

334
00:41:04,310 --> 00:41:12,740
The two dependent values. So again, if we're doing the age with the siblings age, you would subtract one from the other in a data step.

335
00:41:14,160 --> 00:41:21,150
Get a difference here called Delta. And then you can run proc and univariate on Delta.

336
00:41:22,260 --> 00:41:30,190
And in the test for a location, there's the sign rent test when you hear. And we have a significant p value.

337
00:41:30,820 --> 00:41:38,420
Again. Okay.

338
00:41:45,090 --> 00:41:48,550
Right. Key takeaways.

339
00:41:48,760 --> 00:41:51,370
This is a table you should study and memorize.

340
00:41:52,510 --> 00:42:01,240
If you've got independent data and your distributions are normal, then use a T test with the class statement.

341
00:42:03,690 --> 00:42:08,130
But you must check for equal variance to find the right test statistic.

342
00:42:14,480 --> 00:42:21,010
Right. That's if you have if you're comparing two groups. If you have multiple groups, you should use broad glm.

343
00:42:24,440 --> 00:42:29,810
And a test if your data is independent and not normally distributed.

344
00:42:30,350 --> 00:42:36,010
Use in power one way. You get that wilcoxon rank sum test.

345
00:42:38,290 --> 00:42:44,730
If your data is dependent and normally distributed. Use a t test with the paired statement.

346
00:42:47,220 --> 00:42:57,090
And finally, if your date is dependent and not normally distributed, calculate the difference and univariate in that sign rank test.

347
00:43:00,880 --> 00:43:07,030
Okay. Right.

348
00:43:09,420 --> 00:43:12,300
Let's talk a little bit more about evaluating continuous variables.

349
00:43:14,530 --> 00:43:20,620
Now in all of these examples, his body weight increase with increase in caloric intake after controlling for other factors.

350
00:43:21,720 --> 00:43:25,930
Well, these are all. Continuous.

351
00:43:25,930 --> 00:43:29,200
Body weights continuous. Caloric intake. Continuous.

352
00:43:30,730 --> 00:43:32,350
So we're not comparing groups now.

353
00:43:32,560 --> 00:43:40,450
We're just trying to see if two continuous variables are related to each other, as well as maybe some other factors thrown in.

354
00:43:43,510 --> 00:43:49,350
Okay. So this is where we're going to really start getting into regression.

355
00:43:51,980 --> 00:43:56,270
So if you had two continuous variables, you could plot them on a scatterplot like this.

356
00:43:58,460 --> 00:44:04,370
And look for a pattern. Clearly, the dots here kind of have an upward trend.

357
00:44:06,430 --> 00:44:11,130
And you can actually. Fit a regression line onto it.

358
00:44:13,070 --> 00:44:19,740
Like what's seen here? And the difference between the.

359
00:44:21,270 --> 00:44:26,070
Regression line, and each point would be the deviation.

360
00:44:26,950 --> 00:44:39,520
Of those points. This line is actually fit under this data so that the sum of the squared deviations is minimized.

361
00:44:40,060 --> 00:44:44,900
That's how it actually chooses the line. Okay.

362
00:44:46,350 --> 00:44:50,490
And there's some. Calculus involved in that to to do it.

363
00:44:51,270 --> 00:44:54,690
But obviously, this is a first class. We're going to have access do it for us.

364
00:44:55,850 --> 00:45:02,490
Okay. But it's important to understand kind of what's happening here with this line.

365
00:45:05,770 --> 00:45:12,520
Correlation is a measure of how related the two variables are.

366
00:45:14,340 --> 00:45:19,020
The most common is the Pearson correlation. And there's a coefficient for that.

367
00:45:20,350 --> 00:45:23,560
Symbolized by air. It ranges from -1 to 1.

368
00:45:26,300 --> 00:45:30,320
It really just indicates the strength of that linear relationship between the two variables.

369
00:45:32,720 --> 00:45:37,190
There's also r squared known as the coefficient of determination.

370
00:45:39,540 --> 00:45:45,960
You can actually interpret R-squared as the proportion of variance in the outcome explained by the variance in the predictor.

371
00:45:47,850 --> 00:45:54,890
This is kind of an important measure. Tells you how good your model is.

372
00:45:57,820 --> 00:46:02,670
Right. So here's some examples of our values of ours.

373
00:46:02,670 --> 00:46:07,230
One, all the points fall exactly on the line.

374
00:46:09,590 --> 00:46:13,940
Minus one. It'd be the same except for the negative slope.

375
00:46:17,760 --> 00:46:23,850
You've got an R point three on this one. So it's pretty, pretty well correlated, but not exactly.

376
00:46:24,810 --> 00:46:29,610
You got a negative .69 on this one. So definitely a negative trend.

377
00:46:29,610 --> 00:46:38,800
But it's, you know, a little scattered. And you've got this ah .15 here where there's barely any, any trend at all.

378
00:46:40,630 --> 00:46:46,640
So the closer to zero, the less correlated they are. Okay.

379
00:46:49,980 --> 00:46:55,500
So we can calculate the Pearson correlation with PROC core.

380
00:47:00,580 --> 00:47:05,350
So this is kind of how it's set up. You'd be looking at whether X and Y here.

381
00:47:07,460 --> 00:47:15,000
Is. Correlated with each other. Why is your outcome an X as your predictor in this case?

382
00:47:16,400 --> 00:47:19,610
You can certainly add more variables into this bar statement if you want.

383
00:47:19,620 --> 00:47:24,680
You can add as many as you want. You'll get a kind of a matrix, a correlation matrix of all of them.

384
00:47:26,150 --> 00:47:32,450
It looks something like this. So in this case, we ran press core on variables weight, height and age.

385
00:47:34,360 --> 00:47:41,020
We'll give you some simple statistics. By default, the means to innovation and so on, as well as this correlation matrix.

386
00:47:42,100 --> 00:47:47,220
So the first value in each box. Is the Pearson correlation coefficient.

387
00:47:48,510 --> 00:47:59,550
It gives you a probability a p value. In other words, where the the null for the test is that the correlation coefficient is equal to zero.

388
00:48:03,140 --> 00:48:06,710
And it also gives you the number of observations which the.

389
00:48:08,840 --> 00:48:15,120
The test is based on. So the number of non missing is basically for the combination of those two variables.

390
00:48:18,290 --> 00:48:23,970
So you can see most of these things are very highly correlated, as you would expect for weight, height and age.

391
00:48:27,750 --> 00:48:31,920
Okay. Correlation is not enough.

392
00:48:31,940 --> 00:48:42,140
However. All of these examples have the same exact mean standard and standard deviation and correlation.

393
00:48:46,770 --> 00:48:51,330
You're all have the exact same stats, but obviously they're very different.

394
00:48:52,350 --> 00:49:01,650
Right. This top left one that looks pretty normal and good, but the other ones have problems.

395
00:49:02,370 --> 00:49:07,220
The upper right one is nonlinear. That's a problem.

396
00:49:09,050 --> 00:49:12,790
The lower left. One has an extreme outlier. Looks like that's skewing.

397
00:49:13,830 --> 00:49:18,910
The regression. And same as the right one, the lower right one here.

398
00:49:19,510 --> 00:49:27,600
That's even more extreme. So the point here is that we're going to have to be concerned with whether or

399
00:49:27,600 --> 00:49:32,460
not we actually have a linear pattern and whether or not we have outliers,

400
00:49:32,970 --> 00:49:44,810
as well as some other things that we'll talk about next week. So one way to get this scatterplot you can use as plot.

401
00:49:46,300 --> 00:49:49,330
We did talk about this a little before. Right. Remember?

402
00:49:52,140 --> 00:49:56,640
You can get a regression line by using the regs statement in practice g plot.

403
00:49:57,910 --> 00:50:03,040
You just have to specify the Y variable and the variable that you're plotting.

404
00:50:05,930 --> 00:50:13,870
I'll give you something like this. So that's a good way to assess your linearity.

405
00:50:15,340 --> 00:50:26,340
Between your two variables. And when we talk about regression, we are going to use fragile M, just like we do for a nova.

406
00:50:27,150 --> 00:50:28,560
It works exactly the same.

407
00:50:29,370 --> 00:50:35,070
The solution option, just like we talked about before, is going to provide our parameter estimates, our T values and P values.

408
00:50:36,520 --> 00:50:42,430
There's also a sealed palm option which will provide the confidence intervals for those parameter estimates.

409
00:50:44,400 --> 00:50:47,670
We're going to talk about regression a lot more next week.

410
00:50:50,970 --> 00:51:00,240
All right. Any questions? All right.

411
00:51:00,240 --> 00:51:01,530
Well, good luck on your lab today.

